I0501 18:48:51.219346      21 e2e.go:116] Starting e2e run "c4933137-f4e4-456c-9f68-8eb0e25c26fe" on Ginkgo node 1
May  1 18:48:51.268: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682966930 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
May  1 18:48:51.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
E0501 18:48:51.505162      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0501 18:48:51.505162      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May  1 18:48:51.508: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  1 18:48:51.604: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  1 18:48:51.718: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  1 18:48:51.718: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
May  1 18:48:51.718: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  1 18:48:51.740: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
May  1 18:48:51.740: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
May  1 18:48:51.740: INFO: e2e test version: v1.25.8
May  1 18:48:51.744: INFO: kube-apiserver version: v1.25.8+27e744f
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
May  1 18:48:51.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 18:48:51.762: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.261 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  1 18:48:51.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    E0501 18:48:51.505162      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    May  1 18:48:51.508: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    May  1 18:48:51.604: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May  1 18:48:51.718: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May  1 18:48:51.718: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    May  1 18:48:51.718: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May  1 18:48:51.740: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    May  1 18:48:51.740: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    May  1 18:48:51.740: INFO: e2e test version: v1.25.8
    May  1 18:48:51.744: INFO: kube-apiserver version: v1.25.8+27e744f
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    May  1 18:48:51.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 18:48:51.762: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:48:51.81
May  1 18:48:51.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 18:48:51.813
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:48:51.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:48:51.902
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 05/01/23 18:48:51.915
May  1 18:48:52.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b" in namespace "projected-6015" to be "Succeeded or Failed"
May  1 18:48:52.246: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.337139ms
May  1 18:48:54.267: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038504556s
May  1 18:48:56.274: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045521153s
May  1 18:48:58.261: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033044282s
May  1 18:49:00.259: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031331605s
May  1 18:49:02.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Running", Reason="", readiness=true. Elapsed: 10.033537235s
May  1 18:49:04.263: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Running", Reason="", readiness=false. Elapsed: 12.03487424s
May  1 18:49:06.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.033512689s
STEP: Saw pod success 05/01/23 18:49:06.262
May  1 18:49:06.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b" satisfied condition "Succeeded or Failed"
May  1 18:49:06.292: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b container client-container: <nil>
STEP: delete the pod 05/01/23 18:49:06.369
May  1 18:49:06.414: INFO: Waiting for pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b to disappear
May  1 18:49:06.425: INFO: Pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 18:49:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6015" for this suite. 05/01/23 18:49:06.441
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":1,"skipped":34,"failed":0}
------------------------------
• [SLOW TEST] [14.664 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:48:51.81
    May  1 18:48:51.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 18:48:51.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:48:51.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:48:51.902
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 05/01/23 18:48:51.915
    May  1 18:48:52.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b" in namespace "projected-6015" to be "Succeeded or Failed"
    May  1 18:48:52.246: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.337139ms
    May  1 18:48:54.267: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038504556s
    May  1 18:48:56.274: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045521153s
    May  1 18:48:58.261: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033044282s
    May  1 18:49:00.259: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031331605s
    May  1 18:49:02.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Running", Reason="", readiness=true. Elapsed: 10.033537235s
    May  1 18:49:04.263: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Running", Reason="", readiness=false. Elapsed: 12.03487424s
    May  1 18:49:06.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.033512689s
    STEP: Saw pod success 05/01/23 18:49:06.262
    May  1 18:49:06.262: INFO: Pod "downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b" satisfied condition "Succeeded or Failed"
    May  1 18:49:06.292: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b container client-container: <nil>
    STEP: delete the pod 05/01/23 18:49:06.369
    May  1 18:49:06.414: INFO: Waiting for pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b to disappear
    May  1 18:49:06.425: INFO: Pod downwardapi-volume-8d68d2f1-0289-4ef3-8999-9af08400d40b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 18:49:06.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6015" for this suite. 05/01/23 18:49:06.441
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:06.476
May  1 18:49:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename limitrange 05/01/23 18:49:06.479
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:06.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:06.581
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 05/01/23 18:49:06.597
STEP: Setting up watch 05/01/23 18:49:06.598
STEP: Submitting a LimitRange 05/01/23 18:49:06.72
STEP: Verifying LimitRange creation was observed 05/01/23 18:49:06.755
STEP: Fetching the LimitRange to ensure it has proper values 05/01/23 18:49:06.756
May  1 18:49:06.778: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  1 18:49:06.779: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/01/23 18:49:06.779
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/01/23 18:49:06.833
May  1 18:49:06.845: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May  1 18:49:06.845: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/01/23 18:49:06.845
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/01/23 18:49:06.897
May  1 18:49:06.909: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May  1 18:49:06.909: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/01/23 18:49:06.91
STEP: Failing to create a Pod with more than max resources 05/01/23 18:49:06.935
STEP: Updating a LimitRange 05/01/23 18:49:06.958
STEP: Verifying LimitRange updating is effective 05/01/23 18:49:06.981
STEP: Creating a Pod with less than former min resources 05/01/23 18:49:09.001
STEP: Failing to create a Pod with more than max resources 05/01/23 18:49:09.047
STEP: Deleting a LimitRange 05/01/23 18:49:09.07
STEP: Verifying the LimitRange was deleted 05/01/23 18:49:09.107
May  1 18:49:14.129: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/01/23 18:49:14.129
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
May  1 18:49:14.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3614" for this suite. 05/01/23 18:49:14.249
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":2,"skipped":37,"failed":0}
------------------------------
• [SLOW TEST] [7.809 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:06.476
    May  1 18:49:06.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename limitrange 05/01/23 18:49:06.479
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:06.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:06.581
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 05/01/23 18:49:06.597
    STEP: Setting up watch 05/01/23 18:49:06.598
    STEP: Submitting a LimitRange 05/01/23 18:49:06.72
    STEP: Verifying LimitRange creation was observed 05/01/23 18:49:06.755
    STEP: Fetching the LimitRange to ensure it has proper values 05/01/23 18:49:06.756
    May  1 18:49:06.778: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  1 18:49:06.779: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/01/23 18:49:06.779
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/01/23 18:49:06.833
    May  1 18:49:06.845: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May  1 18:49:06.845: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/01/23 18:49:06.845
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/01/23 18:49:06.897
    May  1 18:49:06.909: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May  1 18:49:06.909: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/01/23 18:49:06.91
    STEP: Failing to create a Pod with more than max resources 05/01/23 18:49:06.935
    STEP: Updating a LimitRange 05/01/23 18:49:06.958
    STEP: Verifying LimitRange updating is effective 05/01/23 18:49:06.981
    STEP: Creating a Pod with less than former min resources 05/01/23 18:49:09.001
    STEP: Failing to create a Pod with more than max resources 05/01/23 18:49:09.047
    STEP: Deleting a LimitRange 05/01/23 18:49:09.07
    STEP: Verifying the LimitRange was deleted 05/01/23 18:49:09.107
    May  1 18:49:14.129: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/01/23 18:49:14.129
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    May  1 18:49:14.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-3614" for this suite. 05/01/23 18:49:14.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:14.289
May  1 18:49:14.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename init-container 05/01/23 18:49:14.294
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:14.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:14.382
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 05/01/23 18:49:14.394
May  1 18:49:14.395: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 18:49:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1319" for this suite. 05/01/23 18:49:22.558
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":3,"skipped":52,"failed":0}
------------------------------
• [SLOW TEST] [8.296 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:14.289
    May  1 18:49:14.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename init-container 05/01/23 18:49:14.294
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:14.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:14.382
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 05/01/23 18:49:14.394
    May  1 18:49:14.395: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 18:49:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1319" for this suite. 05/01/23 18:49:22.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:22.593
May  1 18:49:22.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 18:49:22.596
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:22.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:22.673
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 05/01/23 18:49:22.686
W0501 18:49:22.743073      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 05/01/23 18:49:22.743
STEP: Orphaning one of the Job's Pods 05/01/23 18:49:28.759
May  1 18:49:29.351: INFO: Successfully updated pod "adopt-release-8sdff"
STEP: Checking that the Job readopts the Pod 05/01/23 18:49:29.352
May  1 18:49:29.355: INFO: Waiting up to 15m0s for pod "adopt-release-8sdff" in namespace "job-7274" to be "adopted"
May  1 18:49:29.376: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 20.946914ms
May  1 18:49:31.388: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 2.032986763s
May  1 18:49:31.389: INFO: Pod "adopt-release-8sdff" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/01/23 18:49:31.389
May  1 18:49:31.944: INFO: Successfully updated pod "adopt-release-8sdff"
STEP: Checking that the Job releases the Pod 05/01/23 18:49:31.944
May  1 18:49:31.945: INFO: Waiting up to 15m0s for pod "adopt-release-8sdff" in namespace "job-7274" to be "released"
May  1 18:49:31.954: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 8.755711ms
May  1 18:49:33.965: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 2.020569402s
May  1 18:49:33.965: INFO: Pod "adopt-release-8sdff" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 18:49:33.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7274" for this suite. 05/01/23 18:49:33.983
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":4,"skipped":104,"failed":0}
------------------------------
• [SLOW TEST] [11.419 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:22.593
    May  1 18:49:22.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 18:49:22.596
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:22.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:22.673
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 05/01/23 18:49:22.686
    W0501 18:49:22.743073      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 05/01/23 18:49:22.743
    STEP: Orphaning one of the Job's Pods 05/01/23 18:49:28.759
    May  1 18:49:29.351: INFO: Successfully updated pod "adopt-release-8sdff"
    STEP: Checking that the Job readopts the Pod 05/01/23 18:49:29.352
    May  1 18:49:29.355: INFO: Waiting up to 15m0s for pod "adopt-release-8sdff" in namespace "job-7274" to be "adopted"
    May  1 18:49:29.376: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 20.946914ms
    May  1 18:49:31.388: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 2.032986763s
    May  1 18:49:31.389: INFO: Pod "adopt-release-8sdff" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/01/23 18:49:31.389
    May  1 18:49:31.944: INFO: Successfully updated pod "adopt-release-8sdff"
    STEP: Checking that the Job releases the Pod 05/01/23 18:49:31.944
    May  1 18:49:31.945: INFO: Waiting up to 15m0s for pod "adopt-release-8sdff" in namespace "job-7274" to be "released"
    May  1 18:49:31.954: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 8.755711ms
    May  1 18:49:33.965: INFO: Pod "adopt-release-8sdff": Phase="Running", Reason="", readiness=true. Elapsed: 2.020569402s
    May  1 18:49:33.965: INFO: Pod "adopt-release-8sdff" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 18:49:33.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7274" for this suite. 05/01/23 18:49:33.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:34.017
May  1 18:49:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-webhook 05/01/23 18:49:34.019
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:34.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:34.102
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/01/23 18:49:34.119
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/01/23 18:49:37.298
STEP: Deploying the custom resource conversion webhook pod 05/01/23 18:49:37.473
STEP: Wait for the deployment to be ready 05/01/23 18:49:37.523
May  1 18:49:37.553: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  1 18:49:39.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 18:49:41.607
STEP: Verifying the service has paired with the endpoint 05/01/23 18:49:41.652
May  1 18:49:42.653: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May  1 18:49:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Creating a v1 custom resource 05/01/23 18:49:45.606
STEP: Create a v2 custom resource 05/01/23 18:49:45.698
STEP: List CRs in v1 05/01/23 18:49:45.884
STEP: List CRs in v2 05/01/23 18:49:45.907
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 18:49:46.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8860" for this suite. 05/01/23 18:49:46.555
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":5,"skipped":141,"failed":0}
------------------------------
• [SLOW TEST] [12.836 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:34.017
    May  1 18:49:34.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-webhook 05/01/23 18:49:34.019
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:34.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:34.102
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/01/23 18:49:34.119
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/01/23 18:49:37.298
    STEP: Deploying the custom resource conversion webhook pod 05/01/23 18:49:37.473
    STEP: Wait for the deployment to be ready 05/01/23 18:49:37.523
    May  1 18:49:37.553: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    May  1 18:49:39.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 49, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 18:49:41.607
    STEP: Verifying the service has paired with the endpoint 05/01/23 18:49:41.652
    May  1 18:49:42.653: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May  1 18:49:42.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Creating a v1 custom resource 05/01/23 18:49:45.606
    STEP: Create a v2 custom resource 05/01/23 18:49:45.698
    STEP: List CRs in v1 05/01/23 18:49:45.884
    STEP: List CRs in v2 05/01/23 18:49:45.907
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 18:49:46.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8860" for this suite. 05/01/23 18:49:46.555
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:46.857
May  1 18:49:46.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 18:49:46.86
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:46.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:46.953
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
May  1 18:49:47.115: INFO: Waiting up to 2m0s for pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" in namespace "var-expansion-420" to be "container 0 failed with reason CreateContainerConfigError"
May  1 18:49:47.127: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199938ms
May  1 18:49:49.171: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056317426s
May  1 18:49:51.141: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025768672s
May  1 18:49:51.141: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  1 18:49:51.141: INFO: Deleting pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" in namespace "var-expansion-420"
May  1 18:49:51.178: INFO: Wait up to 5m0s for pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 18:49:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-420" for this suite. 05/01/23 18:49:55.229
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":6,"skipped":142,"failed":0}
------------------------------
• [SLOW TEST] [8.423 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:46.857
    May  1 18:49:46.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 18:49:46.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:46.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:46.953
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    May  1 18:49:47.115: INFO: Waiting up to 2m0s for pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" in namespace "var-expansion-420" to be "container 0 failed with reason CreateContainerConfigError"
    May  1 18:49:47.127: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 12.199938ms
    May  1 18:49:49.171: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056317426s
    May  1 18:49:51.141: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025768672s
    May  1 18:49:51.141: INFO: Pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  1 18:49:51.141: INFO: Deleting pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" in namespace "var-expansion-420"
    May  1 18:49:51.178: INFO: Wait up to 5m0s for pod "var-expansion-1461c1f0-4c17-42b5-9ee3-8be002018e54" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 18:49:55.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-420" for this suite. 05/01/23 18:49:55.229
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:49:55.281
May  1 18:49:55.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 18:49:55.285
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:55.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:55.402
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-9720 05/01/23 18:49:55.445
STEP: creating service affinity-nodeport in namespace services-9720 05/01/23 18:49:55.446
STEP: creating replication controller affinity-nodeport in namespace services-9720 05/01/23 18:49:55.566
I0501 18:49:55.743166      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9720, replica count: 3
I0501 18:49:58.794520      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 18:50:01.804443      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 18:50:04.820986      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 18:50:07.821400      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 18:50:07.877: INFO: Creating new exec pod
May  1 18:50:07.941: INFO: Waiting up to 5m0s for pod "execpod-affinityt695t" in namespace "services-9720" to be "running"
May  1 18:50:07.954: INFO: Pod "execpod-affinityt695t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.810049ms
May  1 18:50:09.966: INFO: Pod "execpod-affinityt695t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024361313s
May  1 18:50:11.966: INFO: Pod "execpod-affinityt695t": Phase="Running", Reason="", readiness=true. Elapsed: 4.024611253s
May  1 18:50:11.966: INFO: Pod "execpod-affinityt695t" satisfied condition "running"
May  1 18:50:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May  1 18:50:13.687: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May  1 18:50:13.687: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 18:50:13.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.210.43 80'
May  1 18:50:14.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.210.43 80\nConnection to 172.21.210.43 80 port [tcp/http] succeeded!\n"
May  1 18:50:14.166: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 18:50:14.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.124 30290'
May  1 18:50:14.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.124 30290\nConnection to 10.45.145.124 30290 port [tcp/*] succeeded!\n"
May  1 18:50:14.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 18:50:14.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30290'
May  1 18:50:15.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30290\nConnection to 10.45.145.126 30290 port [tcp/*] succeeded!\n"
May  1 18:50:15.276: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 18:50:15.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30290/ ; done'
May  1 18:50:15.981: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n"
May  1 18:50:15.982: INFO: stdout: "\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6"
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
May  1 18:50:15.982: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9720, will wait for the garbage collector to delete the pods 05/01/23 18:50:16.009
May  1 18:50:16.106: INFO: Deleting ReplicationController affinity-nodeport took: 29.165452ms
May  1 18:50:16.308: INFO: Terminating ReplicationController affinity-nodeport pods took: 201.3362ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 18:50:20.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9720" for this suite. 05/01/23 18:50:20.431
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":7,"skipped":145,"failed":0}
------------------------------
• [SLOW TEST] [25.182 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:49:55.281
    May  1 18:49:55.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 18:49:55.285
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:49:55.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:49:55.402
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-9720 05/01/23 18:49:55.445
    STEP: creating service affinity-nodeport in namespace services-9720 05/01/23 18:49:55.446
    STEP: creating replication controller affinity-nodeport in namespace services-9720 05/01/23 18:49:55.566
    I0501 18:49:55.743166      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9720, replica count: 3
    I0501 18:49:58.794520      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 18:50:01.804443      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 18:50:04.820986      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 18:50:07.821400      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 18:50:07.877: INFO: Creating new exec pod
    May  1 18:50:07.941: INFO: Waiting up to 5m0s for pod "execpod-affinityt695t" in namespace "services-9720" to be "running"
    May  1 18:50:07.954: INFO: Pod "execpod-affinityt695t": Phase="Pending", Reason="", readiness=false. Elapsed: 12.810049ms
    May  1 18:50:09.966: INFO: Pod "execpod-affinityt695t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024361313s
    May  1 18:50:11.966: INFO: Pod "execpod-affinityt695t": Phase="Running", Reason="", readiness=true. Elapsed: 4.024611253s
    May  1 18:50:11.966: INFO: Pod "execpod-affinityt695t" satisfied condition "running"
    May  1 18:50:12.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    May  1 18:50:13.687: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May  1 18:50:13.687: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 18:50:13.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.210.43 80'
    May  1 18:50:14.166: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.210.43 80\nConnection to 172.21.210.43 80 port [tcp/http] succeeded!\n"
    May  1 18:50:14.166: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 18:50:14.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.124 30290'
    May  1 18:50:14.814: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.124 30290\nConnection to 10.45.145.124 30290 port [tcp/*] succeeded!\n"
    May  1 18:50:14.814: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 18:50:14.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30290'
    May  1 18:50:15.276: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30290\nConnection to 10.45.145.126 30290 port [tcp/*] succeeded!\n"
    May  1 18:50:15.276: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 18:50:15.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9720 exec execpod-affinityt695t -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30290/ ; done'
    May  1 18:50:15.981: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30290/\n"
    May  1 18:50:15.982: INFO: stdout: "\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6\naffinity-nodeport-85lm6"
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Received response from host: affinity-nodeport-85lm6
    May  1 18:50:15.982: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9720, will wait for the garbage collector to delete the pods 05/01/23 18:50:16.009
    May  1 18:50:16.106: INFO: Deleting ReplicationController affinity-nodeport took: 29.165452ms
    May  1 18:50:16.308: INFO: Terminating ReplicationController affinity-nodeport pods took: 201.3362ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 18:50:20.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9720" for this suite. 05/01/23 18:50:20.431
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:50:20.465
May  1 18:50:20.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 18:50:20.469
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:20.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:20.586
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May  1 18:50:20.602: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0501 18:50:20.635382      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May  1 18:50:20.652: INFO: Pod name sample-pod: Found 0 pods out of 1
May  1 18:50:25.670: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 18:50:25.67
May  1 18:50:25.671: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-b2cwf" in namespace "deployment-2399" to be "running"
May  1 18:50:25.684: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.646178ms
May  1 18:50:27.699: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028561082s
May  1 18:50:29.699: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028300882s
May  1 18:50:31.700: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028820023s
May  1 18:50:33.748: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.076954266s
May  1 18:50:33.748: INFO: Pod "test-rolling-update-controller-b2cwf" satisfied condition "running"
May  1 18:50:33.748: INFO: Creating deployment "test-rolling-update-deployment"
May  1 18:50:33.768: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  1 18:50:33.806: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  1 18:50:35.879: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  1 18:50:35.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 18:50:37.916: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 18:50:37.955: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2399  4266001a-d8a7-4eea-aebe-0396c93d51e8 60102 1 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a42a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 18:50:33 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-01 18:50:37 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  1 18:50:37.972: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2399  e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b 60090 1 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4266001a-d8a7-4eea-aebe-0396c93d51e8 0xc0025cdb67 0xc0025cdb68}] [] [{kube-controller-manager Update apps/v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4266001a-d8a7-4eea-aebe-0396c93d51e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cdc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  1 18:50:37.972: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  1 18:50:37.972: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2399  c5790537-5609-4050-a8b7-da297321099c 60101 2 2023-05-01 18:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4266001a-d8a7-4eea-aebe-0396c93d51e8 0xc0025cda37 0xc0025cda38}] [] [{e2e.test Update apps/v1 2023-05-01 18:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4266001a-d8a7-4eea-aebe-0396c93d51e8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0025cdaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 18:50:37.989: INFO: Pod "test-rolling-update-deployment-78f575d8ff-8mxz9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-8mxz9 test-rolling-update-deployment-78f575d8ff- deployment-2399  9258f38c-8f2c-4271-bc98-ef6b421b4d02 60087 0 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:8fc61c101db06e5ae9a0ca38c7b4f8319db9e7579d7fbc3632c1cc0af130e2d4 cni.projectcalico.org/podIP:172.30.38.224/32 cni.projectcalico.org/podIPs:172.30.38.224/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b 0xc003af2087 0xc003af2088}] [] [{kube-controller-manager Update v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 18:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 18:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2lbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2lbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-q4tnz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.224,StartTime:2023-05-01 18:50:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 18:50:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://1e87eceb8ecf59d737ae3b328d3810e8173a5016a7004d2e2e65e688ef4ed722,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 18:50:37.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2399" for this suite. 05/01/23 18:50:38.007
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":8,"skipped":145,"failed":0}
------------------------------
• [SLOW TEST] [17.610 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:50:20.465
    May  1 18:50:20.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 18:50:20.469
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:20.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:20.586
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May  1 18:50:20.602: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0501 18:50:20.635382      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May  1 18:50:20.652: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  1 18:50:25.670: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 18:50:25.67
    May  1 18:50:25.671: INFO: Waiting up to 5m0s for pod "test-rolling-update-controller-b2cwf" in namespace "deployment-2399" to be "running"
    May  1 18:50:25.684: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 13.646178ms
    May  1 18:50:27.699: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028561082s
    May  1 18:50:29.699: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028300882s
    May  1 18:50:31.700: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028820023s
    May  1 18:50:33.748: INFO: Pod "test-rolling-update-controller-b2cwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.076954266s
    May  1 18:50:33.748: INFO: Pod "test-rolling-update-controller-b2cwf" satisfied condition "running"
    May  1 18:50:33.748: INFO: Creating deployment "test-rolling-update-deployment"
    May  1 18:50:33.768: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May  1 18:50:33.806: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May  1 18:50:35.879: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May  1 18:50:35.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 18, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 18:50:37.916: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 18:50:37.955: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2399  4266001a-d8a7-4eea-aebe-0396c93d51e8 60102 1 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a42a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 18:50:33 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-05-01 18:50:37 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  1 18:50:37.972: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2399  e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b 60090 1 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 4266001a-d8a7-4eea-aebe-0396c93d51e8 0xc0025cdb67 0xc0025cdb68}] [] [{kube-controller-manager Update apps/v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4266001a-d8a7-4eea-aebe-0396c93d51e8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cdc18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  1 18:50:37.972: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May  1 18:50:37.972: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2399  c5790537-5609-4050-a8b7-da297321099c 60101 2 2023-05-01 18:50:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 4266001a-d8a7-4eea-aebe-0396c93d51e8 0xc0025cda37 0xc0025cda38}] [] [{e2e.test Update apps/v1 2023-05-01 18:50:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4266001a-d8a7-4eea-aebe-0396c93d51e8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0025cdaf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 18:50:37.989: INFO: Pod "test-rolling-update-deployment-78f575d8ff-8mxz9" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-8mxz9 test-rolling-update-deployment-78f575d8ff- deployment-2399  9258f38c-8f2c-4271-bc98-ef6b421b4d02 60087 0 2023-05-01 18:50:33 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:8fc61c101db06e5ae9a0ca38c7b4f8319db9e7579d7fbc3632c1cc0af130e2d4 cni.projectcalico.org/podIP:172.30.38.224/32 cni.projectcalico.org/podIPs:172.30.38.224/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.224"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.224"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b 0xc003af2087 0xc003af2088}] [] [{kube-controller-manager Update v1 2023-05-01 18:50:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e0b14fe1-d6e9-441c-9a6c-3d17749b6b1b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 18:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 18:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 18:50:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2lbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2lbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-q4tnz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 18:50:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.224,StartTime:2023-05-01 18:50:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 18:50:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://1e87eceb8ecf59d737ae3b328d3810e8173a5016a7004d2e2e65e688ef4ed722,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 18:50:37.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2399" for this suite. 05/01/23 18:50:38.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:50:38.082
May  1 18:50:38.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 18:50:38.084
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:38.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:38.171
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 05/01/23 18:50:38.184
May  1 18:50:38.425: INFO: Waiting up to 5m0s for pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4" in namespace "downward-api-4602" to be "Succeeded or Failed"
May  1 18:50:38.437: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.883288ms
May  1 18:50:40.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038835673s
May  1 18:50:42.448: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022612826s
May  1 18:50:44.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038998946s
STEP: Saw pod success 05/01/23 18:50:44.464
May  1 18:50:44.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4" satisfied condition "Succeeded or Failed"
May  1 18:50:44.487: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 container dapi-container: <nil>
STEP: delete the pod 05/01/23 18:50:44.58
May  1 18:50:44.609: INFO: Waiting for pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 to disappear
May  1 18:50:44.619: INFO: Pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  1 18:50:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4602" for this suite. 05/01/23 18:50:44.636
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":9,"skipped":190,"failed":0}
------------------------------
• [SLOW TEST] [6.586 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:50:38.082
    May  1 18:50:38.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 18:50:38.084
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:38.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:38.171
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 05/01/23 18:50:38.184
    May  1 18:50:38.425: INFO: Waiting up to 5m0s for pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4" in namespace "downward-api-4602" to be "Succeeded or Failed"
    May  1 18:50:38.437: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.883288ms
    May  1 18:50:40.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038835673s
    May  1 18:50:42.448: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022612826s
    May  1 18:50:44.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038998946s
    STEP: Saw pod success 05/01/23 18:50:44.464
    May  1 18:50:44.464: INFO: Pod "downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4" satisfied condition "Succeeded or Failed"
    May  1 18:50:44.487: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 container dapi-container: <nil>
    STEP: delete the pod 05/01/23 18:50:44.58
    May  1 18:50:44.609: INFO: Waiting for pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 to disappear
    May  1 18:50:44.619: INFO: Pod downward-api-9ba226bb-30ed-4e38-a54d-0a56cf9914d4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  1 18:50:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4602" for this suite. 05/01/23 18:50:44.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:50:44.676
May  1 18:50:44.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replication-controller 05/01/23 18:50:44.679
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:44.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:44.769
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690 05/01/23 18:50:44.786
W0501 18:50:44.846441      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May  1 18:50:44.862: INFO: Pod name my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Found 0 pods out of 1
May  1 18:50:49.875: INFO: Pod name my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Found 1 pods out of 1
May  1 18:50:49.875: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" are running
May  1 18:50:49.875: INFO: Waiting up to 5m0s for pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" in namespace "replication-controller-2099" to be "running"
May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2": Phase="Running", Reason="", readiness=true. Elapsed: 12.380139ms
May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" satisfied condition "running"
May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:44 +0000 UTC Reason: Message:}])
May  1 18:50:49.888: INFO: Trying to dial the pod
May  1 18:50:54.942: INFO: Controller my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Got expected result from replica 1 [my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2]: "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  1 18:50:54.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2099" for this suite. 05/01/23 18:50:54.96
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":10,"skipped":201,"failed":0}
------------------------------
• [SLOW TEST] [10.318 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:50:44.676
    May  1 18:50:44.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replication-controller 05/01/23 18:50:44.679
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:44.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:44.769
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690 05/01/23 18:50:44.786
    W0501 18:50:44.846441      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May  1 18:50:44.862: INFO: Pod name my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Found 0 pods out of 1
    May  1 18:50:49.875: INFO: Pod name my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Found 1 pods out of 1
    May  1 18:50:49.875: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690" are running
    May  1 18:50:49.875: INFO: Waiting up to 5m0s for pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" in namespace "replication-controller-2099" to be "running"
    May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2": Phase="Running", Reason="", readiness=true. Elapsed: 12.380139ms
    May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" satisfied condition "running"
    May  1 18:50:49.888: INFO: Pod "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:44 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 18:50:44 +0000 UTC Reason: Message:}])
    May  1 18:50:49.888: INFO: Trying to dial the pod
    May  1 18:50:54.942: INFO: Controller my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690: Got expected result from replica 1 [my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2]: "my-hostname-basic-feb21cd7-5abf-42bc-a45f-ee421b383690-b44w2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  1 18:50:54.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2099" for this suite. 05/01/23 18:50:54.96
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:50:54.994
May  1 18:50:54.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 18:50:54.997
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:55.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:55.145
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 05/01/23 18:50:55.166
STEP: watching for the ServiceAccount to be added 05/01/23 18:50:55.202
STEP: patching the ServiceAccount 05/01/23 18:50:55.209
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/01/23 18:50:55.239
STEP: deleting the ServiceAccount 05/01/23 18:50:55.266
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 18:50:55.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4705" for this suite. 05/01/23 18:50:55.362
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":11,"skipped":205,"failed":0}
------------------------------
• [0.435 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:50:54.994
    May  1 18:50:54.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 18:50:54.997
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:55.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:55.145
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 05/01/23 18:50:55.166
    STEP: watching for the ServiceAccount to be added 05/01/23 18:50:55.202
    STEP: patching the ServiceAccount 05/01/23 18:50:55.209
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/01/23 18:50:55.239
    STEP: deleting the ServiceAccount 05/01/23 18:50:55.266
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 18:50:55.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4705" for this suite. 05/01/23 18:50:55.362
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:50:55.441
May  1 18:50:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 18:50:55.442
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:55.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:55.592
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 05/01/23 18:50:55.64
May  1 18:50:55.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3" in namespace "downward-api-3817" to be "Succeeded or Failed"
May  1 18:50:55.749: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.124041ms
May  1 18:50:57.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025391379s
May  1 18:50:59.769: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031682586s
May  1 18:51:01.765: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02797972s
May  1 18:51:03.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024909848s
STEP: Saw pod success 05/01/23 18:51:03.762
May  1 18:51:03.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3" satisfied condition "Succeeded or Failed"
May  1 18:51:03.773: INFO: Trying to get logs from node 10.45.145.126 pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 container client-container: <nil>
STEP: delete the pod 05/01/23 18:51:03.843
May  1 18:51:03.893: INFO: Waiting for pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 to disappear
May  1 18:51:03.906: INFO: Pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 18:51:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3817" for this suite. 05/01/23 18:51:03.926
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":12,"skipped":270,"failed":0}
------------------------------
• [SLOW TEST] [8.522 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:50:55.441
    May  1 18:50:55.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 18:50:55.442
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:50:55.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:50:55.592
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 05/01/23 18:50:55.64
    May  1 18:50:55.736: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3" in namespace "downward-api-3817" to be "Succeeded or Failed"
    May  1 18:50:55.749: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.124041ms
    May  1 18:50:57.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025391379s
    May  1 18:50:59.769: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031682586s
    May  1 18:51:01.765: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02797972s
    May  1 18:51:03.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024909848s
    STEP: Saw pod success 05/01/23 18:51:03.762
    May  1 18:51:03.762: INFO: Pod "downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3" satisfied condition "Succeeded or Failed"
    May  1 18:51:03.773: INFO: Trying to get logs from node 10.45.145.126 pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 container client-container: <nil>
    STEP: delete the pod 05/01/23 18:51:03.843
    May  1 18:51:03.893: INFO: Waiting for pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 to disappear
    May  1 18:51:03.906: INFO: Pod downwardapi-volume-19700552-87c6-4163-9b1a-a3dfd5bd6ec3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 18:51:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3817" for this suite. 05/01/23 18:51:03.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:03.967
May  1 18:51:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir-wrapper 05/01/23 18:51:03.969
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:04.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:04.047
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May  1 18:51:04.226: INFO: Waiting up to 5m0s for pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe" in namespace "emptydir-wrapper-3725" to be "running and ready"
May  1 18:51:04.240: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.75931ms
May  1 18:51:04.240: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Pending, waiting for it to be Running (with Ready = true)
May  1 18:51:06.252: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026462634s
May  1 18:51:06.253: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Pending, waiting for it to be Running (with Ready = true)
May  1 18:51:08.253: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Running", Reason="", readiness=true. Elapsed: 4.027231056s
May  1 18:51:08.253: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Running (Ready = true)
May  1 18:51:08.254: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/01/23 18:51:08.279
STEP: Cleaning up the configmap 05/01/23 18:51:08.309
STEP: Cleaning up the pod 05/01/23 18:51:08.357
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  1 18:51:08.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3725" for this suite. 05/01/23 18:51:08.414
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":13,"skipped":309,"failed":0}
------------------------------
• [4.479 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:03.967
    May  1 18:51:03.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir-wrapper 05/01/23 18:51:03.969
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:04.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:04.047
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May  1 18:51:04.226: INFO: Waiting up to 5m0s for pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe" in namespace "emptydir-wrapper-3725" to be "running and ready"
    May  1 18:51:04.240: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 13.75931ms
    May  1 18:51:04.240: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Pending, waiting for it to be Running (with Ready = true)
    May  1 18:51:06.252: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026462634s
    May  1 18:51:06.253: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Pending, waiting for it to be Running (with Ready = true)
    May  1 18:51:08.253: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe": Phase="Running", Reason="", readiness=true. Elapsed: 4.027231056s
    May  1 18:51:08.253: INFO: The phase of Pod pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe is Running (Ready = true)
    May  1 18:51:08.254: INFO: Pod "pod-secrets-95565c62-160b-47f6-a5c0-266eb9681bfe" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/01/23 18:51:08.279
    STEP: Cleaning up the configmap 05/01/23 18:51:08.309
    STEP: Cleaning up the pod 05/01/23 18:51:08.357
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  1 18:51:08.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3725" for this suite. 05/01/23 18:51:08.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:08.453
May  1 18:51:08.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 18:51:08.455
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:08.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:08.544
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 05/01/23 18:51:08.557
May  1 18:51:08.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c" in namespace "projected-6465" to be "Succeeded or Failed"
May  1 18:51:08.753: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.881843ms
May  1 18:51:10.768: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02884425s
May  1 18:51:12.815: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075109201s
May  1 18:51:14.775: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03573909s
STEP: Saw pod success 05/01/23 18:51:14.775
May  1 18:51:14.776: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c" satisfied condition "Succeeded or Failed"
May  1 18:51:14.810: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c container client-container: <nil>
STEP: delete the pod 05/01/23 18:51:14.869
May  1 18:51:14.920: INFO: Waiting for pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c to disappear
May  1 18:51:14.933: INFO: Pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 18:51:14.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6465" for this suite. 05/01/23 18:51:14.956
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":14,"skipped":335,"failed":0}
------------------------------
• [SLOW TEST] [6.570 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:08.453
    May  1 18:51:08.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 18:51:08.455
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:08.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:08.544
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 05/01/23 18:51:08.557
    May  1 18:51:08.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c" in namespace "projected-6465" to be "Succeeded or Failed"
    May  1 18:51:08.753: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.881843ms
    May  1 18:51:10.768: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02884425s
    May  1 18:51:12.815: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075109201s
    May  1 18:51:14.775: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03573909s
    STEP: Saw pod success 05/01/23 18:51:14.775
    May  1 18:51:14.776: INFO: Pod "downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c" satisfied condition "Succeeded or Failed"
    May  1 18:51:14.810: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c container client-container: <nil>
    STEP: delete the pod 05/01/23 18:51:14.869
    May  1 18:51:14.920: INFO: Waiting for pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c to disappear
    May  1 18:51:14.933: INFO: Pod downwardapi-volume-6f9f1c94-d7ec-48d6-a760-6b2b69d8f97c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 18:51:14.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6465" for this suite. 05/01/23 18:51:14.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:15.03
May  1 18:51:15.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 18:51:15.033
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:15.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:15.16
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 05/01/23 18:51:15.19
May  1 18:51:15.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132" in namespace "downward-api-8018" to be "Succeeded or Failed"
May  1 18:51:15.386: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110525ms
May  1 18:51:17.399: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026454995s
May  1 18:51:19.400: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027455518s
May  1 18:51:21.430: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058328911s
STEP: Saw pod success 05/01/23 18:51:21.43
May  1 18:51:21.431: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132" satisfied condition "Succeeded or Failed"
May  1 18:51:21.442: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 container client-container: <nil>
STEP: delete the pod 05/01/23 18:51:21.486
May  1 18:51:21.526: INFO: Waiting for pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 to disappear
May  1 18:51:21.536: INFO: Pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 18:51:21.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8018" for this suite. 05/01/23 18:51:21.577
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":15,"skipped":372,"failed":0}
------------------------------
• [SLOW TEST] [6.580 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:15.03
    May  1 18:51:15.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 18:51:15.033
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:15.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:15.16
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 05/01/23 18:51:15.19
    May  1 18:51:15.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132" in namespace "downward-api-8018" to be "Succeeded or Failed"
    May  1 18:51:15.386: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110525ms
    May  1 18:51:17.399: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026454995s
    May  1 18:51:19.400: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027455518s
    May  1 18:51:21.430: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058328911s
    STEP: Saw pod success 05/01/23 18:51:21.43
    May  1 18:51:21.431: INFO: Pod "downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132" satisfied condition "Succeeded or Failed"
    May  1 18:51:21.442: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 container client-container: <nil>
    STEP: delete the pod 05/01/23 18:51:21.486
    May  1 18:51:21.526: INFO: Waiting for pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 to disappear
    May  1 18:51:21.536: INFO: Pod downwardapi-volume-feeb542e-b07d-4008-be92-f1ec33d9d132 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 18:51:21.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8018" for this suite. 05/01/23 18:51:21.577
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:21.611
May  1 18:51:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context 05/01/23 18:51:21.615
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:21.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:21.741
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/01/23 18:51:21.76
May  1 18:51:21.888: INFO: Waiting up to 5m0s for pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222" in namespace "security-context-1775" to be "Succeeded or Failed"
May  1 18:51:21.912: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 23.960198ms
May  1 18:51:23.927: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039401439s
May  1 18:51:25.925: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037027862s
May  1 18:51:27.969: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081364484s
STEP: Saw pod success 05/01/23 18:51:27.969
May  1 18:51:27.969: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222" satisfied condition "Succeeded or Failed"
May  1 18:51:28.030: INFO: Trying to get logs from node 10.45.145.124 pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 container test-container: <nil>
STEP: delete the pod 05/01/23 18:51:28.118
May  1 18:51:28.155: INFO: Waiting for pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 to disappear
May  1 18:51:28.168: INFO: Pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 18:51:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1775" for this suite. 05/01/23 18:51:28.188
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":16,"skipped":372,"failed":0}
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:21.611
    May  1 18:51:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context 05/01/23 18:51:21.615
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:21.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:21.741
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/01/23 18:51:21.76
    May  1 18:51:21.888: INFO: Waiting up to 5m0s for pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222" in namespace "security-context-1775" to be "Succeeded or Failed"
    May  1 18:51:21.912: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 23.960198ms
    May  1 18:51:23.927: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039401439s
    May  1 18:51:25.925: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037027862s
    May  1 18:51:27.969: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.081364484s
    STEP: Saw pod success 05/01/23 18:51:27.969
    May  1 18:51:27.969: INFO: Pod "security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222" satisfied condition "Succeeded or Failed"
    May  1 18:51:28.030: INFO: Trying to get logs from node 10.45.145.124 pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 container test-container: <nil>
    STEP: delete the pod 05/01/23 18:51:28.118
    May  1 18:51:28.155: INFO: Waiting for pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 to disappear
    May  1 18:51:28.168: INFO: Pod security-context-7c1b8a79-68d0-4d5a-be69-256355ac8222 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 18:51:28.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-1775" for this suite. 05/01/23 18:51:28.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:28.245
May  1 18:51:28.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replication-controller 05/01/23 18:51:28.25
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:28.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:28.322
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
May  1 18:51:28.335: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/01/23 18:51:29.387
STEP: Checking rc "condition-test" has the desired failure condition set 05/01/23 18:51:29.418
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/01/23 18:51:30.453
May  1 18:51:30.510: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/01/23 18:51:30.51
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  1 18:51:30.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2124" for this suite. 05/01/23 18:51:30.542
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":17,"skipped":386,"failed":0}
------------------------------
• [2.343 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:28.245
    May  1 18:51:28.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replication-controller 05/01/23 18:51:28.25
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:28.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:28.322
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    May  1 18:51:28.335: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/01/23 18:51:29.387
    STEP: Checking rc "condition-test" has the desired failure condition set 05/01/23 18:51:29.418
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/01/23 18:51:30.453
    May  1 18:51:30.510: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/01/23 18:51:30.51
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  1 18:51:30.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2124" for this suite. 05/01/23 18:51:30.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:30.59
May  1 18:51:30.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 18:51:30.592
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:30.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:30.682
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 05/01/23 18:51:30.697
STEP: Getting a ResourceQuota 05/01/23 18:51:30.715
STEP: Listing all ResourceQuotas with LabelSelector 05/01/23 18:51:30.726
STEP: Patching the ResourceQuota 05/01/23 18:51:30.744
STEP: Deleting a Collection of ResourceQuotas 05/01/23 18:51:30.772
STEP: Verifying the deleted ResourceQuota 05/01/23 18:51:30.814
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 18:51:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7420" for this suite. 05/01/23 18:51:30.841
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":18,"skipped":400,"failed":0}
------------------------------
• [0.291 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:30.59
    May  1 18:51:30.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 18:51:30.592
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:30.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:30.682
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 05/01/23 18:51:30.697
    STEP: Getting a ResourceQuota 05/01/23 18:51:30.715
    STEP: Listing all ResourceQuotas with LabelSelector 05/01/23 18:51:30.726
    STEP: Patching the ResourceQuota 05/01/23 18:51:30.744
    STEP: Deleting a Collection of ResourceQuotas 05/01/23 18:51:30.772
    STEP: Verifying the deleted ResourceQuota 05/01/23 18:51:30.814
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 18:51:30.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7420" for this suite. 05/01/23 18:51:30.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:30.891
May  1 18:51:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-pred 05/01/23 18:51:30.895
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:31.015
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  1 18:51:31.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  1 18:51:31.072: INFO: Waiting for terminating namespaces to be deleted...
May  1 18:51:31.158: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.124 before test
May  1 18:51:31.217: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.217: INFO: 	Container calico-node ready: true, restart count 0
May  1 18:51:31.217: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.217: INFO: 	Container calico-typha ready: true, restart count 1
May  1 18:51:31.217: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 18:51:31.218: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 18:51:31.218: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 18:51:31.218: INFO: 	Container pause ready: true, restart count 0
May  1 18:51:31.218: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 18:51:31.218: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container tuned ready: true, restart count 0
May  1 18:51:31.218: INFO: dns-default-6rl9c from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container dns ready: true, restart count 0
May  1 18:51:31.218: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.218: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.218: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 18:51:31.219: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.219: INFO: 	Container node-ca ready: true, restart count 0
May  1 18:51:31.219: INFO: ingress-canary-splcp from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.219: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 18:51:31.219: INFO: router-default-dc48bc679-xgv8x from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.219: INFO: 	Container router ready: true, restart count 0
May  1 18:51:31.220: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.220: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 18:51:31.220: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.220: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 17:00:24 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.220: INFO: 	Container alertmanager ready: true, restart count 1
May  1 18:51:31.220: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 18:51:31.220: INFO: 	Container config-reloader ready: true, restart count 0
May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 18:51:31.221: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 18:51:31.221: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.221: INFO: 	Container node-exporter ready: true, restart count 0
May  1 18:51:31.221: INFO: prometheus-adapter-d8df9dbf9-td74g from openshift-monitoring started at 2023-05-01 17:00:19 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.221: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 18:51:31.221: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 17:00:30 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.221: INFO: 	Container config-reloader ready: true, restart count 0
May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container prometheus ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 18:51:31.222: INFO: prometheus-operator-admission-webhook-98cbdbf8f-kfvjz from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.222: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 18:51:31.222: INFO: thanos-querier-6dcb8b776-gt2g9 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 18:51:31.223: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 18:51:31.223: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 18:51:31.223: INFO: 	Container thanos-query ready: true, restart count 0
May  1 18:51:31.223: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.223: INFO: 	Container kube-multus ready: true, restart count 0
May  1 18:51:31.223: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.223: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 18:51:31.223: INFO: multus-admission-controller-6b76fd464f-7xkp5 from openshift-multus started at 2023-05-01 16:58:13 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.223: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 18:51:31.223: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.224: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 18:51:31.224: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.224: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 18:51:31.224: INFO: collect-profiles-28049415-7qz4w from openshift-operator-lifecycle-manager started at 2023-05-01 18:15:00 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 18:51:31.224: INFO: collect-profiles-28049430-g7fpm from openshift-operator-lifecycle-manager started at 2023-05-01 18:30:00 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 18:51:31.224: INFO: collect-profiles-28049445-zcr2d from openshift-operator-lifecycle-manager started at 2023-05-01 18:45:00 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 18:51:31.224: INFO: packageserver-596c56d895-crb8b from openshift-operator-lifecycle-manager started at 2023-05-01 16:57:21 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.224: INFO: 	Container packageserver ready: true, restart count 0
May  1 18:51:31.225: INFO: condition-test-dkjqr from replication-controller-2124 started at 2023-05-01 18:51:29 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.225: INFO: 	Container httpd ready: false, restart count 0
May  1 18:51:31.225: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.225: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  1 18:51:31.225: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 18:51:31.225: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 18:51:31.225: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.126 before test
May  1 18:51:31.289: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.289: INFO: 	Container calico-node ready: true, restart count 0
May  1 18:51:31.289: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.289: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 18:51:31.289: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 18:51:31.290: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 18:51:31.290: INFO: 	Container pause ready: true, restart count 0
May  1 18:51:31.290: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 18:51:31.290: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container vpn ready: true, restart count 0
May  1 18:51:31.290: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container tuned ready: true, restart count 0
May  1 18:51:31.290: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container console ready: true, restart count 0
May  1 18:51:31.290: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.290: INFO: 	Container download-server ready: true, restart count 0
May  1 18:51:31.290: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container dns ready: true, restart count 0
May  1 18:51:31.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.291: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 18:51:31.291: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container registry ready: true, restart count 0
May  1 18:51:31.291: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container node-ca ready: true, restart count 0
May  1 18:51:31.291: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container pvc-permissions ready: false, restart count 0
May  1 18:51:31.291: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 18:51:31.291: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.291: INFO: 	Container router ready: true, restart count 0
May  1 18:51:31.292: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.292: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.292: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.292: INFO: 	Container alertmanager ready: true, restart count 1
May  1 18:51:31.292: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container config-reloader ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 18:51:31.292: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 18:51:31.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  1 18:51:31.292: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.293: INFO: 	Container node-exporter ready: true, restart count 0
May  1 18:51:31.293: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 18:51:31.293: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May  1 18:51:31.293: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.293: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 18:51:31.293: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.293: INFO: 	Container config-reloader ready: true, restart count 0
May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container prometheus ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 18:51:31.294: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container prometheus-operator ready: true, restart count 0
May  1 18:51:31.294: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.294: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 18:51:31.294: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container reload ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container telemeter-client ready: true, restart count 0
May  1 18:51:31.294: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container thanos-query ready: true, restart count 0
May  1 18:51:31.295: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.295: INFO: 	Container kube-multus ready: true, restart count 0
May  1 18:51:31.295: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.295: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 18:51:31.295: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 18:51:31.295: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.295: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 18:51:31.295: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.295: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 18:51:31.295: INFO: condition-test-hvsfw from replication-controller-2124 started at 2023-05-01 18:51:29 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.296: INFO: 	Container httpd ready: false, restart count 0
May  1 18:51:31.296: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.296: INFO: 	Container e2e ready: true, restart count 0
May  1 18:51:31.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 18:51:31.296: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 18:51:31.296: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 18:51:31.296: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.296: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  1 18:51:31.296: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.71 before test
May  1 18:51:31.388: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.388: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  1 18:51:31.388: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.388: INFO: 	Container calico-node ready: true, restart count 0
May  1 18:51:31.388: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.388: INFO: 	Container calico-typha ready: true, restart count 0
May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 18:51:31.389: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  1 18:51:31.389: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 18:51:31.389: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 18:51:31.389: INFO: 	Container pause ready: true, restart count 0
May  1 18:51:31.389: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
May  1 18:51:31.389: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
May  1 18:51:31.389: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  1 18:51:31.389: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 18:51:31.389: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May  1 18:51:31.389: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May  1 18:51:31.389: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container tuned ready: true, restart count 0
May  1 18:51:31.389: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May  1 18:51:31.389: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May  1 18:51:31.389: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container cluster-storage-operator ready: true, restart count 1
May  1 18:51:31.389: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 18:51:31.389: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 18:51:31.389: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
May  1 18:51:31.389: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container webhook ready: true, restart count 0
May  1 18:51:31.389: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container webhook ready: true, restart count 0
May  1 18:51:31.389: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container console-operator ready: true, restart count 1
May  1 18:51:31.389: INFO: 	Container conversion-webhook-server ready: true, restart count 2
May  1 18:51:31.389: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container console ready: true, restart count 0
May  1 18:51:31.389: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container download-server ready: true, restart count 0
May  1 18:51:31.389: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container dns-operator ready: true, restart count 0
May  1 18:51:31.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.389: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container dns ready: true, restart count 0
May  1 18:51:31.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.389: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 18:51:31.389: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May  1 18:51:31.389: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container node-ca ready: true, restart count 0
May  1 18:51:31.389: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.389: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 18:51:31.389: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container ingress-operator ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container insights-operator ready: true, restart count 1
May  1 18:51:31.390: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
May  1 18:51:31.390: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container migrator ready: true, restart count 0
May  1 18:51:31.390: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
May  1 18:51:31.390: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
May  1 18:51:31.390: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container marketplace-operator ready: true, restart count 0
May  1 18:51:31.390: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
May  1 18:51:31.390: INFO: redhat-operators-smvrd from openshift-marketplace started at 2023-05-01 18:51:19 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
May  1 18:51:31.390: INFO: redhat-operators-wcvpn from openshift-marketplace started at 2023-05-01 18:36:42 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
May  1 18:51:31.390: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container node-exporter ready: true, restart count 0
May  1 18:51:31.390: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 18:51:31.390: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-multus ready: true, restart count 0
May  1 18:51:31.390: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 18:51:31.390: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container check-endpoints ready: true, restart count 0
May  1 18:51:31.390: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 18:51:31.390: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container network-operator ready: true, restart count 1
May  1 18:51:31.390: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container catalog-operator ready: true, restart count 0
May  1 18:51:31.390: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container olm-operator ready: true, restart count 0
May  1 18:51:31.390: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container package-server-manager ready: true, restart count 0
May  1 18:51:31.390: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container packageserver ready: true, restart count 0
May  1 18:51:31.390: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container metrics ready: true, restart count 2
May  1 18:51:31.390: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container push-gateway ready: true, restart count 0
May  1 18:51:31.390: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container service-ca-operator ready: true, restart count 1
May  1 18:51:31.390: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container service-ca-controller ready: true, restart count 0
May  1 18:51:31.390: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 18:51:31.390: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 18:51:31.390: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 18:51:31.390: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
May  1 18:51:31.391: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.45.145.124 05/01/23 18:51:31.519
STEP: verifying the node has the label node 10.45.145.126 05/01/23 18:51:31.613
STEP: verifying the node has the label node 10.45.145.71 05/01/23 18:51:31.655
May  1 18:51:31.792: INFO: Pod calico-kube-controllers-79f474fb8-jn672 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod calico-node-4s5vb requesting resource cpu=250m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod calico-node-m6plq requesting resource cpu=250m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod calico-node-slcg9 requesting resource cpu=250m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod calico-typha-75ff8c8c66-4zgxv requesting resource cpu=250m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod calico-typha-75ff8c8c66-6z5c6 requesting resource cpu=250m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-72pdm requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-x9sv5 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-zbpz5 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm requesting resource cpu=5m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 requesting resource cpu=5m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod ibm-file-plugin-5fdffc884f-4bkrn requesting resource cpu=50m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-cpgdp requesting resource cpu=5m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-ncf96 requesting resource cpu=5m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-qbrsx requesting resource cpu=5m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.124 requesting resource cpu=26m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.126 requesting resource cpu=26m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.71 requesting resource cpu=26m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibm-storage-metrics-agent-7f994d5df5-6j4zc requesting resource cpu=60m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibm-storage-watcher-545798cc6f-lrgms requesting resource cpu=50m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-cmlmj requesting resource cpu=50m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-f82m7 requesting resource cpu=50m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-ffmmx requesting resource cpu=50m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-plugin-5f677b8577-9mpkl requesting resource cpu=50m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod vpn-597d5865cf-9xpdf requesting resource cpu=5m on Node 10.45.145.126
May  1 18:51:31.792: INFO: Pod cluster-node-tuning-operator-6567656d4-hddjg requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod tuned-hfhlr requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.792: INFO: Pod tuned-lpz6l requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.792: INFO: Pod tuned-q8c2h requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod cluster-samples-operator-7d69df847f-nlkl4 requesting resource cpu=20m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod cluster-storage-operator-6b6bc9bd94-7mshg requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-567b6b4d78-4z8tw requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-567b6b4d78-86bnm requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-operator-7985c7d9c-sr9rq requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod csi-snapshot-webhook-7f5c5bc774-cnqv7 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod csi-snapshot-webhook-7f5c5bc774-csznr requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod console-operator-5bcc5564c6-g78ch requesting resource cpu=20m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod console-568f9d5f69-6h2kn requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod console-568f9d5f69-v7kx2 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod downloads-85fbdb68d8-tzwqx requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod downloads-85fbdb68d8-v9dbs requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod dns-operator-5496b6bfdb-8qxdd requesting resource cpu=20m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod dns-default-28psr requesting resource cpu=60m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod dns-default-6rl9c requesting resource cpu=60m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod dns-default-h4p5f requesting resource cpu=60m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod node-resolver-b2nzd requesting resource cpu=5m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod node-resolver-k54ph requesting resource cpu=5m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod node-resolver-krvqc requesting resource cpu=5m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod cluster-image-registry-operator-6dc45cdc69-wxffb requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod image-registry-848dd668cf-w4pll requesting resource cpu=100m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod node-ca-4pw98 requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod node-ca-7wbwj requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod node-ca-xjld6 requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod ingress-canary-k9br4 requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod ingress-canary-splcp requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod ingress-canary-wxrz9 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod ingress-operator-5bdd9bbb4d-rg9xh requesting resource cpu=20m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod router-default-dc48bc679-bqjs8 requesting resource cpu=100m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod router-default-dc48bc679-xgv8x requesting resource cpu=100m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod insights-operator-5dd7768ccd-kgnwj requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-77zfm requesting resource cpu=110m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-xbgzr requesting resource cpu=110m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-xcd7n requesting resource cpu=110m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod kube-storage-version-migrator-operator-8485684586-ln9xn requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod migrator-59fb996c9c-rgzx9 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod certified-operators-kw674 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod community-operators-qz7gr requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod marketplace-operator-65dd67f5bd-7k26x requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod redhat-marketplace-gsdhv requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod redhat-operators-smvrd requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod redhat-operators-wcvpn requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod cluster-monitoring-operator-68fccd9857-q6vhq requesting resource cpu=11m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod kube-state-metrics-7fddd57764-fp7dw requesting resource cpu=4m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod node-exporter-c7g5p requesting resource cpu=9m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod node-exporter-rv48f requesting resource cpu=9m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod node-exporter-xpp24 requesting resource cpu=9m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod openshift-state-metrics-75944bd6bd-qpcd4 requesting resource cpu=3m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod prometheus-adapter-d8df9dbf9-6l9wt requesting resource cpu=1m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod prometheus-adapter-d8df9dbf9-td74g requesting resource cpu=1m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod prometheus-operator-6c947f76fc-lzm9g requesting resource cpu=6m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod prometheus-operator-admission-webhook-98cbdbf8f-kfvjz requesting resource cpu=5m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod prometheus-operator-admission-webhook-98cbdbf8f-nnznl requesting resource cpu=5m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod telemeter-client-5c79955f45-4vgg9 requesting resource cpu=3m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod thanos-querier-6dcb8b776-gt2g9 requesting resource cpu=15m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod thanos-querier-6dcb8b776-sl7w4 requesting resource cpu=15m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod multus-7gn4h requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod multus-7qs8w requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-5rf4h requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-7wpc7 requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-v48tc requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.794: INFO: Pod multus-admission-controller-6b76fd464f-7xkp5 requesting resource cpu=20m on Node 10.45.145.124
May  1 18:51:31.794: INFO: Pod multus-admission-controller-6b76fd464f-pxt8v requesting resource cpu=20m on Node 10.45.145.126
May  1 18:51:31.794: INFO: Pod multus-hrxxk requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.794: INFO: Pod network-metrics-daemon-4w594 requesting resource cpu=20m on Node 10.45.145.124
May  1 18:51:31.794: INFO: Pod network-metrics-daemon-7dbgd requesting resource cpu=20m on Node 10.45.145.71
May  1 18:51:31.794: INFO: Pod network-metrics-daemon-mxrp7 requesting resource cpu=20m on Node 10.45.145.126
May  1 18:51:31.794: INFO: Pod network-check-source-5f544d4b86-mwpjv requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.794: INFO: Pod network-check-target-7gwwh requesting resource cpu=10m on Node 10.45.145.126
May  1 18:51:31.794: INFO: Pod network-check-target-8nwlk requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.794: INFO: Pod network-check-target-z87pw requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.794: INFO: Pod network-operator-7ccbcfd5c6-pl4m5 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod catalog-operator-779888458b-hmq8s requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod olm-operator-6bd5767fdb-2wvmg requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod package-server-manager-65bf985c9c-x4lwk requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod packageserver-596c56d895-crb8b requesting resource cpu=10m on Node 10.45.145.124
May  1 18:51:31.795: INFO: Pod packageserver-596c56d895-jj7sp requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod metrics-6d46d44d8f-kjrzq requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod push-gateway-5465b544cc-wcgd9 requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod service-ca-operator-658f9bdfcb-csr6d requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod service-ca-77bcb8d48b-rcv9l requesting resource cpu=10m on Node 10.45.145.71
May  1 18:51:31.795: INFO: Pod condition-test-dkjqr requesting resource cpu=0m on Node 10.45.145.124
May  1 18:51:31.796: INFO: Pod condition-test-hvsfw requesting resource cpu=0m on Node 10.45.145.126
May  1 18:51:31.796: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.45.145.124
May  1 18:51:31.796: INFO: Pod sonobuoy-e2e-job-9a58787263364bb6 requesting resource cpu=0m on Node 10.45.145.126
May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng requesting resource cpu=0m on Node 10.45.145.124
May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k requesting resource cpu=0m on Node 10.45.145.126
May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 requesting resource cpu=0m on Node 10.45.145.71
May  1 18:51:31.796: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.45.145.126
May  1 18:51:31.796: INFO: Pod tigera-operator-765c48479c-g55zf requesting resource cpu=100m on Node 10.45.145.71
STEP: Starting Pods to consume most of the cluster CPU. 05/01/23 18:51:31.796
May  1 18:51:31.797: INFO: Creating a pod which consumes cpu=2060m on Node 10.45.145.126
May  1 18:51:31.869: INFO: Creating a pod which consumes cpu=1633m on Node 10.45.145.71
May  1 18:51:31.930: INFO: Creating a pod which consumes cpu=1977m on Node 10.45.145.124
May  1 18:51:32.003: INFO: Waiting up to 5m0s for pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809" in namespace "sched-pred-8635" to be "running"
May  1 18:51:32.032: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 29.472555ms
May  1 18:51:34.064: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060687333s
May  1 18:51:36.050: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047001993s
May  1 18:51:38.049: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Running", Reason="", readiness=true. Elapsed: 6.04657521s
May  1 18:51:38.049: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809" satisfied condition "running"
May  1 18:51:38.050: INFO: Waiting up to 5m0s for pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81" in namespace "sched-pred-8635" to be "running"
May  1 18:51:38.061: INFO: Pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81": Phase="Running", Reason="", readiness=true. Elapsed: 11.783324ms
May  1 18:51:38.061: INFO: Pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81" satisfied condition "running"
May  1 18:51:38.061: INFO: Waiting up to 5m0s for pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6" in namespace "sched-pred-8635" to be "running"
May  1 18:51:38.075: INFO: Pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6": Phase="Running", Reason="", readiness=true. Elapsed: 13.211991ms
May  1 18:51:38.075: INFO: Pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/01/23 18:51:38.075
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a74497742c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81 to 10.45.145.71] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a783b2479c], Reason = [AddedInterface], Message = [Add eth0 [172.30.244.96/32] from k8s-pod-network] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a79a51f989], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7e35d53e6], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 1.225444627s (1.225459837s including waiting)] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7f0c27262], Reason = [Created], Message = [Created container filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7f2fc5d63], Reason = [Started], Message = [Started container filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a740f67b39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809 to 10.45.145.126] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a79683a6f0], Reason = [AddedInterface], Message = [Add eth0 [172.30.38.225/32] from k8s-pod-network] 05/01/23 18:51:38.091
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a7c6e2bc89], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a859ed469e], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 2.46688282s (2.466908441s including waiting)] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a882e7cad6], Reason = [Created], Message = [Created container filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a889703963], Reason = [Started], Message = [Started container filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a748d0748b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6 to 10.45.145.124] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a79a9182bd], Reason = [AddedInterface], Message = [Add eth0 [172.30.42.126/32] from k8s-pod-network] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7bcce708e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7daba3d59], Reason = [Created], Message = [Created container filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7e1fede9a], Reason = [Started], Message = [Started container filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6] 05/01/23 18:51:38.092
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175b19a8b6632573], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 05/01/23 18:51:38.178
STEP: removing the label node off the node 10.45.145.124 05/01/23 18:51:39.174
STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.224
STEP: removing the label node off the node 10.45.145.126 05/01/23 18:51:39.246
STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.324
STEP: removing the label node off the node 10.45.145.71 05/01/23 18:51:39.34
STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.404
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  1 18:51:39.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8635" for this suite. 05/01/23 18:51:39.483
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":19,"skipped":416,"failed":0}
------------------------------
• [SLOW TEST] [8.646 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:30.891
    May  1 18:51:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-pred 05/01/23 18:51:30.895
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:31.015
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  1 18:51:31.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  1 18:51:31.072: INFO: Waiting for terminating namespaces to be deleted...
    May  1 18:51:31.158: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.124 before test
    May  1 18:51:31.217: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.217: INFO: 	Container calico-node ready: true, restart count 0
    May  1 18:51:31.217: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.217: INFO: 	Container calico-typha ready: true, restart count 1
    May  1 18:51:31.217: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 18:51:31.218: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 18:51:31.218: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 18:51:31.218: INFO: 	Container pause ready: true, restart count 0
    May  1 18:51:31.218: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 18:51:31.218: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container tuned ready: true, restart count 0
    May  1 18:51:31.218: INFO: dns-default-6rl9c from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container dns ready: true, restart count 0
    May  1 18:51:31.218: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.218: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.218: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 18:51:31.219: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.219: INFO: 	Container node-ca ready: true, restart count 0
    May  1 18:51:31.219: INFO: ingress-canary-splcp from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.219: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 18:51:31.219: INFO: router-default-dc48bc679-xgv8x from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.219: INFO: 	Container router ready: true, restart count 0
    May  1 18:51:31.220: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.220: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 18:51:31.220: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.220: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 17:00:24 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.220: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 18:51:31.220: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 18:51:31.220: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 18:51:31.221: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 18:51:31.221: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.221: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 18:51:31.221: INFO: prometheus-adapter-d8df9dbf9-td74g from openshift-monitoring started at 2023-05-01 17:00:19 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.221: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 18:51:31.221: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 17:00:30 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.221: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 18:51:31.221: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container prometheus ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 18:51:31.222: INFO: prometheus-operator-admission-webhook-98cbdbf8f-kfvjz from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.222: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 18:51:31.222: INFO: thanos-querier-6dcb8b776-gt2g9 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 18:51:31.222: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 18:51:31.223: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 18:51:31.223: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 18:51:31.223: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 18:51:31.223: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.223: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 18:51:31.223: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.223: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 18:51:31.223: INFO: multus-admission-controller-6b76fd464f-7xkp5 from openshift-multus started at 2023-05-01 16:58:13 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.223: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 18:51:31.223: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.223: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.224: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 18:51:31.224: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.224: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 18:51:31.224: INFO: collect-profiles-28049415-7qz4w from openshift-operator-lifecycle-manager started at 2023-05-01 18:15:00 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 18:51:31.224: INFO: collect-profiles-28049430-g7fpm from openshift-operator-lifecycle-manager started at 2023-05-01 18:30:00 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 18:51:31.224: INFO: collect-profiles-28049445-zcr2d from openshift-operator-lifecycle-manager started at 2023-05-01 18:45:00 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.224: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 18:51:31.224: INFO: packageserver-596c56d895-crb8b from openshift-operator-lifecycle-manager started at 2023-05-01 16:57:21 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.224: INFO: 	Container packageserver ready: true, restart count 0
    May  1 18:51:31.225: INFO: condition-test-dkjqr from replication-controller-2124 started at 2023-05-01 18:51:29 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.225: INFO: 	Container httpd ready: false, restart count 0
    May  1 18:51:31.225: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.225: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  1 18:51:31.225: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 18:51:31.225: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 18:51:31.225: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.126 before test
    May  1 18:51:31.289: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.289: INFO: 	Container calico-node ready: true, restart count 0
    May  1 18:51:31.289: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.289: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 18:51:31.289: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 18:51:31.290: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 18:51:31.290: INFO: 	Container pause ready: true, restart count 0
    May  1 18:51:31.290: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 18:51:31.290: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container vpn ready: true, restart count 0
    May  1 18:51:31.290: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container tuned ready: true, restart count 0
    May  1 18:51:31.290: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container console ready: true, restart count 0
    May  1 18:51:31.290: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.290: INFO: 	Container download-server ready: true, restart count 0
    May  1 18:51:31.290: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container dns ready: true, restart count 0
    May  1 18:51:31.291: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.291: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 18:51:31.291: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container registry ready: true, restart count 0
    May  1 18:51:31.291: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container node-ca ready: true, restart count 0
    May  1 18:51:31.291: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container pvc-permissions ready: false, restart count 0
    May  1 18:51:31.291: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 18:51:31.291: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.291: INFO: 	Container router ready: true, restart count 0
    May  1 18:51:31.292: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.292: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.292: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.292: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 18:51:31.292: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 18:51:31.292: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 18:51:31.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  1 18:51:31.292: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.293: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 18:51:31.293: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 18:51:31.293: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May  1 18:51:31.293: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.293: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 18:51:31.293: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.293: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.293: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container prometheus ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 18:51:31.294: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  1 18:51:31.294: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.294: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 18:51:31.294: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
    May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container reload ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container telemeter-client ready: true, restart count 0
    May  1 18:51:31.294: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.294: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 18:51:31.295: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.295: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 18:51:31.295: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.295: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 18:51:31.295: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 18:51:31.295: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.295: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.295: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 18:51:31.295: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.295: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 18:51:31.295: INFO: condition-test-hvsfw from replication-controller-2124 started at 2023-05-01 18:51:29 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.296: INFO: 	Container httpd ready: false, restart count 0
    May  1 18:51:31.296: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.296: INFO: 	Container e2e ready: true, restart count 0
    May  1 18:51:31.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 18:51:31.296: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 18:51:31.296: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 18:51:31.296: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.296: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    May  1 18:51:31.296: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.71 before test
    May  1 18:51:31.388: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.388: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  1 18:51:31.388: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.388: INFO: 	Container calico-node ready: true, restart count 0
    May  1 18:51:31.388: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.388: INFO: 	Container calico-typha ready: true, restart count 0
    May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 18:51:31.389: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 18:51:31.389: INFO: 	Container pause ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    May  1 18:51:31.389: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 18:51:31.389: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    May  1 18:51:31.389: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    May  1 18:51:31.389: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container tuned ready: true, restart count 0
    May  1 18:51:31.389: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    May  1 18:51:31.389: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    May  1 18:51:31.389: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    May  1 18:51:31.389: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 18:51:31.389: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 18:51:31.389: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    May  1 18:51:31.389: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container webhook ready: true, restart count 0
    May  1 18:51:31.389: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container webhook ready: true, restart count 0
    May  1 18:51:31.389: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container console-operator ready: true, restart count 1
    May  1 18:51:31.389: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    May  1 18:51:31.389: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container console ready: true, restart count 0
    May  1 18:51:31.389: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container download-server ready: true, restart count 0
    May  1 18:51:31.389: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container dns-operator ready: true, restart count 0
    May  1 18:51:31.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.389: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container dns ready: true, restart count 0
    May  1 18:51:31.389: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.389: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 18:51:31.389: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    May  1 18:51:31.389: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container node-ca ready: true, restart count 0
    May  1 18:51:31.389: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.389: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 18:51:31.389: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container ingress-operator ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container insights-operator ready: true, restart count 1
    May  1 18:51:31.390: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    May  1 18:51:31.390: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container migrator ready: true, restart count 0
    May  1 18:51:31.390: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
    May  1 18:51:31.390: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
    May  1 18:51:31.390: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container marketplace-operator ready: true, restart count 0
    May  1 18:51:31.390: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
    May  1 18:51:31.390: INFO: redhat-operators-smvrd from openshift-marketplace started at 2023-05-01 18:51:19 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
    May  1 18:51:31.390: INFO: redhat-operators-wcvpn from openshift-marketplace started at 2023-05-01 18:36:42 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container registry-server ready: true, restart count 0
    May  1 18:51:31.390: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 18:51:31.390: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 18:51:31.390: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 18:51:31.390: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 18:51:31.390: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container check-endpoints ready: true, restart count 0
    May  1 18:51:31.390: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 18:51:31.390: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container network-operator ready: true, restart count 1
    May  1 18:51:31.390: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container catalog-operator ready: true, restart count 0
    May  1 18:51:31.390: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container olm-operator ready: true, restart count 0
    May  1 18:51:31.390: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container package-server-manager ready: true, restart count 0
    May  1 18:51:31.390: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container packageserver ready: true, restart count 0
    May  1 18:51:31.390: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container metrics ready: true, restart count 2
    May  1 18:51:31.390: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container push-gateway ready: true, restart count 0
    May  1 18:51:31.390: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container service-ca-operator ready: true, restart count 1
    May  1 18:51:31.390: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container service-ca-controller ready: true, restart count 0
    May  1 18:51:31.390: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 18:51:31.390: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 18:51:31.390: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 18:51:31.390: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
    May  1 18:51:31.391: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.45.145.124 05/01/23 18:51:31.519
    STEP: verifying the node has the label node 10.45.145.126 05/01/23 18:51:31.613
    STEP: verifying the node has the label node 10.45.145.71 05/01/23 18:51:31.655
    May  1 18:51:31.792: INFO: Pod calico-kube-controllers-79f474fb8-jn672 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod calico-node-4s5vb requesting resource cpu=250m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod calico-node-m6plq requesting resource cpu=250m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod calico-node-slcg9 requesting resource cpu=250m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod calico-typha-75ff8c8c66-4zgxv requesting resource cpu=250m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod calico-typha-75ff8c8c66-6z5c6 requesting resource cpu=250m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-72pdm requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-x9sv5 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod managed-storage-validation-webhooks-77db8bf5b-zbpz5 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm requesting resource cpu=5m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 requesting resource cpu=5m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod ibm-file-plugin-5fdffc884f-4bkrn requesting resource cpu=50m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-cpgdp requesting resource cpu=5m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-ncf96 requesting resource cpu=5m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod ibm-keepalived-watcher-qbrsx requesting resource cpu=5m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.124 requesting resource cpu=26m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.126 requesting resource cpu=26m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod ibm-master-proxy-static-10.45.145.71 requesting resource cpu=26m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibm-storage-metrics-agent-7f994d5df5-6j4zc requesting resource cpu=60m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibm-storage-watcher-545798cc6f-lrgms requesting resource cpu=50m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-cmlmj requesting resource cpu=50m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-f82m7 requesting resource cpu=50m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-driver-ffmmx requesting resource cpu=50m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod ibmcloud-block-storage-plugin-5f677b8577-9mpkl requesting resource cpu=50m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod vpn-597d5865cf-9xpdf requesting resource cpu=5m on Node 10.45.145.126
    May  1 18:51:31.792: INFO: Pod cluster-node-tuning-operator-6567656d4-hddjg requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod tuned-hfhlr requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.792: INFO: Pod tuned-lpz6l requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.792: INFO: Pod tuned-q8c2h requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod cluster-samples-operator-7d69df847f-nlkl4 requesting resource cpu=20m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod cluster-storage-operator-6b6bc9bd94-7mshg requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-567b6b4d78-4z8tw requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-567b6b4d78-86bnm requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod csi-snapshot-controller-operator-7985c7d9c-sr9rq requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod csi-snapshot-webhook-7f5c5bc774-cnqv7 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod csi-snapshot-webhook-7f5c5bc774-csznr requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod console-operator-5bcc5564c6-g78ch requesting resource cpu=20m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod console-568f9d5f69-6h2kn requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod console-568f9d5f69-v7kx2 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod downloads-85fbdb68d8-tzwqx requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod downloads-85fbdb68d8-v9dbs requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod dns-operator-5496b6bfdb-8qxdd requesting resource cpu=20m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod dns-default-28psr requesting resource cpu=60m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod dns-default-6rl9c requesting resource cpu=60m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod dns-default-h4p5f requesting resource cpu=60m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod node-resolver-b2nzd requesting resource cpu=5m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod node-resolver-k54ph requesting resource cpu=5m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod node-resolver-krvqc requesting resource cpu=5m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod cluster-image-registry-operator-6dc45cdc69-wxffb requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod image-registry-848dd668cf-w4pll requesting resource cpu=100m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod node-ca-4pw98 requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod node-ca-7wbwj requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod node-ca-xjld6 requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod ingress-canary-k9br4 requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod ingress-canary-splcp requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod ingress-canary-wxrz9 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod ingress-operator-5bdd9bbb4d-rg9xh requesting resource cpu=20m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod router-default-dc48bc679-bqjs8 requesting resource cpu=100m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod router-default-dc48bc679-xgv8x requesting resource cpu=100m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod insights-operator-5dd7768ccd-kgnwj requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-77zfm requesting resource cpu=110m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-xbgzr requesting resource cpu=110m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod openshift-kube-proxy-xcd7n requesting resource cpu=110m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod kube-storage-version-migrator-operator-8485684586-ln9xn requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod migrator-59fb996c9c-rgzx9 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod certified-operators-kw674 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod community-operators-qz7gr requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod marketplace-operator-65dd67f5bd-7k26x requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod redhat-marketplace-gsdhv requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod redhat-operators-smvrd requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod redhat-operators-wcvpn requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod cluster-monitoring-operator-68fccd9857-q6vhq requesting resource cpu=11m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod kube-state-metrics-7fddd57764-fp7dw requesting resource cpu=4m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod node-exporter-c7g5p requesting resource cpu=9m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod node-exporter-rv48f requesting resource cpu=9m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod node-exporter-xpp24 requesting resource cpu=9m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod openshift-state-metrics-75944bd6bd-qpcd4 requesting resource cpu=3m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod prometheus-adapter-d8df9dbf9-6l9wt requesting resource cpu=1m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod prometheus-adapter-d8df9dbf9-td74g requesting resource cpu=1m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod prometheus-operator-6c947f76fc-lzm9g requesting resource cpu=6m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod prometheus-operator-admission-webhook-98cbdbf8f-kfvjz requesting resource cpu=5m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod prometheus-operator-admission-webhook-98cbdbf8f-nnznl requesting resource cpu=5m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod telemeter-client-5c79955f45-4vgg9 requesting resource cpu=3m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod thanos-querier-6dcb8b776-gt2g9 requesting resource cpu=15m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod thanos-querier-6dcb8b776-sl7w4 requesting resource cpu=15m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod multus-7gn4h requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod multus-7qs8w requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-5rf4h requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-7wpc7 requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.793: INFO: Pod multus-additional-cni-plugins-v48tc requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.794: INFO: Pod multus-admission-controller-6b76fd464f-7xkp5 requesting resource cpu=20m on Node 10.45.145.124
    May  1 18:51:31.794: INFO: Pod multus-admission-controller-6b76fd464f-pxt8v requesting resource cpu=20m on Node 10.45.145.126
    May  1 18:51:31.794: INFO: Pod multus-hrxxk requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.794: INFO: Pod network-metrics-daemon-4w594 requesting resource cpu=20m on Node 10.45.145.124
    May  1 18:51:31.794: INFO: Pod network-metrics-daemon-7dbgd requesting resource cpu=20m on Node 10.45.145.71
    May  1 18:51:31.794: INFO: Pod network-metrics-daemon-mxrp7 requesting resource cpu=20m on Node 10.45.145.126
    May  1 18:51:31.794: INFO: Pod network-check-source-5f544d4b86-mwpjv requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.794: INFO: Pod network-check-target-7gwwh requesting resource cpu=10m on Node 10.45.145.126
    May  1 18:51:31.794: INFO: Pod network-check-target-8nwlk requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.794: INFO: Pod network-check-target-z87pw requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.794: INFO: Pod network-operator-7ccbcfd5c6-pl4m5 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod catalog-operator-779888458b-hmq8s requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod olm-operator-6bd5767fdb-2wvmg requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod package-server-manager-65bf985c9c-x4lwk requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod packageserver-596c56d895-crb8b requesting resource cpu=10m on Node 10.45.145.124
    May  1 18:51:31.795: INFO: Pod packageserver-596c56d895-jj7sp requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod metrics-6d46d44d8f-kjrzq requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod push-gateway-5465b544cc-wcgd9 requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod service-ca-operator-658f9bdfcb-csr6d requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod service-ca-77bcb8d48b-rcv9l requesting resource cpu=10m on Node 10.45.145.71
    May  1 18:51:31.795: INFO: Pod condition-test-dkjqr requesting resource cpu=0m on Node 10.45.145.124
    May  1 18:51:31.796: INFO: Pod condition-test-hvsfw requesting resource cpu=0m on Node 10.45.145.126
    May  1 18:51:31.796: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.45.145.124
    May  1 18:51:31.796: INFO: Pod sonobuoy-e2e-job-9a58787263364bb6 requesting resource cpu=0m on Node 10.45.145.126
    May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng requesting resource cpu=0m on Node 10.45.145.124
    May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k requesting resource cpu=0m on Node 10.45.145.126
    May  1 18:51:31.796: INFO: Pod sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 requesting resource cpu=0m on Node 10.45.145.71
    May  1 18:51:31.796: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.45.145.126
    May  1 18:51:31.796: INFO: Pod tigera-operator-765c48479c-g55zf requesting resource cpu=100m on Node 10.45.145.71
    STEP: Starting Pods to consume most of the cluster CPU. 05/01/23 18:51:31.796
    May  1 18:51:31.797: INFO: Creating a pod which consumes cpu=2060m on Node 10.45.145.126
    May  1 18:51:31.869: INFO: Creating a pod which consumes cpu=1633m on Node 10.45.145.71
    May  1 18:51:31.930: INFO: Creating a pod which consumes cpu=1977m on Node 10.45.145.124
    May  1 18:51:32.003: INFO: Waiting up to 5m0s for pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809" in namespace "sched-pred-8635" to be "running"
    May  1 18:51:32.032: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 29.472555ms
    May  1 18:51:34.064: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060687333s
    May  1 18:51:36.050: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047001993s
    May  1 18:51:38.049: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809": Phase="Running", Reason="", readiness=true. Elapsed: 6.04657521s
    May  1 18:51:38.049: INFO: Pod "filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809" satisfied condition "running"
    May  1 18:51:38.050: INFO: Waiting up to 5m0s for pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81" in namespace "sched-pred-8635" to be "running"
    May  1 18:51:38.061: INFO: Pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81": Phase="Running", Reason="", readiness=true. Elapsed: 11.783324ms
    May  1 18:51:38.061: INFO: Pod "filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81" satisfied condition "running"
    May  1 18:51:38.061: INFO: Waiting up to 5m0s for pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6" in namespace "sched-pred-8635" to be "running"
    May  1 18:51:38.075: INFO: Pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6": Phase="Running", Reason="", readiness=true. Elapsed: 13.211991ms
    May  1 18:51:38.075: INFO: Pod "filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/01/23 18:51:38.075
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a74497742c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81 to 10.45.145.71] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a783b2479c], Reason = [AddedInterface], Message = [Add eth0 [172.30.244.96/32] from k8s-pod-network] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a79a51f989], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7e35d53e6], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 1.225444627s (1.225459837s including waiting)] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7f0c27262], Reason = [Created], Message = [Created container filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81.175b19a7f2fc5d63], Reason = [Started], Message = [Started container filler-pod-28d3a0fc-f13c-4b94-840c-00a35f53cd81] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a740f67b39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809 to 10.45.145.126] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a79683a6f0], Reason = [AddedInterface], Message = [Add eth0 [172.30.38.225/32] from k8s-pod-network] 05/01/23 18:51:38.091
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a7c6e2bc89], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.8"] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a859ed469e], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.8" in 2.46688282s (2.466908441s including waiting)] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a882e7cad6], Reason = [Created], Message = [Created container filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809.175b19a889703963], Reason = [Started], Message = [Started container filler-pod-7b094750-f0bd-45e2-a9e0-f4244ce68809] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a748d0748b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8635/filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6 to 10.45.145.124] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a79a9182bd], Reason = [AddedInterface], Message = [Add eth0 [172.30.42.126/32] from k8s-pod-network] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7bcce708e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7daba3d59], Reason = [Created], Message = [Created container filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6.175b19a7e1fede9a], Reason = [Started], Message = [Started container filler-pod-a4331eb6-7c1a-4bf3-af6a-6de5709f22f6] 05/01/23 18:51:38.092
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175b19a8b6632573], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 05/01/23 18:51:38.178
    STEP: removing the label node off the node 10.45.145.124 05/01/23 18:51:39.174
    STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.224
    STEP: removing the label node off the node 10.45.145.126 05/01/23 18:51:39.246
    STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.324
    STEP: removing the label node off the node 10.45.145.71 05/01/23 18:51:39.34
    STEP: verifying the node doesn't have the label node 05/01/23 18:51:39.404
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  1 18:51:39.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8635" for this suite. 05/01/23 18:51:39.483
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:39.541
May  1 18:51:39.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 18:51:39.549
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:39.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:39.678
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May  1 18:51:40.265: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cb89923b-e8e9-47b2-8eae-81aca958fcaf", Controller:(*bool)(0xc001ea7a52), BlockOwnerDeletion:(*bool)(0xc001ea7a53)}}
May  1 18:51:40.288: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1406b37b-20a3-40ea-88d3-20007baf410f", Controller:(*bool)(0xc001ea7d36), BlockOwnerDeletion:(*bool)(0xc001ea7d37)}}
May  1 18:51:40.312: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6e0f8018-1d1c-47af-881c-60f1da0b90a2", Controller:(*bool)(0xc002c84016), BlockOwnerDeletion:(*bool)(0xc002c84017)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 18:51:45.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9877" for this suite. 05/01/23 18:51:45.401
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":20,"skipped":440,"failed":0}
------------------------------
• [SLOW TEST] [5.923 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:39.541
    May  1 18:51:39.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 18:51:39.549
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:39.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:39.678
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May  1 18:51:40.265: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cb89923b-e8e9-47b2-8eae-81aca958fcaf", Controller:(*bool)(0xc001ea7a52), BlockOwnerDeletion:(*bool)(0xc001ea7a53)}}
    May  1 18:51:40.288: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1406b37b-20a3-40ea-88d3-20007baf410f", Controller:(*bool)(0xc001ea7d36), BlockOwnerDeletion:(*bool)(0xc001ea7d37)}}
    May  1 18:51:40.312: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6e0f8018-1d1c-47af-881c-60f1da0b90a2", Controller:(*bool)(0xc002c84016), BlockOwnerDeletion:(*bool)(0xc002c84017)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 18:51:45.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9877" for this suite. 05/01/23 18:51:45.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:45.471
May  1 18:51:45.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 18:51:45.474
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:45.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:45.571
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/01/23 18:51:45.598
May  1 18:51:45.716: INFO: Waiting up to 5m0s for pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479" in namespace "emptydir-7860" to be "Succeeded or Failed"
May  1 18:51:45.746: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 29.653544ms
May  1 18:51:47.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042426031s
May  1 18:51:49.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043112741s
May  1 18:51:51.760: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043403545s
May  1 18:51:53.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042407795s
STEP: Saw pod success 05/01/23 18:51:53.759
May  1 18:51:53.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479" satisfied condition "Succeeded or Failed"
May  1 18:51:53.771: INFO: Trying to get logs from node 10.45.145.124 pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 container test-container: <nil>
STEP: delete the pod 05/01/23 18:51:53.803
May  1 18:51:53.836: INFO: Waiting for pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 to disappear
May  1 18:51:53.846: INFO: Pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 18:51:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7860" for this suite. 05/01/23 18:51:53.866
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":21,"skipped":459,"failed":0}
------------------------------
• [SLOW TEST] [8.425 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:45.471
    May  1 18:51:45.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 18:51:45.474
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:45.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:45.571
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/01/23 18:51:45.598
    May  1 18:51:45.716: INFO: Waiting up to 5m0s for pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479" in namespace "emptydir-7860" to be "Succeeded or Failed"
    May  1 18:51:45.746: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 29.653544ms
    May  1 18:51:47.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042426031s
    May  1 18:51:49.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043112741s
    May  1 18:51:51.760: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043403545s
    May  1 18:51:53.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042407795s
    STEP: Saw pod success 05/01/23 18:51:53.759
    May  1 18:51:53.759: INFO: Pod "pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479" satisfied condition "Succeeded or Failed"
    May  1 18:51:53.771: INFO: Trying to get logs from node 10.45.145.124 pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 container test-container: <nil>
    STEP: delete the pod 05/01/23 18:51:53.803
    May  1 18:51:53.836: INFO: Waiting for pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 to disappear
    May  1 18:51:53.846: INFO: Pod pod-c436fd35-60e0-4cca-9b3f-77aa67c6d479 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 18:51:53.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7860" for this suite. 05/01/23 18:51:53.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:53.896
May  1 18:51:53.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 18:51:53.899
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:53.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:53.978
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/01/23 18:51:54.031
STEP: Verify that the required pods have come up. 05/01/23 18:51:54.055
May  1 18:51:54.068: INFO: Pod name sample-pod: Found 0 pods out of 1
May  1 18:51:59.096: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 18:51:59.096
STEP: Getting /status 05/01/23 18:51:59.096
May  1 18:51:59.153: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/01/23 18:51:59.153
May  1 18:51:59.248: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/01/23 18:51:59.248
May  1 18:51:59.255: INFO: Observed &ReplicaSet event: ADDED
May  1 18:51:59.256: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.256: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.257: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.257: INFO: Found replicaset test-rs in namespace replicaset-3630 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  1 18:51:59.257: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/01/23 18:51:59.257
May  1 18:51:59.257: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  1 18:51:59.284: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/01/23 18:51:59.284
May  1 18:51:59.291: INFO: Observed &ReplicaSet event: ADDED
May  1 18:51:59.292: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.292: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.293: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.293: INFO: Observed replicaset test-rs in namespace replicaset-3630 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  1 18:51:59.293: INFO: Observed &ReplicaSet event: MODIFIED
May  1 18:51:59.293: INFO: Found replicaset test-rs in namespace replicaset-3630 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May  1 18:51:59.293: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 18:51:59.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3630" for this suite. 05/01/23 18:51:59.338
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":22,"skipped":470,"failed":0}
------------------------------
• [SLOW TEST] [5.476 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:53.896
    May  1 18:51:53.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 18:51:53.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:53.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:53.978
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/01/23 18:51:54.031
    STEP: Verify that the required pods have come up. 05/01/23 18:51:54.055
    May  1 18:51:54.068: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  1 18:51:59.096: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 18:51:59.096
    STEP: Getting /status 05/01/23 18:51:59.096
    May  1 18:51:59.153: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/01/23 18:51:59.153
    May  1 18:51:59.248: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/01/23 18:51:59.248
    May  1 18:51:59.255: INFO: Observed &ReplicaSet event: ADDED
    May  1 18:51:59.256: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.256: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.257: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.257: INFO: Found replicaset test-rs in namespace replicaset-3630 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  1 18:51:59.257: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/01/23 18:51:59.257
    May  1 18:51:59.257: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  1 18:51:59.284: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/01/23 18:51:59.284
    May  1 18:51:59.291: INFO: Observed &ReplicaSet event: ADDED
    May  1 18:51:59.292: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.292: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.293: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.293: INFO: Observed replicaset test-rs in namespace replicaset-3630 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  1 18:51:59.293: INFO: Observed &ReplicaSet event: MODIFIED
    May  1 18:51:59.293: INFO: Found replicaset test-rs in namespace replicaset-3630 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May  1 18:51:59.293: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 18:51:59.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3630" for this suite. 05/01/23 18:51:59.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:51:59.375
May  1 18:51:59.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 18:51:59.377
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:59.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:59.562
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 05/01/23 18:51:59.797
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 18:51:59.831
May  1 18:51:59.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 18:51:59.870: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 18:52:00.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 18:52:00.914: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 18:52:01.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 18:52:01.928: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 18:52:02.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  1 18:52:02.914: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 18:52:03.903: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 18:52:03.903: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 18:52:04.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 18:52:04.902: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 18:52:05.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 18:52:05.909: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 18:52:06.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 18:52:06.915: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 18:52:07.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 18:52:07.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/01/23 18:52:07.925
May  1 18:52:08.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 18:52:08.078: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/01/23 18:52:08.078
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 18:52:08.149
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1570, will wait for the garbage collector to delete the pods 05/01/23 18:52:08.149
May  1 18:52:08.325: INFO: Deleting DaemonSet.extensions daemon-set took: 56.102207ms
May  1 18:52:08.526: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.979784ms
May  1 18:52:13.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 18:52:13.436: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 18:52:13.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61868"},"items":null}

May  1 18:52:13.469: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61868"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 18:52:13.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1570" for this suite. 05/01/23 18:52:13.533
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":23,"skipped":508,"failed":0}
------------------------------
• [SLOW TEST] [14.188 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:51:59.375
    May  1 18:51:59.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 18:51:59.377
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:51:59.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:51:59.562
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 05/01/23 18:51:59.797
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 18:51:59.831
    May  1 18:51:59.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 18:51:59.870: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 18:52:00.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 18:52:00.914: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 18:52:01.928: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 18:52:01.928: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 18:52:02.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  1 18:52:02.914: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 18:52:03.903: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 18:52:03.903: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 18:52:04.902: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 18:52:04.902: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 18:52:05.909: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 18:52:05.909: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 18:52:06.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 18:52:06.915: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 18:52:07.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 18:52:07.905: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/01/23 18:52:07.925
    May  1 18:52:08.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 18:52:08.078: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/01/23 18:52:08.078
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 18:52:08.149
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1570, will wait for the garbage collector to delete the pods 05/01/23 18:52:08.149
    May  1 18:52:08.325: INFO: Deleting DaemonSet.extensions daemon-set took: 56.102207ms
    May  1 18:52:08.526: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.979784ms
    May  1 18:52:13.436: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 18:52:13.436: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 18:52:13.458: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"61868"},"items":null}

    May  1 18:52:13.469: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"61868"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 18:52:13.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1570" for this suite. 05/01/23 18:52:13.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:52:13.566
May  1 18:52:13.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 18:52:13.568
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:52:13.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:52:13.654
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/01/23 18:52:13.667
May  1 18:52:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 18:52:32.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 18:53:54.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3085" for this suite. 05/01/23 18:53:54.974
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":24,"skipped":519,"failed":0}
------------------------------
• [SLOW TEST] [101.437 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:52:13.566
    May  1 18:52:13.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 18:52:13.568
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:52:13.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:52:13.654
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/01/23 18:52:13.667
    May  1 18:52:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 18:52:32.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 18:53:54.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3085" for this suite. 05/01/23 18:53:54.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:53:55.011
May  1 18:53:55.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 18:53:55.014
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:53:55.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:53:55.089
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 05/01/23 18:53:55.111
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 18:53:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4751" for this suite. 05/01/23 18:53:55.239
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":25,"skipped":533,"failed":0}
------------------------------
• [0.272 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:53:55.011
    May  1 18:53:55.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 18:53:55.014
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:53:55.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:53:55.089
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 05/01/23 18:53:55.111
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 18:53:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4751" for this suite. 05/01/23 18:53:55.239
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 18:53:55.289
May  1 18:53:55.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 18:53:55.291
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:53:55.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:53:55.362
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1220 05/01/23 18:53:55.38
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 05/01/23 18:53:55.405
STEP: Creating stateful set ss in namespace statefulset-1220 05/01/23 18:53:55.438
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1220 05/01/23 18:53:55.503
May  1 18:53:55.526: INFO: Found 0 stateful pods, waiting for 1
May  1 18:54:05.566: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/01/23 18:54:05.566
May  1 18:54:05.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 18:54:06.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 18:54:06.609: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 18:54:06.609: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 18:54:06.628: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  1 18:54:16.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  1 18:54:16.653: INFO: Waiting for statefulset status.replicas updated to 0
May  1 18:54:16.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997813s
May  1 18:54:17.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.974500514s
May  1 18:54:18.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.950174077s
May  1 18:54:19.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.931482705s
May  1 18:54:20.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.913204519s
May  1 18:54:21.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.886752893s
May  1 18:54:22.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.847708866s
May  1 18:54:23.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.821244997s
May  1 18:54:24.938: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.8028268s
May  1 18:54:25.963: INFO: Verifying statefulset ss doesn't scale past 1 for another 783.53852ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1220 05/01/23 18:54:26.964
May  1 18:54:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:54:27.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 18:54:27.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 18:54:27.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 18:54:27.553: INFO: Found 1 stateful pods, waiting for 3
May  1 18:54:37.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 18:54:37.576: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  1 18:54:37.576: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/01/23 18:54:37.576
STEP: Scale down will halt with unhealthy stateful pod 05/01/23 18:54:37.577
May  1 18:54:37.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 18:54:38.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 18:54:38.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 18:54:38.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 18:54:38.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 18:54:39.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 18:54:39.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 18:54:39.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 18:54:39.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 18:54:39.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 18:54:39.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 18:54:39.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 18:54:39.833: INFO: Waiting for statefulset status.replicas updated to 0
May  1 18:54:39.852: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May  1 18:54:49.931: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  1 18:54:49.931: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  1 18:54:49.931: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  1 18:54:49.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997731s
May  1 18:54:51.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982176605s
May  1 18:54:52.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961351609s
May  1 18:54:53.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.936923785s
May  1 18:54:54.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914504035s
May  1 18:54:55.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.863019548s
May  1 18:54:56.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.84015267s
May  1 18:54:57.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.817757196s
May  1 18:54:58.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.795187288s
May  1 18:54:59.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 773.346215ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1220 05/01/23 18:55:00.223
May  1 18:55:00.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:00.795: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 18:55:00.795: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 18:55:00.795: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 18:55:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:01.410: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 18:55:01.410: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 18:55:01.410: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 18:55:01.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:02.040: INFO: rc: 1
May  1 18:55:02.040: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container is not created or running

error:
exit status 1
May  1 18:55:12.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:12.208: INFO: rc: 1
May  1 18:55:12.208: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:55:22.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:22.407: INFO: rc: 1
May  1 18:55:22.407: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:55:32.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:32.615: INFO: rc: 1
May  1 18:55:32.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:55:42.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:42.763: INFO: rc: 1
May  1 18:55:42.763: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:55:52.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:55:53.054: INFO: rc: 1
May  1 18:55:53.054: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:03.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:03.331: INFO: rc: 1
May  1 18:56:03.331: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:13.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:13.571: INFO: rc: 1
May  1 18:56:13.571: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:23.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:23.946: INFO: rc: 1
May  1 18:56:23.946: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:33.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:34.167: INFO: rc: 1
May  1 18:56:34.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:44.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:44.459: INFO: rc: 1
May  1 18:56:44.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:56:54.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:56:54.612: INFO: rc: 1
May  1 18:56:54.612: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:04.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:04.842: INFO: rc: 1
May  1 18:57:04.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:14.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:15.016: INFO: rc: 1
May  1 18:57:15.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:25.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:25.331: INFO: rc: 1
May  1 18:57:25.331: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:35.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:35.541: INFO: rc: 1
May  1 18:57:35.541: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:45.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:45.832: INFO: rc: 1
May  1 18:57:45.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:57:55.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:57:56.024: INFO: rc: 1
May  1 18:57:56.024: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:06.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:06.293: INFO: rc: 1
May  1 18:58:06.293: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:16.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:16.513: INFO: rc: 1
May  1 18:58:16.513: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:26.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:26.750: INFO: rc: 1
May  1 18:58:26.750: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:36.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:36.924: INFO: rc: 1
May  1 18:58:36.924: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:46.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:47.176: INFO: rc: 1
May  1 18:58:47.176: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:58:57.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:58:57.338: INFO: rc: 1
May  1 18:58:57.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:07.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:07.603: INFO: rc: 1
May  1 18:59:07.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:17.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:17.797: INFO: rc: 1
May  1 18:59:17.797: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:27.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:28.209: INFO: rc: 1
May  1 18:59:28.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:38.416: INFO: rc: 1
May  1 18:59:38.416: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:48.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:48.644: INFO: rc: 1
May  1 18:59:48.644: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 18:59:58.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 18:59:58.847: INFO: rc: 1
May  1 18:59:58.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
May  1 19:00:08.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 19:00:09.053: INFO: rc: 1
May  1 19:00:09.054: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
May  1 19:00:09.054: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/01/23 19:00:09.203
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 19:00:09.204: INFO: Deleting all statefulset in ns statefulset-1220
May  1 19:00:09.225: INFO: Scaling statefulset ss to 0
May  1 19:00:09.339: INFO: Waiting for statefulset status.replicas updated to 0
May  1 19:00:09.356: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 19:00:09.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1220" for this suite. 05/01/23 19:00:09.485
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":26,"skipped":545,"failed":0}
------------------------------
• [SLOW TEST] [374.234 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 18:53:55.289
    May  1 18:53:55.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 18:53:55.291
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 18:53:55.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 18:53:55.362
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1220 05/01/23 18:53:55.38
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/01/23 18:53:55.405
    STEP: Creating stateful set ss in namespace statefulset-1220 05/01/23 18:53:55.438
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1220 05/01/23 18:53:55.503
    May  1 18:53:55.526: INFO: Found 0 stateful pods, waiting for 1
    May  1 18:54:05.566: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/01/23 18:54:05.566
    May  1 18:54:05.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 18:54:06.608: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 18:54:06.609: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 18:54:06.609: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 18:54:06.628: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  1 18:54:16.653: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  1 18:54:16.653: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 18:54:16.744: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997813s
    May  1 18:54:17.771: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.974500514s
    May  1 18:54:18.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.950174077s
    May  1 18:54:19.809: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.931482705s
    May  1 18:54:20.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.913204519s
    May  1 18:54:21.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.886752893s
    May  1 18:54:22.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.847708866s
    May  1 18:54:23.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.821244997s
    May  1 18:54:24.938: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.8028268s
    May  1 18:54:25.963: INFO: Verifying statefulset ss doesn't scale past 1 for another 783.53852ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1220 05/01/23 18:54:26.964
    May  1 18:54:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:54:27.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 18:54:27.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 18:54:27.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 18:54:27.553: INFO: Found 1 stateful pods, waiting for 3
    May  1 18:54:37.576: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 18:54:37.576: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  1 18:54:37.576: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/01/23 18:54:37.576
    STEP: Scale down will halt with unhealthy stateful pod 05/01/23 18:54:37.577
    May  1 18:54:37.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 18:54:38.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 18:54:38.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 18:54:38.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 18:54:38.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 18:54:39.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 18:54:39.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 18:54:39.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 18:54:39.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 18:54:39.833: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 18:54:39.833: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 18:54:39.833: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 18:54:39.833: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 18:54:39.852: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    May  1 18:54:49.931: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  1 18:54:49.931: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  1 18:54:49.931: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  1 18:54:49.994: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997731s
    May  1 18:54:51.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982176605s
    May  1 18:54:52.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.961351609s
    May  1 18:54:53.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.936923785s
    May  1 18:54:54.113: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.914504035s
    May  1 18:54:55.135: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.863019548s
    May  1 18:54:56.158: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.84015267s
    May  1 18:54:57.180: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.817757196s
    May  1 18:54:58.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.795187288s
    May  1 18:54:59.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 773.346215ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1220 05/01/23 18:55:00.223
    May  1 18:55:00.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:00.795: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 18:55:00.795: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 18:55:00.795: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 18:55:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:01.410: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 18:55:01.410: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 18:55:01.410: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 18:55:01.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:02.040: INFO: rc: 1
    May  1 18:55:02.040: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    error: Internal error occurred: error executing command in container: container is not created or running

    error:
    exit status 1
    May  1 18:55:12.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:12.208: INFO: rc: 1
    May  1 18:55:12.208: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:55:22.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:22.407: INFO: rc: 1
    May  1 18:55:22.407: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:55:32.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:32.615: INFO: rc: 1
    May  1 18:55:32.615: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:55:42.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:42.763: INFO: rc: 1
    May  1 18:55:42.763: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:55:52.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:55:53.054: INFO: rc: 1
    May  1 18:55:53.054: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:03.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:03.331: INFO: rc: 1
    May  1 18:56:03.331: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:13.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:13.571: INFO: rc: 1
    May  1 18:56:13.571: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:23.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:23.946: INFO: rc: 1
    May  1 18:56:23.946: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:33.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:34.167: INFO: rc: 1
    May  1 18:56:34.167: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:44.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:44.459: INFO: rc: 1
    May  1 18:56:44.459: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:56:54.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:56:54.612: INFO: rc: 1
    May  1 18:56:54.612: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:04.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:04.842: INFO: rc: 1
    May  1 18:57:04.842: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:14.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:15.016: INFO: rc: 1
    May  1 18:57:15.016: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:25.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:25.331: INFO: rc: 1
    May  1 18:57:25.331: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:35.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:35.541: INFO: rc: 1
    May  1 18:57:35.541: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:45.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:45.832: INFO: rc: 1
    May  1 18:57:45.832: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:57:55.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:57:56.024: INFO: rc: 1
    May  1 18:57:56.024: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:06.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:06.293: INFO: rc: 1
    May  1 18:58:06.293: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:16.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:16.513: INFO: rc: 1
    May  1 18:58:16.513: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:26.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:26.750: INFO: rc: 1
    May  1 18:58:26.750: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:36.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:36.924: INFO: rc: 1
    May  1 18:58:36.924: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:46.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:47.176: INFO: rc: 1
    May  1 18:58:47.176: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:58:57.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:58:57.338: INFO: rc: 1
    May  1 18:58:57.338: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:07.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:07.603: INFO: rc: 1
    May  1 18:59:07.603: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:17.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:17.797: INFO: rc: 1
    May  1 18:59:17.797: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:27.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:28.209: INFO: rc: 1
    May  1 18:59:28.210: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:38.416: INFO: rc: 1
    May  1 18:59:38.416: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:48.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:48.644: INFO: rc: 1
    May  1 18:59:48.644: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 18:59:58.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 18:59:58.847: INFO: rc: 1
    May  1 18:59:58.847: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
    Command stdout:

    stderr:
    Error from server (NotFound): pods "ss-2" not found

    error:
    exit status 1
    May  1 19:00:08.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-1220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 19:00:09.053: INFO: rc: 1
    May  1 19:00:09.054: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
    May  1 19:00:09.054: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/01/23 19:00:09.203
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 19:00:09.204: INFO: Deleting all statefulset in ns statefulset-1220
    May  1 19:00:09.225: INFO: Scaling statefulset ss to 0
    May  1 19:00:09.339: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 19:00:09.356: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 19:00:09.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1220" for this suite. 05/01/23 19:00:09.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:09.525
May  1 19:00:09.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:00:09.527
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:09.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:09.793
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 05/01/23 19:00:09.817
May  1 19:00:09.968: INFO: Waiting up to 5m0s for pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c" in namespace "downward-api-9398" to be "running and ready"
May  1 19:00:09.991: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.789182ms
May  1 19:00:09.991: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:12.023: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05449006s
May  1 19:00:12.023: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:14.021: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Running", Reason="", readiness=true. Elapsed: 4.052166703s
May  1 19:00:14.021: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Running (Ready = true)
May  1 19:00:14.021: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c" satisfied condition "running and ready"
May  1 19:00:14.822: INFO: Successfully updated pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 19:00:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9398" for this suite. 05/01/23 19:00:16.937
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":27,"skipped":551,"failed":0}
------------------------------
• [SLOW TEST] [7.434 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:09.525
    May  1 19:00:09.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:00:09.527
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:09.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:09.793
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 05/01/23 19:00:09.817
    May  1 19:00:09.968: INFO: Waiting up to 5m0s for pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c" in namespace "downward-api-9398" to be "running and ready"
    May  1 19:00:09.991: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.789182ms
    May  1 19:00:09.991: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:12.023: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05449006s
    May  1 19:00:12.023: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:14.021: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c": Phase="Running", Reason="", readiness=true. Elapsed: 4.052166703s
    May  1 19:00:14.021: INFO: The phase of Pod annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c is Running (Ready = true)
    May  1 19:00:14.021: INFO: Pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c" satisfied condition "running and ready"
    May  1 19:00:14.822: INFO: Successfully updated pod "annotationupdatedf4b3f5f-e1c9-4a97-b5fe-d3a59768012c"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 19:00:16.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9398" for this suite. 05/01/23 19:00:16.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:16.964
May  1 19:00:16.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption 05/01/23 19:00:16.966
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:17.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:17.101
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 05/01/23 19:00:17.129
STEP: Waiting for the pdb to be processed 05/01/23 19:00:17.172
STEP: updating the pdb 05/01/23 19:00:19.263
STEP: Waiting for the pdb to be processed 05/01/23 19:00:19.328
STEP: patching the pdb 05/01/23 19:00:21.382
STEP: Waiting for the pdb to be processed 05/01/23 19:00:21.447
STEP: Waiting for the pdb to be deleted 05/01/23 19:00:23.534
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  1 19:00:23.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9424" for this suite. 05/01/23 19:00:23.573
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":28,"skipped":579,"failed":0}
------------------------------
• [SLOW TEST] [6.638 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:16.964
    May  1 19:00:16.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption 05/01/23 19:00:16.966
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:17.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:17.101
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 05/01/23 19:00:17.129
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:17.172
    STEP: updating the pdb 05/01/23 19:00:19.263
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:19.328
    STEP: patching the pdb 05/01/23 19:00:21.382
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:21.447
    STEP: Waiting for the pdb to be deleted 05/01/23 19:00:23.534
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  1 19:00:23.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9424" for this suite. 05/01/23 19:00:23.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:23.608
May  1 19:00:23.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:00:23.611
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:23.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:23.701
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
May  1 19:00:23.773: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-390f24dc-dfb7-4357-8b54-a1ccea0c0cea 05/01/23 19:00:23.774
STEP: Creating secret with name s-test-opt-upd-89efbede-f6ad-4897-a91e-ce932a32d038 05/01/23 19:00:23.809
STEP: Creating the pod 05/01/23 19:00:23.832
May  1 19:00:23.908: INFO: Waiting up to 5m0s for pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57" in namespace "secrets-5067" to be "running and ready"
May  1 19:00:23.926: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Pending", Reason="", readiness=false. Elapsed: 17.932128ms
May  1 19:00:23.926: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:25.947: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038483983s
May  1 19:00:25.947: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:27.951: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Running", Reason="", readiness=true. Elapsed: 4.042335698s
May  1 19:00:27.951: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Running (Ready = true)
May  1 19:00:27.951: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-390f24dc-dfb7-4357-8b54-a1ccea0c0cea 05/01/23 19:00:28.165
STEP: Updating secret s-test-opt-upd-89efbede-f6ad-4897-a91e-ce932a32d038 05/01/23 19:00:28.189
STEP: Creating secret with name s-test-opt-create-9c5e9a63-dddf-407e-ad62-468a15bf5e3e 05/01/23 19:00:28.231
STEP: waiting to observe update in volume 05/01/23 19:00:28.25
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 19:00:30.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5067" for this suite. 05/01/23 19:00:30.469
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":29,"skipped":616,"failed":0}
------------------------------
• [SLOW TEST] [6.886 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:23.608
    May  1 19:00:23.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:00:23.611
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:23.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:23.701
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    May  1 19:00:23.773: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-390f24dc-dfb7-4357-8b54-a1ccea0c0cea 05/01/23 19:00:23.774
    STEP: Creating secret with name s-test-opt-upd-89efbede-f6ad-4897-a91e-ce932a32d038 05/01/23 19:00:23.809
    STEP: Creating the pod 05/01/23 19:00:23.832
    May  1 19:00:23.908: INFO: Waiting up to 5m0s for pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57" in namespace "secrets-5067" to be "running and ready"
    May  1 19:00:23.926: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Pending", Reason="", readiness=false. Elapsed: 17.932128ms
    May  1 19:00:23.926: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:25.947: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038483983s
    May  1 19:00:25.947: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:27.951: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57": Phase="Running", Reason="", readiness=true. Elapsed: 4.042335698s
    May  1 19:00:27.951: INFO: The phase of Pod pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57 is Running (Ready = true)
    May  1 19:00:27.951: INFO: Pod "pod-secrets-d9c69d31-105d-40b1-b151-a1193fa9ac57" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-390f24dc-dfb7-4357-8b54-a1ccea0c0cea 05/01/23 19:00:28.165
    STEP: Updating secret s-test-opt-upd-89efbede-f6ad-4897-a91e-ce932a32d038 05/01/23 19:00:28.189
    STEP: Creating secret with name s-test-opt-create-9c5e9a63-dddf-407e-ad62-468a15bf5e3e 05/01/23 19:00:28.231
    STEP: waiting to observe update in volume 05/01/23 19:00:28.25
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:00:30.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5067" for this suite. 05/01/23 19:00:30.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:30.496
May  1 19:00:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:00:30.499
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:30.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:30.588
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May  1 19:00:30.766: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3618 to be scheduled
May  1 19:00:30.790: INFO: 1 pods are not scheduled: [runtimeclass-3618/test-runtimeclass-runtimeclass-3618-preconfigured-handler-vgxqf(6c104276-9ae6-42a8-bbf6-dfbc8478b78b)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  1 19:00:32.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3618" for this suite. 05/01/23 19:00:32.888
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":30,"skipped":624,"failed":0}
------------------------------
• [2.430 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:30.496
    May  1 19:00:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:00:30.499
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:30.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:30.588
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May  1 19:00:30.766: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3618 to be scheduled
    May  1 19:00:30.790: INFO: 1 pods are not scheduled: [runtimeclass-3618/test-runtimeclass-runtimeclass-3618-preconfigured-handler-vgxqf(6c104276-9ae6-42a8-bbf6-dfbc8478b78b)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  1 19:00:32.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3618" for this suite. 05/01/23 19:00:32.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:32.928
May  1 19:00:32.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption 05/01/23 19:00:32.94
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:33.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:33.04
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:33.056
May  1 19:00:33.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption-2 05/01/23 19:00:33.059
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:33.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:33.16
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 05/01/23 19:00:33.2
STEP: Waiting for the pdb to be processed 05/01/23 19:00:35.281
STEP: Waiting for the pdb to be processed 05/01/23 19:00:37.37
STEP: listing a collection of PDBs across all namespaces 05/01/23 19:00:37.443
STEP: listing a collection of PDBs in namespace disruption-4813 05/01/23 19:00:37.53
STEP: deleting a collection of PDBs 05/01/23 19:00:37.606
STEP: Waiting for the PDB collection to be deleted 05/01/23 19:00:37.676
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
May  1 19:00:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-571" for this suite. 05/01/23 19:00:37.739
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  1 19:00:37.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4813" for this suite. 05/01/23 19:00:37.801
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":31,"skipped":645,"failed":0}
------------------------------
• [4.898 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:32.928
    May  1 19:00:32.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption 05/01/23 19:00:32.94
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:33.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:33.04
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:33.056
    May  1 19:00:33.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption-2 05/01/23 19:00:33.059
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:33.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:33.16
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:33.2
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:35.281
    STEP: Waiting for the pdb to be processed 05/01/23 19:00:37.37
    STEP: listing a collection of PDBs across all namespaces 05/01/23 19:00:37.443
    STEP: listing a collection of PDBs in namespace disruption-4813 05/01/23 19:00:37.53
    STEP: deleting a collection of PDBs 05/01/23 19:00:37.606
    STEP: Waiting for the PDB collection to be deleted 05/01/23 19:00:37.676
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    May  1 19:00:37.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-571" for this suite. 05/01/23 19:00:37.739
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  1 19:00:37.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4813" for this suite. 05/01/23 19:00:37.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:37.828
May  1 19:00:37.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:00:37.831
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:37.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:37.921
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 19:00:37.937
May  1 19:00:37.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1077 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
May  1 19:00:38.224: INFO: stderr: ""
May  1 19:00:38.224: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/01/23 19:00:38.224
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
May  1 19:00:38.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1077 delete pods e2e-test-httpd-pod'
May  1 19:00:43.832: INFO: stderr: ""
May  1 19:00:43.832: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:00:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1077" for this suite. 05/01/23 19:00:43.858
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":32,"skipped":654,"failed":0}
------------------------------
• [SLOW TEST] [6.056 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:37.828
    May  1 19:00:37.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:00:37.831
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:37.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:37.921
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 19:00:37.937
    May  1 19:00:37.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1077 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    May  1 19:00:38.224: INFO: stderr: ""
    May  1 19:00:38.224: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/01/23 19:00:38.224
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    May  1 19:00:38.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1077 delete pods e2e-test-httpd-pod'
    May  1 19:00:43.832: INFO: stderr: ""
    May  1 19:00:43.832: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:00:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1077" for this suite. 05/01/23 19:00:43.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:43.886
May  1 19:00:43.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:00:43.888
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:43.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:43.971
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 05/01/23 19:00:44.095
May  1 19:00:44.095: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83" in namespace "kubelet-test-3107" to be "completed"
May  1 19:00:44.114: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 18.924688ms
May  1 19:00:46.196: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100327787s
May  1 19:00:48.131: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03582518s
May  1 19:00:50.139: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044030687s
May  1 19:00:50.139: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  1 19:00:50.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3107" for this suite. 05/01/23 19:00:50.232
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":33,"skipped":659,"failed":0}
------------------------------
• [SLOW TEST] [6.371 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:43.886
    May  1 19:00:43.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:00:43.888
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:43.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:43.971
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 05/01/23 19:00:44.095
    May  1 19:00:44.095: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83" in namespace "kubelet-test-3107" to be "completed"
    May  1 19:00:44.114: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 18.924688ms
    May  1 19:00:46.196: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100327787s
    May  1 19:00:48.131: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03582518s
    May  1 19:00:50.139: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044030687s
    May  1 19:00:50.139: INFO: Pod "agnhost-host-aliasesc6a897e0-7e2f-4bc6-9e79-872742b02a83" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  1 19:00:50.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3107" for this suite. 05/01/23 19:00:50.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:00:50.262
May  1 19:00:50.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pod-network-test 05/01/23 19:00:50.264
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:50.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:50.378
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5785 05/01/23 19:00:50.41
STEP: creating a selector 05/01/23 19:00:50.41
STEP: Creating the service pods in kubernetes 05/01/23 19:00:50.41
May  1 19:00:50.410: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  1 19:00:50.726: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5785" to be "running and ready"
May  1 19:00:50.759: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.458141ms
May  1 19:00:50.759: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:52.817: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091249463s
May  1 19:00:52.817: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:00:54.801: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.074898712s
May  1 19:00:54.801: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:00:56.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.057077978s
May  1 19:00:56.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:00:58.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.053823114s
May  1 19:00:58.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:00.777: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051000877s
May  1 19:01:00.777: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:02.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.061406786s
May  1 19:01:02.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:04.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053909744s
May  1 19:01:04.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:06.778: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051858133s
May  1 19:01:06.778: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:08.781: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.055399357s
May  1 19:01:08.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:10.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.053598629s
May  1 19:01:10.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:01:12.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.07094704s
May  1 19:01:12.797: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  1 19:01:12.798: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  1 19:01:12.832: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5785" to be "running and ready"
May  1 19:01:12.869: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 36.741498ms
May  1 19:01:12.869: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  1 19:01:12.869: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  1 19:01:12.935: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5785" to be "running and ready"
May  1 19:01:12.984: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 48.945792ms
May  1 19:01:12.984: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  1 19:01:12.985: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/01/23 19:01:13.005
May  1 19:01:13.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5785" to be "running"
May  1 19:01:13.149: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.612124ms
May  1 19:01:15.169: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050895237s
May  1 19:01:17.178: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.060638825s
May  1 19:01:17.178: INFO: Pod "test-container-pod" satisfied condition "running"
May  1 19:01:17.197: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5785" to be "running"
May  1 19:01:17.215: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.709688ms
May  1 19:01:17.215: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  1 19:01:17.237: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  1 19:01:17.237: INFO: Going to poll 172.30.42.85 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  1 19:01:17.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.42.85:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:01:17.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:01:17.256: INFO: ExecWithOptions: Clientset creation
May  1 19:01:17.257: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.42.85%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 19:01:17.745: INFO: Found all 1 expected endpoints: [netserver-0]
May  1 19:01:17.745: INFO: Going to poll 172.30.38.229 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  1 19:01:17.768: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.38.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:01:17.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:01:17.770: INFO: ExecWithOptions: Clientset creation
May  1 19:01:17.770: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.38.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 19:01:18.183: INFO: Found all 1 expected endpoints: [netserver-1]
May  1 19:01:18.183: INFO: Going to poll 172.30.244.106 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May  1 19:01:18.202: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.244.106:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:01:18.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:01:18.204: INFO: ExecWithOptions: Clientset creation
May  1 19:01:18.204: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.244.106%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 19:01:18.671: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  1 19:01:18.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5785" for this suite. 05/01/23 19:01:18.718
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":717,"failed":0}
------------------------------
• [SLOW TEST] [28.480 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:00:50.262
    May  1 19:00:50.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pod-network-test 05/01/23 19:00:50.264
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:00:50.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:00:50.378
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5785 05/01/23 19:00:50.41
    STEP: creating a selector 05/01/23 19:00:50.41
    STEP: Creating the service pods in kubernetes 05/01/23 19:00:50.41
    May  1 19:00:50.410: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  1 19:00:50.726: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5785" to be "running and ready"
    May  1 19:00:50.759: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.458141ms
    May  1 19:00:50.759: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:52.817: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.091249463s
    May  1 19:00:52.817: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:00:54.801: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.074898712s
    May  1 19:00:54.801: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:00:56.783: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.057077978s
    May  1 19:00:56.783: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:00:58.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.053823114s
    May  1 19:00:58.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:00.777: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051000877s
    May  1 19:01:00.777: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:02.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.061406786s
    May  1 19:01:02.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:04.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.053909744s
    May  1 19:01:04.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:06.778: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.051858133s
    May  1 19:01:06.778: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:08.781: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.055399357s
    May  1 19:01:08.782: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:10.780: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.053598629s
    May  1 19:01:10.780: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:01:12.797: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.07094704s
    May  1 19:01:12.797: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  1 19:01:12.798: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  1 19:01:12.832: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5785" to be "running and ready"
    May  1 19:01:12.869: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 36.741498ms
    May  1 19:01:12.869: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  1 19:01:12.869: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  1 19:01:12.935: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5785" to be "running and ready"
    May  1 19:01:12.984: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 48.945792ms
    May  1 19:01:12.984: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  1 19:01:12.985: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/01/23 19:01:13.005
    May  1 19:01:13.118: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5785" to be "running"
    May  1 19:01:13.149: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 31.612124ms
    May  1 19:01:15.169: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050895237s
    May  1 19:01:17.178: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.060638825s
    May  1 19:01:17.178: INFO: Pod "test-container-pod" satisfied condition "running"
    May  1 19:01:17.197: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5785" to be "running"
    May  1 19:01:17.215: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 17.709688ms
    May  1 19:01:17.215: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  1 19:01:17.237: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  1 19:01:17.237: INFO: Going to poll 172.30.42.85 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  1 19:01:17.255: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.42.85:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:01:17.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:01:17.256: INFO: ExecWithOptions: Clientset creation
    May  1 19:01:17.257: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.42.85%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 19:01:17.745: INFO: Found all 1 expected endpoints: [netserver-0]
    May  1 19:01:17.745: INFO: Going to poll 172.30.38.229 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  1 19:01:17.768: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.38.229:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:01:17.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:01:17.770: INFO: ExecWithOptions: Clientset creation
    May  1 19:01:17.770: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.38.229%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 19:01:18.183: INFO: Found all 1 expected endpoints: [netserver-1]
    May  1 19:01:18.183: INFO: Going to poll 172.30.244.106 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May  1 19:01:18.202: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.244.106:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5785 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:01:18.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:01:18.204: INFO: ExecWithOptions: Clientset creation
    May  1 19:01:18.204: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5785/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.244.106%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 19:01:18.671: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  1 19:01:18.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-5785" for this suite. 05/01/23 19:01:18.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:01:18.744
May  1 19:01:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:01:18.747
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:18.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:18.835
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:01:18.851
May  1 19:01:18.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f" in namespace "projected-1879" to be "Succeeded or Failed"
May  1 19:01:18.968: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016084ms
May  1 19:01:20.995: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044608133s
May  1 19:01:22.987: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036907192s
May  1 19:01:25.001: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050988133s
May  1 19:01:26.990: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040073469s
STEP: Saw pod success 05/01/23 19:01:26.99
May  1 19:01:26.991: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f" satisfied condition "Succeeded or Failed"
May  1 19:01:27.023: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f container client-container: <nil>
STEP: delete the pod 05/01/23 19:01:27.083
May  1 19:01:27.154: INFO: Waiting for pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f to disappear
May  1 19:01:27.171: INFO: Pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 19:01:27.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1879" for this suite. 05/01/23 19:01:27.191
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":35,"skipped":725,"failed":0}
------------------------------
• [SLOW TEST] [8.471 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:01:18.744
    May  1 19:01:18.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:01:18.747
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:18.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:18.835
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:01:18.851
    May  1 19:01:18.950: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f" in namespace "projected-1879" to be "Succeeded or Failed"
    May  1 19:01:18.968: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.016084ms
    May  1 19:01:20.995: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044608133s
    May  1 19:01:22.987: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036907192s
    May  1 19:01:25.001: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050988133s
    May  1 19:01:26.990: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.040073469s
    STEP: Saw pod success 05/01/23 19:01:26.99
    May  1 19:01:26.991: INFO: Pod "downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f" satisfied condition "Succeeded or Failed"
    May  1 19:01:27.023: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f container client-container: <nil>
    STEP: delete the pod 05/01/23 19:01:27.083
    May  1 19:01:27.154: INFO: Waiting for pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f to disappear
    May  1 19:01:27.171: INFO: Pod downwardapi-volume-fa882543-49e5-4b0a-87aa-e04c8efd386f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 19:01:27.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1879" for this suite. 05/01/23 19:01:27.191
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:01:27.217
May  1 19:01:27.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:01:27.22
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:27.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:27.302
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 05/01/23 19:01:27.322
May  1 19:01:27.425: INFO: Waiting up to 5m0s for pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37" in namespace "pods-5772" to be "running and ready"
May  1 19:01:27.446: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Pending", Reason="", readiness=false. Elapsed: 21.008955ms
May  1 19:01:27.446: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:01:29.486: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061671752s
May  1 19:01:29.486: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:01:31.468: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Running", Reason="", readiness=true. Elapsed: 4.043131015s
May  1 19:01:31.468: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Running (Ready = true)
May  1 19:01:31.468: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37" satisfied condition "running and ready"
May  1 19:01:31.504: INFO: Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 has hostIP: 10.45.145.124
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:01:31.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5772" for this suite. 05/01/23 19:01:31.522
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":36,"skipped":726,"failed":0}
------------------------------
• [4.332 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:01:27.217
    May  1 19:01:27.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:01:27.22
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:27.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:27.302
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 05/01/23 19:01:27.322
    May  1 19:01:27.425: INFO: Waiting up to 5m0s for pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37" in namespace "pods-5772" to be "running and ready"
    May  1 19:01:27.446: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Pending", Reason="", readiness=false. Elapsed: 21.008955ms
    May  1 19:01:27.446: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:01:29.486: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061671752s
    May  1 19:01:29.486: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:01:31.468: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37": Phase="Running", Reason="", readiness=true. Elapsed: 4.043131015s
    May  1 19:01:31.468: INFO: The phase of Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 is Running (Ready = true)
    May  1 19:01:31.468: INFO: Pod "pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37" satisfied condition "running and ready"
    May  1 19:01:31.504: INFO: Pod pod-hostip-73655997-1cd0-40f8-b81b-506c6add6b37 has hostIP: 10.45.145.124
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:01:31.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5772" for this suite. 05/01/23 19:01:31.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:01:31.552
May  1 19:01:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename init-container 05/01/23 19:01:31.552
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:31.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:31.68
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 05/01/23 19:01:31.691
May  1 19:01:31.691: INFO: PodSpec: initContainers in spec.initContainers
May  1 19:02:18.579: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7124037f-3613-47e4-bae8-e5fac9da37e8", GenerateName:"", Namespace:"init-container-2699", SelfLink:"", UID:"9cd4b934-92ac-4245-b4ee-c0856966caa3", ResourceVersion:"66482", Generation:0, CreationTimestamp:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"691702488"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"867c1e2653d859c4e12ef11a32f28b77fa13dc8dd398b01595cb6c1371926714", "cni.projectcalico.org/podIP":"172.30.42.92/32", "cni.projectcalico.org/podIPs":"172.30.42.92/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.92\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.92\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be0a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 2, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be108), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-x7gf4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003a1e020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba0c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba120), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba060), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001ea6188), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.45.145.124", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0039e8000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ea6240)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ea6260)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001ea627c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001ea6280), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e3a060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.145.124", PodIP:"172.30.42.92", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.42.92"}}, StartTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0039e80e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0039e8150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://536e4cb70f1d69fdadfb4a66c9b83ce9172addc264f90d972058d04bd239acb8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a1e0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a1e080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc001ea62ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 19:02:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2699" for this suite. 05/01/23 19:02:18.606
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":37,"skipped":753,"failed":0}
------------------------------
• [SLOW TEST] [47.080 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:01:31.552
    May  1 19:01:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename init-container 05/01/23 19:01:31.552
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:01:31.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:01:31.68
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 05/01/23 19:01:31.691
    May  1 19:01:31.691: INFO: PodSpec: initContainers in spec.initContainers
    May  1 19:02:18.579: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7124037f-3613-47e4-bae8-e5fac9da37e8", GenerateName:"", Namespace:"init-container-2699", SelfLink:"", UID:"9cd4b934-92ac-4245-b4ee-c0856966caa3", ResourceVersion:"66482", Generation:0, CreationTimestamp:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"691702488"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"867c1e2653d859c4e12ef11a32f28b77fa13dc8dd398b01595cb6c1371926714", "cni.projectcalico.org/podIP":"172.30.42.92/32", "cni.projectcalico.org/podIPs":"172.30.42.92/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.92\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.92\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be0a8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 1, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be0d8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 1, 19, 2, 18, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0010be108), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-x7gf4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003a1e020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba0c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba120), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-x7gf4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0038ba060), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001ea6188), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.45.145.124", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0039e8000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ea6240)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001ea6260)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001ea627c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001ea6280), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e3a060), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.145.124", PodIP:"172.30.42.92", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.42.92"}}, StartTime:time.Date(2023, time.May, 1, 19, 1, 31, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0039e80e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0039e8150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://536e4cb70f1d69fdadfb4a66c9b83ce9172addc264f90d972058d04bd239acb8", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a1e0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003a1e080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc001ea62ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 19:02:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2699" for this suite. 05/01/23 19:02:18.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:02:18.643
May  1 19:02:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:02:18.645
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:18.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:18.743
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-f6774fb5-1769-4b9e-b7a3-b85e90aa85f6 05/01/23 19:02:18.768
STEP: Creating a pod to test consume secrets 05/01/23 19:02:18.787
May  1 19:02:18.867: INFO: Waiting up to 5m0s for pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2" in namespace "secrets-7437" to be "Succeeded or Failed"
May  1 19:02:18.892: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.458527ms
May  1 19:02:20.919: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0516291s
May  1 19:02:22.910: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042972131s
May  1 19:02:24.916: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048756224s
STEP: Saw pod success 05/01/23 19:02:24.916
May  1 19:02:24.917: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2" satisfied condition "Succeeded or Failed"
May  1 19:02:24.934: INFO: Trying to get logs from node 10.45.145.126 pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 19:02:25.075
May  1 19:02:25.146: INFO: Waiting for pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 to disappear
May  1 19:02:25.163: INFO: Pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 19:02:25.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7437" for this suite. 05/01/23 19:02:25.182
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":38,"skipped":834,"failed":0}
------------------------------
• [SLOW TEST] [6.568 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:02:18.643
    May  1 19:02:18.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:02:18.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:18.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:18.743
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-f6774fb5-1769-4b9e-b7a3-b85e90aa85f6 05/01/23 19:02:18.768
    STEP: Creating a pod to test consume secrets 05/01/23 19:02:18.787
    May  1 19:02:18.867: INFO: Waiting up to 5m0s for pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2" in namespace "secrets-7437" to be "Succeeded or Failed"
    May  1 19:02:18.892: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.458527ms
    May  1 19:02:20.919: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0516291s
    May  1 19:02:22.910: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042972131s
    May  1 19:02:24.916: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048756224s
    STEP: Saw pod success 05/01/23 19:02:24.916
    May  1 19:02:24.917: INFO: Pod "pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2" satisfied condition "Succeeded or Failed"
    May  1 19:02:24.934: INFO: Trying to get logs from node 10.45.145.126 pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 19:02:25.075
    May  1 19:02:25.146: INFO: Waiting for pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 to disappear
    May  1 19:02:25.163: INFO: Pod pod-secrets-df84ce17-63cd-42b0-97ff-f58c3445a1a2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:02:25.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7437" for this suite. 05/01/23 19:02:25.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:02:25.219
May  1 19:02:25.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 19:02:25.222
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:25.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:25.347
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May  1 19:02:25.469: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  1 19:02:30.490: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 19:02:30.49
May  1 19:02:30.490: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/01/23 19:02:30.592
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 19:02:34.677: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8001  56145281-c459-49fd-9371-a7b729887f33 66781 1 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af81b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:02:30 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-01 19:02:33 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  1 19:02:34.691: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8001  fb6d2a47-63fc-4793-98a1-15ce488deab5 66769 1 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 56145281-c459-49fd-9371-a7b729887f33 0xc009610a57 0xc009610a58}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56145281-c459-49fd-9371-a7b729887f33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009610b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  1 19:02:34.719: INFO: Pod "test-cleanup-deployment-69cb9c5497-pszgx" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-pszgx test-cleanup-deployment-69cb9c5497- deployment-8001  83d25e5c-8265-47ae-818a-a405c62847ee 66768 0 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:f733ca35b7689e6e7b55e01613e7aa80a4b5fe785106b52f05543e46b56e3d4a cni.projectcalico.org/podIP:172.30.42.91/32 cni.projectcalico.org/podIPs:172.30.42.91/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.91"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.91"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 fb6d2a47-63fc-4793-98a1-15ce488deab5 0xc00a2593e7 0xc00a2593e8}] [] [{kube-controller-manager Update v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb6d2a47-63fc-4793-98a1-15ce488deab5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6k8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6k8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-65rxl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.91,StartTime:2023-05-01 19:02:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://44121cea91dabf6c90c8b0dce404f668ccd13393c2b9a9db9d835e9876c10db9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 19:02:34.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8001" for this suite. 05/01/23 19:02:34.739
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":39,"skipped":841,"failed":0}
------------------------------
• [SLOW TEST] [9.547 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:02:25.219
    May  1 19:02:25.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 19:02:25.222
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:25.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:25.347
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May  1 19:02:25.469: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May  1 19:02:30.490: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 19:02:30.49
    May  1 19:02:30.490: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/01/23 19:02:30.592
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 19:02:34.677: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8001  56145281-c459-49fd-9371-a7b729887f33 66781 1 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af81b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:02:30 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-05-01 19:02:33 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  1 19:02:34.691: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8001  fb6d2a47-63fc-4793-98a1-15ce488deab5 66769 1 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 56145281-c459-49fd-9371-a7b729887f33 0xc009610a57 0xc009610a58}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56145281-c459-49fd-9371-a7b729887f33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009610b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:02:34.719: INFO: Pod "test-cleanup-deployment-69cb9c5497-pszgx" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-pszgx test-cleanup-deployment-69cb9c5497- deployment-8001  83d25e5c-8265-47ae-818a-a405c62847ee 66768 0 2023-05-01 19:02:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:f733ca35b7689e6e7b55e01613e7aa80a4b5fe785106b52f05543e46b56e3d4a cni.projectcalico.org/podIP:172.30.42.91/32 cni.projectcalico.org/podIPs:172.30.42.91/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.91"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.91"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 fb6d2a47-63fc-4793-98a1-15ce488deab5 0xc00a2593e7 0xc00a2593e8}] [] [{kube-controller-manager Update v1 2023-05-01 19:02:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb6d2a47-63fc-4793-98a1-15ce488deab5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:02:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:02:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6k8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6k8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-65rxl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:02:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.91,StartTime:2023-05-01 19:02:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:02:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://44121cea91dabf6c90c8b0dce404f668ccd13393c2b9a9db9d835e9876c10db9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.91,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 19:02:34.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8001" for this suite. 05/01/23 19:02:34.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:02:34.782
May  1 19:02:34.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:02:34.783
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:34.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:34.872
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7386 05/01/23 19:02:34.889
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/01/23 19:02:34.947
STEP: creating service externalsvc in namespace services-7386 05/01/23 19:02:34.948
STEP: creating replication controller externalsvc in namespace services-7386 05/01/23 19:02:35.049
I0501 19:02:35.080538      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7386, replica count: 2
I0501 19:02:38.132105      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/01/23 19:02:38.146
May  1 19:02:38.218: INFO: Creating new exec pod
May  1 19:02:38.287: INFO: Waiting up to 5m0s for pod "execpodm9psv" in namespace "services-7386" to be "running"
May  1 19:02:38.308: INFO: Pod "execpodm9psv": Phase="Pending", Reason="", readiness=false. Elapsed: 21.136159ms
May  1 19:02:40.327: INFO: Pod "execpodm9psv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040135471s
May  1 19:02:42.327: INFO: Pod "execpodm9psv": Phase="Running", Reason="", readiness=true. Elapsed: 4.040128439s
May  1 19:02:42.327: INFO: Pod "execpodm9psv" satisfied condition "running"
May  1 19:02:42.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7386 exec execpodm9psv -- /bin/sh -x -c nslookup clusterip-service.services-7386.svc.cluster.local'
May  1 19:02:42.905: INFO: stderr: "+ nslookup clusterip-service.services-7386.svc.cluster.local\n"
May  1 19:02:42.905: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7386.svc.cluster.local\tcanonical name = externalsvc.services-7386.svc.cluster.local.\nName:\texternalsvc.services-7386.svc.cluster.local\nAddress: 172.21.160.235\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7386, will wait for the garbage collector to delete the pods 05/01/23 19:02:42.905
May  1 19:02:43.051: INFO: Deleting ReplicationController externalsvc took: 56.740082ms
May  1 19:02:43.152: INFO: Terminating ReplicationController externalsvc pods took: 101.031492ms
May  1 19:02:46.979: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:02:47.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7386" for this suite. 05/01/23 19:02:47.098
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":40,"skipped":893,"failed":0}
------------------------------
• [SLOW TEST] [12.393 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:02:34.782
    May  1 19:02:34.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:02:34.783
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:34.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:34.872
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7386 05/01/23 19:02:34.889
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/01/23 19:02:34.947
    STEP: creating service externalsvc in namespace services-7386 05/01/23 19:02:34.948
    STEP: creating replication controller externalsvc in namespace services-7386 05/01/23 19:02:35.049
    I0501 19:02:35.080538      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7386, replica count: 2
    I0501 19:02:38.132105      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/01/23 19:02:38.146
    May  1 19:02:38.218: INFO: Creating new exec pod
    May  1 19:02:38.287: INFO: Waiting up to 5m0s for pod "execpodm9psv" in namespace "services-7386" to be "running"
    May  1 19:02:38.308: INFO: Pod "execpodm9psv": Phase="Pending", Reason="", readiness=false. Elapsed: 21.136159ms
    May  1 19:02:40.327: INFO: Pod "execpodm9psv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040135471s
    May  1 19:02:42.327: INFO: Pod "execpodm9psv": Phase="Running", Reason="", readiness=true. Elapsed: 4.040128439s
    May  1 19:02:42.327: INFO: Pod "execpodm9psv" satisfied condition "running"
    May  1 19:02:42.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7386 exec execpodm9psv -- /bin/sh -x -c nslookup clusterip-service.services-7386.svc.cluster.local'
    May  1 19:02:42.905: INFO: stderr: "+ nslookup clusterip-service.services-7386.svc.cluster.local\n"
    May  1 19:02:42.905: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-7386.svc.cluster.local\tcanonical name = externalsvc.services-7386.svc.cluster.local.\nName:\texternalsvc.services-7386.svc.cluster.local\nAddress: 172.21.160.235\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7386, will wait for the garbage collector to delete the pods 05/01/23 19:02:42.905
    May  1 19:02:43.051: INFO: Deleting ReplicationController externalsvc took: 56.740082ms
    May  1 19:02:43.152: INFO: Terminating ReplicationController externalsvc pods took: 101.031492ms
    May  1 19:02:46.979: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:02:47.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7386" for this suite. 05/01/23 19:02:47.098
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:02:47.189
May  1 19:02:47.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:02:47.192
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:47.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:47.353
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 05/01/23 19:02:47.379
May  1 19:02:47.502: INFO: Waiting up to 5m0s for pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e" in namespace "projected-4597" to be "running and ready"
May  1 19:02:47.574: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Pending", Reason="", readiness=false. Elapsed: 71.428619ms
May  1 19:02:47.574: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Pending, waiting for it to be Running (with Ready = true)
May  1 19:02:49.595: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092131686s
May  1 19:02:49.595: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Pending, waiting for it to be Running (with Ready = true)
May  1 19:02:51.593: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Running", Reason="", readiness=true. Elapsed: 4.090747287s
May  1 19:02:51.593: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Running (Ready = true)
May  1 19:02:51.593: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e" satisfied condition "running and ready"
May  1 19:02:52.289: INFO: Successfully updated pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 19:02:54.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4597" for this suite. 05/01/23 19:02:54.422
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":41,"skipped":897,"failed":0}
------------------------------
• [SLOW TEST] [7.260 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:02:47.189
    May  1 19:02:47.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:02:47.192
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:47.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:47.353
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 05/01/23 19:02:47.379
    May  1 19:02:47.502: INFO: Waiting up to 5m0s for pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e" in namespace "projected-4597" to be "running and ready"
    May  1 19:02:47.574: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Pending", Reason="", readiness=false. Elapsed: 71.428619ms
    May  1 19:02:47.574: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:02:49.595: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092131686s
    May  1 19:02:49.595: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:02:51.593: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e": Phase="Running", Reason="", readiness=true. Elapsed: 4.090747287s
    May  1 19:02:51.593: INFO: The phase of Pod labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e is Running (Ready = true)
    May  1 19:02:51.593: INFO: Pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e" satisfied condition "running and ready"
    May  1 19:02:52.289: INFO: Successfully updated pod "labelsupdate15b40337-a095-4b4a-b9eb-f53abd49d63e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 19:02:54.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4597" for this suite. 05/01/23 19:02:54.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:02:54.449
May  1 19:02:54.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sysctl 05/01/23 19:02:54.452
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:54.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:54.595
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/01/23 19:02:54.614
STEP: Watching for error events or started pod 05/01/23 19:02:54.735
STEP: Waiting for pod completion 05/01/23 19:02:58.767
May  1 19:02:58.767: INFO: Waiting up to 3m0s for pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1" in namespace "sysctl-3722" to be "completed"
May  1 19:02:58.785: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.538596ms
May  1 19:03:00.803: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036083362s
May  1 19:03:00.803: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/01/23 19:03:00.847
STEP: Getting logs from the pod 05/01/23 19:03:00.847
STEP: Checking that the sysctl is actually updated 05/01/23 19:03:01.039
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 19:03:01.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3722" for this suite. 05/01/23 19:03:01.058
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":42,"skipped":902,"failed":0}
------------------------------
• [SLOW TEST] [6.636 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:02:54.449
    May  1 19:02:54.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sysctl 05/01/23 19:02:54.452
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:02:54.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:02:54.595
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/01/23 19:02:54.614
    STEP: Watching for error events or started pod 05/01/23 19:02:54.735
    STEP: Waiting for pod completion 05/01/23 19:02:58.767
    May  1 19:02:58.767: INFO: Waiting up to 3m0s for pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1" in namespace "sysctl-3722" to be "completed"
    May  1 19:02:58.785: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.538596ms
    May  1 19:03:00.803: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036083362s
    May  1 19:03:00.803: INFO: Pod "sysctl-b239ada9-3fc7-45de-aa0b-3d7b63505eb1" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/01/23 19:03:00.847
    STEP: Getting logs from the pod 05/01/23 19:03:00.847
    STEP: Checking that the sysctl is actually updated 05/01/23 19:03:01.039
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 19:03:01.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3722" for this suite. 05/01/23 19:03:01.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:01.088
May  1 19:03:01.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:03:01.09
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:01.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:01.335
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-387/configmap-test-2e276718-e7a8-4ea3-b146-237c688c05ed 05/01/23 19:03:01.351
STEP: Creating a pod to test consume configMaps 05/01/23 19:03:01.373
May  1 19:03:01.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b" in namespace "configmap-387" to be "Succeeded or Failed"
May  1 19:03:01.520: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.398757ms
May  1 19:03:03.563: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076960791s
May  1 19:03:05.573: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086785144s
May  1 19:03:07.540: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053946936s
May  1 19:03:09.540: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054384964s
STEP: Saw pod success 05/01/23 19:03:09.54
May  1 19:03:09.541: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b" satisfied condition "Succeeded or Failed"
May  1 19:03:09.557: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b container env-test: <nil>
STEP: delete the pod 05/01/23 19:03:09.598
May  1 19:03:09.651: INFO: Waiting for pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b to disappear
May  1 19:03:09.669: INFO: Pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:03:09.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-387" for this suite. 05/01/23 19:03:09.702
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":43,"skipped":912,"failed":0}
------------------------------
• [SLOW TEST] [8.641 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:01.088
    May  1 19:03:01.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:03:01.09
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:01.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:01.335
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-387/configmap-test-2e276718-e7a8-4ea3-b146-237c688c05ed 05/01/23 19:03:01.351
    STEP: Creating a pod to test consume configMaps 05/01/23 19:03:01.373
    May  1 19:03:01.486: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b" in namespace "configmap-387" to be "Succeeded or Failed"
    May  1 19:03:01.520: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 34.398757ms
    May  1 19:03:03.563: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076960791s
    May  1 19:03:05.573: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086785144s
    May  1 19:03:07.540: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053946936s
    May  1 19:03:09.540: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054384964s
    STEP: Saw pod success 05/01/23 19:03:09.54
    May  1 19:03:09.541: INFO: Pod "pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b" satisfied condition "Succeeded or Failed"
    May  1 19:03:09.557: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b container env-test: <nil>
    STEP: delete the pod 05/01/23 19:03:09.598
    May  1 19:03:09.651: INFO: Waiting for pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b to disappear
    May  1 19:03:09.669: INFO: Pod pod-configmaps-0ba4f4d4-ae42-4c2d-9875-eabfb39fc04b no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:03:09.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-387" for this suite. 05/01/23 19:03:09.702
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:09.734
May  1 19:03:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename containers 05/01/23 19:03:09.737
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:09.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:09.818
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
May  1 19:03:09.893: INFO: Waiting up to 5m0s for pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9" in namespace "containers-1691" to be "running"
May  1 19:03:09.923: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Pending", Reason="", readiness=false. Elapsed: 29.193682ms
May  1 19:03:11.938: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044272926s
May  1 19:03:14.198: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Running", Reason="", readiness=true. Elapsed: 4.304700328s
May  1 19:03:14.198: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  1 19:03:16.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1691" for this suite. 05/01/23 19:03:18.262
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":44,"skipped":932,"failed":0}
------------------------------
• [SLOW TEST] [9.054 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:09.734
    May  1 19:03:09.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename containers 05/01/23 19:03:09.737
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:09.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:09.818
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    May  1 19:03:09.893: INFO: Waiting up to 5m0s for pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9" in namespace "containers-1691" to be "running"
    May  1 19:03:09.923: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Pending", Reason="", readiness=false. Elapsed: 29.193682ms
    May  1 19:03:11.938: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044272926s
    May  1 19:03:14.198: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9": Phase="Running", Reason="", readiness=true. Elapsed: 4.304700328s
    May  1 19:03:14.198: INFO: Pod "client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  1 19:03:16.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1691" for this suite. 05/01/23 19:03:18.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:18.789
May  1 19:03:18.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-pred 05/01/23 19:03:18.791
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:19.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:19.481
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  1 19:03:19.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  1 19:03:19.660: INFO: Waiting for terminating namespaces to be deleted...
May  1 19:03:19.718: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.124 before test
May  1 19:03:19.810: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container calico-node ready: true, restart count 0
May  1 19:03:19.810: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container calico-typha ready: true, restart count 1
May  1 19:03:19.810: INFO: client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9 from containers-1691 started at 2023-05-01 19:03:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container agnhost-container ready: true, restart count 0
May  1 19:03:19.810: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 19:03:19.810: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 19:03:19.810: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container pause ready: true, restart count 0
May  1 19:03:19.810: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 19:03:19.810: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container tuned ready: true, restart count 0
May  1 19:03:19.810: INFO: dns-default-6rl9c from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container dns ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 19:03:19.810: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container node-ca ready: true, restart count 0
May  1 19:03:19.810: INFO: ingress-canary-splcp from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 19:03:19.810: INFO: router-default-dc48bc679-xgv8x from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container router ready: true, restart count 0
May  1 19:03:19.810: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 17:00:24 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container alertmanager ready: true, restart count 1
May  1 19:03:19.810: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container config-reloader ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container node-exporter ready: true, restart count 0
May  1 19:03:19.810: INFO: prometheus-adapter-d8df9dbf9-td74g from openshift-monitoring started at 2023-05-01 17:00:19 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 19:03:19.810: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 17:00:30 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.810: INFO: 	Container config-reloader ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container prometheus ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 19:03:19.811: INFO: prometheus-operator-admission-webhook-98cbdbf8f-kfvjz from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 19:03:19.811: INFO: thanos-querier-6dcb8b776-gt2g9 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container thanos-query ready: true, restart count 0
May  1 19:03:19.811: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-multus ready: true, restart count 0
May  1 19:03:19.811: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 19:03:19.811: INFO: multus-admission-controller-6b76fd464f-7xkp5 from openshift-multus started at 2023-05-01 16:58:13 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 19:03:19.811: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 19:03:19.811: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 19:03:19.811: INFO: collect-profiles-28049430-g7fpm from openshift-operator-lifecycle-manager started at 2023-05-01 18:30:00 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 19:03:19.811: INFO: collect-profiles-28049445-zcr2d from openshift-operator-lifecycle-manager started at 2023-05-01 18:45:00 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 19:03:19.811: INFO: collect-profiles-28049460-xjcc8 from openshift-operator-lifecycle-manager started at 2023-05-01 19:00:00 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 19:03:19.811: INFO: packageserver-596c56d895-crb8b from openshift-operator-lifecycle-manager started at 2023-05-01 16:57:21 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container packageserver ready: true, restart count 0
May  1 19:03:19.811: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  1 19:03:19.811: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.811: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 19:03:19.811: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 19:03:19.811: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.126 before test
May  1 19:03:19.913: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container calico-node ready: true, restart count 0
May  1 19:03:19.914: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 19:03:19.914: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 19:03:19.914: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 19:03:19.914: INFO: 	Container pause ready: true, restart count 0
May  1 19:03:19.914: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 19:03:19.914: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container vpn ready: true, restart count 0
May  1 19:03:19.914: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.914: INFO: 	Container tuned ready: true, restart count 0
May  1 19:03:19.915: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container console ready: true, restart count 0
May  1 19:03:19.915: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container download-server ready: true, restart count 0
May  1 19:03:19.915: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container dns ready: true, restart count 0
May  1 19:03:19.915: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.915: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 19:03:19.915: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container registry ready: true, restart count 0
May  1 19:03:19.915: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.915: INFO: 	Container node-ca ready: true, restart count 0
May  1 19:03:19.915: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.916: INFO: 	Container pvc-permissions ready: false, restart count 0
May  1 19:03:19.916: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.916: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 19:03:19.916: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.916: INFO: 	Container router ready: true, restart count 0
May  1 19:03:19.916: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.916: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.916: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.916: INFO: 	Container alertmanager ready: true, restart count 1
May  1 19:03:19.916: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 19:03:19.916: INFO: 	Container config-reloader ready: true, restart count 0
May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 19:03:19.916: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 19:03:19.916: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  1 19:03:19.917: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container node-exporter ready: true, restart count 0
May  1 19:03:19.917: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May  1 19:03:19.917: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.917: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 19:03:19.917: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.917: INFO: 	Container config-reloader ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container prometheus ready: true, restart count 0
May  1 19:03:19.917: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 19:03:19.918: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container prometheus-operator ready: true, restart count 0
May  1 19:03:19.918: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.918: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 19:03:19.918: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container reload ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container telemeter-client ready: true, restart count 0
May  1 19:03:19.918: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 19:03:19.918: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 19:03:19.919: INFO: 	Container thanos-query ready: true, restart count 0
May  1 19:03:19.919: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container kube-multus ready: true, restart count 0
May  1 19:03:19.919: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 19:03:19.919: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.919: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 19:03:19.919: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:19.919: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 19:03:19.919: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 19:03:19.919: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.919: INFO: 	Container e2e ready: true, restart count 0
May  1 19:03:19.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 19:03:19.919: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 19:03:19.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 19:03:19.920: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 19:03:19.920: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
May  1 19:03:19.920: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  1 19:03:19.920: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.71 before test
May  1 19:03:20.044: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  1 19:03:20.044: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container calico-node ready: true, restart count 0
May  1 19:03:20.044: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container calico-typha ready: true, restart count 0
May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 19:03:20.044: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  1 19:03:20.044: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 19:03:20.044: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 19:03:20.044: INFO: 	Container pause ready: true, restart count 0
May  1 19:03:20.044: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
May  1 19:03:20.044: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
May  1 19:03:20.044: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  1 19:03:20.044: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 19:03:20.044: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May  1 19:03:20.044: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May  1 19:03:20.044: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container tuned ready: true, restart count 0
May  1 19:03:20.044: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May  1 19:03:20.044: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May  1 19:03:20.044: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container cluster-storage-operator ready: true, restart count 1
May  1 19:03:20.044: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.044: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 19:03:20.044: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 19:03:20.045: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container webhook ready: true, restart count 0
May  1 19:03:20.045: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container webhook ready: true, restart count 0
May  1 19:03:20.045: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container console-operator ready: true, restart count 1
May  1 19:03:20.045: INFO: 	Container conversion-webhook-server ready: true, restart count 2
May  1 19:03:20.045: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container console ready: true, restart count 0
May  1 19:03:20.045: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container download-server ready: true, restart count 0
May  1 19:03:20.045: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container dns-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container dns ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 19:03:20.045: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container node-ca ready: true, restart count 0
May  1 19:03:20.045: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 19:03:20.045: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container ingress-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container insights-operator ready: true, restart count 1
May  1 19:03:20.045: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
May  1 19:03:20.045: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container migrator ready: true, restart count 0
May  1 19:03:20.045: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
May  1 19:03:20.045: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
May  1 19:03:20.045: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container marketplace-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
May  1 19:03:20.045: INFO: redhat-operators-wcvpn from openshift-marketplace started at 2023-05-01 18:36:42 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
May  1 19:03:20.045: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container node-exporter ready: true, restart count 0
May  1 19:03:20.045: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 19:03:20.045: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-multus ready: true, restart count 0
May  1 19:03:20.045: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 19:03:20.045: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 19:03:20.045: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container check-endpoints ready: true, restart count 0
May  1 19:03:20.045: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 19:03:20.045: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.045: INFO: 	Container network-operator ready: true, restart count 1
May  1 19:03:20.046: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container catalog-operator ready: true, restart count 0
May  1 19:03:20.046: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container olm-operator ready: true, restart count 0
May  1 19:03:20.046: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container package-server-manager ready: true, restart count 0
May  1 19:03:20.046: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container packageserver ready: true, restart count 0
May  1 19:03:20.046: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container metrics ready: true, restart count 2
May  1 19:03:20.046: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container push-gateway ready: true, restart count 0
May  1 19:03:20.046: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container service-ca-operator ready: true, restart count 1
May  1 19:03:20.046: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container service-ca-controller ready: true, restart count 0
May  1 19:03:20.046: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 19:03:20.046: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 19:03:20.046: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
May  1 19:03:20.046: INFO: 	Container tigera-operator ready: true, restart count 2
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/01/23 19:03:20.046
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175b1a4c3cf02bcd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 05/01/23 19:03:20.596
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  1 19:03:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7003" for this suite. 05/01/23 19:03:21.56
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":45,"skipped":937,"failed":0}
------------------------------
• [2.818 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:18.789
    May  1 19:03:18.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-pred 05/01/23 19:03:18.791
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:19.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:19.481
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  1 19:03:19.548: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  1 19:03:19.660: INFO: Waiting for terminating namespaces to be deleted...
    May  1 19:03:19.718: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.124 before test
    May  1 19:03:19.810: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container calico-node ready: true, restart count 0
    May  1 19:03:19.810: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container calico-typha ready: true, restart count 1
    May  1 19:03:19.810: INFO: client-containers-5ef999ad-c07b-4f5d-bd15-ed00e76915d9 from containers-1691 started at 2023-05-01 19:03:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container agnhost-container ready: true, restart count 0
    May  1 19:03:19.810: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-mkqd4 from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 19:03:19.810: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 19:03:19.810: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container pause ready: true, restart count 0
    May  1 19:03:19.810: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 19:03:19.810: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container tuned ready: true, restart count 0
    May  1 19:03:19.810: INFO: dns-default-6rl9c from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container dns ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 19:03:19.810: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container node-ca ready: true, restart count 0
    May  1 19:03:19.810: INFO: ingress-canary-splcp from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 19:03:19.810: INFO: router-default-dc48bc679-xgv8x from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container router ready: true, restart count 0
    May  1 19:03:19.810: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 17:00:24 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 19:03:19.810: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 19:03:19.810: INFO: prometheus-adapter-d8df9dbf9-td74g from openshift-monitoring started at 2023-05-01 17:00:19 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 19:03:19.810: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 17:00:30 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.810: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.810: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container prometheus ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 19:03:19.811: INFO: prometheus-operator-admission-webhook-98cbdbf8f-kfvjz from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 19:03:19.811: INFO: thanos-querier-6dcb8b776-gt2g9 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 19:03:19.811: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 19:03:19.811: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 19:03:19.811: INFO: multus-admission-controller-6b76fd464f-7xkp5 from openshift-multus started at 2023-05-01 16:58:13 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 19:03:19.811: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 19:03:19.811: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 19:03:19.811: INFO: collect-profiles-28049430-g7fpm from openshift-operator-lifecycle-manager started at 2023-05-01 18:30:00 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 19:03:19.811: INFO: collect-profiles-28049445-zcr2d from openshift-operator-lifecycle-manager started at 2023-05-01 18:45:00 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 19:03:19.811: INFO: collect-profiles-28049460-xjcc8 from openshift-operator-lifecycle-manager started at 2023-05-01 19:00:00 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 19:03:19.811: INFO: packageserver-596c56d895-crb8b from openshift-operator-lifecycle-manager started at 2023-05-01 16:57:21 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container packageserver ready: true, restart count 0
    May  1 19:03:19.811: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  1 19:03:19.811: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.811: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 19:03:19.811: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 19:03:19.811: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.126 before test
    May  1 19:03:19.913: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container calico-node ready: true, restart count 0
    May  1 19:03:19.914: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 19:03:19.914: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 19:03:19.914: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 19:03:19.914: INFO: 	Container pause ready: true, restart count 0
    May  1 19:03:19.914: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 19:03:19.914: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container vpn ready: true, restart count 0
    May  1 19:03:19.914: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.914: INFO: 	Container tuned ready: true, restart count 0
    May  1 19:03:19.915: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container console ready: true, restart count 0
    May  1 19:03:19.915: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container download-server ready: true, restart count 0
    May  1 19:03:19.915: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container dns ready: true, restart count 0
    May  1 19:03:19.915: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.915: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 19:03:19.915: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container registry ready: true, restart count 0
    May  1 19:03:19.915: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.915: INFO: 	Container node-ca ready: true, restart count 0
    May  1 19:03:19.915: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.916: INFO: 	Container pvc-permissions ready: false, restart count 0
    May  1 19:03:19.916: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.916: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 19:03:19.916: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.916: INFO: 	Container router ready: true, restart count 0
    May  1 19:03:19.916: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.916: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.916: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.916: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 19:03:19.916: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 19:03:19.916: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.916: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 19:03:19.916: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 19:03:19.916: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  1 19:03:19.917: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 19:03:19.917: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May  1 19:03:19.917: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.917: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 19:03:19.917: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.917: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container prometheus ready: true, restart count 0
    May  1 19:03:19.917: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 19:03:19.918: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  1 19:03:19.918: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.918: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 19:03:19.918: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
    May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container reload ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container telemeter-client ready: true, restart count 0
    May  1 19:03:19.918: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 19:03:19.918: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 19:03:19.919: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 19:03:19.919: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 19:03:19.919: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 19:03:19.919: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.919: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 19:03:19.919: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:19.919: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 19:03:19.919: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 19:03:19.919: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.919: INFO: 	Container e2e ready: true, restart count 0
    May  1 19:03:19.919: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 19:03:19.919: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 19:03:19.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 19:03:19.920: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 19:03:19.920: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
    May  1 19:03:19.920: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    May  1 19:03:19.920: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.71 before test
    May  1 19:03:20.044: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  1 19:03:20.044: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container calico-node ready: true, restart count 0
    May  1 19:03:20.044: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container calico-typha ready: true, restart count 0
    May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 19:03:20.044: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 19:03:20.044: INFO: 	Container pause ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    May  1 19:03:20.044: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 19:03:20.044: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    May  1 19:03:20.044: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    May  1 19:03:20.044: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container tuned ready: true, restart count 0
    May  1 19:03:20.044: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    May  1 19:03:20.044: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    May  1 19:03:20.044: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    May  1 19:03:20.044: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.044: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 19:03:20.044: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 19:03:20.045: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container webhook ready: true, restart count 0
    May  1 19:03:20.045: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container webhook ready: true, restart count 0
    May  1 19:03:20.045: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container console-operator ready: true, restart count 1
    May  1 19:03:20.045: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    May  1 19:03:20.045: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container console ready: true, restart count 0
    May  1 19:03:20.045: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container download-server ready: true, restart count 0
    May  1 19:03:20.045: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container dns-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container dns ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 19:03:20.045: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container node-ca ready: true, restart count 0
    May  1 19:03:20.045: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 19:03:20.045: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container ingress-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container insights-operator ready: true, restart count 1
    May  1 19:03:20.045: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    May  1 19:03:20.045: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container migrator ready: true, restart count 0
    May  1 19:03:20.045: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
    May  1 19:03:20.045: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
    May  1 19:03:20.045: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container marketplace-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
    May  1 19:03:20.045: INFO: redhat-operators-wcvpn from openshift-marketplace started at 2023-05-01 18:36:42 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container registry-server ready: true, restart count 0
    May  1 19:03:20.045: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 19:03:20.045: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 19:03:20.045: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 19:03:20.045: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 19:03:20.045: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 19:03:20.045: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container check-endpoints ready: true, restart count 0
    May  1 19:03:20.045: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 19:03:20.045: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.045: INFO: 	Container network-operator ready: true, restart count 1
    May  1 19:03:20.046: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container catalog-operator ready: true, restart count 0
    May  1 19:03:20.046: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container olm-operator ready: true, restart count 0
    May  1 19:03:20.046: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container package-server-manager ready: true, restart count 0
    May  1 19:03:20.046: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container packageserver ready: true, restart count 0
    May  1 19:03:20.046: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container metrics ready: true, restart count 2
    May  1 19:03:20.046: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container push-gateway ready: true, restart count 0
    May  1 19:03:20.046: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container service-ca-operator ready: true, restart count 1
    May  1 19:03:20.046: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container service-ca-controller ready: true, restart count 0
    May  1 19:03:20.046: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 19:03:20.046: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 19:03:20.046: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
    May  1 19:03:20.046: INFO: 	Container tigera-operator ready: true, restart count 2
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/01/23 19:03:20.046
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175b1a4c3cf02bcd], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 05/01/23 19:03:20.596
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:03:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7003" for this suite. 05/01/23 19:03:21.56
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:21.607
May  1 19:03:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:03:21.611
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:21.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:21.723
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 05/01/23 19:03:21.74
May  1 19:03:21.958: INFO: Waiting up to 5m0s for pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe" in namespace "downward-api-5603" to be "running and ready"
May  1 19:03:22.039: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Pending", Reason="", readiness=false. Elapsed: 80.548456ms
May  1 19:03:22.039: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Pending, waiting for it to be Running (with Ready = true)
May  1 19:03:24.073: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115029245s
May  1 19:03:24.073: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Pending, waiting for it to be Running (with Ready = true)
May  1 19:03:26.070: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Running", Reason="", readiness=true. Elapsed: 4.111538622s
May  1 19:03:26.070: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Running (Ready = true)
May  1 19:03:26.070: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe" satisfied condition "running and ready"
May  1 19:03:26.766: INFO: Successfully updated pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 19:03:28.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5603" for this suite. 05/01/23 19:03:28.87
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":46,"skipped":937,"failed":0}
------------------------------
• [SLOW TEST] [7.301 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:21.607
    May  1 19:03:21.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:03:21.611
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:21.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:21.723
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 05/01/23 19:03:21.74
    May  1 19:03:21.958: INFO: Waiting up to 5m0s for pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe" in namespace "downward-api-5603" to be "running and ready"
    May  1 19:03:22.039: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Pending", Reason="", readiness=false. Elapsed: 80.548456ms
    May  1 19:03:22.039: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:03:24.073: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115029245s
    May  1 19:03:24.073: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:03:26.070: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe": Phase="Running", Reason="", readiness=true. Elapsed: 4.111538622s
    May  1 19:03:26.070: INFO: The phase of Pod labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe is Running (Ready = true)
    May  1 19:03:26.070: INFO: Pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe" satisfied condition "running and ready"
    May  1 19:03:26.766: INFO: Successfully updated pod "labelsupdate896857e3-5083-4c88-a0bb-25349b6801fe"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 19:03:28.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5603" for this suite. 05/01/23 19:03:28.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:28.927
May  1 19:03:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 19:03:28.93
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:29.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:29.032
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 05/01/23 19:03:29.046
STEP: Counting existing ResourceQuota 05/01/23 19:03:35.073
STEP: Creating a ResourceQuota 05/01/23 19:03:40.092
STEP: Ensuring resource quota status is calculated 05/01/23 19:03:40.119
STEP: Creating a Secret 05/01/23 19:03:42.15
STEP: Ensuring resource quota status captures secret creation 05/01/23 19:03:42.198
STEP: Deleting a secret 05/01/23 19:03:44.217
STEP: Ensuring resource quota status released usage 05/01/23 19:03:44.237
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 19:03:46.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6163" for this suite. 05/01/23 19:03:46.285
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":47,"skipped":976,"failed":0}
------------------------------
• [SLOW TEST] [17.384 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:28.927
    May  1 19:03:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 19:03:28.93
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:29.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:29.032
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 05/01/23 19:03:29.046
    STEP: Counting existing ResourceQuota 05/01/23 19:03:35.073
    STEP: Creating a ResourceQuota 05/01/23 19:03:40.092
    STEP: Ensuring resource quota status is calculated 05/01/23 19:03:40.119
    STEP: Creating a Secret 05/01/23 19:03:42.15
    STEP: Ensuring resource quota status captures secret creation 05/01/23 19:03:42.198
    STEP: Deleting a secret 05/01/23 19:03:44.217
    STEP: Ensuring resource quota status released usage 05/01/23 19:03:44.237
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 19:03:46.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6163" for this suite. 05/01/23 19:03:46.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:03:46.328
May  1 19:03:46.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 19:03:46.33
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:46.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:46.458
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 05/01/23 19:03:46.504
STEP: Patching the Job 05/01/23 19:03:46.542
STEP: Watching for Job to be patched 05/01/23 19:03:46.611
May  1 19:03:46.624: INFO: Event ADDED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
May  1 19:03:46.624: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
May  1 19:03:46.625: INFO: Event MODIFIED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/01/23 19:03:46.625
STEP: Watching for Job to be updated 05/01/23 19:03:46.716
May  1 19:03:46.746: INFO: Event MODIFIED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:03:46.746: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/01/23 19:03:46.746
May  1 19:03:46.787: INFO: Job: e2e-2sqx2 as labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2]
STEP: Waiting for job to complete 05/01/23 19:03:46.787
STEP: Delete a job collection with a labelselector 05/01/23 19:04:02.8
STEP: Watching for Job to be deleted 05/01/23 19:04:02.827
May  1 19:04:02.842: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:04:02.846: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May  1 19:04:02.847: INFO: Event DELETED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/01/23 19:04:02.847
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 19:04:02.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7084" for this suite. 05/01/23 19:04:02.907
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":48,"skipped":1059,"failed":0}
------------------------------
• [SLOW TEST] [16.600 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:03:46.328
    May  1 19:03:46.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 19:03:46.33
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:03:46.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:03:46.458
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 05/01/23 19:03:46.504
    STEP: Patching the Job 05/01/23 19:03:46.542
    STEP: Watching for Job to be patched 05/01/23 19:03:46.611
    May  1 19:03:46.624: INFO: Event ADDED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  1 19:03:46.624: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
    May  1 19:03:46.625: INFO: Event MODIFIED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/01/23 19:03:46.625
    STEP: Watching for Job to be updated 05/01/23 19:03:46.716
    May  1 19:03:46.746: INFO: Event MODIFIED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:03:46.746: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/01/23 19:03:46.746
    May  1 19:03:46.787: INFO: Job: e2e-2sqx2 as labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2]
    STEP: Waiting for job to complete 05/01/23 19:03:46.787
    STEP: Delete a job collection with a labelselector 05/01/23 19:04:02.8
    STEP: Watching for Job to be deleted 05/01/23 19:04:02.827
    May  1 19:04:02.842: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:04:02.846: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:04:02.847: INFO: Event MODIFIED observed for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May  1 19:04:02.847: INFO: Event DELETED found for Job e2e-2sqx2 in namespace job-7084 with labels: map[e2e-2sqx2:patched e2e-job-label:e2e-2sqx2] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/01/23 19:04:02.847
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 19:04:02.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7084" for this suite. 05/01/23 19:04:02.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:04:02.93
May  1 19:04:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename conformance-tests 05/01/23 19:04:02.931
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:04:03.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:04:03.041
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/01/23 19:04:03.058
May  1 19:04:03.058: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
May  1 19:04:03.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-7979" for this suite. 05/01/23 19:04:03.252
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":49,"skipped":1079,"failed":0}
------------------------------
• [0.344 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:04:02.93
    May  1 19:04:02.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename conformance-tests 05/01/23 19:04:02.931
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:04:03.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:04:03.041
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/01/23 19:04:03.058
    May  1 19:04:03.058: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    May  1 19:04:03.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-7979" for this suite. 05/01/23 19:04:03.252
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:04:03.274
May  1 19:04:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:04:03.276
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:04:03.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:04:03.344
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  1 19:04:03.467: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 19:05:03.768: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:03.794
May  1 19:05:03.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption-path 05/01/23 19:05:03.797
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:03.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:03.931
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
May  1 19:05:04.036: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May  1 19:05:04.067: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
May  1 19:05:04.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5877" for this suite. 05/01/23 19:05:04.202
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  1 19:05:04.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4882" for this suite. 05/01/23 19:05:04.373
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":50,"skipped":1082,"failed":0}
------------------------------
• [SLOW TEST] [61.360 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:04:03.274
    May  1 19:04:03.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:04:03.276
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:04:03.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:04:03.344
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  1 19:04:03.467: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 19:05:03.768: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:03.794
    May  1 19:05:03.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption-path 05/01/23 19:05:03.797
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:03.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:03.931
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    May  1 19:05:04.036: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May  1 19:05:04.067: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    May  1 19:05:04.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-5877" for this suite. 05/01/23 19:05:04.202
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:05:04.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4882" for this suite. 05/01/23 19:05:04.373
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:04.637
May  1 19:05:04.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:05:04.639
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:04.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:04.744
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-509cdaf7-09eb-4bc3-bab8-6378bf24e466 05/01/23 19:05:04.759
STEP: Creating a pod to test consume secrets 05/01/23 19:05:04.778
May  1 19:05:04.883: INFO: Waiting up to 5m0s for pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d" in namespace "secrets-9023" to be "Succeeded or Failed"
May  1 19:05:04.937: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 53.993754ms
May  1 19:05:06.966: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08290152s
May  1 19:05:08.978: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094622908s
May  1 19:05:10.973: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089388074s
STEP: Saw pod success 05/01/23 19:05:10.973
May  1 19:05:10.973: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d" satisfied condition "Succeeded or Failed"
May  1 19:05:11.012: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 19:05:11.184
May  1 19:05:11.242: INFO: Waiting for pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d to disappear
May  1 19:05:11.294: INFO: Pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 19:05:11.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9023" for this suite. 05/01/23 19:05:11.36
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":51,"skipped":1095,"failed":0}
------------------------------
• [SLOW TEST] [6.767 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:04.637
    May  1 19:05:04.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:05:04.639
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:04.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:04.744
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-509cdaf7-09eb-4bc3-bab8-6378bf24e466 05/01/23 19:05:04.759
    STEP: Creating a pod to test consume secrets 05/01/23 19:05:04.778
    May  1 19:05:04.883: INFO: Waiting up to 5m0s for pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d" in namespace "secrets-9023" to be "Succeeded or Failed"
    May  1 19:05:04.937: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 53.993754ms
    May  1 19:05:06.966: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08290152s
    May  1 19:05:08.978: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094622908s
    May  1 19:05:10.973: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089388074s
    STEP: Saw pod success 05/01/23 19:05:10.973
    May  1 19:05:10.973: INFO: Pod "pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d" satisfied condition "Succeeded or Failed"
    May  1 19:05:11.012: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 19:05:11.184
    May  1 19:05:11.242: INFO: Waiting for pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d to disappear
    May  1 19:05:11.294: INFO: Pod pod-secrets-6722e368-b751-42e7-b50a-80197f53ad0d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:05:11.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9023" for this suite. 05/01/23 19:05:11.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:11.406
May  1 19:05:11.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:05:11.408
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:11.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:11.628
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 05/01/23 19:05:11.641
May  1 19:05:11.842: INFO: Waiting up to 5m0s for pod "pod-8089633a-382c-4d14-88d0-45da798817c5" in namespace "emptydir-3455" to be "Succeeded or Failed"
May  1 19:05:11.919: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 76.660166ms
May  1 19:05:13.939: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096957052s
May  1 19:05:15.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104194424s
May  1 19:05:17.968: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.126012121s
May  1 19:05:19.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.103787669s
STEP: Saw pod success 05/01/23 19:05:19.946
May  1 19:05:19.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5" satisfied condition "Succeeded or Failed"
May  1 19:05:19.963: INFO: Trying to get logs from node 10.45.145.124 pod pod-8089633a-382c-4d14-88d0-45da798817c5 container test-container: <nil>
STEP: delete the pod 05/01/23 19:05:20.022
May  1 19:05:20.085: INFO: Waiting for pod pod-8089633a-382c-4d14-88d0-45da798817c5 to disappear
May  1 19:05:20.101: INFO: Pod pod-8089633a-382c-4d14-88d0-45da798817c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:05:20.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3455" for this suite. 05/01/23 19:05:20.123
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":1118,"failed":0}
------------------------------
• [SLOW TEST] [8.810 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:11.406
    May  1 19:05:11.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:05:11.408
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:11.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:11.628
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/01/23 19:05:11.641
    May  1 19:05:11.842: INFO: Waiting up to 5m0s for pod "pod-8089633a-382c-4d14-88d0-45da798817c5" in namespace "emptydir-3455" to be "Succeeded or Failed"
    May  1 19:05:11.919: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 76.660166ms
    May  1 19:05:13.939: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096957052s
    May  1 19:05:15.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.104194424s
    May  1 19:05:17.968: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.126012121s
    May  1 19:05:19.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.103787669s
    STEP: Saw pod success 05/01/23 19:05:19.946
    May  1 19:05:19.946: INFO: Pod "pod-8089633a-382c-4d14-88d0-45da798817c5" satisfied condition "Succeeded or Failed"
    May  1 19:05:19.963: INFO: Trying to get logs from node 10.45.145.124 pod pod-8089633a-382c-4d14-88d0-45da798817c5 container test-container: <nil>
    STEP: delete the pod 05/01/23 19:05:20.022
    May  1 19:05:20.085: INFO: Waiting for pod pod-8089633a-382c-4d14-88d0-45da798817c5 to disappear
    May  1 19:05:20.101: INFO: Pod pod-8089633a-382c-4d14-88d0-45da798817c5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:05:20.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3455" for this suite. 05/01/23 19:05:20.123
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:20.217
May  1 19:05:20.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replication-controller 05/01/23 19:05:20.219
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:20.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:20.372
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 05/01/23 19:05:20.385
May  1 19:05:20.482: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4981" to be "running and ready"
May  1 19:05:20.512: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 30.231815ms
May  1 19:05:20.513: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  1 19:05:22.543: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061093113s
May  1 19:05:22.543: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May  1 19:05:24.535: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.052411503s
May  1 19:05:24.535: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May  1 19:05:24.535: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/01/23 19:05:24.554
STEP: Then the orphan pod is adopted 05/01/23 19:05:24.577
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  1 19:05:25.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4981" for this suite. 05/01/23 19:05:25.66
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":53,"skipped":1120,"failed":0}
------------------------------
• [SLOW TEST] [5.468 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:20.217
    May  1 19:05:20.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replication-controller 05/01/23 19:05:20.219
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:20.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:20.372
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/01/23 19:05:20.385
    May  1 19:05:20.482: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-4981" to be "running and ready"
    May  1 19:05:20.512: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 30.231815ms
    May  1 19:05:20.513: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:05:22.543: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061093113s
    May  1 19:05:22.543: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:05:24.535: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.052411503s
    May  1 19:05:24.535: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May  1 19:05:24.535: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/01/23 19:05:24.554
    STEP: Then the orphan pod is adopted 05/01/23 19:05:24.577
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  1 19:05:25.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4981" for this suite. 05/01/23 19:05:25.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:25.713
May  1 19:05:25.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:05:25.718
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:25.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:25.818
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:05:25.953
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:05:27.44
STEP: Deploying the webhook pod 05/01/23 19:05:27.508
STEP: Wait for the deployment to be ready 05/01/23 19:05:27.553
May  1 19:05:27.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:05:29.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:05:31.754
STEP: Verifying the service has paired with the endpoint 05/01/23 19:05:31.806
May  1 19:05:32.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 05/01/23 19:05:32.821
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/01/23 19:05:32.937
STEP: Creating a configMap that should not be mutated 05/01/23 19:05:32.96
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/01/23 19:05:33.008
STEP: Creating a configMap that should be mutated 05/01/23 19:05:33.034
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:05:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7424" for this suite. 05/01/23 19:05:33.175
STEP: Destroying namespace "webhook-7424-markers" for this suite. 05/01/23 19:05:33.208
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":54,"skipped":1169,"failed":0}
------------------------------
• [SLOW TEST] [7.679 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:25.713
    May  1 19:05:25.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:05:25.718
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:25.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:25.818
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:05:25.953
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:05:27.44
    STEP: Deploying the webhook pod 05/01/23 19:05:27.508
    STEP: Wait for the deployment to be ready 05/01/23 19:05:27.553
    May  1 19:05:27.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:05:29.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 5, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:05:31.754
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:05:31.806
    May  1 19:05:32.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 05/01/23 19:05:32.821
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/01/23 19:05:32.937
    STEP: Creating a configMap that should not be mutated 05/01/23 19:05:32.96
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/01/23 19:05:33.008
    STEP: Creating a configMap that should be mutated 05/01/23 19:05:33.034
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:05:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7424" for this suite. 05/01/23 19:05:33.175
    STEP: Destroying namespace "webhook-7424-markers" for this suite. 05/01/23 19:05:33.208
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:33.411
May  1 19:05:33.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename discovery 05/01/23 19:05:33.432
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:33.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:33.549
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/01/23 19:05:33.567
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May  1 19:05:34.487: INFO: Checking APIGroup: apiregistration.k8s.io
May  1 19:05:34.493: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May  1 19:05:34.493: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May  1 19:05:34.493: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May  1 19:05:34.493: INFO: Checking APIGroup: apps
May  1 19:05:34.500: INFO: PreferredVersion.GroupVersion: apps/v1
May  1 19:05:34.500: INFO: Versions found [{apps/v1 v1}]
May  1 19:05:34.500: INFO: apps/v1 matches apps/v1
May  1 19:05:34.500: INFO: Checking APIGroup: events.k8s.io
May  1 19:05:34.506: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May  1 19:05:34.506: INFO: Versions found [{events.k8s.io/v1 v1}]
May  1 19:05:34.506: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May  1 19:05:34.506: INFO: Checking APIGroup: authentication.k8s.io
May  1 19:05:34.511: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May  1 19:05:34.511: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May  1 19:05:34.511: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May  1 19:05:34.511: INFO: Checking APIGroup: authorization.k8s.io
May  1 19:05:34.522: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May  1 19:05:34.522: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May  1 19:05:34.522: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May  1 19:05:34.522: INFO: Checking APIGroup: autoscaling
May  1 19:05:34.527: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May  1 19:05:34.527: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
May  1 19:05:34.527: INFO: autoscaling/v2 matches autoscaling/v2
May  1 19:05:34.527: INFO: Checking APIGroup: batch
May  1 19:05:34.532: INFO: PreferredVersion.GroupVersion: batch/v1
May  1 19:05:34.532: INFO: Versions found [{batch/v1 v1}]
May  1 19:05:34.532: INFO: batch/v1 matches batch/v1
May  1 19:05:34.532: INFO: Checking APIGroup: certificates.k8s.io
May  1 19:05:34.542: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May  1 19:05:34.542: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May  1 19:05:34.542: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May  1 19:05:34.542: INFO: Checking APIGroup: networking.k8s.io
May  1 19:05:34.548: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May  1 19:05:34.549: INFO: Versions found [{networking.k8s.io/v1 v1}]
May  1 19:05:34.549: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May  1 19:05:34.549: INFO: Checking APIGroup: policy
May  1 19:05:34.553: INFO: PreferredVersion.GroupVersion: policy/v1
May  1 19:05:34.553: INFO: Versions found [{policy/v1 v1}]
May  1 19:05:34.553: INFO: policy/v1 matches policy/v1
May  1 19:05:34.553: INFO: Checking APIGroup: rbac.authorization.k8s.io
May  1 19:05:34.558: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May  1 19:05:34.558: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May  1 19:05:34.558: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May  1 19:05:34.558: INFO: Checking APIGroup: storage.k8s.io
May  1 19:05:34.569: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May  1 19:05:34.569: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May  1 19:05:34.569: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May  1 19:05:34.569: INFO: Checking APIGroup: admissionregistration.k8s.io
May  1 19:05:34.574: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May  1 19:05:34.574: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May  1 19:05:34.574: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May  1 19:05:34.574: INFO: Checking APIGroup: apiextensions.k8s.io
May  1 19:05:34.579: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May  1 19:05:34.579: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May  1 19:05:34.579: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May  1 19:05:34.579: INFO: Checking APIGroup: scheduling.k8s.io
May  1 19:05:34.589: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May  1 19:05:34.589: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May  1 19:05:34.589: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May  1 19:05:34.589: INFO: Checking APIGroup: coordination.k8s.io
May  1 19:05:34.593: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May  1 19:05:34.593: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May  1 19:05:34.593: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May  1 19:05:34.593: INFO: Checking APIGroup: node.k8s.io
May  1 19:05:34.599: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May  1 19:05:34.599: INFO: Versions found [{node.k8s.io/v1 v1}]
May  1 19:05:34.599: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May  1 19:05:34.599: INFO: Checking APIGroup: discovery.k8s.io
May  1 19:05:34.604: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May  1 19:05:34.604: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May  1 19:05:34.604: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May  1 19:05:34.604: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May  1 19:05:34.609: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May  1 19:05:34.609: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May  1 19:05:34.609: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May  1 19:05:34.609: INFO: Checking APIGroup: apps.openshift.io
May  1 19:05:34.614: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
May  1 19:05:34.614: INFO: Versions found [{apps.openshift.io/v1 v1}]
May  1 19:05:34.614: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
May  1 19:05:34.614: INFO: Checking APIGroup: authorization.openshift.io
May  1 19:05:34.621: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
May  1 19:05:34.621: INFO: Versions found [{authorization.openshift.io/v1 v1}]
May  1 19:05:34.621: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
May  1 19:05:34.621: INFO: Checking APIGroup: build.openshift.io
May  1 19:05:34.626: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
May  1 19:05:34.626: INFO: Versions found [{build.openshift.io/v1 v1}]
May  1 19:05:34.626: INFO: build.openshift.io/v1 matches build.openshift.io/v1
May  1 19:05:34.626: INFO: Checking APIGroup: image.openshift.io
May  1 19:05:34.631: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
May  1 19:05:34.631: INFO: Versions found [{image.openshift.io/v1 v1}]
May  1 19:05:34.631: INFO: image.openshift.io/v1 matches image.openshift.io/v1
May  1 19:05:34.631: INFO: Checking APIGroup: oauth.openshift.io
May  1 19:05:34.636: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
May  1 19:05:34.636: INFO: Versions found [{oauth.openshift.io/v1 v1}]
May  1 19:05:34.636: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
May  1 19:05:34.636: INFO: Checking APIGroup: project.openshift.io
May  1 19:05:34.641: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
May  1 19:05:34.641: INFO: Versions found [{project.openshift.io/v1 v1}]
May  1 19:05:34.641: INFO: project.openshift.io/v1 matches project.openshift.io/v1
May  1 19:05:34.641: INFO: Checking APIGroup: quota.openshift.io
May  1 19:05:34.646: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
May  1 19:05:34.646: INFO: Versions found [{quota.openshift.io/v1 v1}]
May  1 19:05:34.646: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
May  1 19:05:34.646: INFO: Checking APIGroup: route.openshift.io
May  1 19:05:34.654: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
May  1 19:05:34.654: INFO: Versions found [{route.openshift.io/v1 v1}]
May  1 19:05:34.654: INFO: route.openshift.io/v1 matches route.openshift.io/v1
May  1 19:05:34.654: INFO: Checking APIGroup: security.openshift.io
May  1 19:05:34.667: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
May  1 19:05:34.668: INFO: Versions found [{security.openshift.io/v1 v1}]
May  1 19:05:34.668: INFO: security.openshift.io/v1 matches security.openshift.io/v1
May  1 19:05:34.668: INFO: Checking APIGroup: template.openshift.io
May  1 19:05:34.680: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
May  1 19:05:34.680: INFO: Versions found [{template.openshift.io/v1 v1}]
May  1 19:05:34.680: INFO: template.openshift.io/v1 matches template.openshift.io/v1
May  1 19:05:34.680: INFO: Checking APIGroup: user.openshift.io
May  1 19:05:34.687: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
May  1 19:05:34.687: INFO: Versions found [{user.openshift.io/v1 v1}]
May  1 19:05:34.687: INFO: user.openshift.io/v1 matches user.openshift.io/v1
May  1 19:05:34.687: INFO: Checking APIGroup: packages.operators.coreos.com
May  1 19:05:34.695: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
May  1 19:05:34.695: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
May  1 19:05:34.695: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
May  1 19:05:34.695: INFO: Checking APIGroup: config.openshift.io
May  1 19:05:34.701: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
May  1 19:05:34.701: INFO: Versions found [{config.openshift.io/v1 v1}]
May  1 19:05:34.701: INFO: config.openshift.io/v1 matches config.openshift.io/v1
May  1 19:05:34.701: INFO: Checking APIGroup: operator.openshift.io
May  1 19:05:34.706: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
May  1 19:05:34.706: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
May  1 19:05:34.706: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
May  1 19:05:34.706: INFO: Checking APIGroup: apiserver.openshift.io
May  1 19:05:34.711: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
May  1 19:05:34.711: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
May  1 19:05:34.711: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
May  1 19:05:34.711: INFO: Checking APIGroup: cloudcredential.openshift.io
May  1 19:05:34.716: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
May  1 19:05:34.716: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
May  1 19:05:34.716: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
May  1 19:05:34.716: INFO: Checking APIGroup: console.openshift.io
May  1 19:05:34.721: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
May  1 19:05:34.721: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
May  1 19:05:34.721: INFO: console.openshift.io/v1 matches console.openshift.io/v1
May  1 19:05:34.721: INFO: Checking APIGroup: crd.projectcalico.org
May  1 19:05:34.726: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May  1 19:05:34.726: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May  1 19:05:34.726: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May  1 19:05:34.726: INFO: Checking APIGroup: imageregistry.operator.openshift.io
May  1 19:05:34.733: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
May  1 19:05:34.733: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
May  1 19:05:34.733: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
May  1 19:05:34.733: INFO: Checking APIGroup: ingress.operator.openshift.io
May  1 19:05:34.738: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
May  1 19:05:34.738: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
May  1 19:05:34.738: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
May  1 19:05:34.738: INFO: Checking APIGroup: k8s.cni.cncf.io
May  1 19:05:34.743: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
May  1 19:05:34.743: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
May  1 19:05:34.743: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
May  1 19:05:34.744: INFO: Checking APIGroup: machineconfiguration.openshift.io
May  1 19:05:34.749: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
May  1 19:05:34.749: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
May  1 19:05:34.749: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
May  1 19:05:34.749: INFO: Checking APIGroup: monitoring.coreos.com
May  1 19:05:34.753: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
May  1 19:05:34.753: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
May  1 19:05:34.753: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
May  1 19:05:34.753: INFO: Checking APIGroup: network.operator.openshift.io
May  1 19:05:34.758: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
May  1 19:05:34.758: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
May  1 19:05:34.758: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
May  1 19:05:34.758: INFO: Checking APIGroup: operator.tigera.io
May  1 19:05:34.764: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
May  1 19:05:34.764: INFO: Versions found [{operator.tigera.io/v1 v1}]
May  1 19:05:34.764: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
May  1 19:05:34.764: INFO: Checking APIGroup: operators.coreos.com
May  1 19:05:34.770: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
May  1 19:05:34.770: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
May  1 19:05:34.770: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
May  1 19:05:34.770: INFO: Checking APIGroup: performance.openshift.io
May  1 19:05:34.775: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
May  1 19:05:34.775: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
May  1 19:05:34.775: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
May  1 19:05:34.775: INFO: Checking APIGroup: samples.operator.openshift.io
May  1 19:05:34.780: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
May  1 19:05:34.780: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
May  1 19:05:34.780: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
May  1 19:05:34.780: INFO: Checking APIGroup: security.internal.openshift.io
May  1 19:05:34.787: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
May  1 19:05:34.787: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
May  1 19:05:34.787: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
May  1 19:05:34.787: INFO: Checking APIGroup: snapshot.storage.k8s.io
May  1 19:05:34.792: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May  1 19:05:34.792: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
May  1 19:05:34.792: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May  1 19:05:34.792: INFO: Checking APIGroup: tuned.openshift.io
May  1 19:05:34.798: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
May  1 19:05:34.798: INFO: Versions found [{tuned.openshift.io/v1 v1}]
May  1 19:05:34.798: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
May  1 19:05:34.798: INFO: Checking APIGroup: controlplane.operator.openshift.io
May  1 19:05:34.803: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
May  1 19:05:34.803: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
May  1 19:05:34.803: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
May  1 19:05:34.803: INFO: Checking APIGroup: ibm.com
May  1 19:05:34.810: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
May  1 19:05:34.810: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
May  1 19:05:34.810: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
May  1 19:05:34.810: INFO: Checking APIGroup: migration.k8s.io
May  1 19:05:34.816: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
May  1 19:05:34.816: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
May  1 19:05:34.816: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
May  1 19:05:34.816: INFO: Checking APIGroup: whereabouts.cni.cncf.io
May  1 19:05:34.821: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
May  1 19:05:34.821: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
May  1 19:05:34.821: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
May  1 19:05:34.821: INFO: Checking APIGroup: helm.openshift.io
May  1 19:05:34.826: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
May  1 19:05:34.826: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
May  1 19:05:34.826: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
May  1 19:05:34.826: INFO: Checking APIGroup: metrics.k8s.io
May  1 19:05:34.841: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May  1 19:05:34.841: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May  1 19:05:34.841: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
May  1 19:05:34.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7342" for this suite. 05/01/23 19:05:34.904
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":55,"skipped":1200,"failed":0}
------------------------------
• [1.543 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:33.411
    May  1 19:05:33.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename discovery 05/01/23 19:05:33.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:33.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:33.549
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/01/23 19:05:33.567
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May  1 19:05:34.487: INFO: Checking APIGroup: apiregistration.k8s.io
    May  1 19:05:34.493: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May  1 19:05:34.493: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May  1 19:05:34.493: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May  1 19:05:34.493: INFO: Checking APIGroup: apps
    May  1 19:05:34.500: INFO: PreferredVersion.GroupVersion: apps/v1
    May  1 19:05:34.500: INFO: Versions found [{apps/v1 v1}]
    May  1 19:05:34.500: INFO: apps/v1 matches apps/v1
    May  1 19:05:34.500: INFO: Checking APIGroup: events.k8s.io
    May  1 19:05:34.506: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May  1 19:05:34.506: INFO: Versions found [{events.k8s.io/v1 v1}]
    May  1 19:05:34.506: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May  1 19:05:34.506: INFO: Checking APIGroup: authentication.k8s.io
    May  1 19:05:34.511: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May  1 19:05:34.511: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May  1 19:05:34.511: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May  1 19:05:34.511: INFO: Checking APIGroup: authorization.k8s.io
    May  1 19:05:34.522: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May  1 19:05:34.522: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May  1 19:05:34.522: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May  1 19:05:34.522: INFO: Checking APIGroup: autoscaling
    May  1 19:05:34.527: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May  1 19:05:34.527: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    May  1 19:05:34.527: INFO: autoscaling/v2 matches autoscaling/v2
    May  1 19:05:34.527: INFO: Checking APIGroup: batch
    May  1 19:05:34.532: INFO: PreferredVersion.GroupVersion: batch/v1
    May  1 19:05:34.532: INFO: Versions found [{batch/v1 v1}]
    May  1 19:05:34.532: INFO: batch/v1 matches batch/v1
    May  1 19:05:34.532: INFO: Checking APIGroup: certificates.k8s.io
    May  1 19:05:34.542: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May  1 19:05:34.542: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May  1 19:05:34.542: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May  1 19:05:34.542: INFO: Checking APIGroup: networking.k8s.io
    May  1 19:05:34.548: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May  1 19:05:34.549: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May  1 19:05:34.549: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May  1 19:05:34.549: INFO: Checking APIGroup: policy
    May  1 19:05:34.553: INFO: PreferredVersion.GroupVersion: policy/v1
    May  1 19:05:34.553: INFO: Versions found [{policy/v1 v1}]
    May  1 19:05:34.553: INFO: policy/v1 matches policy/v1
    May  1 19:05:34.553: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May  1 19:05:34.558: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May  1 19:05:34.558: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May  1 19:05:34.558: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May  1 19:05:34.558: INFO: Checking APIGroup: storage.k8s.io
    May  1 19:05:34.569: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May  1 19:05:34.569: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May  1 19:05:34.569: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May  1 19:05:34.569: INFO: Checking APIGroup: admissionregistration.k8s.io
    May  1 19:05:34.574: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May  1 19:05:34.574: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May  1 19:05:34.574: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May  1 19:05:34.574: INFO: Checking APIGroup: apiextensions.k8s.io
    May  1 19:05:34.579: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May  1 19:05:34.579: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May  1 19:05:34.579: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May  1 19:05:34.579: INFO: Checking APIGroup: scheduling.k8s.io
    May  1 19:05:34.589: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May  1 19:05:34.589: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May  1 19:05:34.589: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May  1 19:05:34.589: INFO: Checking APIGroup: coordination.k8s.io
    May  1 19:05:34.593: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May  1 19:05:34.593: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May  1 19:05:34.593: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May  1 19:05:34.593: INFO: Checking APIGroup: node.k8s.io
    May  1 19:05:34.599: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May  1 19:05:34.599: INFO: Versions found [{node.k8s.io/v1 v1}]
    May  1 19:05:34.599: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May  1 19:05:34.599: INFO: Checking APIGroup: discovery.k8s.io
    May  1 19:05:34.604: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May  1 19:05:34.604: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May  1 19:05:34.604: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May  1 19:05:34.604: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May  1 19:05:34.609: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    May  1 19:05:34.609: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    May  1 19:05:34.609: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    May  1 19:05:34.609: INFO: Checking APIGroup: apps.openshift.io
    May  1 19:05:34.614: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    May  1 19:05:34.614: INFO: Versions found [{apps.openshift.io/v1 v1}]
    May  1 19:05:34.614: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    May  1 19:05:34.614: INFO: Checking APIGroup: authorization.openshift.io
    May  1 19:05:34.621: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    May  1 19:05:34.621: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    May  1 19:05:34.621: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    May  1 19:05:34.621: INFO: Checking APIGroup: build.openshift.io
    May  1 19:05:34.626: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    May  1 19:05:34.626: INFO: Versions found [{build.openshift.io/v1 v1}]
    May  1 19:05:34.626: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    May  1 19:05:34.626: INFO: Checking APIGroup: image.openshift.io
    May  1 19:05:34.631: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    May  1 19:05:34.631: INFO: Versions found [{image.openshift.io/v1 v1}]
    May  1 19:05:34.631: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    May  1 19:05:34.631: INFO: Checking APIGroup: oauth.openshift.io
    May  1 19:05:34.636: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    May  1 19:05:34.636: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    May  1 19:05:34.636: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    May  1 19:05:34.636: INFO: Checking APIGroup: project.openshift.io
    May  1 19:05:34.641: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    May  1 19:05:34.641: INFO: Versions found [{project.openshift.io/v1 v1}]
    May  1 19:05:34.641: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    May  1 19:05:34.641: INFO: Checking APIGroup: quota.openshift.io
    May  1 19:05:34.646: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    May  1 19:05:34.646: INFO: Versions found [{quota.openshift.io/v1 v1}]
    May  1 19:05:34.646: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    May  1 19:05:34.646: INFO: Checking APIGroup: route.openshift.io
    May  1 19:05:34.654: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    May  1 19:05:34.654: INFO: Versions found [{route.openshift.io/v1 v1}]
    May  1 19:05:34.654: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    May  1 19:05:34.654: INFO: Checking APIGroup: security.openshift.io
    May  1 19:05:34.667: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    May  1 19:05:34.668: INFO: Versions found [{security.openshift.io/v1 v1}]
    May  1 19:05:34.668: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    May  1 19:05:34.668: INFO: Checking APIGroup: template.openshift.io
    May  1 19:05:34.680: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    May  1 19:05:34.680: INFO: Versions found [{template.openshift.io/v1 v1}]
    May  1 19:05:34.680: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    May  1 19:05:34.680: INFO: Checking APIGroup: user.openshift.io
    May  1 19:05:34.687: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    May  1 19:05:34.687: INFO: Versions found [{user.openshift.io/v1 v1}]
    May  1 19:05:34.687: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    May  1 19:05:34.687: INFO: Checking APIGroup: packages.operators.coreos.com
    May  1 19:05:34.695: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    May  1 19:05:34.695: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    May  1 19:05:34.695: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    May  1 19:05:34.695: INFO: Checking APIGroup: config.openshift.io
    May  1 19:05:34.701: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    May  1 19:05:34.701: INFO: Versions found [{config.openshift.io/v1 v1}]
    May  1 19:05:34.701: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    May  1 19:05:34.701: INFO: Checking APIGroup: operator.openshift.io
    May  1 19:05:34.706: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    May  1 19:05:34.706: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.706: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    May  1 19:05:34.706: INFO: Checking APIGroup: apiserver.openshift.io
    May  1 19:05:34.711: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    May  1 19:05:34.711: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    May  1 19:05:34.711: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    May  1 19:05:34.711: INFO: Checking APIGroup: cloudcredential.openshift.io
    May  1 19:05:34.716: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    May  1 19:05:34.716: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    May  1 19:05:34.716: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    May  1 19:05:34.716: INFO: Checking APIGroup: console.openshift.io
    May  1 19:05:34.721: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    May  1 19:05:34.721: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.721: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    May  1 19:05:34.721: INFO: Checking APIGroup: crd.projectcalico.org
    May  1 19:05:34.726: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    May  1 19:05:34.726: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    May  1 19:05:34.726: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    May  1 19:05:34.726: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    May  1 19:05:34.733: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    May  1 19:05:34.733: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    May  1 19:05:34.733: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    May  1 19:05:34.733: INFO: Checking APIGroup: ingress.operator.openshift.io
    May  1 19:05:34.738: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    May  1 19:05:34.738: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    May  1 19:05:34.738: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    May  1 19:05:34.738: INFO: Checking APIGroup: k8s.cni.cncf.io
    May  1 19:05:34.743: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    May  1 19:05:34.743: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    May  1 19:05:34.743: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    May  1 19:05:34.744: INFO: Checking APIGroup: machineconfiguration.openshift.io
    May  1 19:05:34.749: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    May  1 19:05:34.749: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    May  1 19:05:34.749: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    May  1 19:05:34.749: INFO: Checking APIGroup: monitoring.coreos.com
    May  1 19:05:34.753: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    May  1 19:05:34.753: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    May  1 19:05:34.753: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    May  1 19:05:34.753: INFO: Checking APIGroup: network.operator.openshift.io
    May  1 19:05:34.758: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    May  1 19:05:34.758: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    May  1 19:05:34.758: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    May  1 19:05:34.758: INFO: Checking APIGroup: operator.tigera.io
    May  1 19:05:34.764: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    May  1 19:05:34.764: INFO: Versions found [{operator.tigera.io/v1 v1}]
    May  1 19:05:34.764: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    May  1 19:05:34.764: INFO: Checking APIGroup: operators.coreos.com
    May  1 19:05:34.770: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    May  1 19:05:34.770: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    May  1 19:05:34.770: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    May  1 19:05:34.770: INFO: Checking APIGroup: performance.openshift.io
    May  1 19:05:34.775: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    May  1 19:05:34.775: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.775: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    May  1 19:05:34.775: INFO: Checking APIGroup: samples.operator.openshift.io
    May  1 19:05:34.780: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    May  1 19:05:34.780: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    May  1 19:05:34.780: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    May  1 19:05:34.780: INFO: Checking APIGroup: security.internal.openshift.io
    May  1 19:05:34.787: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    May  1 19:05:34.787: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    May  1 19:05:34.787: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    May  1 19:05:34.787: INFO: Checking APIGroup: snapshot.storage.k8s.io
    May  1 19:05:34.792: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    May  1 19:05:34.792: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    May  1 19:05:34.792: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    May  1 19:05:34.792: INFO: Checking APIGroup: tuned.openshift.io
    May  1 19:05:34.798: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    May  1 19:05:34.798: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    May  1 19:05:34.798: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    May  1 19:05:34.798: INFO: Checking APIGroup: controlplane.operator.openshift.io
    May  1 19:05:34.803: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    May  1 19:05:34.803: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.803: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    May  1 19:05:34.803: INFO: Checking APIGroup: ibm.com
    May  1 19:05:34.810: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    May  1 19:05:34.810: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    May  1 19:05:34.810: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    May  1 19:05:34.810: INFO: Checking APIGroup: migration.k8s.io
    May  1 19:05:34.816: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    May  1 19:05:34.816: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.816: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    May  1 19:05:34.816: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    May  1 19:05:34.821: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    May  1 19:05:34.821: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    May  1 19:05:34.821: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    May  1 19:05:34.821: INFO: Checking APIGroup: helm.openshift.io
    May  1 19:05:34.826: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    May  1 19:05:34.826: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    May  1 19:05:34.826: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    May  1 19:05:34.826: INFO: Checking APIGroup: metrics.k8s.io
    May  1 19:05:34.841: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May  1 19:05:34.841: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May  1 19:05:34.841: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    May  1 19:05:34.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7342" for this suite. 05/01/23 19:05:34.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:34.958
May  1 19:05:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 19:05:34.963
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:35.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:35.075
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3254 05/01/23 19:05:35.118
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
May  1 19:05:35.281: INFO: Found 0 stateful pods, waiting for 1
May  1 19:05:45.302: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/01/23 19:05:45.352
W0501 19:05:45.395664      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  1 19:05:45.430: INFO: Found 1 stateful pods, waiting for 2
May  1 19:05:55.456: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 19:05:55.456: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/01/23 19:05:55.498
STEP: Delete all of the StatefulSets 05/01/23 19:05:55.531
STEP: Verify that StatefulSets have been deleted 05/01/23 19:05:55.581
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 19:05:55.606: INFO: Deleting all statefulset in ns statefulset-3254
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 19:05:55.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3254" for this suite. 05/01/23 19:05:55.705
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":56,"skipped":1222,"failed":0}
------------------------------
• [SLOW TEST] [20.773 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:34.958
    May  1 19:05:34.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 19:05:34.963
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:35.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:35.075
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3254 05/01/23 19:05:35.118
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    May  1 19:05:35.281: INFO: Found 0 stateful pods, waiting for 1
    May  1 19:05:45.302: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/01/23 19:05:45.352
    W0501 19:05:45.395664      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  1 19:05:45.430: INFO: Found 1 stateful pods, waiting for 2
    May  1 19:05:55.456: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 19:05:55.456: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/01/23 19:05:55.498
    STEP: Delete all of the StatefulSets 05/01/23 19:05:55.531
    STEP: Verify that StatefulSets have been deleted 05/01/23 19:05:55.581
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 19:05:55.606: INFO: Deleting all statefulset in ns statefulset-3254
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 19:05:55.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3254" for this suite. 05/01/23 19:05:55.705
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:05:55.732
May  1 19:05:55.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 19:05:55.736
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:55.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:55.836
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 05/01/23 19:05:55.856
May  1 19:05:55.934: INFO: Waiting up to 5m0s for pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f" in namespace "var-expansion-3028" to be "Succeeded or Failed"
May  1 19:05:55.981: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 47.137252ms
May  1 19:05:58.042: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108241217s
May  1 19:06:00.029: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095512332s
May  1 19:06:02.016: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082242475s
STEP: Saw pod success 05/01/23 19:06:02.016
May  1 19:06:02.017: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f" satisfied condition "Succeeded or Failed"
May  1 19:06:02.044: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f container dapi-container: <nil>
STEP: delete the pod 05/01/23 19:06:02.169
May  1 19:06:02.251: INFO: Waiting for pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f to disappear
May  1 19:06:02.281: INFO: Pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 19:06:02.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3028" for this suite. 05/01/23 19:06:02.343
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":57,"skipped":1224,"failed":0}
------------------------------
• [SLOW TEST] [6.657 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:05:55.732
    May  1 19:05:55.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 19:05:55.736
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:05:55.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:05:55.836
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 05/01/23 19:05:55.856
    May  1 19:05:55.934: INFO: Waiting up to 5m0s for pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f" in namespace "var-expansion-3028" to be "Succeeded or Failed"
    May  1 19:05:55.981: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 47.137252ms
    May  1 19:05:58.042: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108241217s
    May  1 19:06:00.029: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095512332s
    May  1 19:06:02.016: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082242475s
    STEP: Saw pod success 05/01/23 19:06:02.016
    May  1 19:06:02.017: INFO: Pod "var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f" satisfied condition "Succeeded or Failed"
    May  1 19:06:02.044: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f container dapi-container: <nil>
    STEP: delete the pod 05/01/23 19:06:02.169
    May  1 19:06:02.251: INFO: Waiting for pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f to disappear
    May  1 19:06:02.281: INFO: Pod var-expansion-baace09a-0daf-4a4d-867f-d68325628b9f no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 19:06:02.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3028" for this suite. 05/01/23 19:06:02.343
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:06:02.391
May  1 19:06:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 19:06:02.394
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:02.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:02.496
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 05/01/23 19:06:02.514
STEP: Creating a ResourceQuota 05/01/23 19:06:07.536
STEP: Ensuring resource quota status is calculated 05/01/23 19:06:07.563
STEP: Creating a Pod that fits quota 05/01/23 19:06:09.597
STEP: Ensuring ResourceQuota status captures the pod usage 05/01/23 19:06:09.694
STEP: Not allowing a pod to be created that exceeds remaining quota 05/01/23 19:06:11.713
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/01/23 19:06:11.75
STEP: Ensuring a pod cannot update its resource requirements 05/01/23 19:06:11.781
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/01/23 19:06:11.826
STEP: Deleting the pod 05/01/23 19:06:13.845
STEP: Ensuring resource quota status released the pod usage 05/01/23 19:06:13.901
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 19:06:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5530" for this suite. 05/01/23 19:06:15.951
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":58,"skipped":1226,"failed":0}
------------------------------
• [SLOW TEST] [13.604 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:06:02.391
    May  1 19:06:02.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 19:06:02.394
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:02.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:02.496
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 05/01/23 19:06:02.514
    STEP: Creating a ResourceQuota 05/01/23 19:06:07.536
    STEP: Ensuring resource quota status is calculated 05/01/23 19:06:07.563
    STEP: Creating a Pod that fits quota 05/01/23 19:06:09.597
    STEP: Ensuring ResourceQuota status captures the pod usage 05/01/23 19:06:09.694
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/01/23 19:06:11.713
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/01/23 19:06:11.75
    STEP: Ensuring a pod cannot update its resource requirements 05/01/23 19:06:11.781
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/01/23 19:06:11.826
    STEP: Deleting the pod 05/01/23 19:06:13.845
    STEP: Ensuring resource quota status released the pod usage 05/01/23 19:06:13.901
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 19:06:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5530" for this suite. 05/01/23 19:06:15.951
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:06:15.996
May  1 19:06:15.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename proxy 05/01/23 19:06:16
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:16.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:16.18
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May  1 19:06:16.197: INFO: Creating pod...
May  1 19:06:16.322: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8473" to be "running"
May  1 19:06:16.361: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 39.118028ms
May  1 19:06:18.402: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080074788s
May  1 19:06:20.394: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.072144354s
May  1 19:06:20.394: INFO: Pod "agnhost" satisfied condition "running"
May  1 19:06:20.394: INFO: Creating service...
May  1 19:06:20.467: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=DELETE
May  1 19:06:20.535: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  1 19:06:20.536: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=OPTIONS
May  1 19:06:20.598: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  1 19:06:20.598: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=PATCH
May  1 19:06:20.634: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  1 19:06:20.634: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=POST
May  1 19:06:20.661: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  1 19:06:20.661: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=PUT
May  1 19:06:20.696: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  1 19:06:20.696: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=DELETE
May  1 19:06:20.737: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  1 19:06:20.737: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=OPTIONS
May  1 19:06:20.774: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  1 19:06:20.774: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=PATCH
May  1 19:06:20.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  1 19:06:20.812: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=POST
May  1 19:06:20.855: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  1 19:06:20.856: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=PUT
May  1 19:06:20.929: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  1 19:06:20.929: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=GET
May  1 19:06:20.966: INFO: http.Client request:GET StatusCode:301
May  1 19:06:20.966: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=GET
May  1 19:06:20.992: INFO: http.Client request:GET StatusCode:301
May  1 19:06:20.992: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=HEAD
May  1 19:06:21.012: INFO: http.Client request:HEAD StatusCode:301
May  1 19:06:21.012: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=HEAD
May  1 19:06:21.038: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  1 19:06:21.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8473" for this suite. 05/01/23 19:06:21.059
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":59,"skipped":1227,"failed":0}
------------------------------
• [SLOW TEST] [5.100 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:06:15.996
    May  1 19:06:15.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename proxy 05/01/23 19:06:16
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:16.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:16.18
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May  1 19:06:16.197: INFO: Creating pod...
    May  1 19:06:16.322: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8473" to be "running"
    May  1 19:06:16.361: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 39.118028ms
    May  1 19:06:18.402: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080074788s
    May  1 19:06:20.394: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.072144354s
    May  1 19:06:20.394: INFO: Pod "agnhost" satisfied condition "running"
    May  1 19:06:20.394: INFO: Creating service...
    May  1 19:06:20.467: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=DELETE
    May  1 19:06:20.535: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  1 19:06:20.536: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=OPTIONS
    May  1 19:06:20.598: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  1 19:06:20.598: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=PATCH
    May  1 19:06:20.634: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  1 19:06:20.634: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=POST
    May  1 19:06:20.661: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  1 19:06:20.661: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=PUT
    May  1 19:06:20.696: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  1 19:06:20.696: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=DELETE
    May  1 19:06:20.737: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  1 19:06:20.737: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May  1 19:06:20.774: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  1 19:06:20.774: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=PATCH
    May  1 19:06:20.811: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  1 19:06:20.812: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=POST
    May  1 19:06:20.855: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  1 19:06:20.856: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=PUT
    May  1 19:06:20.929: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  1 19:06:20.929: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=GET
    May  1 19:06:20.966: INFO: http.Client request:GET StatusCode:301
    May  1 19:06:20.966: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=GET
    May  1 19:06:20.992: INFO: http.Client request:GET StatusCode:301
    May  1 19:06:20.992: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/pods/agnhost/proxy?method=HEAD
    May  1 19:06:21.012: INFO: http.Client request:HEAD StatusCode:301
    May  1 19:06:21.012: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8473/services/e2e-proxy-test-service/proxy?method=HEAD
    May  1 19:06:21.038: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  1 19:06:21.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-8473" for this suite. 05/01/23 19:06:21.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:06:21.104
May  1 19:06:21.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 19:06:21.105
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:21.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:21.227
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
May  1 19:06:21.346: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  1 19:06:26.378: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 19:06:26.378
May  1 19:06:26.378: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  1 19:06:28.403: INFO: Creating deployment "test-rollover-deployment"
May  1 19:06:28.450: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  1 19:06:30.497: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  1 19:06:30.526: INFO: Ensure that both replica sets have 1 created replica
May  1 19:06:30.590: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  1 19:06:30.652: INFO: Updating deployment test-rollover-deployment
May  1 19:06:30.652: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  1 19:06:32.704: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  1 19:06:32.733: INFO: Make sure deployment "test-rollover-deployment" is complete
May  1 19:06:32.760: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:32.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:34.789: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:34.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:36.791: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:36.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:38.796: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:38.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:40.808: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:40.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:42.789: INFO: all replica sets need to contain the pod-template-hash label
May  1 19:06:42.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:44.810: INFO: 
May  1 19:06:44.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:06:46.796: INFO: 
May  1 19:06:46.796: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 19:06:46.840: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3708  d7d511d8-9c20-4b32-8267-283ccb886e99 70290 2 2023-05-01 19:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038370c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:06:28 +0000 UTC,LastTransitionTime:2023-05-01 19:06:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-01 19:06:44 +0000 UTC,LastTransitionTime:2023-05-01 19:06:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  1 19:06:46.861: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3708  47609184-9015-4057-b802-5c101ce579ec 70280 2 2023-05-01 19:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfeb7 0xc00d4cfeb8}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d4cff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  1 19:06:46.861: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  1 19:06:46.861: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3708  ff2cf348-2b69-4dcd-a67f-bc49f8f00d23 70289 2 2023-05-01 19:06:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfc67 0xc00d4cfc68}] [] [{e2e.test Update apps/v1 2023-05-01 19:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d4cfd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 19:06:46.861: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3708  457e72c7-9646-42a9-b6b8-7797c309a62f 70180 2 2023-05-01 19:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfd97 0xc00d4cfd98}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d4cfe48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 19:06:46.879: INFO: Pod "test-rollover-deployment-6d45fd857b-nx4q2" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-nx4q2 test-rollover-deployment-6d45fd857b- deployment-3708  8bcc4927-9135-4370-a003-af3bb0564328 70222 0 2023-05-01 19:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:4b10b2c13f105a3389cb66616cfc2eedcde1e9b569c34cf8ed968290a0fdbb7b cni.projectcalico.org/podIP:172.30.38.236/32 cni.projectcalico.org/podIPs:172.30.38.236/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.236"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.236"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 47609184-9015-4057-b802-5c101ce579ec 0xc003837497 0xc003837498}] [] [{kube-controller-manager Update v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47609184-9015-4057-b802-5c101ce579ec\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:06:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz7ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz7ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qqfr5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.236,StartTime:2023-05-01 19:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:06:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://44362fd763bbc4b8d87df221132e279a9706c6175e88279952babf5d4d2268e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 19:06:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3708" for this suite. 05/01/23 19:06:46.919
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":60,"skipped":1258,"failed":0}
------------------------------
• [SLOW TEST] [25.858 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:06:21.104
    May  1 19:06:21.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 19:06:21.105
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:21.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:21.227
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    May  1 19:06:21.346: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May  1 19:06:26.378: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 19:06:26.378
    May  1 19:06:26.378: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May  1 19:06:28.403: INFO: Creating deployment "test-rollover-deployment"
    May  1 19:06:28.450: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May  1 19:06:30.497: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May  1 19:06:30.526: INFO: Ensure that both replica sets have 1 created replica
    May  1 19:06:30.590: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May  1 19:06:30.652: INFO: Updating deployment test-rollover-deployment
    May  1 19:06:30.652: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May  1 19:06:32.704: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May  1 19:06:32.733: INFO: Make sure deployment "test-rollover-deployment" is complete
    May  1 19:06:32.760: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:32.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:34.789: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:34.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:36.791: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:36.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:38.796: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:38.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:40.808: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:40.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:42.789: INFO: all replica sets need to contain the pod-template-hash label
    May  1 19:06:42.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:44.810: INFO: 
    May  1 19:06:44.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 6, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 6, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:06:46.796: INFO: 
    May  1 19:06:46.796: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 19:06:46.840: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3708  d7d511d8-9c20-4b32-8267-283ccb886e99 70290 2 2023-05-01 19:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038370c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:06:28 +0000 UTC,LastTransitionTime:2023-05-01 19:06:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-05-01 19:06:44 +0000 UTC,LastTransitionTime:2023-05-01 19:06:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  1 19:06:46.861: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3708  47609184-9015-4057-b802-5c101ce579ec 70280 2 2023-05-01 19:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfeb7 0xc00d4cfeb8}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d4cff68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:06:46.861: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May  1 19:06:46.861: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3708  ff2cf348-2b69-4dcd-a67f-bc49f8f00d23 70289 2 2023-05-01 19:06:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfc67 0xc00d4cfc68}] [] [{e2e.test Update apps/v1 2023-05-01 19:06:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00d4cfd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:06:46.861: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3708  457e72c7-9646-42a9-b6b8-7797c309a62f 70180 2 2023-05-01 19:06:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment d7d511d8-9c20-4b32-8267-283ccb886e99 0xc00d4cfd97 0xc00d4cfd98}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d511d8-9c20-4b32-8267-283ccb886e99\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d4cfe48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:06:46.879: INFO: Pod "test-rollover-deployment-6d45fd857b-nx4q2" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-nx4q2 test-rollover-deployment-6d45fd857b- deployment-3708  8bcc4927-9135-4370-a003-af3bb0564328 70222 0 2023-05-01 19:06:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:4b10b2c13f105a3389cb66616cfc2eedcde1e9b569c34cf8ed968290a0fdbb7b cni.projectcalico.org/podIP:172.30.38.236/32 cni.projectcalico.org/podIPs:172.30.38.236/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.236"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.236"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 47609184-9015-4057-b802-5c101ce579ec 0xc003837497 0xc003837498}] [] [{kube-controller-manager Update v1 2023-05-01 19:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"47609184-9015-4057-b802-5c101ce579ec\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:06:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:06:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.236\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz7ph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz7ph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-qqfr5,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:06:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.236,StartTime:2023-05-01 19:06:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:06:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://44362fd763bbc4b8d87df221132e279a9706c6175e88279952babf5d4d2268e2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.236,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 19:06:46.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3708" for this suite. 05/01/23 19:06:46.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:06:46.971
May  1 19:06:46.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replication-controller 05/01/23 19:06:46.973
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:47.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:47.113
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 05/01/23 19:06:47.15
STEP: waiting for RC to be added 05/01/23 19:06:47.178
STEP: waiting for available Replicas 05/01/23 19:06:47.18
STEP: patching ReplicationController 05/01/23 19:06:53.31
STEP: waiting for RC to be modified 05/01/23 19:06:53.346
STEP: patching ReplicationController status 05/01/23 19:06:53.347
STEP: waiting for RC to be modified 05/01/23 19:06:53.394
STEP: waiting for available Replicas 05/01/23 19:06:53.394
STEP: fetching ReplicationController status 05/01/23 19:06:53.408
STEP: patching ReplicationController scale 05/01/23 19:06:53.425
STEP: waiting for RC to be modified 05/01/23 19:06:53.475
STEP: waiting for ReplicationController's scale to be the max amount 05/01/23 19:06:53.478
STEP: fetching ReplicationController; ensuring that it's patched 05/01/23 19:06:59.909
STEP: updating ReplicationController status 05/01/23 19:06:59.938
STEP: waiting for RC to be modified 05/01/23 19:06:59.966
STEP: listing all ReplicationControllers 05/01/23 19:06:59.967
STEP: checking that ReplicationController has expected values 05/01/23 19:07:00
STEP: deleting ReplicationControllers by collection 05/01/23 19:07:00.001
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/01/23 19:07:00.051
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  1 19:07:00.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7298" for this suite. 05/01/23 19:07:00.308
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":61,"skipped":1278,"failed":0}
------------------------------
• [SLOW TEST] [13.379 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:06:46.971
    May  1 19:06:46.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replication-controller 05/01/23 19:06:46.973
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:06:47.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:06:47.113
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 05/01/23 19:06:47.15
    STEP: waiting for RC to be added 05/01/23 19:06:47.178
    STEP: waiting for available Replicas 05/01/23 19:06:47.18
    STEP: patching ReplicationController 05/01/23 19:06:53.31
    STEP: waiting for RC to be modified 05/01/23 19:06:53.346
    STEP: patching ReplicationController status 05/01/23 19:06:53.347
    STEP: waiting for RC to be modified 05/01/23 19:06:53.394
    STEP: waiting for available Replicas 05/01/23 19:06:53.394
    STEP: fetching ReplicationController status 05/01/23 19:06:53.408
    STEP: patching ReplicationController scale 05/01/23 19:06:53.425
    STEP: waiting for RC to be modified 05/01/23 19:06:53.475
    STEP: waiting for ReplicationController's scale to be the max amount 05/01/23 19:06:53.478
    STEP: fetching ReplicationController; ensuring that it's patched 05/01/23 19:06:59.909
    STEP: updating ReplicationController status 05/01/23 19:06:59.938
    STEP: waiting for RC to be modified 05/01/23 19:06:59.966
    STEP: listing all ReplicationControllers 05/01/23 19:06:59.967
    STEP: checking that ReplicationController has expected values 05/01/23 19:07:00
    STEP: deleting ReplicationControllers by collection 05/01/23 19:07:00.001
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/01/23 19:07:00.051
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  1 19:07:00.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7298" for this suite. 05/01/23 19:07:00.308
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:07:00.36
May  1 19:07:00.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-runtime 05/01/23 19:07:00.362
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:00.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:00.474
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 05/01/23 19:07:00.517
STEP: wait for the container to reach Failed 05/01/23 19:07:00.729
STEP: get the container status 05/01/23 19:07:05.945
STEP: the container should be terminated 05/01/23 19:07:05.964
STEP: the termination message should be set 05/01/23 19:07:05.964
May  1 19:07:05.964: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/01/23 19:07:05.964
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  1 19:07:06.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6644" for this suite. 05/01/23 19:07:06.087
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":62,"skipped":1280,"failed":0}
------------------------------
• [SLOW TEST] [5.753 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:07:00.36
    May  1 19:07:00.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-runtime 05/01/23 19:07:00.362
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:00.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:00.474
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 05/01/23 19:07:00.517
    STEP: wait for the container to reach Failed 05/01/23 19:07:00.729
    STEP: get the container status 05/01/23 19:07:05.945
    STEP: the container should be terminated 05/01/23 19:07:05.964
    STEP: the termination message should be set 05/01/23 19:07:05.964
    May  1 19:07:05.964: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/01/23 19:07:05.964
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  1 19:07:06.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6644" for this suite. 05/01/23 19:07:06.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:07:06.124
May  1 19:07:06.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 19:07:06.125
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:06.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:06.303
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May  1 19:07:06.318: INFO: Creating deployment "test-recreate-deployment"
May  1 19:07:06.342: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  1 19:07:06.399: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  1 19:07:08.466: INFO: Waiting deployment "test-recreate-deployment" to complete
May  1 19:07:08.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:07:10.508: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  1 19:07:10.539: INFO: Updating deployment test-recreate-deployment
May  1 19:07:10.539: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 19:07:10.860: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9762  c79f5b9e-af80-4ba2-a0e7-7086285701e5 70741 2 2023-05-01 19:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af80c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-01 19:07:10 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-01 19:07:10 +0000 UTC,LastTransitionTime:2023-05-01 19:07:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May  1 19:07:10.878: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9762  882f1845-4d2c-48a0-9c95-591d4208613c 70739 1 2023-05-01 19:07:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c79f5b9e-af80-4ba2-a0e7-7086285701e5 0xc002a48f40 0xc002a48f41}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c79f5b9e-af80-4ba2-a0e7-7086285701e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a48fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 19:07:10.878: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  1 19:07:10.879: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9762  485cb27e-c4ae-47da-b974-da3c6250ac01 70730 2 2023-05-01 19:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c79f5b9e-af80-4ba2-a0e7-7086285701e5 0xc002a48e27 0xc002a48e28}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c79f5b9e-af80-4ba2-a0e7-7086285701e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a48ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 19:07:10.916: INFO: Pod "test-recreate-deployment-9d58999df-v7bql" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-v7bql test-recreate-deployment-9d58999df- deployment-9762  a51dea65-e633-49d4-a940-e313f7c870d3 70742 0 2023-05-01 19:07:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 882f1845-4d2c-48a0-9c95-591d4208613c 0xc002a49727 0xc002a49728}] [] [{kube-controller-manager Update v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"882f1845-4d2c-48a0-9c95-591d4208613c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfqvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfqvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xclhd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:07:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 19:07:10.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9762" for this suite. 05/01/23 19:07:10.991
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":63,"skipped":1322,"failed":0}
------------------------------
• [4.890 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:07:06.124
    May  1 19:07:06.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 19:07:06.125
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:06.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:06.303
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May  1 19:07:06.318: INFO: Creating deployment "test-recreate-deployment"
    May  1 19:07:06.342: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May  1 19:07:06.399: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May  1 19:07:08.466: INFO: Waiting deployment "test-recreate-deployment" to complete
    May  1 19:07:08.483: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 7, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:07:10.508: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May  1 19:07:10.539: INFO: Updating deployment test-recreate-deployment
    May  1 19:07:10.539: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 19:07:10.860: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9762  c79f5b9e-af80-4ba2-a0e7-7086285701e5 70741 2 2023-05-01 19:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af80c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-01 19:07:10 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-05-01 19:07:10 +0000 UTC,LastTransitionTime:2023-05-01 19:07:06 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May  1 19:07:10.878: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-9762  882f1845-4d2c-48a0-9c95-591d4208613c 70739 1 2023-05-01 19:07:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c79f5b9e-af80-4ba2-a0e7-7086285701e5 0xc002a48f40 0xc002a48f41}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c79f5b9e-af80-4ba2-a0e7-7086285701e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a48fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:07:10.878: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May  1 19:07:10.879: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-9762  485cb27e-c4ae-47da-b974-da3c6250ac01 70730 2 2023-05-01 19:07:06 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c79f5b9e-af80-4ba2-a0e7-7086285701e5 0xc002a48e27 0xc002a48e28}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c79f5b9e-af80-4ba2-a0e7-7086285701e5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002a48ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:07:10.916: INFO: Pod "test-recreate-deployment-9d58999df-v7bql" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-v7bql test-recreate-deployment-9d58999df- deployment-9762  a51dea65-e633-49d4-a940-e313f7c870d3 70742 0 2023-05-01 19:07:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 882f1845-4d2c-48a0-9c95-591d4208613c 0xc002a49727 0xc002a49728}] [] [{kube-controller-manager Update v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"882f1845-4d2c-48a0-9c95-591d4208613c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:07:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfqvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfqvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-xclhd,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:07:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:07:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 19:07:10.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9762" for this suite. 05/01/23 19:07:10.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:07:11.02
May  1 19:07:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename taint-single-pod 05/01/23 19:07:11.023
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:11.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:11.158
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
May  1 19:07:11.173: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 19:08:11.558: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
May  1 19:08:11.586: INFO: Starting informer...
STEP: Starting pod... 05/01/23 19:08:11.587
May  1 19:08:11.882: INFO: Pod is running on 10.45.145.124. Tainting Node
STEP: Trying to apply a taint on the Node 05/01/23 19:08:11.882
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 19:08:11.935
STEP: Waiting short time to make sure Pod is queued for deletion 05/01/23 19:08:11.952
May  1 19:08:11.952: INFO: Pod wasn't evicted. Proceeding
May  1 19:08:11.953: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 19:08:12.002
STEP: Waiting some time to make sure that toleration time passed. 05/01/23 19:08:12.281
May  1 19:09:27.387: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
May  1 19:09:27.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-163" for this suite. 05/01/23 19:09:27.408
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":64,"skipped":1342,"failed":0}
------------------------------
• [SLOW TEST] [136.417 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:07:11.02
    May  1 19:07:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename taint-single-pod 05/01/23 19:07:11.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:07:11.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:07:11.158
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    May  1 19:07:11.173: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 19:08:11.558: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    May  1 19:08:11.586: INFO: Starting informer...
    STEP: Starting pod... 05/01/23 19:08:11.587
    May  1 19:08:11.882: INFO: Pod is running on 10.45.145.124. Tainting Node
    STEP: Trying to apply a taint on the Node 05/01/23 19:08:11.882
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 19:08:11.935
    STEP: Waiting short time to make sure Pod is queued for deletion 05/01/23 19:08:11.952
    May  1 19:08:11.952: INFO: Pod wasn't evicted. Proceeding
    May  1 19:08:11.953: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 19:08:12.002
    STEP: Waiting some time to make sure that toleration time passed. 05/01/23 19:08:12.281
    May  1 19:09:27.387: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:09:27.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-163" for this suite. 05/01/23 19:09:27.408
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:09:27.437
May  1 19:09:27.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:09:27.438
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:27.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:27.541
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
May  1 19:09:27.703: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-b80b2e1b-9b2f-4aa4-b8af-a76b82916f0d 05/01/23 19:09:27.703
STEP: Creating the pod 05/01/23 19:09:27.747
May  1 19:09:27.905: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7" in namespace "configmap-7402" to be "running"
May  1 19:09:27.933: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Pending", Reason="", readiness=false. Elapsed: 27.848069ms
May  1 19:09:29.958: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052443444s
May  1 19:09:31.977: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Running", Reason="", readiness=false. Elapsed: 4.071631071s
May  1 19:09:31.977: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7" satisfied condition "running"
STEP: Waiting for pod with text data 05/01/23 19:09:31.977
STEP: Waiting for pod with binary data 05/01/23 19:09:32.097
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:09:32.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7402" for this suite. 05/01/23 19:09:32.222
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":65,"skipped":1345,"failed":0}
------------------------------
• [4.824 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:09:27.437
    May  1 19:09:27.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:09:27.438
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:27.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:27.541
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    May  1 19:09:27.703: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-b80b2e1b-9b2f-4aa4-b8af-a76b82916f0d 05/01/23 19:09:27.703
    STEP: Creating the pod 05/01/23 19:09:27.747
    May  1 19:09:27.905: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7" in namespace "configmap-7402" to be "running"
    May  1 19:09:27.933: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Pending", Reason="", readiness=false. Elapsed: 27.848069ms
    May  1 19:09:29.958: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052443444s
    May  1 19:09:31.977: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7": Phase="Running", Reason="", readiness=false. Elapsed: 4.071631071s
    May  1 19:09:31.977: INFO: Pod "pod-configmaps-7cb87f44-643c-424b-bf49-f9e7821745b7" satisfied condition "running"
    STEP: Waiting for pod with text data 05/01/23 19:09:31.977
    STEP: Waiting for pod with binary data 05/01/23 19:09:32.097
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:09:32.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7402" for this suite. 05/01/23 19:09:32.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:09:32.262
May  1 19:09:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:09:32.264
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:32.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:32.395
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 05/01/23 19:09:32.408
May  1 19:09:32.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-885 create -f -'
May  1 19:09:33.889: INFO: stderr: ""
May  1 19:09:33.889: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/01/23 19:09:33.889
May  1 19:09:34.913: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:34.913: INFO: Found 0 / 1
May  1 19:09:35.922: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:35.922: INFO: Found 0 / 1
May  1 19:09:36.915: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:36.915: INFO: Found 0 / 1
May  1 19:09:37.910: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:37.910: INFO: Found 1 / 1
May  1 19:09:37.910: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/01/23 19:09:37.91
May  1 19:09:37.926: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:37.926: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  1 19:09:37.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-885 patch pod agnhost-primary-97f5x -p {"metadata":{"annotations":{"x":"y"}}}'
May  1 19:09:38.276: INFO: stderr: ""
May  1 19:09:38.276: INFO: stdout: "pod/agnhost-primary-97f5x patched\n"
STEP: checking annotations 05/01/23 19:09:38.276
May  1 19:09:38.304: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:09:38.304: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:09:38.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-885" for this suite. 05/01/23 19:09:38.328
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":66,"skipped":1350,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:09:32.262
    May  1 19:09:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:09:32.264
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:32.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:32.395
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 05/01/23 19:09:32.408
    May  1 19:09:32.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-885 create -f -'
    May  1 19:09:33.889: INFO: stderr: ""
    May  1 19:09:33.889: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/01/23 19:09:33.889
    May  1 19:09:34.913: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:34.913: INFO: Found 0 / 1
    May  1 19:09:35.922: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:35.922: INFO: Found 0 / 1
    May  1 19:09:36.915: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:36.915: INFO: Found 0 / 1
    May  1 19:09:37.910: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:37.910: INFO: Found 1 / 1
    May  1 19:09:37.910: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/01/23 19:09:37.91
    May  1 19:09:37.926: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:37.926: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  1 19:09:37.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-885 patch pod agnhost-primary-97f5x -p {"metadata":{"annotations":{"x":"y"}}}'
    May  1 19:09:38.276: INFO: stderr: ""
    May  1 19:09:38.276: INFO: stdout: "pod/agnhost-primary-97f5x patched\n"
    STEP: checking annotations 05/01/23 19:09:38.276
    May  1 19:09:38.304: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:09:38.304: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:09:38.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-885" for this suite. 05/01/23 19:09:38.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:09:38.355
May  1 19:09:38.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:09:38.359
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:38.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:38.455
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:09:38.574
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:09:39.531
STEP: Deploying the webhook pod 05/01/23 19:09:39.584
STEP: Wait for the deployment to be ready 05/01/23 19:09:39.678
May  1 19:09:39.742: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:09:41.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:09:43.797
STEP: Verifying the service has paired with the endpoint 05/01/23 19:09:43.834
May  1 19:09:44.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/01/23 19:09:44.851
STEP: create a namespace for the webhook 05/01/23 19:09:44.926
STEP: create a configmap should be unconditionally rejected by the webhook 05/01/23 19:09:44.969
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:09:45.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2863" for this suite. 05/01/23 19:09:45.153
STEP: Destroying namespace "webhook-2863-markers" for this suite. 05/01/23 19:09:45.179
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":67,"skipped":1382,"failed":0}
------------------------------
• [SLOW TEST] [7.005 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:09:38.355
    May  1 19:09:38.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:09:38.359
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:38.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:38.455
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:09:38.574
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:09:39.531
    STEP: Deploying the webhook pod 05/01/23 19:09:39.584
    STEP: Wait for the deployment to be ready 05/01/23 19:09:39.678
    May  1 19:09:39.742: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:09:41.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 9, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:09:43.797
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:09:43.834
    May  1 19:09:44.838: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/01/23 19:09:44.851
    STEP: create a namespace for the webhook 05/01/23 19:09:44.926
    STEP: create a configmap should be unconditionally rejected by the webhook 05/01/23 19:09:44.969
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:09:45.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2863" for this suite. 05/01/23 19:09:45.153
    STEP: Destroying namespace "webhook-2863-markers" for this suite. 05/01/23 19:09:45.179
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:09:45.37
May  1 19:09:45.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:09:45.373
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:45.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:45.508
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4273 05/01/23 19:09:45.543
STEP: changing the ExternalName service to type=NodePort 05/01/23 19:09:45.573
STEP: creating replication controller externalname-service in namespace services-4273 05/01/23 19:09:45.696
I0501 19:09:45.733673      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4273, replica count: 2
I0501 19:09:48.787537      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 19:09:48.787: INFO: Creating new exec pod
May  1 19:09:48.863: INFO: Waiting up to 5m0s for pod "execpoddcprv" in namespace "services-4273" to be "running"
May  1 19:09:48.879: INFO: Pod "execpoddcprv": Phase="Pending", Reason="", readiness=false. Elapsed: 15.923557ms
May  1 19:09:50.922: INFO: Pod "execpoddcprv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059158869s
May  1 19:09:52.898: INFO: Pod "execpoddcprv": Phase="Running", Reason="", readiness=true. Elapsed: 4.035064726s
May  1 19:09:52.898: INFO: Pod "execpoddcprv" satisfied condition "running"
May  1 19:09:53.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  1 19:09:54.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  1 19:09:54.372: INFO: stdout: "externalname-service-jqbsh"
May  1 19:09:54.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.147.8 80'
May  1 19:09:54.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.147.8 80\nConnection to 172.21.147.8 80 port [tcp/http] succeeded!\n"
May  1 19:09:54.871: INFO: stdout: "externalname-service-4gfz9"
May  1 19:09:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 31471'
May  1 19:09:55.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 31471\nConnection to 10.45.145.71 31471 port [tcp/*] succeeded!\n"
May  1 19:09:55.306: INFO: stdout: "externalname-service-jqbsh"
May  1 19:09:55.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
May  1 19:09:55.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
May  1 19:09:55.951: INFO: stdout: ""
May  1 19:09:56.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
May  1 19:09:57.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
May  1 19:09:57.650: INFO: stdout: ""
May  1 19:09:57.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
May  1 19:09:59.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
May  1 19:09:59.010: INFO: stdout: "externalname-service-4gfz9"
May  1 19:09:59.010: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:09:59.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4273" for this suite. 05/01/23 19:09:59.13
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":68,"skipped":1387,"failed":0}
------------------------------
• [SLOW TEST] [13.787 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:09:45.37
    May  1 19:09:45.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:09:45.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:45.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:45.508
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4273 05/01/23 19:09:45.543
    STEP: changing the ExternalName service to type=NodePort 05/01/23 19:09:45.573
    STEP: creating replication controller externalname-service in namespace services-4273 05/01/23 19:09:45.696
    I0501 19:09:45.733673      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4273, replica count: 2
    I0501 19:09:48.787537      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 19:09:48.787: INFO: Creating new exec pod
    May  1 19:09:48.863: INFO: Waiting up to 5m0s for pod "execpoddcprv" in namespace "services-4273" to be "running"
    May  1 19:09:48.879: INFO: Pod "execpoddcprv": Phase="Pending", Reason="", readiness=false. Elapsed: 15.923557ms
    May  1 19:09:50.922: INFO: Pod "execpoddcprv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059158869s
    May  1 19:09:52.898: INFO: Pod "execpoddcprv": Phase="Running", Reason="", readiness=true. Elapsed: 4.035064726s
    May  1 19:09:52.898: INFO: Pod "execpoddcprv" satisfied condition "running"
    May  1 19:09:53.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  1 19:09:54.372: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  1 19:09:54.372: INFO: stdout: "externalname-service-jqbsh"
    May  1 19:09:54.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.147.8 80'
    May  1 19:09:54.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.147.8 80\nConnection to 172.21.147.8 80 port [tcp/http] succeeded!\n"
    May  1 19:09:54.871: INFO: stdout: "externalname-service-4gfz9"
    May  1 19:09:54.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 31471'
    May  1 19:09:55.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 31471\nConnection to 10.45.145.71 31471 port [tcp/*] succeeded!\n"
    May  1 19:09:55.306: INFO: stdout: "externalname-service-jqbsh"
    May  1 19:09:55.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
    May  1 19:09:55.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
    May  1 19:09:55.951: INFO: stdout: ""
    May  1 19:09:56.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
    May  1 19:09:57.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
    May  1 19:09:57.650: INFO: stdout: ""
    May  1 19:09:57.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-4273 exec execpoddcprv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31471'
    May  1 19:09:59.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31471\nConnection to 10.45.145.126 31471 port [tcp/*] succeeded!\n"
    May  1 19:09:59.010: INFO: stdout: "externalname-service-4gfz9"
    May  1 19:09:59.010: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:09:59.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4273" for this suite. 05/01/23 19:09:59.13
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:09:59.163
May  1 19:09:59.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename namespaces 05/01/23 19:09:59.167
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:59.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:59.295
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 05/01/23 19:09:59.31
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:59.41
STEP: Creating a pod in the namespace 05/01/23 19:09:59.427
W0501 19:09:59.516400      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for the pod to have running status 05/01/23 19:09:59.516
May  1 19:09:59.516: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8261" to be "running"
May  1 19:09:59.567: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 50.490056ms
May  1 19:10:01.584: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.067888102s
May  1 19:10:01.584: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/01/23 19:10:01.584
STEP: Waiting for the namespace to be removed. 05/01/23 19:10:01.611
STEP: Recreating the namespace 05/01/23 19:10:19.624
STEP: Verifying there are no pods in the namespace 05/01/23 19:10:19.72
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  1 19:10:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3166" for this suite. 05/01/23 19:10:19.821
STEP: Destroying namespace "nsdeletetest-8261" for this suite. 05/01/23 19:10:19.851
May  1 19:10:19.864: INFO: Namespace nsdeletetest-8261 was already deleted
STEP: Destroying namespace "nsdeletetest-9124" for this suite. 05/01/23 19:10:19.864
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":69,"skipped":1394,"failed":0}
------------------------------
• [SLOW TEST] [20.721 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:09:59.163
    May  1 19:09:59.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename namespaces 05/01/23 19:09:59.167
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:59.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:09:59.295
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 05/01/23 19:09:59.31
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:09:59.41
    STEP: Creating a pod in the namespace 05/01/23 19:09:59.427
    W0501 19:09:59.516400      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for the pod to have running status 05/01/23 19:09:59.516
    May  1 19:09:59.516: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8261" to be "running"
    May  1 19:09:59.567: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 50.490056ms
    May  1 19:10:01.584: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.067888102s
    May  1 19:10:01.584: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/01/23 19:10:01.584
    STEP: Waiting for the namespace to be removed. 05/01/23 19:10:01.611
    STEP: Recreating the namespace 05/01/23 19:10:19.624
    STEP: Verifying there are no pods in the namespace 05/01/23 19:10:19.72
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:10:19.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3166" for this suite. 05/01/23 19:10:19.821
    STEP: Destroying namespace "nsdeletetest-8261" for this suite. 05/01/23 19:10:19.851
    May  1 19:10:19.864: INFO: Namespace nsdeletetest-8261 was already deleted
    STEP: Destroying namespace "nsdeletetest-9124" for this suite. 05/01/23 19:10:19.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:19.889
May  1 19:10:19.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 19:10:19.891
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:19.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:19.968
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/01/23 19:10:19.982
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/01/23 19:10:19.988
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/01/23 19:10:19.989
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/01/23 19:10:19.989
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/01/23 19:10:19.996
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/01/23 19:10:19.996
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/01/23 19:10:20.001
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:10:20.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9967" for this suite. 05/01/23 19:10:20.019
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":70,"skipped":1412,"failed":0}
------------------------------
• [0.158 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:19.889
    May  1 19:10:19.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 19:10:19.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:19.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:19.968
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/01/23 19:10:19.982
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/01/23 19:10:19.988
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/01/23 19:10:19.989
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/01/23 19:10:19.989
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/01/23 19:10:19.996
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/01/23 19:10:19.996
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/01/23 19:10:20.001
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:10:20.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9967" for this suite. 05/01/23 19:10:20.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:20.049
May  1 19:10:20.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:10:20.052
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:20.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:20.146
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:10:20.241
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:10:21.684
STEP: Deploying the webhook pod 05/01/23 19:10:21.729
STEP: Wait for the deployment to be ready 05/01/23 19:10:21.769
May  1 19:10:21.798: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:10:23.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:10:25.875
STEP: Verifying the service has paired with the endpoint 05/01/23 19:10:25.994
May  1 19:10:26.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 05/01/23 19:10:27.016
STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.111
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/01/23 19:10:27.152
STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.229
STEP: Patching a validating webhook configuration's rules to include the create operation 05/01/23 19:10:27.279
STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.344
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:10:27.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-216" for this suite. 05/01/23 19:10:27.468
STEP: Destroying namespace "webhook-216-markers" for this suite. 05/01/23 19:10:27.501
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":71,"skipped":1426,"failed":0}
------------------------------
• [SLOW TEST] [7.656 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:20.049
    May  1 19:10:20.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:10:20.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:20.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:20.146
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:10:20.241
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:10:21.684
    STEP: Deploying the webhook pod 05/01/23 19:10:21.729
    STEP: Wait for the deployment to be ready 05/01/23 19:10:21.769
    May  1 19:10:21.798: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:10:23.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:10:25.875
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:10:25.994
    May  1 19:10:26.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 05/01/23 19:10:27.016
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.111
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/01/23 19:10:27.152
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.229
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/01/23 19:10:27.279
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:10:27.344
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:10:27.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-216" for this suite. 05/01/23 19:10:27.468
    STEP: Destroying namespace "webhook-216-markers" for this suite. 05/01/23 19:10:27.501
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:27.705
May  1 19:10:27.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:10:27.707
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:27.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:27.833
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:10:27.978
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:10:29.077
STEP: Deploying the webhook pod 05/01/23 19:10:29.121
STEP: Wait for the deployment to be ready 05/01/23 19:10:29.161
May  1 19:10:29.219: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:10:31.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:10:33.318
STEP: Verifying the service has paired with the endpoint 05/01/23 19:10:33.37
May  1 19:10:34.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/01/23 19:10:34.397
STEP: create a pod that should be updated by the webhook 05/01/23 19:10:34.46
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:10:34.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6846" for this suite. 05/01/23 19:10:34.762
STEP: Destroying namespace "webhook-6846-markers" for this suite. 05/01/23 19:10:34.786
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":72,"skipped":1427,"failed":0}
------------------------------
• [SLOW TEST] [7.241 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:27.705
    May  1 19:10:27.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:10:27.707
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:27.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:27.833
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:10:27.978
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:10:29.077
    STEP: Deploying the webhook pod 05/01/23 19:10:29.121
    STEP: Wait for the deployment to be ready 05/01/23 19:10:29.161
    May  1 19:10:29.219: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:10:31.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 10, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:10:33.318
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:10:33.37
    May  1 19:10:34.374: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/01/23 19:10:34.397
    STEP: create a pod that should be updated by the webhook 05/01/23 19:10:34.46
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:10:34.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6846" for this suite. 05/01/23 19:10:34.762
    STEP: Destroying namespace "webhook-6846-markers" for this suite. 05/01/23 19:10:34.786
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:34.947
May  1 19:10:34.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replication-controller 05/01/23 19:10:34.95
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:35.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:35.092
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 05/01/23 19:10:35.113
STEP: When the matched label of one of its pods change 05/01/23 19:10:35.146
May  1 19:10:35.167: INFO: Pod name pod-release: Found 0 pods out of 1
May  1 19:10:40.186: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/01/23 19:10:40.287
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
May  1 19:10:41.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-26" for this suite. 05/01/23 19:10:41.421
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":73,"skipped":1427,"failed":0}
------------------------------
• [SLOW TEST] [6.526 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:34.947
    May  1 19:10:34.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replication-controller 05/01/23 19:10:34.95
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:35.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:35.092
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 05/01/23 19:10:35.113
    STEP: When the matched label of one of its pods change 05/01/23 19:10:35.146
    May  1 19:10:35.167: INFO: Pod name pod-release: Found 0 pods out of 1
    May  1 19:10:40.186: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/01/23 19:10:40.287
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    May  1 19:10:41.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-26" for this suite. 05/01/23 19:10:41.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:41.48
May  1 19:10:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:10:41.483
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:41.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:41.597
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-081af8ee-5184-43e5-b0db-5308032da1fe 05/01/23 19:10:41.612
STEP: Creating a pod to test consume secrets 05/01/23 19:10:41.646
May  1 19:10:41.759: INFO: Waiting up to 5m0s for pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9" in namespace "secrets-4664" to be "Succeeded or Failed"
May  1 19:10:41.785: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.316933ms
May  1 19:10:43.812: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052866322s
May  1 19:10:45.809: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049581765s
May  1 19:10:47.807: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048242434s
May  1 19:10:49.804: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045105529s
STEP: Saw pod success 05/01/23 19:10:49.804
May  1 19:10:49.805: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9" satisfied condition "Succeeded or Failed"
May  1 19:10:49.826: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 container secret-env-test: <nil>
STEP: delete the pod 05/01/23 19:10:49.882
May  1 19:10:49.933: INFO: Waiting for pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 to disappear
May  1 19:10:49.950: INFO: Pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  1 19:10:49.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4664" for this suite. 05/01/23 19:10:49.975
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":74,"skipped":1445,"failed":0}
------------------------------
• [SLOW TEST] [8.517 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:41.48
    May  1 19:10:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:10:41.483
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:41.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:41.597
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-081af8ee-5184-43e5-b0db-5308032da1fe 05/01/23 19:10:41.612
    STEP: Creating a pod to test consume secrets 05/01/23 19:10:41.646
    May  1 19:10:41.759: INFO: Waiting up to 5m0s for pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9" in namespace "secrets-4664" to be "Succeeded or Failed"
    May  1 19:10:41.785: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.316933ms
    May  1 19:10:43.812: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052866322s
    May  1 19:10:45.809: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049581765s
    May  1 19:10:47.807: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048242434s
    May  1 19:10:49.804: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045105529s
    STEP: Saw pod success 05/01/23 19:10:49.804
    May  1 19:10:49.805: INFO: Pod "pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9" satisfied condition "Succeeded or Failed"
    May  1 19:10:49.826: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 container secret-env-test: <nil>
    STEP: delete the pod 05/01/23 19:10:49.882
    May  1 19:10:49.933: INFO: Waiting for pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 to disappear
    May  1 19:10:49.950: INFO: Pod pod-secrets-143b95a6-ae61-406c-8309-ceb4f15ed5b9 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:10:49.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4664" for this suite. 05/01/23 19:10:49.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:50.004
May  1 19:10:50.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:10:50.006
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:50.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:50.095
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:10:50.113
May  1 19:10:50.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e" in namespace "projected-3843" to be "Succeeded or Failed"
May  1 19:10:50.247: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.190345ms
May  1 19:10:52.274: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046230301s
May  1 19:10:54.269: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040787769s
May  1 19:10:56.279: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050719402s
STEP: Saw pod success 05/01/23 19:10:56.279
May  1 19:10:56.279: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e" satisfied condition "Succeeded or Failed"
May  1 19:10:56.310: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e container client-container: <nil>
STEP: delete the pod 05/01/23 19:10:56.349
May  1 19:10:56.418: INFO: Waiting for pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e to disappear
May  1 19:10:56.438: INFO: Pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 19:10:56.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3843" for this suite. 05/01/23 19:10:56.468
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":75,"skipped":1456,"failed":0}
------------------------------
• [SLOW TEST] [6.491 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:50.004
    May  1 19:10:50.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:10:50.006
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:50.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:50.095
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:10:50.113
    May  1 19:10:50.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e" in namespace "projected-3843" to be "Succeeded or Failed"
    May  1 19:10:50.247: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.190345ms
    May  1 19:10:52.274: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046230301s
    May  1 19:10:54.269: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040787769s
    May  1 19:10:56.279: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050719402s
    STEP: Saw pod success 05/01/23 19:10:56.279
    May  1 19:10:56.279: INFO: Pod "downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e" satisfied condition "Succeeded or Failed"
    May  1 19:10:56.310: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e container client-container: <nil>
    STEP: delete the pod 05/01/23 19:10:56.349
    May  1 19:10:56.418: INFO: Waiting for pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e to disappear
    May  1 19:10:56.438: INFO: Pod downwardapi-volume-c26d19bc-0eaf-4cc4-bf71-1fa348d3991e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 19:10:56.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3843" for this suite. 05/01/23 19:10:56.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:10:56.498
May  1 19:10:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:10:56.501
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:56.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:56.58
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 05/01/23 19:10:56.594
May  1 19:10:56.698: INFO: Waiting up to 5m0s for pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90" in namespace "downward-api-896" to be "Succeeded or Failed"
May  1 19:10:56.720: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 21.084589ms
May  1 19:10:58.754: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055817511s
May  1 19:11:00.738: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039526527s
May  1 19:11:02.750: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051247502s
STEP: Saw pod success 05/01/23 19:11:02.75
May  1 19:11:02.750: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90" satisfied condition "Succeeded or Failed"
May  1 19:11:02.769: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 container dapi-container: <nil>
STEP: delete the pod 05/01/23 19:11:02.815
May  1 19:11:02.866: INFO: Waiting for pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 to disappear
May  1 19:11:02.887: INFO: Pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  1 19:11:02.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-896" for this suite. 05/01/23 19:11:02.906
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":76,"skipped":1461,"failed":0}
------------------------------
• [SLOW TEST] [6.430 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:10:56.498
    May  1 19:10:56.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:10:56.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:10:56.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:10:56.58
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 05/01/23 19:10:56.594
    May  1 19:10:56.698: INFO: Waiting up to 5m0s for pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90" in namespace "downward-api-896" to be "Succeeded or Failed"
    May  1 19:10:56.720: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 21.084589ms
    May  1 19:10:58.754: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055817511s
    May  1 19:11:00.738: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039526527s
    May  1 19:11:02.750: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051247502s
    STEP: Saw pod success 05/01/23 19:11:02.75
    May  1 19:11:02.750: INFO: Pod "downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90" satisfied condition "Succeeded or Failed"
    May  1 19:11:02.769: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 container dapi-container: <nil>
    STEP: delete the pod 05/01/23 19:11:02.815
    May  1 19:11:02.866: INFO: Waiting for pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 to disappear
    May  1 19:11:02.887: INFO: Pod downward-api-4627dd00-80d9-4f18-b4b9-95fe656b0f90 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  1 19:11:02.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-896" for this suite. 05/01/23 19:11:02.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:02.936
May  1 19:11:02.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 19:11:02.939
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:03.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:03.033
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/01/23 19:11:03.047
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/01/23 19:11:03.047
STEP: creating a pod to probe DNS 05/01/23 19:11:03.047
STEP: submitting the pod to kubernetes 05/01/23 19:11:03.048
May  1 19:11:03.186: INFO: Waiting up to 15m0s for pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566" in namespace "dns-6510" to be "running"
May  1 19:11:03.216: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 29.25128ms
May  1 19:11:05.239: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052345705s
May  1 19:11:07.237: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051204209s
May  1 19:11:09.236: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049345163s
May  1 19:11:11.242: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056191162s
May  1 19:11:13.235: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048240174s
May  1 19:11:15.240: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053360274s
May  1 19:11:17.233: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04657585s
May  1 19:11:19.247: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Running", Reason="", readiness=true. Elapsed: 16.060914743s
May  1 19:11:19.247: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566" satisfied condition "running"
STEP: retrieving the pod 05/01/23 19:11:19.247
STEP: looking for the results for each expected name from probers 05/01/23 19:11:19.266
May  1 19:11:19.461: INFO: DNS probes using dns-6510/dns-test-501ee746-9661-4e06-9ee7-c908b57aa566 succeeded

STEP: deleting the pod 05/01/23 19:11:19.461
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 19:11:19.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6510" for this suite. 05/01/23 19:11:19.586
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":77,"skipped":1492,"failed":0}
------------------------------
• [SLOW TEST] [16.674 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:02.936
    May  1 19:11:02.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 19:11:02.939
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:03.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:03.033
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/01/23 19:11:03.047
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/01/23 19:11:03.047
    STEP: creating a pod to probe DNS 05/01/23 19:11:03.047
    STEP: submitting the pod to kubernetes 05/01/23 19:11:03.048
    May  1 19:11:03.186: INFO: Waiting up to 15m0s for pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566" in namespace "dns-6510" to be "running"
    May  1 19:11:03.216: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 29.25128ms
    May  1 19:11:05.239: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052345705s
    May  1 19:11:07.237: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051204209s
    May  1 19:11:09.236: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049345163s
    May  1 19:11:11.242: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056191162s
    May  1 19:11:13.235: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 10.048240174s
    May  1 19:11:15.240: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053360274s
    May  1 19:11:17.233: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04657585s
    May  1 19:11:19.247: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566": Phase="Running", Reason="", readiness=true. Elapsed: 16.060914743s
    May  1 19:11:19.247: INFO: Pod "dns-test-501ee746-9661-4e06-9ee7-c908b57aa566" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 19:11:19.247
    STEP: looking for the results for each expected name from probers 05/01/23 19:11:19.266
    May  1 19:11:19.461: INFO: DNS probes using dns-6510/dns-test-501ee746-9661-4e06-9ee7-c908b57aa566 succeeded

    STEP: deleting the pod 05/01/23 19:11:19.461
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 19:11:19.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6510" for this suite. 05/01/23 19:11:19.586
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:19.612
May  1 19:11:19.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:11:19.614
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:19.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:19.723
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  1 19:11:23.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8106" for this suite. 05/01/23 19:11:23.981
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":78,"skipped":1496,"failed":0}
------------------------------
• [4.439 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:19.612
    May  1 19:11:19.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:11:19.614
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:19.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:19.723
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  1 19:11:23.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8106" for this suite. 05/01/23 19:11:23.981
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:24.056
May  1 19:11:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:11:24.063
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:24.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:24.159
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/01/23 19:11:24.193
STEP: submitting the pod to kubernetes 05/01/23 19:11:24.194
STEP: verifying QOS class is set on the pod 05/01/23 19:11:24.331
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
May  1 19:11:24.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7108" for this suite. 05/01/23 19:11:24.441
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":79,"skipped":1512,"failed":0}
------------------------------
• [0.448 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:24.056
    May  1 19:11:24.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:11:24.063
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:24.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:24.159
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/01/23 19:11:24.193
    STEP: submitting the pod to kubernetes 05/01/23 19:11:24.194
    STEP: verifying QOS class is set on the pod 05/01/23 19:11:24.331
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    May  1 19:11:24.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7108" for this suite. 05/01/23 19:11:24.441
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:24.505
May  1 19:11:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:11:24.507
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:24.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:24.68
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  1 19:11:24.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8960" for this suite. 05/01/23 19:11:24.898
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":80,"skipped":1515,"failed":0}
------------------------------
• [0.464 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:24.505
    May  1 19:11:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:11:24.507
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:24.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:24.68
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  1 19:11:24.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8960" for this suite. 05/01/23 19:11:24.898
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:24.97
May  1 19:11:24.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:11:24.976
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:25.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:25.205
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 05/01/23 19:11:25.242
May  1 19:11:25.383: INFO: Waiting up to 5m0s for pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6" in namespace "downward-api-3835" to be "Succeeded or Failed"
May  1 19:11:25.411: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.48705ms
May  1 19:11:27.433: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049789769s
May  1 19:11:29.448: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065086713s
May  1 19:11:31.436: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053284509s
May  1 19:11:33.452: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069099647s
STEP: Saw pod success 05/01/23 19:11:33.452
May  1 19:11:33.453: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6" satisfied condition "Succeeded or Failed"
May  1 19:11:33.493: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 container dapi-container: <nil>
STEP: delete the pod 05/01/23 19:11:33.619
May  1 19:11:33.746: INFO: Waiting for pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 to disappear
May  1 19:11:33.777: INFO: Pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  1 19:11:33.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3835" for this suite. 05/01/23 19:11:33.801
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":81,"skipped":1516,"failed":0}
------------------------------
• [SLOW TEST] [8.862 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:24.97
    May  1 19:11:24.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:11:24.976
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:25.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:25.205
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 05/01/23 19:11:25.242
    May  1 19:11:25.383: INFO: Waiting up to 5m0s for pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6" in namespace "downward-api-3835" to be "Succeeded or Failed"
    May  1 19:11:25.411: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.48705ms
    May  1 19:11:27.433: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049789769s
    May  1 19:11:29.448: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065086713s
    May  1 19:11:31.436: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053284509s
    May  1 19:11:33.452: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069099647s
    STEP: Saw pod success 05/01/23 19:11:33.452
    May  1 19:11:33.453: INFO: Pod "downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6" satisfied condition "Succeeded or Failed"
    May  1 19:11:33.493: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 container dapi-container: <nil>
    STEP: delete the pod 05/01/23 19:11:33.619
    May  1 19:11:33.746: INFO: Waiting for pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 to disappear
    May  1 19:11:33.777: INFO: Pod downward-api-672f38a1-1e4e-417c-bbe0-5f7555c437a6 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  1 19:11:33.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3835" for this suite. 05/01/23 19:11:33.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:33.844
May  1 19:11:33.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:11:33.846
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:33.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:33.987
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 05/01/23 19:11:34.009
May  1 19:11:34.128: INFO: Waiting up to 5m0s for pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f" in namespace "emptydir-3399" to be "Succeeded or Failed"
May  1 19:11:34.158: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 29.514811ms
May  1 19:11:36.188: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060282296s
May  1 19:11:38.214: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086264389s
May  1 19:11:40.177: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048972683s
STEP: Saw pod success 05/01/23 19:11:40.177
May  1 19:11:40.177: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f" satisfied condition "Succeeded or Failed"
May  1 19:11:40.202: INFO: Trying to get logs from node 10.45.145.124 pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f container test-container: <nil>
STEP: delete the pod 05/01/23 19:11:40.246
May  1 19:11:40.300: INFO: Waiting for pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f to disappear
May  1 19:11:40.329: INFO: Pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:11:40.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3399" for this suite. 05/01/23 19:11:40.348
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":82,"skipped":1550,"failed":0}
------------------------------
• [SLOW TEST] [6.524 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:33.844
    May  1 19:11:33.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:11:33.846
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:33.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:33.987
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/01/23 19:11:34.009
    May  1 19:11:34.128: INFO: Waiting up to 5m0s for pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f" in namespace "emptydir-3399" to be "Succeeded or Failed"
    May  1 19:11:34.158: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 29.514811ms
    May  1 19:11:36.188: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060282296s
    May  1 19:11:38.214: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.086264389s
    May  1 19:11:40.177: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048972683s
    STEP: Saw pod success 05/01/23 19:11:40.177
    May  1 19:11:40.177: INFO: Pod "pod-82ec5302-6291-45e7-99b4-79a7f31b035f" satisfied condition "Succeeded or Failed"
    May  1 19:11:40.202: INFO: Trying to get logs from node 10.45.145.124 pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f container test-container: <nil>
    STEP: delete the pod 05/01/23 19:11:40.246
    May  1 19:11:40.300: INFO: Waiting for pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f to disappear
    May  1 19:11:40.329: INFO: Pod pod-82ec5302-6291-45e7-99b4-79a7f31b035f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:11:40.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3399" for this suite. 05/01/23 19:11:40.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:11:40.374
May  1 19:11:40.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:11:40.376
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:40.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:40.511
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
May  1 19:11:40.610: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f8dcbbcb-d61d-4c0e-882c-8c739841dd35 05/01/23 19:11:40.61
STEP: Creating the pod 05/01/23 19:11:40.643
May  1 19:11:40.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab" in namespace "projected-8790" to be "running and ready"
May  1 19:11:40.851: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Pending", Reason="", readiness=false. Elapsed: 58.739748ms
May  1 19:11:40.851: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Pending, waiting for it to be Running (with Ready = true)
May  1 19:11:42.868: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076140678s
May  1 19:11:42.868: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Pending, waiting for it to be Running (with Ready = true)
May  1 19:11:44.870: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Running", Reason="", readiness=true. Elapsed: 4.077545074s
May  1 19:11:44.870: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Running (Ready = true)
May  1 19:11:44.870: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-f8dcbbcb-d61d-4c0e-882c-8c739841dd35 05/01/23 19:11:44.939
STEP: waiting to observe update in volume 05/01/23 19:11:44.983
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 19:12:57.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8790" for this suite. 05/01/23 19:12:57.797
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":83,"skipped":1568,"failed":0}
------------------------------
• [SLOW TEST] [77.463 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:11:40.374
    May  1 19:11:40.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:11:40.376
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:11:40.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:11:40.511
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    May  1 19:11:40.610: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-f8dcbbcb-d61d-4c0e-882c-8c739841dd35 05/01/23 19:11:40.61
    STEP: Creating the pod 05/01/23 19:11:40.643
    May  1 19:11:40.792: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab" in namespace "projected-8790" to be "running and ready"
    May  1 19:11:40.851: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Pending", Reason="", readiness=false. Elapsed: 58.739748ms
    May  1 19:11:40.851: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:11:42.868: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076140678s
    May  1 19:11:42.868: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:11:44.870: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab": Phase="Running", Reason="", readiness=true. Elapsed: 4.077545074s
    May  1 19:11:44.870: INFO: The phase of Pod pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab is Running (Ready = true)
    May  1 19:11:44.870: INFO: Pod "pod-projected-configmaps-1bc31742-4d96-44f8-a2c8-88b16c1a81ab" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-f8dcbbcb-d61d-4c0e-882c-8c739841dd35 05/01/23 19:11:44.939
    STEP: waiting to observe update in volume 05/01/23 19:11:44.983
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 19:12:57.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8790" for this suite. 05/01/23 19:12:57.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:12:57.842
May  1 19:12:57.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 19:12:57.848
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:12:57.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:12:57.992
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 05/01/23 19:12:58.008
STEP: Ensure pods equal to paralellism count is attached to the job 05/01/23 19:12:58.031
STEP: patching /status 05/01/23 19:13:02.053
STEP: updating /status 05/01/23 19:13:02.088
STEP: get /status 05/01/23 19:13:02.126
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 19:13:02.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4799" for this suite. 05/01/23 19:13:02.165
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":84,"skipped":1582,"failed":0}
------------------------------
• [4.362 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:12:57.842
    May  1 19:12:57.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 19:12:57.848
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:12:57.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:12:57.992
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 05/01/23 19:12:58.008
    STEP: Ensure pods equal to paralellism count is attached to the job 05/01/23 19:12:58.031
    STEP: patching /status 05/01/23 19:13:02.053
    STEP: updating /status 05/01/23 19:13:02.088
    STEP: get /status 05/01/23 19:13:02.126
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 19:13:02.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4799" for this suite. 05/01/23 19:13:02.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:13:02.207
May  1 19:13:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svc-latency 05/01/23 19:13:02.209
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:13:02.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:13:02.299
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May  1 19:13:02.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1343 05/01/23 19:13:02.332
I0501 19:13:02.358907      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1343, replica count: 1
I0501 19:13:03.410418      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 19:13:04.410898      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 19:13:05.411519      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 19:13:05.556: INFO: Created: latency-svc-xsfsc
May  1 19:13:05.583: INFO: Got endpoints: latency-svc-xsfsc [70.896567ms]
May  1 19:13:05.655: INFO: Created: latency-svc-fk7hj
May  1 19:13:05.668: INFO: Got endpoints: latency-svc-fk7hj [81.450481ms]
May  1 19:13:05.678: INFO: Created: latency-svc-k2hnl
May  1 19:13:05.691: INFO: Got endpoints: latency-svc-k2hnl [105.647401ms]
May  1 19:13:05.704: INFO: Created: latency-svc-4x2x7
May  1 19:13:05.722: INFO: Got endpoints: latency-svc-4x2x7 [136.317637ms]
May  1 19:13:05.725: INFO: Created: latency-svc-drwcz
May  1 19:13:05.746: INFO: Got endpoints: latency-svc-drwcz [159.287343ms]
May  1 19:13:05.757: INFO: Created: latency-svc-rfnzb
May  1 19:13:05.779: INFO: Got endpoints: latency-svc-rfnzb [189.439043ms]
May  1 19:13:05.790: INFO: Created: latency-svc-8984p
May  1 19:13:05.809: INFO: Got endpoints: latency-svc-8984p [222.589378ms]
May  1 19:13:05.819: INFO: Created: latency-svc-n4xps
May  1 19:13:05.868: INFO: Got endpoints: latency-svc-n4xps [280.844564ms]
May  1 19:13:05.871: INFO: Created: latency-svc-2rdxf
May  1 19:13:05.873: INFO: Got endpoints: latency-svc-2rdxf [285.058157ms]
May  1 19:13:05.901: INFO: Created: latency-svc-s2vpb
May  1 19:13:05.913: INFO: Created: latency-svc-lx9zd
May  1 19:13:05.947: INFO: Created: latency-svc-xrldr
May  1 19:13:05.958: INFO: Created: latency-svc-4b7mf
May  1 19:13:06.012: INFO: Got endpoints: latency-svc-s2vpb [423.538512ms]
May  1 19:13:06.013: INFO: Got endpoints: latency-svc-4b7mf [421.782245ms]
May  1 19:13:06.013: INFO: Got endpoints: latency-svc-lx9zd [423.186119ms]
May  1 19:13:06.055: INFO: Got endpoints: latency-svc-xrldr [464.644023ms]
May  1 19:13:06.097: INFO: Created: latency-svc-c4rdx
May  1 19:13:06.098: INFO: Got endpoints: latency-svc-c4rdx [506.200169ms]
May  1 19:13:06.098: INFO: Created: latency-svc-xzncj
May  1 19:13:06.114: INFO: Created: latency-svc-vhlzl
May  1 19:13:06.114: INFO: Got endpoints: latency-svc-vhlzl [522.134401ms]
May  1 19:13:06.116: INFO: Created: latency-svc-6xp2p
May  1 19:13:06.116: INFO: Got endpoints: latency-svc-6xp2p [525.847283ms]
May  1 19:13:06.136: INFO: Created: latency-svc-4rpnp
May  1 19:13:06.137: INFO: Got endpoints: latency-svc-xzncj [469.101204ms]
May  1 19:13:06.157: INFO: Got endpoints: latency-svc-4rpnp [465.613563ms]
May  1 19:13:06.159: INFO: Created: latency-svc-mn97z
May  1 19:13:06.223: INFO: Created: latency-svc-cz4qw
May  1 19:13:06.226: INFO: Created: latency-svc-4dw8c
May  1 19:13:06.233: INFO: Got endpoints: latency-svc-4dw8c [453.806188ms]
May  1 19:13:06.234: INFO: Got endpoints: latency-svc-cz4qw [487.831346ms]
May  1 19:13:06.235: INFO: Got endpoints: latency-svc-mn97z [512.430195ms]
May  1 19:13:06.246: INFO: Created: latency-svc-5nh65
May  1 19:13:06.246: INFO: Created: latency-svc-kkcdw
May  1 19:13:06.246: INFO: Got endpoints: latency-svc-kkcdw [436.817876ms]
May  1 19:13:06.261: INFO: Got endpoints: latency-svc-5nh65 [392.712693ms]
May  1 19:13:06.298: INFO: Created: latency-svc-hkn5q
May  1 19:13:06.334: INFO: Got endpoints: latency-svc-hkn5q [98.164628ms]
May  1 19:13:06.338: INFO: Created: latency-svc-dxxwc
May  1 19:13:06.367: INFO: Got endpoints: latency-svc-dxxwc [353.325518ms]
May  1 19:13:06.371: INFO: Created: latency-svc-2926j
May  1 19:13:06.396: INFO: Got endpoints: latency-svc-2926j [382.384123ms]
May  1 19:13:06.404: INFO: Created: latency-svc-sw7v6
May  1 19:13:06.434: INFO: Got endpoints: latency-svc-sw7v6 [378.792533ms]
May  1 19:13:06.441: INFO: Created: latency-svc-jxg8c
May  1 19:13:06.452: INFO: Got endpoints: latency-svc-jxg8c [353.383759ms]
May  1 19:13:06.458: INFO: Created: latency-svc-q6sct
May  1 19:13:06.488: INFO: Got endpoints: latency-svc-q6sct [374.425375ms]
May  1 19:13:06.490: INFO: Created: latency-svc-29t6n
May  1 19:13:06.500: INFO: Got endpoints: latency-svc-29t6n [384.284939ms]
May  1 19:13:06.507: INFO: Created: latency-svc-pbm6h
May  1 19:13:06.523: INFO: Got endpoints: latency-svc-pbm6h [385.819438ms]
May  1 19:13:06.538: INFO: Created: latency-svc-gmv9d
May  1 19:13:06.553: INFO: Got endpoints: latency-svc-gmv9d [396.141557ms]
May  1 19:13:06.565: INFO: Created: latency-svc-qmcpk
May  1 19:13:06.589: INFO: Got endpoints: latency-svc-qmcpk [355.969617ms]
May  1 19:13:06.601: INFO: Created: latency-svc-pvlhc
May  1 19:13:06.606: INFO: Created: latency-svc-t2zmx
May  1 19:13:06.636: INFO: Created: latency-svc-2f7k6
May  1 19:13:06.638: INFO: Got endpoints: latency-svc-t2zmx [764.661643ms]
May  1 19:13:06.639: INFO: Got endpoints: latency-svc-pvlhc [404.100572ms]
May  1 19:13:06.672: INFO: Got endpoints: latency-svc-2f7k6 [659.645272ms]
May  1 19:13:06.677: INFO: Created: latency-svc-v9kl4
May  1 19:13:06.691: INFO: Got endpoints: latency-svc-v9kl4 [441.840337ms]
May  1 19:13:06.696: INFO: Created: latency-svc-4sdds
May  1 19:13:06.720: INFO: Got endpoints: latency-svc-4sdds [458.412908ms]
May  1 19:13:06.725: INFO: Created: latency-svc-sxzb6
May  1 19:13:06.764: INFO: Got endpoints: latency-svc-sxzb6 [426.924351ms]
May  1 19:13:06.764: INFO: Created: latency-svc-8t88d
May  1 19:13:06.787: INFO: Got endpoints: latency-svc-8t88d [419.588593ms]
May  1 19:13:06.818: INFO: Created: latency-svc-89x8q
May  1 19:13:06.848: INFO: Got endpoints: latency-svc-89x8q [452.323477ms]
May  1 19:13:06.851: INFO: Created: latency-svc-65zk7
May  1 19:13:06.946: INFO: Got endpoints: latency-svc-65zk7 [511.549029ms]
May  1 19:13:07.007: INFO: Created: latency-svc-t6d7k
May  1 19:13:07.064: INFO: Got endpoints: latency-svc-t6d7k [611.410528ms]
May  1 19:13:07.087: INFO: Created: latency-svc-mrw6r
May  1 19:13:07.157: INFO: Got endpoints: latency-svc-mrw6r [669.083581ms]
May  1 19:13:07.206: INFO: Created: latency-svc-8d2v4
May  1 19:13:07.306: INFO: Got endpoints: latency-svc-8d2v4 [805.4082ms]
May  1 19:13:07.311: INFO: Created: latency-svc-k9k79
May  1 19:13:07.354: INFO: Created: latency-svc-zs598
May  1 19:13:07.363: INFO: Got endpoints: latency-svc-k9k79 [839.921978ms]
May  1 19:13:07.398: INFO: Created: latency-svc-z7qvr
May  1 19:13:07.414: INFO: Got endpoints: latency-svc-zs598 [861.162952ms]
May  1 19:13:07.421: INFO: Got endpoints: latency-svc-z7qvr [831.304949ms]
May  1 19:13:07.427: INFO: Created: latency-svc-tdn69
May  1 19:13:07.467: INFO: Created: latency-svc-r6s6h
May  1 19:13:07.494: INFO: Got endpoints: latency-svc-tdn69 [855.14651ms]
May  1 19:13:07.528: INFO: Got endpoints: latency-svc-r6s6h [889.174235ms]
May  1 19:13:07.542: INFO: Created: latency-svc-b7lmz
May  1 19:13:07.584: INFO: Created: latency-svc-z99lc
May  1 19:13:07.595: INFO: Got endpoints: latency-svc-b7lmz [922.33097ms]
May  1 19:13:07.614: INFO: Got endpoints: latency-svc-z99lc [923.198658ms]
May  1 19:13:07.615: INFO: Created: latency-svc-zsxfj
May  1 19:13:07.673: INFO: Got endpoints: latency-svc-zsxfj [952.303616ms]
May  1 19:13:07.674: INFO: Created: latency-svc-2gzgv
May  1 19:13:07.717: INFO: Got endpoints: latency-svc-2gzgv [952.815238ms]
May  1 19:13:08.319: INFO: Created: latency-svc-x4d6t
May  1 19:13:08.320: INFO: Created: latency-svc-d2w97
May  1 19:13:08.321: INFO: Created: latency-svc-gv7zr
May  1 19:13:08.335: INFO: Created: latency-svc-82f8r
May  1 19:13:08.336: INFO: Created: latency-svc-5zv9v
May  1 19:13:08.338: INFO: Created: latency-svc-q6v4b
May  1 19:13:08.339: INFO: Created: latency-svc-zwk9n
May  1 19:13:08.340: INFO: Created: latency-svc-vbqrp
May  1 19:13:08.340: INFO: Created: latency-svc-9pc77
May  1 19:13:08.341: INFO: Created: latency-svc-2wrn7
May  1 19:13:08.342: INFO: Created: latency-svc-bcvd2
May  1 19:13:08.343: INFO: Created: latency-svc-bx8qf
May  1 19:13:08.344: INFO: Created: latency-svc-f5v4z
May  1 19:13:08.344: INFO: Created: latency-svc-rl5jx
May  1 19:13:08.345: INFO: Created: latency-svc-546cp
May  1 19:13:08.393: INFO: Got endpoints: latency-svc-x4d6t [864.926985ms]
May  1 19:13:08.397: INFO: Got endpoints: latency-svc-d2w97 [982.903034ms]
May  1 19:13:08.398: INFO: Got endpoints: latency-svc-82f8r [903.615165ms]
May  1 19:13:08.399: INFO: Got endpoints: latency-svc-gv7zr [784.521415ms]
May  1 19:13:08.410: INFO: Got endpoints: latency-svc-q6v4b [815.324765ms]
May  1 19:13:08.455: INFO: Got endpoints: latency-svc-9pc77 [738.139337ms]
May  1 19:13:08.457: INFO: Got endpoints: latency-svc-bx8qf [1.035668094s]
May  1 19:13:08.459: INFO: Created: latency-svc-j82v2
May  1 19:13:08.490: INFO: Got endpoints: latency-svc-bcvd2 [1.183760692s]
May  1 19:13:08.492: INFO: Got endpoints: latency-svc-f5v4z [1.334798647s]
May  1 19:13:08.492: INFO: Got endpoints: latency-svc-5zv9v [1.128885711s]
May  1 19:13:08.502: INFO: Created: latency-svc-qdjgj
May  1 19:13:08.509: INFO: Got endpoints: latency-svc-zwk9n [1.722344183s]
May  1 19:13:08.509: INFO: Got endpoints: latency-svc-vbqrp [836.421529ms]
May  1 19:13:08.551: INFO: Created: latency-svc-544fx
May  1 19:13:08.566: INFO: Got endpoints: latency-svc-j82v2 [168.534615ms]
May  1 19:13:08.571: INFO: Got endpoints: latency-svc-2wrn7 [1.507446691s]
May  1 19:13:08.572: INFO: Got endpoints: latency-svc-qdjgj [179.194881ms]
May  1 19:13:08.574: INFO: Got endpoints: latency-svc-rl5jx [1.627603446s]
May  1 19:13:08.575: INFO: Got endpoints: latency-svc-546cp [1.726449728s]
May  1 19:13:08.612: INFO: Created: latency-svc-nnrlf
May  1 19:13:08.615: INFO: Got endpoints: latency-svc-544fx [215.560246ms]
May  1 19:13:08.632: INFO: Got endpoints: latency-svc-nnrlf [221.588717ms]
May  1 19:13:08.665: INFO: Created: latency-svc-vx7dl
May  1 19:13:08.685: INFO: Got endpoints: latency-svc-vx7dl [287.223918ms]
May  1 19:13:08.700: INFO: Created: latency-svc-tnp2q
May  1 19:13:08.745: INFO: Got endpoints: latency-svc-tnp2q [289.665423ms]
May  1 19:13:08.756: INFO: Created: latency-svc-sgbfw
May  1 19:13:08.775: INFO: Got endpoints: latency-svc-sgbfw [318.40309ms]
May  1 19:13:08.803: INFO: Created: latency-svc-c82fz
May  1 19:13:08.861: INFO: Got endpoints: latency-svc-c82fz [370.499977ms]
May  1 19:13:08.862: INFO: Created: latency-svc-rdpfb
May  1 19:13:08.908: INFO: Got endpoints: latency-svc-rdpfb [415.458224ms]
May  1 19:13:08.922: INFO: Created: latency-svc-sxczh
May  1 19:13:08.940: INFO: Got endpoints: latency-svc-sxczh [447.814768ms]
May  1 19:13:08.942: INFO: Created: latency-svc-mv69h
May  1 19:13:08.957: INFO: Created: latency-svc-67tsv
May  1 19:13:08.983: INFO: Got endpoints: latency-svc-mv69h [474.028035ms]
May  1 19:13:09.000: INFO: Created: latency-svc-ggv2p
May  1 19:13:09.024: INFO: Got endpoints: latency-svc-ggv2p [456.99199ms]
May  1 19:13:09.024: INFO: Got endpoints: latency-svc-67tsv [514.706877ms]
May  1 19:13:09.046: INFO: Created: latency-svc-8q448
May  1 19:13:09.073: INFO: Created: latency-svc-bcgt4
May  1 19:13:09.074: INFO: Got endpoints: latency-svc-8q448 [502.907983ms]
May  1 19:13:09.106: INFO: Created: latency-svc-4fh6p
May  1 19:13:09.118: INFO: Got endpoints: latency-svc-bcgt4 [545.237656ms]
May  1 19:13:09.124: INFO: Got endpoints: latency-svc-4fh6p [549.811964ms]
May  1 19:13:09.138: INFO: Created: latency-svc-7ctxn
May  1 19:13:09.160: INFO: Got endpoints: latency-svc-7ctxn [585.17091ms]
May  1 19:13:09.164: INFO: Created: latency-svc-jtb6x
May  1 19:13:09.232: INFO: Got endpoints: latency-svc-jtb6x [616.736557ms]
May  1 19:13:09.243: INFO: Created: latency-svc-2cl56
May  1 19:13:09.271: INFO: Created: latency-svc-sjltf
May  1 19:13:09.300: INFO: Got endpoints: latency-svc-2cl56 [665.684562ms]
May  1 19:13:09.343: INFO: Created: latency-svc-7j55f
May  1 19:13:09.355: INFO: Got endpoints: latency-svc-sjltf [670.097209ms]
May  1 19:13:09.395: INFO: Got endpoints: latency-svc-7j55f [649.751788ms]
May  1 19:13:09.408: INFO: Created: latency-svc-tmwbd
May  1 19:13:09.446: INFO: Got endpoints: latency-svc-tmwbd [670.219269ms]
May  1 19:13:09.528: INFO: Created: latency-svc-tnm55
May  1 19:13:09.553: INFO: Got endpoints: latency-svc-tnm55 [691.966368ms]
May  1 19:13:09.566: INFO: Created: latency-svc-hf2lr
May  1 19:13:09.591: INFO: Got endpoints: latency-svc-hf2lr [683.309259ms]
May  1 19:13:09.624: INFO: Created: latency-svc-pc2c5
May  1 19:13:09.641: INFO: Got endpoints: latency-svc-pc2c5 [700.627002ms]
May  1 19:13:09.654: INFO: Created: latency-svc-lnct9
May  1 19:13:09.684: INFO: Created: latency-svc-c5t5n
May  1 19:13:09.731: INFO: Got endpoints: latency-svc-lnct9 [747.103121ms]
May  1 19:13:09.738: INFO: Got endpoints: latency-svc-c5t5n [714.490644ms]
May  1 19:13:09.774: INFO: Created: latency-svc-z4dgj
May  1 19:13:09.835: INFO: Got endpoints: latency-svc-z4dgj [810.923958ms]
May  1 19:13:09.894: INFO: Created: latency-svc-7wjm8
May  1 19:13:09.917: INFO: Got endpoints: latency-svc-7wjm8 [841.606161ms]
May  1 19:13:10.081: INFO: Created: latency-svc-gsmcn
May  1 19:13:10.129: INFO: Got endpoints: latency-svc-gsmcn [1.004858829s]
May  1 19:13:10.131: INFO: Created: latency-svc-ck46l
May  1 19:13:10.175: INFO: Got endpoints: latency-svc-ck46l [1.054964312s]
May  1 19:13:10.183: INFO: Created: latency-svc-txnrk
May  1 19:13:10.223: INFO: Got endpoints: latency-svc-txnrk [1.063330656s]
May  1 19:13:10.272: INFO: Created: latency-svc-r2hjk
May  1 19:13:10.354: INFO: Got endpoints: latency-svc-r2hjk [1.121562906s]
May  1 19:13:10.458: INFO: Created: latency-svc-5ckl8
May  1 19:13:10.565: INFO: Created: latency-svc-4jwp9
May  1 19:13:10.566: INFO: Got endpoints: latency-svc-5ckl8 [1.266032056s]
May  1 19:13:10.703: INFO: Got endpoints: latency-svc-4jwp9 [1.347864433s]
May  1 19:13:10.771: INFO: Created: latency-svc-dhm87
May  1 19:13:10.819: INFO: Created: latency-svc-s2v47
May  1 19:13:10.878: INFO: Got endpoints: latency-svc-dhm87 [1.482687487s]
May  1 19:13:10.882: INFO: Got endpoints: latency-svc-s2v47 [1.436438776s]
May  1 19:13:11.022: INFO: Created: latency-svc-96jzm
May  1 19:13:11.107: INFO: Got endpoints: latency-svc-96jzm [1.553566104s]
May  1 19:13:11.163: INFO: Created: latency-svc-p2mcr
May  1 19:13:11.165: INFO: Got endpoints: latency-svc-p2mcr [1.573451655s]
May  1 19:13:11.179: INFO: Created: latency-svc-bzssk
May  1 19:13:11.197: INFO: Created: latency-svc-bmp7h
May  1 19:13:11.250: INFO: Got endpoints: latency-svc-bmp7h [1.514991748s]
May  1 19:13:11.250: INFO: Got endpoints: latency-svc-bzssk [1.609124919s]
May  1 19:13:11.252: INFO: Created: latency-svc-r2mgq
May  1 19:13:11.271: INFO: Got endpoints: latency-svc-r2mgq [1.435916728s]
May  1 19:13:11.327: INFO: Created: latency-svc-mmkx9
May  1 19:13:11.337: INFO: Created: latency-svc-x7f28
May  1 19:13:11.357: INFO: Created: latency-svc-9bn4n
May  1 19:13:11.363: INFO: Created: latency-svc-6wf52
May  1 19:13:11.364: INFO: Created: latency-svc-pvpk8
May  1 19:13:11.364: INFO: Got endpoints: latency-svc-pvpk8 [1.625299137s]
May  1 19:13:11.368: INFO: Got endpoints: latency-svc-mmkx9 [1.239508194s]
May  1 19:13:11.381: INFO: Got endpoints: latency-svc-6wf52 [1.464207238s]
May  1 19:13:11.406: INFO: Got endpoints: latency-svc-x7f28 [1.230922734s]
May  1 19:13:11.415: INFO: Created: latency-svc-csks4
May  1 19:13:11.419: INFO: Got endpoints: latency-svc-9bn4n [1.195323705s]
May  1 19:13:11.421: INFO: Got endpoints: latency-svc-csks4 [1.06682898s]
May  1 19:13:11.435: INFO: Created: latency-svc-5bcqh
May  1 19:13:11.459: INFO: Got endpoints: latency-svc-5bcqh [890.821761ms]
May  1 19:13:11.460: INFO: Created: latency-svc-w9584
May  1 19:13:11.501: INFO: Got endpoints: latency-svc-w9584 [797.979451ms]
May  1 19:13:11.507: INFO: Created: latency-svc-6mlvr
May  1 19:13:11.541: INFO: Got endpoints: latency-svc-6mlvr [663.111454ms]
May  1 19:13:11.572: INFO: Created: latency-svc-rfnnx
May  1 19:13:11.604: INFO: Created: latency-svc-6b7zb
May  1 19:13:11.612: INFO: Got endpoints: latency-svc-rfnnx [729.576809ms]
May  1 19:13:11.636: INFO: Created: latency-svc-k4xhm
May  1 19:13:11.697: INFO: Got endpoints: latency-svc-k4xhm [531.129507ms]
May  1 19:13:11.699: INFO: Got endpoints: latency-svc-6b7zb [592.541738ms]
May  1 19:13:11.835: INFO: Created: latency-svc-ctnrh
May  1 19:13:11.884: INFO: Got endpoints: latency-svc-ctnrh [634.186589ms]
May  1 19:13:11.871: INFO: Created: latency-svc-hmxcr
May  1 19:13:11.885: INFO: Got endpoints: latency-svc-hmxcr [613.628622ms]
May  1 19:13:11.872: INFO: Created: latency-svc-c8nf8
May  1 19:13:11.889: INFO: Created: latency-svc-gq6kj
May  1 19:13:11.889: INFO: Got endpoints: latency-svc-gq6kj [638.719317ms]
May  1 19:13:11.929: INFO: Got endpoints: latency-svc-c8nf8 [564.361688ms]
May  1 19:13:11.934: INFO: Created: latency-svc-8f4cv
May  1 19:13:12.017: INFO: Got endpoints: latency-svc-8f4cv [647.183787ms]
May  1 19:13:12.069: INFO: Created: latency-svc-xqbmk
May  1 19:13:12.084: INFO: Created: latency-svc-hgf22
May  1 19:13:12.223: INFO: Created: latency-svc-bmzxb
May  1 19:13:12.263: INFO: Got endpoints: latency-svc-xqbmk [857.488954ms]
May  1 19:13:12.395: INFO: Created: latency-svc-nxdzb
May  1 19:13:12.398: INFO: Got endpoints: latency-svc-hgf22 [978.919314ms]
May  1 19:13:12.399: INFO: Got endpoints: latency-svc-nxdzb [977.482543ms]
May  1 19:13:12.399: INFO: Got endpoints: latency-svc-bmzxb [987.8063ms]
May  1 19:13:12.420: INFO: Created: latency-svc-28pzf
May  1 19:13:12.450: INFO: Got endpoints: latency-svc-28pzf [991.385625ms]
May  1 19:13:12.462: INFO: Created: latency-svc-trbts
May  1 19:13:12.509: INFO: Got endpoints: latency-svc-trbts [1.007621569s]
May  1 19:13:12.535: INFO: Created: latency-svc-9m5f5
May  1 19:13:12.568: INFO: Got endpoints: latency-svc-9m5f5 [1.026765981s]
May  1 19:13:12.570: INFO: Created: latency-svc-m764g
May  1 19:13:12.617: INFO: Got endpoints: latency-svc-m764g [1.005675871s]
May  1 19:13:12.661: INFO: Created: latency-svc-2xn9b
May  1 19:13:12.664: INFO: Got endpoints: latency-svc-2xn9b [965.938544ms]
May  1 19:13:12.673: INFO: Created: latency-svc-rjtcm
May  1 19:13:12.699: INFO: Got endpoints: latency-svc-rjtcm [998.931767ms]
May  1 19:13:12.711: INFO: Created: latency-svc-8r8xv
May  1 19:13:12.733: INFO: Got endpoints: latency-svc-8r8xv [848.236139ms]
May  1 19:13:12.765: INFO: Created: latency-svc-7fzll
May  1 19:13:12.854: INFO: Got endpoints: latency-svc-7fzll [967.748183ms]
May  1 19:13:12.889: INFO: Created: latency-svc-vst7j
May  1 19:13:12.929: INFO: Got endpoints: latency-svc-vst7j [1.040415242s]
May  1 19:13:12.939: INFO: Created: latency-svc-qw7js
May  1 19:13:12.968: INFO: Got endpoints: latency-svc-qw7js [1.031240664s]
May  1 19:13:12.970: INFO: Created: latency-svc-mwdx6
May  1 19:13:13.005: INFO: Created: latency-svc-lnw5c
May  1 19:13:13.040: INFO: Got endpoints: latency-svc-mwdx6 [1.023574347s]
May  1 19:13:13.041: INFO: Got endpoints: latency-svc-lnw5c [768.800982ms]
May  1 19:13:13.068: INFO: Created: latency-svc-fwv64
May  1 19:13:13.102: INFO: Got endpoints: latency-svc-fwv64 [703.019507ms]
May  1 19:13:13.167: INFO: Created: latency-svc-hpctt
May  1 19:13:13.192: INFO: Got endpoints: latency-svc-hpctt [792.920849ms]
May  1 19:13:13.274: INFO: Created: latency-svc-kfl7q
May  1 19:13:13.294: INFO: Got endpoints: latency-svc-kfl7q [894.266592ms]
May  1 19:13:13.337: INFO: Created: latency-svc-fbfpn
May  1 19:13:13.357: INFO: Got endpoints: latency-svc-fbfpn [906.517998ms]
May  1 19:13:13.362: INFO: Created: latency-svc-l4mkp
May  1 19:13:13.384: INFO: Created: latency-svc-9dt74
May  1 19:13:13.387: INFO: Got endpoints: latency-svc-l4mkp [878.214389ms]
May  1 19:13:13.402: INFO: Got endpoints: latency-svc-9dt74 [833.67499ms]
May  1 19:13:13.413: INFO: Created: latency-svc-ljltj
May  1 19:13:13.446: INFO: Created: latency-svc-tbt65
May  1 19:13:13.447: INFO: Got endpoints: latency-svc-ljltj [829.48233ms]
May  1 19:13:13.469: INFO: Got endpoints: latency-svc-tbt65 [805.362707ms]
May  1 19:13:13.479: INFO: Created: latency-svc-sc2dw
May  1 19:13:13.505: INFO: Got endpoints: latency-svc-sc2dw [806.12478ms]
May  1 19:13:13.516: INFO: Created: latency-svc-vpvh7
May  1 19:13:13.537: INFO: Got endpoints: latency-svc-vpvh7 [804.390222ms]
May  1 19:13:13.549: INFO: Created: latency-svc-7ktqp
May  1 19:13:13.568: INFO: Got endpoints: latency-svc-7ktqp [714.002005ms]
May  1 19:13:13.575: INFO: Created: latency-svc-56d4q
May  1 19:13:13.595: INFO: Got endpoints: latency-svc-56d4q [665.449684ms]
May  1 19:13:13.626: INFO: Created: latency-svc-gdsqs
May  1 19:13:13.626: INFO: Got endpoints: latency-svc-gdsqs [656.469635ms]
May  1 19:13:13.690: INFO: Created: latency-svc-cngcp
May  1 19:13:13.766: INFO: Got endpoints: latency-svc-cngcp [725.001803ms]
May  1 19:13:13.781: INFO: Created: latency-svc-df8bs
May  1 19:13:13.829: INFO: Got endpoints: latency-svc-df8bs [788.070071ms]
May  1 19:13:13.830: INFO: Created: latency-svc-kldrn
May  1 19:13:13.871: INFO: Got endpoints: latency-svc-kldrn [769.188448ms]
May  1 19:13:13.920: INFO: Created: latency-svc-6q7kd
May  1 19:13:13.945: INFO: Created: latency-svc-b5hw4
May  1 19:13:13.970: INFO: Got endpoints: latency-svc-6q7kd [777.159727ms]
May  1 19:13:13.974: INFO: Got endpoints: latency-svc-b5hw4 [680.245181ms]
May  1 19:13:14.015: INFO: Created: latency-svc-zjpbk
May  1 19:13:14.055: INFO: Got endpoints: latency-svc-zjpbk [697.980682ms]
May  1 19:13:14.161: INFO: Created: latency-svc-8qctv
May  1 19:13:14.208: INFO: Got endpoints: latency-svc-8qctv [820.695895ms]
May  1 19:13:14.239: INFO: Created: latency-svc-2rdns
May  1 19:13:14.325: INFO: Got endpoints: latency-svc-2rdns [922.685182ms]
May  1 19:13:14.343: INFO: Created: latency-svc-p2hcn
May  1 19:13:14.360: INFO: Created: latency-svc-ff2qc
May  1 19:13:14.363: INFO: Got endpoints: latency-svc-p2hcn [916.376038ms]
May  1 19:13:14.414: INFO: Got endpoints: latency-svc-ff2qc [944.503981ms]
May  1 19:13:14.420: INFO: Created: latency-svc-fhsnv
May  1 19:13:14.434: INFO: Got endpoints: latency-svc-fhsnv [928.667369ms]
May  1 19:13:14.439: INFO: Created: latency-svc-rkptg
May  1 19:13:14.464: INFO: Got endpoints: latency-svc-rkptg [926.192779ms]
May  1 19:13:14.472: INFO: Created: latency-svc-vbvgn
May  1 19:13:14.517: INFO: Got endpoints: latency-svc-vbvgn [948.698241ms]
May  1 19:13:14.526: INFO: Created: latency-svc-sp6ch
May  1 19:13:14.563: INFO: Got endpoints: latency-svc-sp6ch [968.33263ms]
May  1 19:13:14.611: INFO: Created: latency-svc-x2zft
May  1 19:13:14.629: INFO: Got endpoints: latency-svc-x2zft [1.002640761s]
May  1 19:13:14.631: INFO: Created: latency-svc-vrzs5
May  1 19:13:14.682: INFO: Got endpoints: latency-svc-vrzs5 [915.834358ms]
May  1 19:13:14.743: INFO: Created: latency-svc-wsq6x
May  1 19:13:14.793: INFO: Created: latency-svc-wfxgh
May  1 19:13:14.794: INFO: Got endpoints: latency-svc-wfxgh [922.2483ms]
May  1 19:13:14.794: INFO: Got endpoints: latency-svc-wsq6x [964.746179ms]
May  1 19:13:14.821: INFO: Created: latency-svc-h4929
May  1 19:13:14.850: INFO: Got endpoints: latency-svc-h4929 [880.529395ms]
May  1 19:13:14.854: INFO: Created: latency-svc-pfn2m
May  1 19:13:14.889: INFO: Got endpoints: latency-svc-pfn2m [915.057695ms]
May  1 19:13:14.895: INFO: Created: latency-svc-7dlsb
May  1 19:13:14.979: INFO: Got endpoints: latency-svc-7dlsb [924.193749ms]
May  1 19:13:14.995: INFO: Created: latency-svc-m2f9m
May  1 19:13:15.014: INFO: Got endpoints: latency-svc-m2f9m [805.776374ms]
May  1 19:13:15.049: INFO: Created: latency-svc-9skbd
May  1 19:13:15.049: INFO: Got endpoints: latency-svc-9skbd [723.886636ms]
May  1 19:13:15.055: INFO: Created: latency-svc-z4h9w
May  1 19:13:15.081: INFO: Got endpoints: latency-svc-z4h9w [717.81325ms]
May  1 19:13:15.088: INFO: Created: latency-svc-bvnh8
May  1 19:13:15.107: INFO: Got endpoints: latency-svc-bvnh8 [693.352622ms]
May  1 19:13:15.117: INFO: Created: latency-svc-ssrfx
May  1 19:13:15.158: INFO: Got endpoints: latency-svc-ssrfx [723.933954ms]
May  1 19:13:15.169: INFO: Created: latency-svc-5njlt
May  1 19:13:15.196: INFO: Got endpoints: latency-svc-5njlt [731.763097ms]
May  1 19:13:15.202: INFO: Created: latency-svc-sd6js
May  1 19:13:15.224: INFO: Created: latency-svc-m59x7
May  1 19:13:15.228: INFO: Got endpoints: latency-svc-sd6js [710.939757ms]
May  1 19:13:15.244: INFO: Got endpoints: latency-svc-m59x7 [680.128169ms]
May  1 19:13:15.287: INFO: Created: latency-svc-v2pbw
May  1 19:13:15.310: INFO: Got endpoints: latency-svc-v2pbw [680.413246ms]
May  1 19:13:15.314: INFO: Created: latency-svc-prvjg
May  1 19:13:15.360: INFO: Got endpoints: latency-svc-prvjg [677.918211ms]
May  1 19:13:15.364: INFO: Created: latency-svc-4gbhw
May  1 19:13:15.385: INFO: Got endpoints: latency-svc-4gbhw [591.676924ms]
May  1 19:13:15.387: INFO: Created: latency-svc-kwgkw
May  1 19:13:15.414: INFO: Got endpoints: latency-svc-kwgkw [619.960777ms]
May  1 19:13:15.417: INFO: Created: latency-svc-blfm2
May  1 19:13:15.438: INFO: Got endpoints: latency-svc-blfm2 [588.059003ms]
May  1 19:13:15.455: INFO: Created: latency-svc-q7mqp
May  1 19:13:15.483: INFO: Got endpoints: latency-svc-q7mqp [593.638111ms]
May  1 19:13:15.494: INFO: Created: latency-svc-t9gst
May  1 19:13:15.515: INFO: Got endpoints: latency-svc-t9gst [535.93225ms]
May  1 19:13:15.515: INFO: Created: latency-svc-8vmcb
May  1 19:13:15.546: INFO: Got endpoints: latency-svc-8vmcb [532.082528ms]
May  1 19:13:15.547: INFO: Created: latency-svc-jpmdv
May  1 19:13:15.593: INFO: Got endpoints: latency-svc-jpmdv [544.077736ms]
May  1 19:13:15.599: INFO: Created: latency-svc-8d5dm
May  1 19:13:15.670: INFO: Got endpoints: latency-svc-8d5dm [588.785082ms]
May  1 19:13:15.678: INFO: Created: latency-svc-cm9h8
May  1 19:13:15.701: INFO: Got endpoints: latency-svc-cm9h8 [592.994683ms]
May  1 19:13:15.709: INFO: Created: latency-svc-9cf52
May  1 19:13:15.735: INFO: Got endpoints: latency-svc-9cf52 [577.11885ms]
May  1 19:13:15.737: INFO: Created: latency-svc-2kbmk
May  1 19:13:15.810: INFO: Got endpoints: latency-svc-2kbmk [614.85763ms]
May  1 19:13:15.823: INFO: Created: latency-svc-5dqmh
May  1 19:13:15.847: INFO: Got endpoints: latency-svc-5dqmh [618.897082ms]
May  1 19:13:15.847: INFO: Created: latency-svc-p89hp
May  1 19:13:15.870: INFO: Got endpoints: latency-svc-p89hp [626.246911ms]
May  1 19:13:15.870: INFO: Latencies: [81.450481ms 98.164628ms 105.647401ms 136.317637ms 159.287343ms 168.534615ms 179.194881ms 189.439043ms 215.560246ms 221.588717ms 222.589378ms 280.844564ms 285.058157ms 287.223918ms 289.665423ms 318.40309ms 353.325518ms 353.383759ms 355.969617ms 370.499977ms 374.425375ms 378.792533ms 382.384123ms 384.284939ms 385.819438ms 392.712693ms 396.141557ms 404.100572ms 415.458224ms 419.588593ms 421.782245ms 423.186119ms 423.538512ms 426.924351ms 436.817876ms 441.840337ms 447.814768ms 452.323477ms 453.806188ms 456.99199ms 458.412908ms 464.644023ms 465.613563ms 469.101204ms 474.028035ms 487.831346ms 502.907983ms 506.200169ms 511.549029ms 512.430195ms 514.706877ms 522.134401ms 525.847283ms 531.129507ms 532.082528ms 535.93225ms 544.077736ms 545.237656ms 549.811964ms 564.361688ms 577.11885ms 585.17091ms 588.059003ms 588.785082ms 591.676924ms 592.541738ms 592.994683ms 593.638111ms 611.410528ms 613.628622ms 614.85763ms 616.736557ms 618.897082ms 619.960777ms 626.246911ms 634.186589ms 638.719317ms 647.183787ms 649.751788ms 656.469635ms 659.645272ms 663.111454ms 665.449684ms 665.684562ms 669.083581ms 670.097209ms 670.219269ms 677.918211ms 680.128169ms 680.245181ms 680.413246ms 683.309259ms 691.966368ms 693.352622ms 697.980682ms 700.627002ms 703.019507ms 710.939757ms 714.002005ms 714.490644ms 717.81325ms 723.886636ms 723.933954ms 725.001803ms 729.576809ms 731.763097ms 738.139337ms 747.103121ms 764.661643ms 768.800982ms 769.188448ms 777.159727ms 784.521415ms 788.070071ms 792.920849ms 797.979451ms 804.390222ms 805.362707ms 805.4082ms 805.776374ms 806.12478ms 810.923958ms 815.324765ms 820.695895ms 829.48233ms 831.304949ms 833.67499ms 836.421529ms 839.921978ms 841.606161ms 848.236139ms 855.14651ms 857.488954ms 861.162952ms 864.926985ms 878.214389ms 880.529395ms 889.174235ms 890.821761ms 894.266592ms 903.615165ms 906.517998ms 915.057695ms 915.834358ms 916.376038ms 922.2483ms 922.33097ms 922.685182ms 923.198658ms 924.193749ms 926.192779ms 928.667369ms 944.503981ms 948.698241ms 952.303616ms 952.815238ms 964.746179ms 965.938544ms 967.748183ms 968.33263ms 977.482543ms 978.919314ms 982.903034ms 987.8063ms 991.385625ms 998.931767ms 1.002640761s 1.004858829s 1.005675871s 1.007621569s 1.023574347s 1.026765981s 1.031240664s 1.035668094s 1.040415242s 1.054964312s 1.063330656s 1.06682898s 1.121562906s 1.128885711s 1.183760692s 1.195323705s 1.230922734s 1.239508194s 1.266032056s 1.334798647s 1.347864433s 1.435916728s 1.436438776s 1.464207238s 1.482687487s 1.507446691s 1.514991748s 1.553566104s 1.573451655s 1.609124919s 1.625299137s 1.627603446s 1.722344183s 1.726449728s]
May  1 19:13:15.871: INFO: 50 %ile: 717.81325ms
May  1 19:13:15.871: INFO: 90 %ile: 1.183760692s
May  1 19:13:15.871: INFO: 99 %ile: 1.722344183s
May  1 19:13:15.871: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
May  1 19:13:15.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1343" for this suite. 05/01/23 19:13:15.898
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":85,"skipped":1598,"failed":0}
------------------------------
• [SLOW TEST] [13.716 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:13:02.207
    May  1 19:13:02.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svc-latency 05/01/23 19:13:02.209
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:13:02.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:13:02.299
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May  1 19:13:02.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1343 05/01/23 19:13:02.332
    I0501 19:13:02.358907      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1343, replica count: 1
    I0501 19:13:03.410418      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 19:13:04.410898      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 19:13:05.411519      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 19:13:05.556: INFO: Created: latency-svc-xsfsc
    May  1 19:13:05.583: INFO: Got endpoints: latency-svc-xsfsc [70.896567ms]
    May  1 19:13:05.655: INFO: Created: latency-svc-fk7hj
    May  1 19:13:05.668: INFO: Got endpoints: latency-svc-fk7hj [81.450481ms]
    May  1 19:13:05.678: INFO: Created: latency-svc-k2hnl
    May  1 19:13:05.691: INFO: Got endpoints: latency-svc-k2hnl [105.647401ms]
    May  1 19:13:05.704: INFO: Created: latency-svc-4x2x7
    May  1 19:13:05.722: INFO: Got endpoints: latency-svc-4x2x7 [136.317637ms]
    May  1 19:13:05.725: INFO: Created: latency-svc-drwcz
    May  1 19:13:05.746: INFO: Got endpoints: latency-svc-drwcz [159.287343ms]
    May  1 19:13:05.757: INFO: Created: latency-svc-rfnzb
    May  1 19:13:05.779: INFO: Got endpoints: latency-svc-rfnzb [189.439043ms]
    May  1 19:13:05.790: INFO: Created: latency-svc-8984p
    May  1 19:13:05.809: INFO: Got endpoints: latency-svc-8984p [222.589378ms]
    May  1 19:13:05.819: INFO: Created: latency-svc-n4xps
    May  1 19:13:05.868: INFO: Got endpoints: latency-svc-n4xps [280.844564ms]
    May  1 19:13:05.871: INFO: Created: latency-svc-2rdxf
    May  1 19:13:05.873: INFO: Got endpoints: latency-svc-2rdxf [285.058157ms]
    May  1 19:13:05.901: INFO: Created: latency-svc-s2vpb
    May  1 19:13:05.913: INFO: Created: latency-svc-lx9zd
    May  1 19:13:05.947: INFO: Created: latency-svc-xrldr
    May  1 19:13:05.958: INFO: Created: latency-svc-4b7mf
    May  1 19:13:06.012: INFO: Got endpoints: latency-svc-s2vpb [423.538512ms]
    May  1 19:13:06.013: INFO: Got endpoints: latency-svc-4b7mf [421.782245ms]
    May  1 19:13:06.013: INFO: Got endpoints: latency-svc-lx9zd [423.186119ms]
    May  1 19:13:06.055: INFO: Got endpoints: latency-svc-xrldr [464.644023ms]
    May  1 19:13:06.097: INFO: Created: latency-svc-c4rdx
    May  1 19:13:06.098: INFO: Got endpoints: latency-svc-c4rdx [506.200169ms]
    May  1 19:13:06.098: INFO: Created: latency-svc-xzncj
    May  1 19:13:06.114: INFO: Created: latency-svc-vhlzl
    May  1 19:13:06.114: INFO: Got endpoints: latency-svc-vhlzl [522.134401ms]
    May  1 19:13:06.116: INFO: Created: latency-svc-6xp2p
    May  1 19:13:06.116: INFO: Got endpoints: latency-svc-6xp2p [525.847283ms]
    May  1 19:13:06.136: INFO: Created: latency-svc-4rpnp
    May  1 19:13:06.137: INFO: Got endpoints: latency-svc-xzncj [469.101204ms]
    May  1 19:13:06.157: INFO: Got endpoints: latency-svc-4rpnp [465.613563ms]
    May  1 19:13:06.159: INFO: Created: latency-svc-mn97z
    May  1 19:13:06.223: INFO: Created: latency-svc-cz4qw
    May  1 19:13:06.226: INFO: Created: latency-svc-4dw8c
    May  1 19:13:06.233: INFO: Got endpoints: latency-svc-4dw8c [453.806188ms]
    May  1 19:13:06.234: INFO: Got endpoints: latency-svc-cz4qw [487.831346ms]
    May  1 19:13:06.235: INFO: Got endpoints: latency-svc-mn97z [512.430195ms]
    May  1 19:13:06.246: INFO: Created: latency-svc-5nh65
    May  1 19:13:06.246: INFO: Created: latency-svc-kkcdw
    May  1 19:13:06.246: INFO: Got endpoints: latency-svc-kkcdw [436.817876ms]
    May  1 19:13:06.261: INFO: Got endpoints: latency-svc-5nh65 [392.712693ms]
    May  1 19:13:06.298: INFO: Created: latency-svc-hkn5q
    May  1 19:13:06.334: INFO: Got endpoints: latency-svc-hkn5q [98.164628ms]
    May  1 19:13:06.338: INFO: Created: latency-svc-dxxwc
    May  1 19:13:06.367: INFO: Got endpoints: latency-svc-dxxwc [353.325518ms]
    May  1 19:13:06.371: INFO: Created: latency-svc-2926j
    May  1 19:13:06.396: INFO: Got endpoints: latency-svc-2926j [382.384123ms]
    May  1 19:13:06.404: INFO: Created: latency-svc-sw7v6
    May  1 19:13:06.434: INFO: Got endpoints: latency-svc-sw7v6 [378.792533ms]
    May  1 19:13:06.441: INFO: Created: latency-svc-jxg8c
    May  1 19:13:06.452: INFO: Got endpoints: latency-svc-jxg8c [353.383759ms]
    May  1 19:13:06.458: INFO: Created: latency-svc-q6sct
    May  1 19:13:06.488: INFO: Got endpoints: latency-svc-q6sct [374.425375ms]
    May  1 19:13:06.490: INFO: Created: latency-svc-29t6n
    May  1 19:13:06.500: INFO: Got endpoints: latency-svc-29t6n [384.284939ms]
    May  1 19:13:06.507: INFO: Created: latency-svc-pbm6h
    May  1 19:13:06.523: INFO: Got endpoints: latency-svc-pbm6h [385.819438ms]
    May  1 19:13:06.538: INFO: Created: latency-svc-gmv9d
    May  1 19:13:06.553: INFO: Got endpoints: latency-svc-gmv9d [396.141557ms]
    May  1 19:13:06.565: INFO: Created: latency-svc-qmcpk
    May  1 19:13:06.589: INFO: Got endpoints: latency-svc-qmcpk [355.969617ms]
    May  1 19:13:06.601: INFO: Created: latency-svc-pvlhc
    May  1 19:13:06.606: INFO: Created: latency-svc-t2zmx
    May  1 19:13:06.636: INFO: Created: latency-svc-2f7k6
    May  1 19:13:06.638: INFO: Got endpoints: latency-svc-t2zmx [764.661643ms]
    May  1 19:13:06.639: INFO: Got endpoints: latency-svc-pvlhc [404.100572ms]
    May  1 19:13:06.672: INFO: Got endpoints: latency-svc-2f7k6 [659.645272ms]
    May  1 19:13:06.677: INFO: Created: latency-svc-v9kl4
    May  1 19:13:06.691: INFO: Got endpoints: latency-svc-v9kl4 [441.840337ms]
    May  1 19:13:06.696: INFO: Created: latency-svc-4sdds
    May  1 19:13:06.720: INFO: Got endpoints: latency-svc-4sdds [458.412908ms]
    May  1 19:13:06.725: INFO: Created: latency-svc-sxzb6
    May  1 19:13:06.764: INFO: Got endpoints: latency-svc-sxzb6 [426.924351ms]
    May  1 19:13:06.764: INFO: Created: latency-svc-8t88d
    May  1 19:13:06.787: INFO: Got endpoints: latency-svc-8t88d [419.588593ms]
    May  1 19:13:06.818: INFO: Created: latency-svc-89x8q
    May  1 19:13:06.848: INFO: Got endpoints: latency-svc-89x8q [452.323477ms]
    May  1 19:13:06.851: INFO: Created: latency-svc-65zk7
    May  1 19:13:06.946: INFO: Got endpoints: latency-svc-65zk7 [511.549029ms]
    May  1 19:13:07.007: INFO: Created: latency-svc-t6d7k
    May  1 19:13:07.064: INFO: Got endpoints: latency-svc-t6d7k [611.410528ms]
    May  1 19:13:07.087: INFO: Created: latency-svc-mrw6r
    May  1 19:13:07.157: INFO: Got endpoints: latency-svc-mrw6r [669.083581ms]
    May  1 19:13:07.206: INFO: Created: latency-svc-8d2v4
    May  1 19:13:07.306: INFO: Got endpoints: latency-svc-8d2v4 [805.4082ms]
    May  1 19:13:07.311: INFO: Created: latency-svc-k9k79
    May  1 19:13:07.354: INFO: Created: latency-svc-zs598
    May  1 19:13:07.363: INFO: Got endpoints: latency-svc-k9k79 [839.921978ms]
    May  1 19:13:07.398: INFO: Created: latency-svc-z7qvr
    May  1 19:13:07.414: INFO: Got endpoints: latency-svc-zs598 [861.162952ms]
    May  1 19:13:07.421: INFO: Got endpoints: latency-svc-z7qvr [831.304949ms]
    May  1 19:13:07.427: INFO: Created: latency-svc-tdn69
    May  1 19:13:07.467: INFO: Created: latency-svc-r6s6h
    May  1 19:13:07.494: INFO: Got endpoints: latency-svc-tdn69 [855.14651ms]
    May  1 19:13:07.528: INFO: Got endpoints: latency-svc-r6s6h [889.174235ms]
    May  1 19:13:07.542: INFO: Created: latency-svc-b7lmz
    May  1 19:13:07.584: INFO: Created: latency-svc-z99lc
    May  1 19:13:07.595: INFO: Got endpoints: latency-svc-b7lmz [922.33097ms]
    May  1 19:13:07.614: INFO: Got endpoints: latency-svc-z99lc [923.198658ms]
    May  1 19:13:07.615: INFO: Created: latency-svc-zsxfj
    May  1 19:13:07.673: INFO: Got endpoints: latency-svc-zsxfj [952.303616ms]
    May  1 19:13:07.674: INFO: Created: latency-svc-2gzgv
    May  1 19:13:07.717: INFO: Got endpoints: latency-svc-2gzgv [952.815238ms]
    May  1 19:13:08.319: INFO: Created: latency-svc-x4d6t
    May  1 19:13:08.320: INFO: Created: latency-svc-d2w97
    May  1 19:13:08.321: INFO: Created: latency-svc-gv7zr
    May  1 19:13:08.335: INFO: Created: latency-svc-82f8r
    May  1 19:13:08.336: INFO: Created: latency-svc-5zv9v
    May  1 19:13:08.338: INFO: Created: latency-svc-q6v4b
    May  1 19:13:08.339: INFO: Created: latency-svc-zwk9n
    May  1 19:13:08.340: INFO: Created: latency-svc-vbqrp
    May  1 19:13:08.340: INFO: Created: latency-svc-9pc77
    May  1 19:13:08.341: INFO: Created: latency-svc-2wrn7
    May  1 19:13:08.342: INFO: Created: latency-svc-bcvd2
    May  1 19:13:08.343: INFO: Created: latency-svc-bx8qf
    May  1 19:13:08.344: INFO: Created: latency-svc-f5v4z
    May  1 19:13:08.344: INFO: Created: latency-svc-rl5jx
    May  1 19:13:08.345: INFO: Created: latency-svc-546cp
    May  1 19:13:08.393: INFO: Got endpoints: latency-svc-x4d6t [864.926985ms]
    May  1 19:13:08.397: INFO: Got endpoints: latency-svc-d2w97 [982.903034ms]
    May  1 19:13:08.398: INFO: Got endpoints: latency-svc-82f8r [903.615165ms]
    May  1 19:13:08.399: INFO: Got endpoints: latency-svc-gv7zr [784.521415ms]
    May  1 19:13:08.410: INFO: Got endpoints: latency-svc-q6v4b [815.324765ms]
    May  1 19:13:08.455: INFO: Got endpoints: latency-svc-9pc77 [738.139337ms]
    May  1 19:13:08.457: INFO: Got endpoints: latency-svc-bx8qf [1.035668094s]
    May  1 19:13:08.459: INFO: Created: latency-svc-j82v2
    May  1 19:13:08.490: INFO: Got endpoints: latency-svc-bcvd2 [1.183760692s]
    May  1 19:13:08.492: INFO: Got endpoints: latency-svc-f5v4z [1.334798647s]
    May  1 19:13:08.492: INFO: Got endpoints: latency-svc-5zv9v [1.128885711s]
    May  1 19:13:08.502: INFO: Created: latency-svc-qdjgj
    May  1 19:13:08.509: INFO: Got endpoints: latency-svc-zwk9n [1.722344183s]
    May  1 19:13:08.509: INFO: Got endpoints: latency-svc-vbqrp [836.421529ms]
    May  1 19:13:08.551: INFO: Created: latency-svc-544fx
    May  1 19:13:08.566: INFO: Got endpoints: latency-svc-j82v2 [168.534615ms]
    May  1 19:13:08.571: INFO: Got endpoints: latency-svc-2wrn7 [1.507446691s]
    May  1 19:13:08.572: INFO: Got endpoints: latency-svc-qdjgj [179.194881ms]
    May  1 19:13:08.574: INFO: Got endpoints: latency-svc-rl5jx [1.627603446s]
    May  1 19:13:08.575: INFO: Got endpoints: latency-svc-546cp [1.726449728s]
    May  1 19:13:08.612: INFO: Created: latency-svc-nnrlf
    May  1 19:13:08.615: INFO: Got endpoints: latency-svc-544fx [215.560246ms]
    May  1 19:13:08.632: INFO: Got endpoints: latency-svc-nnrlf [221.588717ms]
    May  1 19:13:08.665: INFO: Created: latency-svc-vx7dl
    May  1 19:13:08.685: INFO: Got endpoints: latency-svc-vx7dl [287.223918ms]
    May  1 19:13:08.700: INFO: Created: latency-svc-tnp2q
    May  1 19:13:08.745: INFO: Got endpoints: latency-svc-tnp2q [289.665423ms]
    May  1 19:13:08.756: INFO: Created: latency-svc-sgbfw
    May  1 19:13:08.775: INFO: Got endpoints: latency-svc-sgbfw [318.40309ms]
    May  1 19:13:08.803: INFO: Created: latency-svc-c82fz
    May  1 19:13:08.861: INFO: Got endpoints: latency-svc-c82fz [370.499977ms]
    May  1 19:13:08.862: INFO: Created: latency-svc-rdpfb
    May  1 19:13:08.908: INFO: Got endpoints: latency-svc-rdpfb [415.458224ms]
    May  1 19:13:08.922: INFO: Created: latency-svc-sxczh
    May  1 19:13:08.940: INFO: Got endpoints: latency-svc-sxczh [447.814768ms]
    May  1 19:13:08.942: INFO: Created: latency-svc-mv69h
    May  1 19:13:08.957: INFO: Created: latency-svc-67tsv
    May  1 19:13:08.983: INFO: Got endpoints: latency-svc-mv69h [474.028035ms]
    May  1 19:13:09.000: INFO: Created: latency-svc-ggv2p
    May  1 19:13:09.024: INFO: Got endpoints: latency-svc-ggv2p [456.99199ms]
    May  1 19:13:09.024: INFO: Got endpoints: latency-svc-67tsv [514.706877ms]
    May  1 19:13:09.046: INFO: Created: latency-svc-8q448
    May  1 19:13:09.073: INFO: Created: latency-svc-bcgt4
    May  1 19:13:09.074: INFO: Got endpoints: latency-svc-8q448 [502.907983ms]
    May  1 19:13:09.106: INFO: Created: latency-svc-4fh6p
    May  1 19:13:09.118: INFO: Got endpoints: latency-svc-bcgt4 [545.237656ms]
    May  1 19:13:09.124: INFO: Got endpoints: latency-svc-4fh6p [549.811964ms]
    May  1 19:13:09.138: INFO: Created: latency-svc-7ctxn
    May  1 19:13:09.160: INFO: Got endpoints: latency-svc-7ctxn [585.17091ms]
    May  1 19:13:09.164: INFO: Created: latency-svc-jtb6x
    May  1 19:13:09.232: INFO: Got endpoints: latency-svc-jtb6x [616.736557ms]
    May  1 19:13:09.243: INFO: Created: latency-svc-2cl56
    May  1 19:13:09.271: INFO: Created: latency-svc-sjltf
    May  1 19:13:09.300: INFO: Got endpoints: latency-svc-2cl56 [665.684562ms]
    May  1 19:13:09.343: INFO: Created: latency-svc-7j55f
    May  1 19:13:09.355: INFO: Got endpoints: latency-svc-sjltf [670.097209ms]
    May  1 19:13:09.395: INFO: Got endpoints: latency-svc-7j55f [649.751788ms]
    May  1 19:13:09.408: INFO: Created: latency-svc-tmwbd
    May  1 19:13:09.446: INFO: Got endpoints: latency-svc-tmwbd [670.219269ms]
    May  1 19:13:09.528: INFO: Created: latency-svc-tnm55
    May  1 19:13:09.553: INFO: Got endpoints: latency-svc-tnm55 [691.966368ms]
    May  1 19:13:09.566: INFO: Created: latency-svc-hf2lr
    May  1 19:13:09.591: INFO: Got endpoints: latency-svc-hf2lr [683.309259ms]
    May  1 19:13:09.624: INFO: Created: latency-svc-pc2c5
    May  1 19:13:09.641: INFO: Got endpoints: latency-svc-pc2c5 [700.627002ms]
    May  1 19:13:09.654: INFO: Created: latency-svc-lnct9
    May  1 19:13:09.684: INFO: Created: latency-svc-c5t5n
    May  1 19:13:09.731: INFO: Got endpoints: latency-svc-lnct9 [747.103121ms]
    May  1 19:13:09.738: INFO: Got endpoints: latency-svc-c5t5n [714.490644ms]
    May  1 19:13:09.774: INFO: Created: latency-svc-z4dgj
    May  1 19:13:09.835: INFO: Got endpoints: latency-svc-z4dgj [810.923958ms]
    May  1 19:13:09.894: INFO: Created: latency-svc-7wjm8
    May  1 19:13:09.917: INFO: Got endpoints: latency-svc-7wjm8 [841.606161ms]
    May  1 19:13:10.081: INFO: Created: latency-svc-gsmcn
    May  1 19:13:10.129: INFO: Got endpoints: latency-svc-gsmcn [1.004858829s]
    May  1 19:13:10.131: INFO: Created: latency-svc-ck46l
    May  1 19:13:10.175: INFO: Got endpoints: latency-svc-ck46l [1.054964312s]
    May  1 19:13:10.183: INFO: Created: latency-svc-txnrk
    May  1 19:13:10.223: INFO: Got endpoints: latency-svc-txnrk [1.063330656s]
    May  1 19:13:10.272: INFO: Created: latency-svc-r2hjk
    May  1 19:13:10.354: INFO: Got endpoints: latency-svc-r2hjk [1.121562906s]
    May  1 19:13:10.458: INFO: Created: latency-svc-5ckl8
    May  1 19:13:10.565: INFO: Created: latency-svc-4jwp9
    May  1 19:13:10.566: INFO: Got endpoints: latency-svc-5ckl8 [1.266032056s]
    May  1 19:13:10.703: INFO: Got endpoints: latency-svc-4jwp9 [1.347864433s]
    May  1 19:13:10.771: INFO: Created: latency-svc-dhm87
    May  1 19:13:10.819: INFO: Created: latency-svc-s2v47
    May  1 19:13:10.878: INFO: Got endpoints: latency-svc-dhm87 [1.482687487s]
    May  1 19:13:10.882: INFO: Got endpoints: latency-svc-s2v47 [1.436438776s]
    May  1 19:13:11.022: INFO: Created: latency-svc-96jzm
    May  1 19:13:11.107: INFO: Got endpoints: latency-svc-96jzm [1.553566104s]
    May  1 19:13:11.163: INFO: Created: latency-svc-p2mcr
    May  1 19:13:11.165: INFO: Got endpoints: latency-svc-p2mcr [1.573451655s]
    May  1 19:13:11.179: INFO: Created: latency-svc-bzssk
    May  1 19:13:11.197: INFO: Created: latency-svc-bmp7h
    May  1 19:13:11.250: INFO: Got endpoints: latency-svc-bmp7h [1.514991748s]
    May  1 19:13:11.250: INFO: Got endpoints: latency-svc-bzssk [1.609124919s]
    May  1 19:13:11.252: INFO: Created: latency-svc-r2mgq
    May  1 19:13:11.271: INFO: Got endpoints: latency-svc-r2mgq [1.435916728s]
    May  1 19:13:11.327: INFO: Created: latency-svc-mmkx9
    May  1 19:13:11.337: INFO: Created: latency-svc-x7f28
    May  1 19:13:11.357: INFO: Created: latency-svc-9bn4n
    May  1 19:13:11.363: INFO: Created: latency-svc-6wf52
    May  1 19:13:11.364: INFO: Created: latency-svc-pvpk8
    May  1 19:13:11.364: INFO: Got endpoints: latency-svc-pvpk8 [1.625299137s]
    May  1 19:13:11.368: INFO: Got endpoints: latency-svc-mmkx9 [1.239508194s]
    May  1 19:13:11.381: INFO: Got endpoints: latency-svc-6wf52 [1.464207238s]
    May  1 19:13:11.406: INFO: Got endpoints: latency-svc-x7f28 [1.230922734s]
    May  1 19:13:11.415: INFO: Created: latency-svc-csks4
    May  1 19:13:11.419: INFO: Got endpoints: latency-svc-9bn4n [1.195323705s]
    May  1 19:13:11.421: INFO: Got endpoints: latency-svc-csks4 [1.06682898s]
    May  1 19:13:11.435: INFO: Created: latency-svc-5bcqh
    May  1 19:13:11.459: INFO: Got endpoints: latency-svc-5bcqh [890.821761ms]
    May  1 19:13:11.460: INFO: Created: latency-svc-w9584
    May  1 19:13:11.501: INFO: Got endpoints: latency-svc-w9584 [797.979451ms]
    May  1 19:13:11.507: INFO: Created: latency-svc-6mlvr
    May  1 19:13:11.541: INFO: Got endpoints: latency-svc-6mlvr [663.111454ms]
    May  1 19:13:11.572: INFO: Created: latency-svc-rfnnx
    May  1 19:13:11.604: INFO: Created: latency-svc-6b7zb
    May  1 19:13:11.612: INFO: Got endpoints: latency-svc-rfnnx [729.576809ms]
    May  1 19:13:11.636: INFO: Created: latency-svc-k4xhm
    May  1 19:13:11.697: INFO: Got endpoints: latency-svc-k4xhm [531.129507ms]
    May  1 19:13:11.699: INFO: Got endpoints: latency-svc-6b7zb [592.541738ms]
    May  1 19:13:11.835: INFO: Created: latency-svc-ctnrh
    May  1 19:13:11.884: INFO: Got endpoints: latency-svc-ctnrh [634.186589ms]
    May  1 19:13:11.871: INFO: Created: latency-svc-hmxcr
    May  1 19:13:11.885: INFO: Got endpoints: latency-svc-hmxcr [613.628622ms]
    May  1 19:13:11.872: INFO: Created: latency-svc-c8nf8
    May  1 19:13:11.889: INFO: Created: latency-svc-gq6kj
    May  1 19:13:11.889: INFO: Got endpoints: latency-svc-gq6kj [638.719317ms]
    May  1 19:13:11.929: INFO: Got endpoints: latency-svc-c8nf8 [564.361688ms]
    May  1 19:13:11.934: INFO: Created: latency-svc-8f4cv
    May  1 19:13:12.017: INFO: Got endpoints: latency-svc-8f4cv [647.183787ms]
    May  1 19:13:12.069: INFO: Created: latency-svc-xqbmk
    May  1 19:13:12.084: INFO: Created: latency-svc-hgf22
    May  1 19:13:12.223: INFO: Created: latency-svc-bmzxb
    May  1 19:13:12.263: INFO: Got endpoints: latency-svc-xqbmk [857.488954ms]
    May  1 19:13:12.395: INFO: Created: latency-svc-nxdzb
    May  1 19:13:12.398: INFO: Got endpoints: latency-svc-hgf22 [978.919314ms]
    May  1 19:13:12.399: INFO: Got endpoints: latency-svc-nxdzb [977.482543ms]
    May  1 19:13:12.399: INFO: Got endpoints: latency-svc-bmzxb [987.8063ms]
    May  1 19:13:12.420: INFO: Created: latency-svc-28pzf
    May  1 19:13:12.450: INFO: Got endpoints: latency-svc-28pzf [991.385625ms]
    May  1 19:13:12.462: INFO: Created: latency-svc-trbts
    May  1 19:13:12.509: INFO: Got endpoints: latency-svc-trbts [1.007621569s]
    May  1 19:13:12.535: INFO: Created: latency-svc-9m5f5
    May  1 19:13:12.568: INFO: Got endpoints: latency-svc-9m5f5 [1.026765981s]
    May  1 19:13:12.570: INFO: Created: latency-svc-m764g
    May  1 19:13:12.617: INFO: Got endpoints: latency-svc-m764g [1.005675871s]
    May  1 19:13:12.661: INFO: Created: latency-svc-2xn9b
    May  1 19:13:12.664: INFO: Got endpoints: latency-svc-2xn9b [965.938544ms]
    May  1 19:13:12.673: INFO: Created: latency-svc-rjtcm
    May  1 19:13:12.699: INFO: Got endpoints: latency-svc-rjtcm [998.931767ms]
    May  1 19:13:12.711: INFO: Created: latency-svc-8r8xv
    May  1 19:13:12.733: INFO: Got endpoints: latency-svc-8r8xv [848.236139ms]
    May  1 19:13:12.765: INFO: Created: latency-svc-7fzll
    May  1 19:13:12.854: INFO: Got endpoints: latency-svc-7fzll [967.748183ms]
    May  1 19:13:12.889: INFO: Created: latency-svc-vst7j
    May  1 19:13:12.929: INFO: Got endpoints: latency-svc-vst7j [1.040415242s]
    May  1 19:13:12.939: INFO: Created: latency-svc-qw7js
    May  1 19:13:12.968: INFO: Got endpoints: latency-svc-qw7js [1.031240664s]
    May  1 19:13:12.970: INFO: Created: latency-svc-mwdx6
    May  1 19:13:13.005: INFO: Created: latency-svc-lnw5c
    May  1 19:13:13.040: INFO: Got endpoints: latency-svc-mwdx6 [1.023574347s]
    May  1 19:13:13.041: INFO: Got endpoints: latency-svc-lnw5c [768.800982ms]
    May  1 19:13:13.068: INFO: Created: latency-svc-fwv64
    May  1 19:13:13.102: INFO: Got endpoints: latency-svc-fwv64 [703.019507ms]
    May  1 19:13:13.167: INFO: Created: latency-svc-hpctt
    May  1 19:13:13.192: INFO: Got endpoints: latency-svc-hpctt [792.920849ms]
    May  1 19:13:13.274: INFO: Created: latency-svc-kfl7q
    May  1 19:13:13.294: INFO: Got endpoints: latency-svc-kfl7q [894.266592ms]
    May  1 19:13:13.337: INFO: Created: latency-svc-fbfpn
    May  1 19:13:13.357: INFO: Got endpoints: latency-svc-fbfpn [906.517998ms]
    May  1 19:13:13.362: INFO: Created: latency-svc-l4mkp
    May  1 19:13:13.384: INFO: Created: latency-svc-9dt74
    May  1 19:13:13.387: INFO: Got endpoints: latency-svc-l4mkp [878.214389ms]
    May  1 19:13:13.402: INFO: Got endpoints: latency-svc-9dt74 [833.67499ms]
    May  1 19:13:13.413: INFO: Created: latency-svc-ljltj
    May  1 19:13:13.446: INFO: Created: latency-svc-tbt65
    May  1 19:13:13.447: INFO: Got endpoints: latency-svc-ljltj [829.48233ms]
    May  1 19:13:13.469: INFO: Got endpoints: latency-svc-tbt65 [805.362707ms]
    May  1 19:13:13.479: INFO: Created: latency-svc-sc2dw
    May  1 19:13:13.505: INFO: Got endpoints: latency-svc-sc2dw [806.12478ms]
    May  1 19:13:13.516: INFO: Created: latency-svc-vpvh7
    May  1 19:13:13.537: INFO: Got endpoints: latency-svc-vpvh7 [804.390222ms]
    May  1 19:13:13.549: INFO: Created: latency-svc-7ktqp
    May  1 19:13:13.568: INFO: Got endpoints: latency-svc-7ktqp [714.002005ms]
    May  1 19:13:13.575: INFO: Created: latency-svc-56d4q
    May  1 19:13:13.595: INFO: Got endpoints: latency-svc-56d4q [665.449684ms]
    May  1 19:13:13.626: INFO: Created: latency-svc-gdsqs
    May  1 19:13:13.626: INFO: Got endpoints: latency-svc-gdsqs [656.469635ms]
    May  1 19:13:13.690: INFO: Created: latency-svc-cngcp
    May  1 19:13:13.766: INFO: Got endpoints: latency-svc-cngcp [725.001803ms]
    May  1 19:13:13.781: INFO: Created: latency-svc-df8bs
    May  1 19:13:13.829: INFO: Got endpoints: latency-svc-df8bs [788.070071ms]
    May  1 19:13:13.830: INFO: Created: latency-svc-kldrn
    May  1 19:13:13.871: INFO: Got endpoints: latency-svc-kldrn [769.188448ms]
    May  1 19:13:13.920: INFO: Created: latency-svc-6q7kd
    May  1 19:13:13.945: INFO: Created: latency-svc-b5hw4
    May  1 19:13:13.970: INFO: Got endpoints: latency-svc-6q7kd [777.159727ms]
    May  1 19:13:13.974: INFO: Got endpoints: latency-svc-b5hw4 [680.245181ms]
    May  1 19:13:14.015: INFO: Created: latency-svc-zjpbk
    May  1 19:13:14.055: INFO: Got endpoints: latency-svc-zjpbk [697.980682ms]
    May  1 19:13:14.161: INFO: Created: latency-svc-8qctv
    May  1 19:13:14.208: INFO: Got endpoints: latency-svc-8qctv [820.695895ms]
    May  1 19:13:14.239: INFO: Created: latency-svc-2rdns
    May  1 19:13:14.325: INFO: Got endpoints: latency-svc-2rdns [922.685182ms]
    May  1 19:13:14.343: INFO: Created: latency-svc-p2hcn
    May  1 19:13:14.360: INFO: Created: latency-svc-ff2qc
    May  1 19:13:14.363: INFO: Got endpoints: latency-svc-p2hcn [916.376038ms]
    May  1 19:13:14.414: INFO: Got endpoints: latency-svc-ff2qc [944.503981ms]
    May  1 19:13:14.420: INFO: Created: latency-svc-fhsnv
    May  1 19:13:14.434: INFO: Got endpoints: latency-svc-fhsnv [928.667369ms]
    May  1 19:13:14.439: INFO: Created: latency-svc-rkptg
    May  1 19:13:14.464: INFO: Got endpoints: latency-svc-rkptg [926.192779ms]
    May  1 19:13:14.472: INFO: Created: latency-svc-vbvgn
    May  1 19:13:14.517: INFO: Got endpoints: latency-svc-vbvgn [948.698241ms]
    May  1 19:13:14.526: INFO: Created: latency-svc-sp6ch
    May  1 19:13:14.563: INFO: Got endpoints: latency-svc-sp6ch [968.33263ms]
    May  1 19:13:14.611: INFO: Created: latency-svc-x2zft
    May  1 19:13:14.629: INFO: Got endpoints: latency-svc-x2zft [1.002640761s]
    May  1 19:13:14.631: INFO: Created: latency-svc-vrzs5
    May  1 19:13:14.682: INFO: Got endpoints: latency-svc-vrzs5 [915.834358ms]
    May  1 19:13:14.743: INFO: Created: latency-svc-wsq6x
    May  1 19:13:14.793: INFO: Created: latency-svc-wfxgh
    May  1 19:13:14.794: INFO: Got endpoints: latency-svc-wfxgh [922.2483ms]
    May  1 19:13:14.794: INFO: Got endpoints: latency-svc-wsq6x [964.746179ms]
    May  1 19:13:14.821: INFO: Created: latency-svc-h4929
    May  1 19:13:14.850: INFO: Got endpoints: latency-svc-h4929 [880.529395ms]
    May  1 19:13:14.854: INFO: Created: latency-svc-pfn2m
    May  1 19:13:14.889: INFO: Got endpoints: latency-svc-pfn2m [915.057695ms]
    May  1 19:13:14.895: INFO: Created: latency-svc-7dlsb
    May  1 19:13:14.979: INFO: Got endpoints: latency-svc-7dlsb [924.193749ms]
    May  1 19:13:14.995: INFO: Created: latency-svc-m2f9m
    May  1 19:13:15.014: INFO: Got endpoints: latency-svc-m2f9m [805.776374ms]
    May  1 19:13:15.049: INFO: Created: latency-svc-9skbd
    May  1 19:13:15.049: INFO: Got endpoints: latency-svc-9skbd [723.886636ms]
    May  1 19:13:15.055: INFO: Created: latency-svc-z4h9w
    May  1 19:13:15.081: INFO: Got endpoints: latency-svc-z4h9w [717.81325ms]
    May  1 19:13:15.088: INFO: Created: latency-svc-bvnh8
    May  1 19:13:15.107: INFO: Got endpoints: latency-svc-bvnh8 [693.352622ms]
    May  1 19:13:15.117: INFO: Created: latency-svc-ssrfx
    May  1 19:13:15.158: INFO: Got endpoints: latency-svc-ssrfx [723.933954ms]
    May  1 19:13:15.169: INFO: Created: latency-svc-5njlt
    May  1 19:13:15.196: INFO: Got endpoints: latency-svc-5njlt [731.763097ms]
    May  1 19:13:15.202: INFO: Created: latency-svc-sd6js
    May  1 19:13:15.224: INFO: Created: latency-svc-m59x7
    May  1 19:13:15.228: INFO: Got endpoints: latency-svc-sd6js [710.939757ms]
    May  1 19:13:15.244: INFO: Got endpoints: latency-svc-m59x7 [680.128169ms]
    May  1 19:13:15.287: INFO: Created: latency-svc-v2pbw
    May  1 19:13:15.310: INFO: Got endpoints: latency-svc-v2pbw [680.413246ms]
    May  1 19:13:15.314: INFO: Created: latency-svc-prvjg
    May  1 19:13:15.360: INFO: Got endpoints: latency-svc-prvjg [677.918211ms]
    May  1 19:13:15.364: INFO: Created: latency-svc-4gbhw
    May  1 19:13:15.385: INFO: Got endpoints: latency-svc-4gbhw [591.676924ms]
    May  1 19:13:15.387: INFO: Created: latency-svc-kwgkw
    May  1 19:13:15.414: INFO: Got endpoints: latency-svc-kwgkw [619.960777ms]
    May  1 19:13:15.417: INFO: Created: latency-svc-blfm2
    May  1 19:13:15.438: INFO: Got endpoints: latency-svc-blfm2 [588.059003ms]
    May  1 19:13:15.455: INFO: Created: latency-svc-q7mqp
    May  1 19:13:15.483: INFO: Got endpoints: latency-svc-q7mqp [593.638111ms]
    May  1 19:13:15.494: INFO: Created: latency-svc-t9gst
    May  1 19:13:15.515: INFO: Got endpoints: latency-svc-t9gst [535.93225ms]
    May  1 19:13:15.515: INFO: Created: latency-svc-8vmcb
    May  1 19:13:15.546: INFO: Got endpoints: latency-svc-8vmcb [532.082528ms]
    May  1 19:13:15.547: INFO: Created: latency-svc-jpmdv
    May  1 19:13:15.593: INFO: Got endpoints: latency-svc-jpmdv [544.077736ms]
    May  1 19:13:15.599: INFO: Created: latency-svc-8d5dm
    May  1 19:13:15.670: INFO: Got endpoints: latency-svc-8d5dm [588.785082ms]
    May  1 19:13:15.678: INFO: Created: latency-svc-cm9h8
    May  1 19:13:15.701: INFO: Got endpoints: latency-svc-cm9h8 [592.994683ms]
    May  1 19:13:15.709: INFO: Created: latency-svc-9cf52
    May  1 19:13:15.735: INFO: Got endpoints: latency-svc-9cf52 [577.11885ms]
    May  1 19:13:15.737: INFO: Created: latency-svc-2kbmk
    May  1 19:13:15.810: INFO: Got endpoints: latency-svc-2kbmk [614.85763ms]
    May  1 19:13:15.823: INFO: Created: latency-svc-5dqmh
    May  1 19:13:15.847: INFO: Got endpoints: latency-svc-5dqmh [618.897082ms]
    May  1 19:13:15.847: INFO: Created: latency-svc-p89hp
    May  1 19:13:15.870: INFO: Got endpoints: latency-svc-p89hp [626.246911ms]
    May  1 19:13:15.870: INFO: Latencies: [81.450481ms 98.164628ms 105.647401ms 136.317637ms 159.287343ms 168.534615ms 179.194881ms 189.439043ms 215.560246ms 221.588717ms 222.589378ms 280.844564ms 285.058157ms 287.223918ms 289.665423ms 318.40309ms 353.325518ms 353.383759ms 355.969617ms 370.499977ms 374.425375ms 378.792533ms 382.384123ms 384.284939ms 385.819438ms 392.712693ms 396.141557ms 404.100572ms 415.458224ms 419.588593ms 421.782245ms 423.186119ms 423.538512ms 426.924351ms 436.817876ms 441.840337ms 447.814768ms 452.323477ms 453.806188ms 456.99199ms 458.412908ms 464.644023ms 465.613563ms 469.101204ms 474.028035ms 487.831346ms 502.907983ms 506.200169ms 511.549029ms 512.430195ms 514.706877ms 522.134401ms 525.847283ms 531.129507ms 532.082528ms 535.93225ms 544.077736ms 545.237656ms 549.811964ms 564.361688ms 577.11885ms 585.17091ms 588.059003ms 588.785082ms 591.676924ms 592.541738ms 592.994683ms 593.638111ms 611.410528ms 613.628622ms 614.85763ms 616.736557ms 618.897082ms 619.960777ms 626.246911ms 634.186589ms 638.719317ms 647.183787ms 649.751788ms 656.469635ms 659.645272ms 663.111454ms 665.449684ms 665.684562ms 669.083581ms 670.097209ms 670.219269ms 677.918211ms 680.128169ms 680.245181ms 680.413246ms 683.309259ms 691.966368ms 693.352622ms 697.980682ms 700.627002ms 703.019507ms 710.939757ms 714.002005ms 714.490644ms 717.81325ms 723.886636ms 723.933954ms 725.001803ms 729.576809ms 731.763097ms 738.139337ms 747.103121ms 764.661643ms 768.800982ms 769.188448ms 777.159727ms 784.521415ms 788.070071ms 792.920849ms 797.979451ms 804.390222ms 805.362707ms 805.4082ms 805.776374ms 806.12478ms 810.923958ms 815.324765ms 820.695895ms 829.48233ms 831.304949ms 833.67499ms 836.421529ms 839.921978ms 841.606161ms 848.236139ms 855.14651ms 857.488954ms 861.162952ms 864.926985ms 878.214389ms 880.529395ms 889.174235ms 890.821761ms 894.266592ms 903.615165ms 906.517998ms 915.057695ms 915.834358ms 916.376038ms 922.2483ms 922.33097ms 922.685182ms 923.198658ms 924.193749ms 926.192779ms 928.667369ms 944.503981ms 948.698241ms 952.303616ms 952.815238ms 964.746179ms 965.938544ms 967.748183ms 968.33263ms 977.482543ms 978.919314ms 982.903034ms 987.8063ms 991.385625ms 998.931767ms 1.002640761s 1.004858829s 1.005675871s 1.007621569s 1.023574347s 1.026765981s 1.031240664s 1.035668094s 1.040415242s 1.054964312s 1.063330656s 1.06682898s 1.121562906s 1.128885711s 1.183760692s 1.195323705s 1.230922734s 1.239508194s 1.266032056s 1.334798647s 1.347864433s 1.435916728s 1.436438776s 1.464207238s 1.482687487s 1.507446691s 1.514991748s 1.553566104s 1.573451655s 1.609124919s 1.625299137s 1.627603446s 1.722344183s 1.726449728s]
    May  1 19:13:15.871: INFO: 50 %ile: 717.81325ms
    May  1 19:13:15.871: INFO: 90 %ile: 1.183760692s
    May  1 19:13:15.871: INFO: 99 %ile: 1.722344183s
    May  1 19:13:15.871: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    May  1 19:13:15.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1343" for this suite. 05/01/23 19:13:15.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:13:15.929
May  1 19:13:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:13:15.932
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:13:16.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:13:16.013
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
May  1 19:13:16.133: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-61116d66-5677-4119-a4f9-296adb3b56e8 05/01/23 19:13:16.133
STEP: Creating secret with name s-test-opt-upd-c67d7de6-32d1-4e43-b3d9-5384fc3f8641 05/01/23 19:13:16.155
STEP: Creating the pod 05/01/23 19:13:16.178
May  1 19:13:16.285: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80" in namespace "projected-7569" to be "running and ready"
May  1 19:13:16.333: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 47.606705ms
May  1 19:13:16.333: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:18.364: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07865187s
May  1 19:13:18.364: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:20.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07063323s
May  1 19:13:20.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:22.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070754635s
May  1 19:13:22.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:24.351: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065988799s
May  1 19:13:24.352: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:26.353: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.067302206s
May  1 19:13:26.353: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:28.493: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.207432072s
May  1 19:13:28.493: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:30.354: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 14.068164602s
May  1 19:13:30.354: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:32.372: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.086936297s
May  1 19:13:32.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:34.394: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 18.10863171s
May  1 19:13:34.394: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:36.391: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 20.105899197s
May  1 19:13:36.391: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:38.360: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 22.07495819s
May  1 19:13:38.367: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:40.374: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.088338432s
May  1 19:13:40.374: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:42.422: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 26.136253983s
May  1 19:13:42.425: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:44.411: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 28.125071749s
May  1 19:13:44.411: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:46.364: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 30.078449933s
May  1 19:13:46.364: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:48.393: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 32.107283471s
May  1 19:13:48.393: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:50.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 34.070815976s
May  1 19:13:50.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:52.416: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 36.130845271s
May  1 19:13:52.417: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:54.386: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 38.10082825s
May  1 19:13:54.387: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:56.370: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 40.08414194s
May  1 19:13:56.370: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:13:58.555: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 42.269488055s
May  1 19:13:58.555: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:00.367: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 44.081193199s
May  1 19:14:00.367: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:02.398: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 46.112088277s
May  1 19:14:02.398: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:04.369: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 48.083848107s
May  1 19:14:04.369: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:06.374: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 50.088070584s
May  1 19:14:06.374: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:08.379: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 52.093954314s
May  1 19:14:08.380: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:10.362: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 54.076357909s
May  1 19:14:10.362: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:12.372: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 56.086012847s
May  1 19:14:12.372: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:14.368: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 58.082023064s
May  1 19:14:14.368: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:16.357: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.071015542s
May  1 19:14:16.357: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:18.421: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.135714391s
May  1 19:14:18.421: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:20.379: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.093041251s
May  1 19:14:20.379: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:22.360: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.074659645s
May  1 19:14:22.360: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:24.373: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.087184174s
May  1 19:14:24.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:26.358: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.072599751s
May  1 19:14:26.358: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:28.381: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.095546646s
May  1 19:14:28.383: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:30.365: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.079413787s
May  1 19:14:30.365: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:32.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.07381144s
May  1 19:14:32.359: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:34.353: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.067328748s
May  1 19:14:34.353: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:36.373: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.087747495s
May  1 19:14:36.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:14:38.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Running", Reason="", readiness=true. Elapsed: 1m22.073343055s
May  1 19:14:38.359: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Running (Ready = true)
May  1 19:14:38.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-61116d66-5677-4119-a4f9-296adb3b56e8 05/01/23 19:14:38.594
STEP: Updating secret s-test-opt-upd-c67d7de6-32d1-4e43-b3d9-5384fc3f8641 05/01/23 19:14:38.616
STEP: Creating secret with name s-test-opt-create-37d11537-a758-4338-ad64-1ec66bbda57c 05/01/23 19:14:38.634
STEP: waiting to observe update in volume 05/01/23 19:14:38.669
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 19:15:58.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7569" for this suite. 05/01/23 19:15:58.046
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":86,"skipped":1620,"failed":0}
------------------------------
• [SLOW TEST] [162.142 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:13:15.929
    May  1 19:13:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:13:15.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:13:16.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:13:16.013
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    May  1 19:13:16.133: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-61116d66-5677-4119-a4f9-296adb3b56e8 05/01/23 19:13:16.133
    STEP: Creating secret with name s-test-opt-upd-c67d7de6-32d1-4e43-b3d9-5384fc3f8641 05/01/23 19:13:16.155
    STEP: Creating the pod 05/01/23 19:13:16.178
    May  1 19:13:16.285: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80" in namespace "projected-7569" to be "running and ready"
    May  1 19:13:16.333: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 47.606705ms
    May  1 19:13:16.333: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:18.364: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07865187s
    May  1 19:13:18.364: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:20.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07063323s
    May  1 19:13:20.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:22.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070754635s
    May  1 19:13:22.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:24.351: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065988799s
    May  1 19:13:24.352: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:26.353: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.067302206s
    May  1 19:13:26.353: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:28.493: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.207432072s
    May  1 19:13:28.493: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:30.354: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 14.068164602s
    May  1 19:13:30.354: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:32.372: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.086936297s
    May  1 19:13:32.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:34.394: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 18.10863171s
    May  1 19:13:34.394: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:36.391: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 20.105899197s
    May  1 19:13:36.391: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:38.360: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 22.07495819s
    May  1 19:13:38.367: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:40.374: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.088338432s
    May  1 19:13:40.374: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:42.422: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 26.136253983s
    May  1 19:13:42.425: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:44.411: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 28.125071749s
    May  1 19:13:44.411: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:46.364: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 30.078449933s
    May  1 19:13:46.364: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:48.393: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 32.107283471s
    May  1 19:13:48.393: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:50.356: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 34.070815976s
    May  1 19:13:50.356: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:52.416: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 36.130845271s
    May  1 19:13:52.417: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:54.386: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 38.10082825s
    May  1 19:13:54.387: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:56.370: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 40.08414194s
    May  1 19:13:56.370: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:13:58.555: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 42.269488055s
    May  1 19:13:58.555: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:00.367: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 44.081193199s
    May  1 19:14:00.367: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:02.398: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 46.112088277s
    May  1 19:14:02.398: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:04.369: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 48.083848107s
    May  1 19:14:04.369: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:06.374: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 50.088070584s
    May  1 19:14:06.374: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:08.379: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 52.093954314s
    May  1 19:14:08.380: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:10.362: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 54.076357909s
    May  1 19:14:10.362: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:12.372: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 56.086012847s
    May  1 19:14:12.372: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:14.368: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 58.082023064s
    May  1 19:14:14.368: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:16.357: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.071015542s
    May  1 19:14:16.357: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:18.421: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.135714391s
    May  1 19:14:18.421: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:20.379: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.093041251s
    May  1 19:14:20.379: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:22.360: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.074659645s
    May  1 19:14:22.360: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:24.373: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.087184174s
    May  1 19:14:24.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:26.358: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.072599751s
    May  1 19:14:26.358: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:28.381: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.095546646s
    May  1 19:14:28.383: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:30.365: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.079413787s
    May  1 19:14:30.365: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:32.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.07381144s
    May  1 19:14:32.359: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:34.353: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.067328748s
    May  1 19:14:34.353: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:36.373: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.087747495s
    May  1 19:14:36.373: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:14:38.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80": Phase="Running", Reason="", readiness=true. Elapsed: 1m22.073343055s
    May  1 19:14:38.359: INFO: The phase of Pod pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80 is Running (Ready = true)
    May  1 19:14:38.359: INFO: Pod "pod-projected-secrets-8259ecef-f292-40cd-ac65-dd0190c53d80" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-61116d66-5677-4119-a4f9-296adb3b56e8 05/01/23 19:14:38.594
    STEP: Updating secret s-test-opt-upd-c67d7de6-32d1-4e43-b3d9-5384fc3f8641 05/01/23 19:14:38.616
    STEP: Creating secret with name s-test-opt-create-37d11537-a758-4338-ad64-1ec66bbda57c 05/01/23 19:14:38.634
    STEP: waiting to observe update in volume 05/01/23 19:14:38.669
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 19:15:58.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7569" for this suite. 05/01/23 19:15:58.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:15:58.076
May  1 19:15:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:15:58.078
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:15:58.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:15:58.21
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
May  1 19:15:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:16:16.688
May  1 19:16:16.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 create -f -'
May  1 19:16:19.979: INFO: stderr: ""
May  1 19:16:19.979: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  1 19:16:19.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
May  1 19:16:20.253: INFO: stderr: ""
May  1 19:16:20.253: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May  1 19:16:20.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 apply -f -'
May  1 19:16:21.668: INFO: stderr: ""
May  1 19:16:21.668: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May  1 19:16:21.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
May  1 19:16:21.979: INFO: stderr: ""
May  1 19:16:21.979: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/01/23 19:16:21.979
May  1 19:16:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 explain e2e-test-crd-publish-openapi-7802-crds'
May  1 19:16:24.779: INFO: stderr: ""
May  1 19:16:24.779: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7802-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:16:46.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2209" for this suite. 05/01/23 19:16:46.778
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":87,"skipped":1648,"failed":0}
------------------------------
• [SLOW TEST] [48.734 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:15:58.076
    May  1 19:15:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:15:58.078
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:15:58.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:15:58.21
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    May  1 19:15:58.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:16:16.688
    May  1 19:16:16.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 create -f -'
    May  1 19:16:19.979: INFO: stderr: ""
    May  1 19:16:19.979: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  1 19:16:19.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
    May  1 19:16:20.253: INFO: stderr: ""
    May  1 19:16:20.253: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May  1 19:16:20.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 apply -f -'
    May  1 19:16:21.668: INFO: stderr: ""
    May  1 19:16:21.668: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May  1 19:16:21.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 --namespace=crd-publish-openapi-2209 delete e2e-test-crd-publish-openapi-7802-crds test-cr'
    May  1 19:16:21.979: INFO: stderr: ""
    May  1 19:16:21.979: INFO: stdout: "e2e-test-crd-publish-openapi-7802-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/01/23 19:16:21.979
    May  1 19:16:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-2209 explain e2e-test-crd-publish-openapi-7802-crds'
    May  1 19:16:24.779: INFO: stderr: ""
    May  1 19:16:24.779: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7802-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:16:46.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2209" for this suite. 05/01/23 19:16:46.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:16:46.822
May  1 19:16:46.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:16:46.824
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:16:46.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:16:46.953
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2206 05/01/23 19:16:46.971
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[] 05/01/23 19:16:47.11
May  1 19:16:47.142: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May  1 19:16:48.205: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2206 05/01/23 19:16:48.205
May  1 19:16:48.270: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2206" to be "running and ready"
May  1 19:16:48.302: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 31.752738ms
May  1 19:16:48.302: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:16:50.321: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050904905s
May  1 19:16:50.321: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:16:52.322: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.051966129s
May  1 19:16:52.322: INFO: The phase of Pod pod1 is Running (Ready = true)
May  1 19:16:52.322: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod1:[80]] 05/01/23 19:16:52.343
May  1 19:16:52.416: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/01/23 19:16:52.416
May  1 19:16:52.417: INFO: Creating new exec pod
May  1 19:16:52.478: INFO: Waiting up to 5m0s for pod "execpodmng46" in namespace "services-2206" to be "running"
May  1 19:16:52.524: INFO: Pod "execpodmng46": Phase="Pending", Reason="", readiness=false. Elapsed: 45.909898ms
May  1 19:16:54.546: INFO: Pod "execpodmng46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068525176s
May  1 19:16:56.547: INFO: Pod "execpodmng46": Phase="Running", Reason="", readiness=true. Elapsed: 4.069639196s
May  1 19:16:56.547: INFO: Pod "execpodmng46" satisfied condition "running"
May  1 19:16:57.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  1 19:16:58.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  1 19:16:58.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:16:58.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
May  1 19:16:58.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
May  1 19:16:58.756: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2206 05/01/23 19:16:58.756
May  1 19:16:58.811: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2206" to be "running and ready"
May  1 19:16:58.829: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.351117ms
May  1 19:16:58.829: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:17:00.851: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039675056s
May  1 19:17:00.851: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:17:02.849: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037283073s
May  1 19:17:02.849: INFO: The phase of Pod pod2 is Running (Ready = true)
May  1 19:17:02.849: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod1:[80] pod2:[80]] 05/01/23 19:17:02.877
May  1 19:17:02.959: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/01/23 19:17:02.959
May  1 19:17:03.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  1 19:17:04.588: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  1 19:17:04.588: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:17:04.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
May  1 19:17:05.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
May  1 19:17:05.114: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2206 05/01/23 19:17:05.114
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod2:[80]] 05/01/23 19:17:05.182
May  1 19:17:05.288: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/01/23 19:17:05.288
May  1 19:17:06.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May  1 19:17:07.685: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May  1 19:17:07.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:17:07.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
May  1 19:17:08.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
May  1 19:17:08.164: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2206 05/01/23 19:17:08.164
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[] 05/01/23 19:17:08.213
May  1 19:17:09.333: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:17:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2206" for this suite. 05/01/23 19:17:09.472
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":88,"skipped":1659,"failed":0}
------------------------------
• [SLOW TEST] [22.689 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:16:46.822
    May  1 19:16:46.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:16:46.824
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:16:46.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:16:46.953
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2206 05/01/23 19:16:46.971
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[] 05/01/23 19:16:47.11
    May  1 19:16:47.142: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    May  1 19:16:48.205: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2206 05/01/23 19:16:48.205
    May  1 19:16:48.270: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2206" to be "running and ready"
    May  1 19:16:48.302: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 31.752738ms
    May  1 19:16:48.302: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:16:50.321: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050904905s
    May  1 19:16:50.321: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:16:52.322: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.051966129s
    May  1 19:16:52.322: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  1 19:16:52.322: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod1:[80]] 05/01/23 19:16:52.343
    May  1 19:16:52.416: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/01/23 19:16:52.416
    May  1 19:16:52.417: INFO: Creating new exec pod
    May  1 19:16:52.478: INFO: Waiting up to 5m0s for pod "execpodmng46" in namespace "services-2206" to be "running"
    May  1 19:16:52.524: INFO: Pod "execpodmng46": Phase="Pending", Reason="", readiness=false. Elapsed: 45.909898ms
    May  1 19:16:54.546: INFO: Pod "execpodmng46": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068525176s
    May  1 19:16:56.547: INFO: Pod "execpodmng46": Phase="Running", Reason="", readiness=true. Elapsed: 4.069639196s
    May  1 19:16:56.547: INFO: Pod "execpodmng46" satisfied condition "running"
    May  1 19:16:57.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  1 19:16:58.239: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  1 19:16:58.239: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:16:58.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
    May  1 19:16:58.756: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
    May  1 19:16:58.756: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2206 05/01/23 19:16:58.756
    May  1 19:16:58.811: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2206" to be "running and ready"
    May  1 19:16:58.829: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.351117ms
    May  1 19:16:58.829: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:17:00.851: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039675056s
    May  1 19:17:00.851: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:17:02.849: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.037283073s
    May  1 19:17:02.849: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  1 19:17:02.849: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod1:[80] pod2:[80]] 05/01/23 19:17:02.877
    May  1 19:17:02.959: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/01/23 19:17:02.959
    May  1 19:17:03.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  1 19:17:04.588: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  1 19:17:04.588: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:17:04.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
    May  1 19:17:05.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
    May  1 19:17:05.114: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2206 05/01/23 19:17:05.114
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[pod2:[80]] 05/01/23 19:17:05.182
    May  1 19:17:05.288: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/01/23 19:17:05.288
    May  1 19:17:06.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    May  1 19:17:07.685: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May  1 19:17:07.685: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:17:07.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-2206 exec execpodmng46 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.239 80'
    May  1 19:17:08.164: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.239 80\nConnection to 172.21.141.239 80 port [tcp/http] succeeded!\n"
    May  1 19:17:08.164: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2206 05/01/23 19:17:08.164
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2206 to expose endpoints map[] 05/01/23 19:17:08.213
    May  1 19:17:09.333: INFO: successfully validated that service endpoint-test2 in namespace services-2206 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:17:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2206" for this suite. 05/01/23 19:17:09.472
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:09.511
May  1 19:17:09.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:17:09.515
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:09.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:09.619
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-477ed7e8-55b2-4f2f-9cc8-18bce477f284 05/01/23 19:17:09.647
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:17:09.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8915" for this suite. 05/01/23 19:17:09.728
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":89,"skipped":1659,"failed":0}
------------------------------
• [0.270 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:09.511
    May  1 19:17:09.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:17:09.515
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:09.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:09.619
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-477ed7e8-55b2-4f2f-9cc8-18bce477f284 05/01/23 19:17:09.647
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:17:09.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8915" for this suite. 05/01/23 19:17:09.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:09.794
May  1 19:17:09.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:17:09.799
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:09.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:09.917
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 19:17:10.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5244" for this suite. 05/01/23 19:17:10.279
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":90,"skipped":1680,"failed":0}
------------------------------
• [0.518 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:09.794
    May  1 19:17:09.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:17:09.799
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:09.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:09.917
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:17:10.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5244" for this suite. 05/01/23 19:17:10.279
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:10.319
May  1 19:17:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:17:10.321
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:10.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:10.415
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 05/01/23 19:17:10.446
May  1 19:17:10.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May  1 19:17:10.843: INFO: stderr: ""
May  1 19:17:10.843: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 05/01/23 19:17:10.843
May  1 19:17:10.843: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May  1 19:17:10.843: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2158" to be "running and ready, or succeeded"
May  1 19:17:10.880: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 36.907824ms
May  1 19:17:10.880: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.45.145.124' to be 'Running' but was 'Pending'
May  1 19:17:12.897: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.053670196s
May  1 19:17:12.897: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May  1 19:17:12.897: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/01/23 19:17:12.897
May  1 19:17:12.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator'
May  1 19:17:13.161: INFO: stderr: ""
May  1 19:17:13.161: INFO: stdout: "I0501 19:17:12.633858       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/gjq 486\nI0501 19:17:12.834365       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/7v8k 502\nI0501 19:17:13.034829       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/twdg 242\n"
STEP: limiting log lines 05/01/23 19:17:13.162
May  1 19:17:13.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --tail=1'
May  1 19:17:13.552: INFO: stderr: ""
May  1 19:17:13.552: INFO: stdout: "I0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\n"
May  1 19:17:13.552: INFO: got output "I0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\n"
STEP: limiting log bytes 05/01/23 19:17:13.552
May  1 19:17:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --limit-bytes=1'
May  1 19:17:13.750: INFO: stderr: ""
May  1 19:17:13.750: INFO: stdout: "I"
May  1 19:17:13.750: INFO: got output "I"
STEP: exposing timestamps 05/01/23 19:17:13.75
May  1 19:17:13.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --tail=1 --timestamps'
May  1 19:17:14.030: INFO: stderr: ""
May  1 19:17:14.030: INFO: stdout: "2023-05-01T14:17:13.834945019-05:00 I0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\n"
May  1 19:17:14.030: INFO: got output "2023-05-01T14:17:13.834945019-05:00 I0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\n"
STEP: restricting to a time range 05/01/23 19:17:14.031
May  1 19:17:16.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --since=1s'
May  1 19:17:16.798: INFO: stderr: ""
May  1 19:17:16.798: INFO: stdout: "I0501 19:17:15.835443       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/cxkh 437\nI0501 19:17:16.034443       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/ptn 477\nI0501 19:17:16.233899       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/h8cq 580\nI0501 19:17:16.434562       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/xjdw 254\nI0501 19:17:16.633881       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/8q6n 498\n"
May  1 19:17:16.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --since=24h'
May  1 19:17:17.040: INFO: stderr: ""
May  1 19:17:17.040: INFO: stdout: "I0501 19:17:12.633858       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/gjq 486\nI0501 19:17:12.834365       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/7v8k 502\nI0501 19:17:13.034829       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/twdg 242\nI0501 19:17:13.234417       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/bln9 439\nI0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\nI0501 19:17:13.634333       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/jl64 202\nI0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\nI0501 19:17:14.034422       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/prs7 338\nI0501 19:17:14.259503       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dks 249\nI0501 19:17:14.434819       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/hh5h 512\nI0501 19:17:14.634369       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/wjd 517\nI0501 19:17:14.834643       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/zvgv 581\nI0501 19:17:15.034032       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/286d 571\nI0501 19:17:15.235360       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/kmnx 276\nI0501 19:17:15.433825       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/bcsz 267\nI0501 19:17:15.634148       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/6wjs 200\nI0501 19:17:15.835443       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/cxkh 437\nI0501 19:17:16.034443       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/ptn 477\nI0501 19:17:16.233899       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/h8cq 580\nI0501 19:17:16.434562       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/xjdw 254\nI0501 19:17:16.633881       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/8q6n 498\nI0501 19:17:16.834405       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/27x 569\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
May  1 19:17:17.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 delete pod logs-generator'
May  1 19:17:19.950: INFO: stderr: ""
May  1 19:17:19.950: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:17:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2158" for this suite. 05/01/23 19:17:19.98
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":91,"skipped":1684,"failed":0}
------------------------------
• [SLOW TEST] [9.695 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:10.319
    May  1 19:17:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:17:10.321
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:10.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:10.415
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 05/01/23 19:17:10.446
    May  1 19:17:10.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May  1 19:17:10.843: INFO: stderr: ""
    May  1 19:17:10.843: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 05/01/23 19:17:10.843
    May  1 19:17:10.843: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May  1 19:17:10.843: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2158" to be "running and ready, or succeeded"
    May  1 19:17:10.880: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 36.907824ms
    May  1 19:17:10.880: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.45.145.124' to be 'Running' but was 'Pending'
    May  1 19:17:12.897: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.053670196s
    May  1 19:17:12.897: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May  1 19:17:12.897: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/01/23 19:17:12.897
    May  1 19:17:12.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator'
    May  1 19:17:13.161: INFO: stderr: ""
    May  1 19:17:13.161: INFO: stdout: "I0501 19:17:12.633858       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/gjq 486\nI0501 19:17:12.834365       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/7v8k 502\nI0501 19:17:13.034829       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/twdg 242\n"
    STEP: limiting log lines 05/01/23 19:17:13.162
    May  1 19:17:13.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --tail=1'
    May  1 19:17:13.552: INFO: stderr: ""
    May  1 19:17:13.552: INFO: stdout: "I0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\n"
    May  1 19:17:13.552: INFO: got output "I0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\n"
    STEP: limiting log bytes 05/01/23 19:17:13.552
    May  1 19:17:13.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --limit-bytes=1'
    May  1 19:17:13.750: INFO: stderr: ""
    May  1 19:17:13.750: INFO: stdout: "I"
    May  1 19:17:13.750: INFO: got output "I"
    STEP: exposing timestamps 05/01/23 19:17:13.75
    May  1 19:17:13.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --tail=1 --timestamps'
    May  1 19:17:14.030: INFO: stderr: ""
    May  1 19:17:14.030: INFO: stdout: "2023-05-01T14:17:13.834945019-05:00 I0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\n"
    May  1 19:17:14.030: INFO: got output "2023-05-01T14:17:13.834945019-05:00 I0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\n"
    STEP: restricting to a time range 05/01/23 19:17:14.031
    May  1 19:17:16.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --since=1s'
    May  1 19:17:16.798: INFO: stderr: ""
    May  1 19:17:16.798: INFO: stdout: "I0501 19:17:15.835443       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/cxkh 437\nI0501 19:17:16.034443       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/ptn 477\nI0501 19:17:16.233899       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/h8cq 580\nI0501 19:17:16.434562       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/xjdw 254\nI0501 19:17:16.633881       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/8q6n 498\n"
    May  1 19:17:16.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 logs logs-generator logs-generator --since=24h'
    May  1 19:17:17.040: INFO: stderr: ""
    May  1 19:17:17.040: INFO: stdout: "I0501 19:17:12.633858       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/gjq 486\nI0501 19:17:12.834365       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/7v8k 502\nI0501 19:17:13.034829       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/twdg 242\nI0501 19:17:13.234417       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/bln9 439\nI0501 19:17:13.433733       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/t2n 375\nI0501 19:17:13.634333       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/jl64 202\nI0501 19:17:13.834805       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/wj8 474\nI0501 19:17:14.034422       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/prs7 338\nI0501 19:17:14.259503       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/dks 249\nI0501 19:17:14.434819       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/hh5h 512\nI0501 19:17:14.634369       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/wjd 517\nI0501 19:17:14.834643       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/zvgv 581\nI0501 19:17:15.034032       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/286d 571\nI0501 19:17:15.235360       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/kmnx 276\nI0501 19:17:15.433825       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/bcsz 267\nI0501 19:17:15.634148       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/6wjs 200\nI0501 19:17:15.835443       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/cxkh 437\nI0501 19:17:16.034443       1 logs_generator.go:76] 17 POST /api/v1/namespaces/ns/pods/ptn 477\nI0501 19:17:16.233899       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/h8cq 580\nI0501 19:17:16.434562       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/xjdw 254\nI0501 19:17:16.633881       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/8q6n 498\nI0501 19:17:16.834405       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/27x 569\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    May  1 19:17:17.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-2158 delete pod logs-generator'
    May  1 19:17:19.950: INFO: stderr: ""
    May  1 19:17:19.950: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:17:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2158" for this suite. 05/01/23 19:17:19.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:20.017
May  1 19:17:20.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename init-container 05/01/23 19:17:20.019
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:20.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:20.097
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 05/01/23 19:17:20.115
May  1 19:17:20.115: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 19:17:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3938" for this suite. 05/01/23 19:17:27.302
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":92,"skipped":1708,"failed":0}
------------------------------
• [SLOW TEST] [7.317 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:20.017
    May  1 19:17:20.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename init-container 05/01/23 19:17:20.019
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:20.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:20.097
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 05/01/23 19:17:20.115
    May  1 19:17:20.115: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 19:17:27.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3938" for this suite. 05/01/23 19:17:27.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:27.341
May  1 19:17:27.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename ephemeral-containers-test 05/01/23 19:17:27.343
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:27.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:27.438
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/01/23 19:17:27.471
May  1 19:17:27.581: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2326" to be "running and ready"
May  1 19:17:27.605: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043163ms
May  1 19:17:27.605: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  1 19:17:29.621: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039540607s
May  1 19:17:29.621: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May  1 19:17:31.631: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.049422779s
May  1 19:17:31.631: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May  1 19:17:31.631: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/01/23 19:17:31.649
May  1 19:17:31.692: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2326" to be "container debugger running"
May  1 19:17:31.711: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.756876ms
May  1 19:17:33.731: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.038962506s
May  1 19:17:35.730: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038105719s
May  1 19:17:35.730: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/01/23 19:17:35.73
May  1 19:17:35.731: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2326 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:17:35.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:17:35.732: INFO: ExecWithOptions: Clientset creation
May  1 19:17:35.732: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-2326/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May  1 19:17:36.099: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 19:17:36.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-2326" for this suite. 05/01/23 19:17:36.221
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":93,"skipped":1713,"failed":0}
------------------------------
• [SLOW TEST] [8.913 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:27.341
    May  1 19:17:27.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/01/23 19:17:27.343
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:27.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:27.438
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/01/23 19:17:27.471
    May  1 19:17:27.581: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2326" to be "running and ready"
    May  1 19:17:27.605: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 24.043163ms
    May  1 19:17:27.605: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:17:29.621: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039540607s
    May  1 19:17:29.621: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:17:31.631: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.049422779s
    May  1 19:17:31.631: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May  1 19:17:31.631: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/01/23 19:17:31.649
    May  1 19:17:31.692: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2326" to be "container debugger running"
    May  1 19:17:31.711: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.756876ms
    May  1 19:17:33.731: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.038962506s
    May  1 19:17:35.730: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038105719s
    May  1 19:17:35.730: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/01/23 19:17:35.73
    May  1 19:17:35.731: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2326 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:17:35.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:17:35.732: INFO: ExecWithOptions: Clientset creation
    May  1 19:17:35.732: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-2326/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May  1 19:17:36.099: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 19:17:36.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-2326" for this suite. 05/01/23 19:17:36.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:36.261
May  1 19:17:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:17:36.263
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:36.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:36.359
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 05/01/23 19:17:36.377
May  1 19:17:36.377: INFO: namespace kubectl-9472
May  1 19:17:36.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 create -f -'
May  1 19:17:39.107: INFO: stderr: ""
May  1 19:17:39.107: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/01/23 19:17:39.107
May  1 19:17:40.126: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:17:40.126: INFO: Found 0 / 1
May  1 19:17:41.127: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:17:41.127: INFO: Found 0 / 1
May  1 19:17:42.135: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:17:42.135: INFO: Found 1 / 1
May  1 19:17:42.135: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  1 19:17:42.157: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 19:17:42.157: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  1 19:17:42.157: INFO: wait on agnhost-primary startup in kubectl-9472 
May  1 19:17:42.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 logs agnhost-primary-svmss agnhost-primary'
May  1 19:17:42.519: INFO: stderr: ""
May  1 19:17:42.519: INFO: stdout: "Paused\n"
STEP: exposing RC 05/01/23 19:17:42.519
May  1 19:17:42.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May  1 19:17:42.801: INFO: stderr: ""
May  1 19:17:42.801: INFO: stdout: "service/rm2 exposed\n"
May  1 19:17:42.820: INFO: Service rm2 in namespace kubectl-9472 found.
STEP: exposing service 05/01/23 19:17:44.857
May  1 19:17:44.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May  1 19:17:45.129: INFO: stderr: ""
May  1 19:17:45.129: INFO: stdout: "service/rm3 exposed\n"
May  1 19:17:45.147: INFO: Service rm3 in namespace kubectl-9472 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:17:47.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9472" for this suite. 05/01/23 19:17:47.236
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":94,"skipped":1764,"failed":0}
------------------------------
• [SLOW TEST] [11.005 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:36.261
    May  1 19:17:36.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:17:36.263
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:36.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:36.359
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 05/01/23 19:17:36.377
    May  1 19:17:36.377: INFO: namespace kubectl-9472
    May  1 19:17:36.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 create -f -'
    May  1 19:17:39.107: INFO: stderr: ""
    May  1 19:17:39.107: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/01/23 19:17:39.107
    May  1 19:17:40.126: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:17:40.126: INFO: Found 0 / 1
    May  1 19:17:41.127: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:17:41.127: INFO: Found 0 / 1
    May  1 19:17:42.135: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:17:42.135: INFO: Found 1 / 1
    May  1 19:17:42.135: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  1 19:17:42.157: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 19:17:42.157: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  1 19:17:42.157: INFO: wait on agnhost-primary startup in kubectl-9472 
    May  1 19:17:42.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 logs agnhost-primary-svmss agnhost-primary'
    May  1 19:17:42.519: INFO: stderr: ""
    May  1 19:17:42.519: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/01/23 19:17:42.519
    May  1 19:17:42.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May  1 19:17:42.801: INFO: stderr: ""
    May  1 19:17:42.801: INFO: stdout: "service/rm2 exposed\n"
    May  1 19:17:42.820: INFO: Service rm2 in namespace kubectl-9472 found.
    STEP: exposing service 05/01/23 19:17:44.857
    May  1 19:17:44.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-9472 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May  1 19:17:45.129: INFO: stderr: ""
    May  1 19:17:45.129: INFO: stdout: "service/rm3 exposed\n"
    May  1 19:17:45.147: INFO: Service rm3 in namespace kubectl-9472 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:17:47.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9472" for this suite. 05/01/23 19:17:47.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:47.272
May  1 19:17:47.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:17:47.276
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:47.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:47.353
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 05/01/23 19:17:47.375
STEP: setting up watch 05/01/23 19:17:47.376
STEP: submitting the pod to kubernetes 05/01/23 19:17:47.521
STEP: verifying the pod is in kubernetes 05/01/23 19:17:47.607
STEP: verifying pod creation was observed 05/01/23 19:17:47.624
May  1 19:17:47.624: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0" in namespace "pods-9204" to be "running"
May  1 19:17:47.643: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.165428ms
May  1 19:17:49.661: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037361311s
May  1 19:17:51.662: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038395588s
May  1 19:17:51.663: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0" satisfied condition "running"
STEP: deleting the pod gracefully 05/01/23 19:17:51.679
STEP: verifying pod deletion was observed 05/01/23 19:17:51.713
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:17:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9204" for this suite. 05/01/23 19:17:54.341
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":95,"skipped":1795,"failed":0}
------------------------------
• [SLOW TEST] [7.106 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:47.272
    May  1 19:17:47.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:17:47.276
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:47.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:47.353
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 05/01/23 19:17:47.375
    STEP: setting up watch 05/01/23 19:17:47.376
    STEP: submitting the pod to kubernetes 05/01/23 19:17:47.521
    STEP: verifying the pod is in kubernetes 05/01/23 19:17:47.607
    STEP: verifying pod creation was observed 05/01/23 19:17:47.624
    May  1 19:17:47.624: INFO: Waiting up to 5m0s for pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0" in namespace "pods-9204" to be "running"
    May  1 19:17:47.643: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.165428ms
    May  1 19:17:49.661: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037361311s
    May  1 19:17:51.662: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0": Phase="Running", Reason="", readiness=true. Elapsed: 4.038395588s
    May  1 19:17:51.663: INFO: Pod "pod-submit-remove-f9014ca5-7ba6-4b6c-a375-f55321f522d0" satisfied condition "running"
    STEP: deleting the pod gracefully 05/01/23 19:17:51.679
    STEP: verifying pod deletion was observed 05/01/23 19:17:51.713
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:17:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9204" for this suite. 05/01/23 19:17:54.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:17:54.385
May  1 19:17:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 19:17:54.387
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:54.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:54.477
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 05/01/23 19:17:54.491
STEP: Ensuring job reaches completions 05/01/23 19:17:54.518
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 19:18:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4064" for this suite. 05/01/23 19:18:12.603
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":96,"skipped":1845,"failed":0}
------------------------------
• [SLOW TEST] [18.249 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:17:54.385
    May  1 19:17:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 19:17:54.387
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:17:54.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:17:54.477
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 05/01/23 19:17:54.491
    STEP: Ensuring job reaches completions 05/01/23 19:17:54.518
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 19:18:12.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4064" for this suite. 05/01/23 19:18:12.603
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:18:12.641
May  1 19:18:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 19:18:12.644
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:12.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:12.764
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 05/01/23 19:18:12.785
May  1 19:18:12.923: INFO: Waiting up to 5m0s for pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8" in namespace "var-expansion-8398" to be "Succeeded or Failed"
May  1 19:18:12.961: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.277718ms
May  1 19:18:14.982: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058900093s
May  1 19:18:16.993: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070266796s
May  1 19:18:18.980: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057466017s
STEP: Saw pod success 05/01/23 19:18:18.98
May  1 19:18:18.981: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8" satisfied condition "Succeeded or Failed"
May  1 19:18:18.998: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 container dapi-container: <nil>
STEP: delete the pod 05/01/23 19:18:19.17
May  1 19:18:19.222: INFO: Waiting for pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 to disappear
May  1 19:18:19.247: INFO: Pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 19:18:19.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8398" for this suite. 05/01/23 19:18:19.283
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":97,"skipped":1847,"failed":0}
------------------------------
• [SLOW TEST] [6.691 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:18:12.641
    May  1 19:18:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 19:18:12.644
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:12.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:12.764
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 05/01/23 19:18:12.785
    May  1 19:18:12.923: INFO: Waiting up to 5m0s for pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8" in namespace "var-expansion-8398" to be "Succeeded or Failed"
    May  1 19:18:12.961: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.277718ms
    May  1 19:18:14.982: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058900093s
    May  1 19:18:16.993: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070266796s
    May  1 19:18:18.980: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057466017s
    STEP: Saw pod success 05/01/23 19:18:18.98
    May  1 19:18:18.981: INFO: Pod "var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8" satisfied condition "Succeeded or Failed"
    May  1 19:18:18.998: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 container dapi-container: <nil>
    STEP: delete the pod 05/01/23 19:18:19.17
    May  1 19:18:19.222: INFO: Waiting for pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 to disappear
    May  1 19:18:19.247: INFO: Pod var-expansion-47a54fc3-0c62-4d36-9ef5-d02135c688d8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 19:18:19.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8398" for this suite. 05/01/23 19:18:19.283
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:18:19.333
May  1 19:18:19.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:18:19.336
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:19.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:19.425
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-9008 05/01/23 19:18:19.441
STEP: creating service affinity-clusterip-transition in namespace services-9008 05/01/23 19:18:19.441
STEP: creating replication controller affinity-clusterip-transition in namespace services-9008 05/01/23 19:18:19.506
I0501 19:18:19.555549      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9008, replica count: 3
I0501 19:18:22.608909      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 19:18:25.611541      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 19:18:25.664: INFO: Creating new exec pod
May  1 19:18:25.717: INFO: Waiting up to 5m0s for pod "execpod-affinityj75wx" in namespace "services-9008" to be "running"
May  1 19:18:25.742: INFO: Pod "execpod-affinityj75wx": Phase="Pending", Reason="", readiness=false. Elapsed: 24.892097ms
May  1 19:18:27.761: INFO: Pod "execpod-affinityj75wx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044379297s
May  1 19:18:29.765: INFO: Pod "execpod-affinityj75wx": Phase="Running", Reason="", readiness=true. Elapsed: 4.047462159s
May  1 19:18:29.765: INFO: Pod "execpod-affinityj75wx" satisfied condition "running"
May  1 19:18:30.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May  1 19:18:31.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May  1 19:18:31.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:18:31.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.166.222 80'
May  1 19:18:31.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.166.222 80\nConnection to 172.21.166.222 80 port [tcp/http] succeeded!\n"
May  1 19:18:31.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:18:31.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.166.222:80/ ; done'
May  1 19:18:32.840: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n"
May  1 19:18:32.840: INFO: stdout: "\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm"
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:32.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.166.222:80/ ; done'
May  1 19:18:33.800: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n"
May  1 19:18:33.801: INFO: stdout: "\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm"
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
May  1 19:18:33.801: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9008, will wait for the garbage collector to delete the pods 05/01/23 19:18:33.855
May  1 19:18:33.969: INFO: Deleting ReplicationController affinity-clusterip-transition took: 33.901913ms
May  1 19:18:34.170: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.823741ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:18:37.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9008" for this suite. 05/01/23 19:18:37.995
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":98,"skipped":1850,"failed":0}
------------------------------
• [SLOW TEST] [18.708 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:18:19.333
    May  1 19:18:19.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:18:19.336
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:19.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:19.425
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-9008 05/01/23 19:18:19.441
    STEP: creating service affinity-clusterip-transition in namespace services-9008 05/01/23 19:18:19.441
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9008 05/01/23 19:18:19.506
    I0501 19:18:19.555549      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9008, replica count: 3
    I0501 19:18:22.608909      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 19:18:25.611541      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 19:18:25.664: INFO: Creating new exec pod
    May  1 19:18:25.717: INFO: Waiting up to 5m0s for pod "execpod-affinityj75wx" in namespace "services-9008" to be "running"
    May  1 19:18:25.742: INFO: Pod "execpod-affinityj75wx": Phase="Pending", Reason="", readiness=false. Elapsed: 24.892097ms
    May  1 19:18:27.761: INFO: Pod "execpod-affinityj75wx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044379297s
    May  1 19:18:29.765: INFO: Pod "execpod-affinityj75wx": Phase="Running", Reason="", readiness=true. Elapsed: 4.047462159s
    May  1 19:18:29.765: INFO: Pod "execpod-affinityj75wx" satisfied condition "running"
    May  1 19:18:30.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    May  1 19:18:31.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May  1 19:18:31.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:18:31.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.166.222 80'
    May  1 19:18:31.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.166.222 80\nConnection to 172.21.166.222 80 port [tcp/http] succeeded!\n"
    May  1 19:18:31.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:18:31.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.166.222:80/ ; done'
    May  1 19:18:32.840: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n"
    May  1 19:18:32.840: INFO: stdout: "\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-j5m8l\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-sg6l2\naffinity-clusterip-transition-wtsxm"
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-j5m8l
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-sg6l2
    May  1 19:18:32.840: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:32.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-9008 exec execpod-affinityj75wx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.166.222:80/ ; done'
    May  1 19:18:33.800: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.166.222:80/\n"
    May  1 19:18:33.801: INFO: stdout: "\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm\naffinity-clusterip-transition-wtsxm"
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Received response from host: affinity-clusterip-transition-wtsxm
    May  1 19:18:33.801: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9008, will wait for the garbage collector to delete the pods 05/01/23 19:18:33.855
    May  1 19:18:33.969: INFO: Deleting ReplicationController affinity-clusterip-transition took: 33.901913ms
    May  1 19:18:34.170: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 200.823741ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:18:37.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9008" for this suite. 05/01/23 19:18:37.995
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:18:38.042
May  1 19:18:38.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:18:38.044
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:38.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:38.123
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-60da16eb-8904-4c8a-ac9e-d412c9fd6b2a 05/01/23 19:18:38.141
STEP: Creating a pod to test consume configMaps 05/01/23 19:18:38.22
May  1 19:18:38.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48" in namespace "configmap-7630" to be "Succeeded or Failed"
May  1 19:18:38.364: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.530333ms
May  1 19:18:40.401: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057774306s
May  1 19:18:42.392: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048280201s
May  1 19:18:44.383: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039692331s
STEP: Saw pod success 05/01/23 19:18:44.384
May  1 19:18:44.384: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48" satisfied condition "Succeeded or Failed"
May  1 19:18:44.401: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:18:44.449
May  1 19:18:44.504: INFO: Waiting for pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 to disappear
May  1 19:18:44.519: INFO: Pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:18:44.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7630" for this suite. 05/01/23 19:18:44.555
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1852,"failed":0}
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:18:38.042
    May  1 19:18:38.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:18:38.044
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:38.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:38.123
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-60da16eb-8904-4c8a-ac9e-d412c9fd6b2a 05/01/23 19:18:38.141
    STEP: Creating a pod to test consume configMaps 05/01/23 19:18:38.22
    May  1 19:18:38.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48" in namespace "configmap-7630" to be "Succeeded or Failed"
    May  1 19:18:38.364: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.530333ms
    May  1 19:18:40.401: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057774306s
    May  1 19:18:42.392: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048280201s
    May  1 19:18:44.383: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039692331s
    STEP: Saw pod success 05/01/23 19:18:44.384
    May  1 19:18:44.384: INFO: Pod "pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48" satisfied condition "Succeeded or Failed"
    May  1 19:18:44.401: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:18:44.449
    May  1 19:18:44.504: INFO: Waiting for pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 to disappear
    May  1 19:18:44.519: INFO: Pod pod-configmaps-5b305fa5-e94f-4c8f-b635-a2d50d05ac48 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:18:44.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7630" for this suite. 05/01/23 19:18:44.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:18:44.592
May  1 19:18:44.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 19:18:44.598
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:44.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:44.696
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 19:19:44.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9868" for this suite. 05/01/23 19:19:44.882
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":100,"skipped":1885,"failed":0}
------------------------------
• [SLOW TEST] [60.324 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:18:44.592
    May  1 19:18:44.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 19:18:44.598
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:18:44.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:18:44.696
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 19:19:44.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9868" for this suite. 05/01/23 19:19:44.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:19:44.926
May  1 19:19:44.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:19:44.928
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:19:44.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:19:45.026
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 05/01/23 19:19:45.048
May  1 19:19:45.188: INFO: Waiting up to 5m0s for pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d" in namespace "emptydir-7664" to be "Succeeded or Failed"
May  1 19:19:45.209: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.932629ms
May  1 19:19:47.229: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040705395s
May  1 19:19:49.227: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038957923s
May  1 19:19:51.229: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040769736s
May  1 19:19:53.230: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041866414s
STEP: Saw pod success 05/01/23 19:19:53.23
May  1 19:19:53.230: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d" satisfied condition "Succeeded or Failed"
May  1 19:19:53.262: INFO: Trying to get logs from node 10.45.145.124 pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d container test-container: <nil>
STEP: delete the pod 05/01/23 19:19:53.365
May  1 19:19:53.414: INFO: Waiting for pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d to disappear
May  1 19:19:53.430: INFO: Pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:19:53.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7664" for this suite. 05/01/23 19:19:53.477
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":101,"skipped":1919,"failed":0}
------------------------------
• [SLOW TEST] [8.581 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:19:44.926
    May  1 19:19:44.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:19:44.928
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:19:44.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:19:45.026
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/01/23 19:19:45.048
    May  1 19:19:45.188: INFO: Waiting up to 5m0s for pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d" in namespace "emptydir-7664" to be "Succeeded or Failed"
    May  1 19:19:45.209: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.932629ms
    May  1 19:19:47.229: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040705395s
    May  1 19:19:49.227: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038957923s
    May  1 19:19:51.229: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040769736s
    May  1 19:19:53.230: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.041866414s
    STEP: Saw pod success 05/01/23 19:19:53.23
    May  1 19:19:53.230: INFO: Pod "pod-87287c43-a0c1-4f71-a089-61fb1e83a52d" satisfied condition "Succeeded or Failed"
    May  1 19:19:53.262: INFO: Trying to get logs from node 10.45.145.124 pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d container test-container: <nil>
    STEP: delete the pod 05/01/23 19:19:53.365
    May  1 19:19:53.414: INFO: Waiting for pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d to disappear
    May  1 19:19:53.430: INFO: Pod pod-87287c43-a0c1-4f71-a089-61fb1e83a52d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:19:53.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7664" for this suite. 05/01/23 19:19:53.477
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:19:53.508
May  1 19:19:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-runtime 05/01/23 19:19:53.509
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:19:53.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:19:53.581
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 05/01/23 19:19:53.598
STEP: wait for the container to reach Succeeded 05/01/23 19:19:53.699
STEP: get the container status 05/01/23 19:19:59.834
STEP: the container should be terminated 05/01/23 19:19:59.853
STEP: the termination message should be set 05/01/23 19:19:59.853
May  1 19:19:59.854: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/01/23 19:19:59.854
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  1 19:19:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3595" for this suite. 05/01/23 19:19:59.949
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":102,"skipped":1920,"failed":0}
------------------------------
• [SLOW TEST] [6.471 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:19:53.508
    May  1 19:19:53.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-runtime 05/01/23 19:19:53.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:19:53.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:19:53.581
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 05/01/23 19:19:53.598
    STEP: wait for the container to reach Succeeded 05/01/23 19:19:53.699
    STEP: get the container status 05/01/23 19:19:59.834
    STEP: the container should be terminated 05/01/23 19:19:59.853
    STEP: the termination message should be set 05/01/23 19:19:59.853
    May  1 19:19:59.854: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/01/23 19:19:59.854
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  1 19:19:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3595" for this suite. 05/01/23 19:19:59.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:19:59.98
May  1 19:19:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:19:59.982
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:00.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:00.091
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 05/01/23 19:20:00.113
May  1 19:20:00.310: INFO: Waiting up to 5m0s for pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf" in namespace "downward-api-6934" to be "Succeeded or Failed"
May  1 19:20:00.334: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.042804ms
May  1 19:20:02.362: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051765165s
May  1 19:20:04.354: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043740397s
May  1 19:20:06.365: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054639785s
STEP: Saw pod success 05/01/23 19:20:06.365
May  1 19:20:06.365: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf" satisfied condition "Succeeded or Failed"
May  1 19:20:06.400: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf container dapi-container: <nil>
STEP: delete the pod 05/01/23 19:20:06.499
May  1 19:20:06.592: INFO: Waiting for pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf to disappear
May  1 19:20:06.613: INFO: Pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  1 19:20:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6934" for this suite. 05/01/23 19:20:06.648
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":103,"skipped":1929,"failed":0}
------------------------------
• [SLOW TEST] [6.699 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:19:59.98
    May  1 19:19:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:19:59.982
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:00.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:00.091
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 05/01/23 19:20:00.113
    May  1 19:20:00.310: INFO: Waiting up to 5m0s for pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf" in namespace "downward-api-6934" to be "Succeeded or Failed"
    May  1 19:20:00.334: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.042804ms
    May  1 19:20:02.362: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051765165s
    May  1 19:20:04.354: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043740397s
    May  1 19:20:06.365: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054639785s
    STEP: Saw pod success 05/01/23 19:20:06.365
    May  1 19:20:06.365: INFO: Pod "downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf" satisfied condition "Succeeded or Failed"
    May  1 19:20:06.400: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf container dapi-container: <nil>
    STEP: delete the pod 05/01/23 19:20:06.499
    May  1 19:20:06.592: INFO: Waiting for pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf to disappear
    May  1 19:20:06.613: INFO: Pod downward-api-ebe118ac-b94b-4dca-a829-0a6e6bd02baf no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  1 19:20:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6934" for this suite. 05/01/23 19:20:06.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:20:06.683
May  1 19:20:06.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:20:06.685
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:06.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:06.765
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:20:06.915
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:20:08.034
STEP: Deploying the webhook pod 05/01/23 19:20:08.097
STEP: Wait for the deployment to be ready 05/01/23 19:20:08.153
May  1 19:20:08.215: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:20:10.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:20:12.339
STEP: Verifying the service has paired with the endpoint 05/01/23 19:20:12.456
May  1 19:20:13.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 05/01/23 19:20:13.763
STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:20:13.893
STEP: Deleting the collection of validation webhooks 05/01/23 19:20:14.009
STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:20:14.214
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:20:14.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6387" for this suite. 05/01/23 19:20:14.308
STEP: Destroying namespace "webhook-6387-markers" for this suite. 05/01/23 19:20:14.37
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":104,"skipped":1944,"failed":0}
------------------------------
• [SLOW TEST] [7.932 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:20:06.683
    May  1 19:20:06.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:20:06.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:06.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:06.765
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:20:06.915
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:20:08.034
    STEP: Deploying the webhook pod 05/01/23 19:20:08.097
    STEP: Wait for the deployment to be ready 05/01/23 19:20:08.153
    May  1 19:20:08.215: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:20:10.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 20, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:20:12.339
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:20:12.456
    May  1 19:20:13.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 05/01/23 19:20:13.763
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:20:13.893
    STEP: Deleting the collection of validation webhooks 05/01/23 19:20:14.009
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/01/23 19:20:14.214
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:20:14.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6387" for this suite. 05/01/23 19:20:14.308
    STEP: Destroying namespace "webhook-6387-markers" for this suite. 05/01/23 19:20:14.37
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:20:14.619
May  1 19:20:14.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:20:14.622
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:14.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:14.712
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 05/01/23 19:20:14.731
May  1 19:20:14.835: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a" in namespace "emptydir-4957" to be "running"
May  1 19:20:14.854: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.936413ms
May  1 19:20:16.882: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046051546s
May  1 19:20:18.872: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036798068s
May  1 19:20:18.873: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/01/23 19:20:18.873
May  1 19:20:18.874: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4957 PodName:pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:20:18.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:20:18.876: INFO: ExecWithOptions: Clientset creation
May  1 19:20:18.876: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-4957/pods/pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May  1 19:20:19.294: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:20:19.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4957" for this suite. 05/01/23 19:20:19.326
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":105,"skipped":1945,"failed":0}
------------------------------
• [4.737 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:20:14.619
    May  1 19:20:14.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:20:14.622
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:14.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:14.712
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 05/01/23 19:20:14.731
    May  1 19:20:14.835: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a" in namespace "emptydir-4957" to be "running"
    May  1 19:20:14.854: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.936413ms
    May  1 19:20:16.882: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046051546s
    May  1 19:20:18.872: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036798068s
    May  1 19:20:18.873: INFO: Pod "pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/01/23 19:20:18.873
    May  1 19:20:18.874: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4957 PodName:pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:20:18.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:20:18.876: INFO: ExecWithOptions: Clientset creation
    May  1 19:20:18.876: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-4957/pods/pod-sharedvolume-21cfd7e0-0719-48df-9f41-2ad9d7ad458a/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May  1 19:20:19.294: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:20:19.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4957" for this suite. 05/01/23 19:20:19.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:20:19.365
May  1 19:20:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename subpath 05/01/23 19:20:19.368
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:19.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:19.448
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/01/23 19:20:19.466
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-tqgc 05/01/23 19:20:19.515
STEP: Creating a pod to test atomic-volume-subpath 05/01/23 19:20:19.515
May  1 19:20:19.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tqgc" in namespace "subpath-9037" to be "Succeeded or Failed"
May  1 19:20:19.636: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Pending", Reason="", readiness=false. Elapsed: 33.499389ms
May  1 19:20:21.661: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058603064s
May  1 19:20:23.655: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 4.052216038s
May  1 19:20:25.669: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 6.066578879s
May  1 19:20:27.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 8.053439733s
May  1 19:20:29.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 10.054593873s
May  1 19:20:31.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 12.05376374s
May  1 19:20:33.675: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 14.072584199s
May  1 19:20:35.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 16.054790659s
May  1 19:20:37.655: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 18.052612045s
May  1 19:20:39.653: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 20.050796378s
May  1 19:20:41.659: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 22.056846649s
May  1 19:20:43.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=false. Elapsed: 24.053128216s
May  1 19:20:45.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=false. Elapsed: 26.054444882s
May  1 19:20:47.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.053541984s
STEP: Saw pod success 05/01/23 19:20:47.656
May  1 19:20:47.656: INFO: Pod "pod-subpath-test-secret-tqgc" satisfied condition "Succeeded or Failed"
May  1 19:20:47.677: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-secret-tqgc container test-container-subpath-secret-tqgc: <nil>
STEP: delete the pod 05/01/23 19:20:47.729
May  1 19:20:47.772: INFO: Waiting for pod pod-subpath-test-secret-tqgc to disappear
May  1 19:20:47.805: INFO: Pod pod-subpath-test-secret-tqgc no longer exists
STEP: Deleting pod pod-subpath-test-secret-tqgc 05/01/23 19:20:47.805
May  1 19:20:47.805: INFO: Deleting pod "pod-subpath-test-secret-tqgc" in namespace "subpath-9037"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  1 19:20:47.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9037" for this suite. 05/01/23 19:20:47.852
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":106,"skipped":1962,"failed":0}
------------------------------
• [SLOW TEST] [28.524 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:20:19.365
    May  1 19:20:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename subpath 05/01/23 19:20:19.368
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:19.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:19.448
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/01/23 19:20:19.466
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-tqgc 05/01/23 19:20:19.515
    STEP: Creating a pod to test atomic-volume-subpath 05/01/23 19:20:19.515
    May  1 19:20:19.602: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tqgc" in namespace "subpath-9037" to be "Succeeded or Failed"
    May  1 19:20:19.636: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Pending", Reason="", readiness=false. Elapsed: 33.499389ms
    May  1 19:20:21.661: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058603064s
    May  1 19:20:23.655: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 4.052216038s
    May  1 19:20:25.669: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 6.066578879s
    May  1 19:20:27.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 8.053439733s
    May  1 19:20:29.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 10.054593873s
    May  1 19:20:31.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 12.05376374s
    May  1 19:20:33.675: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 14.072584199s
    May  1 19:20:35.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 16.054790659s
    May  1 19:20:37.655: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 18.052612045s
    May  1 19:20:39.653: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 20.050796378s
    May  1 19:20:41.659: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=true. Elapsed: 22.056846649s
    May  1 19:20:43.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=false. Elapsed: 24.053128216s
    May  1 19:20:45.657: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Running", Reason="", readiness=false. Elapsed: 26.054444882s
    May  1 19:20:47.656: INFO: Pod "pod-subpath-test-secret-tqgc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.053541984s
    STEP: Saw pod success 05/01/23 19:20:47.656
    May  1 19:20:47.656: INFO: Pod "pod-subpath-test-secret-tqgc" satisfied condition "Succeeded or Failed"
    May  1 19:20:47.677: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-secret-tqgc container test-container-subpath-secret-tqgc: <nil>
    STEP: delete the pod 05/01/23 19:20:47.729
    May  1 19:20:47.772: INFO: Waiting for pod pod-subpath-test-secret-tqgc to disappear
    May  1 19:20:47.805: INFO: Pod pod-subpath-test-secret-tqgc no longer exists
    STEP: Deleting pod pod-subpath-test-secret-tqgc 05/01/23 19:20:47.805
    May  1 19:20:47.805: INFO: Deleting pod "pod-subpath-test-secret-tqgc" in namespace "subpath-9037"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  1 19:20:47.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9037" for this suite. 05/01/23 19:20:47.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:20:47.891
May  1 19:20:47.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption 05/01/23 19:20:47.892
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:47.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:47.985
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 05/01/23 19:20:48.022
STEP: Waiting for all pods to be running 05/01/23 19:20:48.25
May  1 19:20:48.274: INFO: running pods: 0 < 3
May  1 19:20:50.293: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  1 19:20:52.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4092" for this suite. 05/01/23 19:20:52.357
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":107,"skipped":1976,"failed":0}
------------------------------
• [4.497 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:20:47.891
    May  1 19:20:47.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption 05/01/23 19:20:47.892
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:47.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:47.985
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 05/01/23 19:20:48.022
    STEP: Waiting for all pods to be running 05/01/23 19:20:48.25
    May  1 19:20:48.274: INFO: running pods: 0 < 3
    May  1 19:20:50.293: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  1 19:20:52.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-4092" for this suite. 05/01/23 19:20:52.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:20:52.395
May  1 19:20:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:20:52.397
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:52.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:52.478
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
May  1 19:20:52.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:21:13.909
May  1 19:21:13.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 create -f -'
May  1 19:21:16.353: INFO: stderr: ""
May  1 19:21:16.353: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  1 19:21:16.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 delete e2e-test-crd-publish-openapi-8976-crds test-cr'
May  1 19:21:16.614: INFO: stderr: ""
May  1 19:21:16.614: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May  1 19:21:16.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 apply -f -'
May  1 19:21:20.109: INFO: stderr: ""
May  1 19:21:20.109: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May  1 19:21:20.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 delete e2e-test-crd-publish-openapi-8976-crds test-cr'
May  1 19:21:20.268: INFO: stderr: ""
May  1 19:21:20.268: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/01/23 19:21:20.268
May  1 19:21:20.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 explain e2e-test-crd-publish-openapi-8976-crds'
May  1 19:21:21.495: INFO: stderr: ""
May  1 19:21:21.495: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8976-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:21:43.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7892" for this suite. 05/01/23 19:21:43.317
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":108,"skipped":2041,"failed":0}
------------------------------
• [SLOW TEST] [50.966 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:20:52.395
    May  1 19:20:52.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:20:52.397
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:20:52.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:20:52.478
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    May  1 19:20:52.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:21:13.909
    May  1 19:21:13.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 create -f -'
    May  1 19:21:16.353: INFO: stderr: ""
    May  1 19:21:16.353: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  1 19:21:16.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 delete e2e-test-crd-publish-openapi-8976-crds test-cr'
    May  1 19:21:16.614: INFO: stderr: ""
    May  1 19:21:16.614: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May  1 19:21:16.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 apply -f -'
    May  1 19:21:20.109: INFO: stderr: ""
    May  1 19:21:20.109: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May  1 19:21:20.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 --namespace=crd-publish-openapi-7892 delete e2e-test-crd-publish-openapi-8976-crds test-cr'
    May  1 19:21:20.268: INFO: stderr: ""
    May  1 19:21:20.268: INFO: stdout: "e2e-test-crd-publish-openapi-8976-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/01/23 19:21:20.268
    May  1 19:21:20.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-7892 explain e2e-test-crd-publish-openapi-8976-crds'
    May  1 19:21:21.495: INFO: stderr: ""
    May  1 19:21:21.495: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8976-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:21:43.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7892" for this suite. 05/01/23 19:21:43.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:21:43.363
May  1 19:21:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 19:21:43.366
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:21:43.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:21:43.561
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 05/01/23 19:21:43.575
STEP: Creating a ResourceQuota 05/01/23 19:21:48.594
STEP: Ensuring resource quota status is calculated 05/01/23 19:21:48.619
STEP: Creating a ReplicaSet 05/01/23 19:21:50.65
STEP: Ensuring resource quota status captures replicaset creation 05/01/23 19:21:50.776
STEP: Deleting a ReplicaSet 05/01/23 19:21:52.8
STEP: Ensuring resource quota status released usage 05/01/23 19:21:52.835
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 19:21:54.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9071" for this suite. 05/01/23 19:21:54.906
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":109,"skipped":2050,"failed":0}
------------------------------
• [SLOW TEST] [11.565 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:21:43.363
    May  1 19:21:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 19:21:43.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:21:43.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:21:43.561
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 05/01/23 19:21:43.575
    STEP: Creating a ResourceQuota 05/01/23 19:21:48.594
    STEP: Ensuring resource quota status is calculated 05/01/23 19:21:48.619
    STEP: Creating a ReplicaSet 05/01/23 19:21:50.65
    STEP: Ensuring resource quota status captures replicaset creation 05/01/23 19:21:50.776
    STEP: Deleting a ReplicaSet 05/01/23 19:21:52.8
    STEP: Ensuring resource quota status released usage 05/01/23 19:21:52.835
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 19:21:54.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9071" for this suite. 05/01/23 19:21:54.906
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:21:54.929
May  1 19:21:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename cronjob 05/01/23 19:21:54.932
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:21:55.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:21:55.026
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/01/23 19:21:55.039
W0501 19:21:55.067824      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 05/01/23 19:21:55.068
STEP: Ensuring exactly one is scheduled 05/01/23 19:22:01.1
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/01/23 19:22:01.116
STEP: Ensuring the job is replaced with a new one 05/01/23 19:22:01.155
STEP: Removing cronjob 05/01/23 19:23:01.172
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  1 19:23:01.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3930" for this suite. 05/01/23 19:23:01.211
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":110,"skipped":2051,"failed":0}
------------------------------
• [SLOW TEST] [66.305 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:21:54.929
    May  1 19:21:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename cronjob 05/01/23 19:21:54.932
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:21:55.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:21:55.026
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/01/23 19:21:55.039
    W0501 19:21:55.067824      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 05/01/23 19:21:55.068
    STEP: Ensuring exactly one is scheduled 05/01/23 19:22:01.1
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/01/23 19:22:01.116
    STEP: Ensuring the job is replaced with a new one 05/01/23 19:22:01.155
    STEP: Removing cronjob 05/01/23 19:23:01.172
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  1 19:23:01.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3930" for this suite. 05/01/23 19:23:01.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:01.244
May  1 19:23:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 19:23:01.247
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:01.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:01.335
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/01/23 19:23:01.347
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1973;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1973;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +notcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_tcp@PTR;sleep 1; done
 05/01/23 19:23:01.423
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1973;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1973;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +notcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_tcp@PTR;sleep 1; done
 05/01/23 19:23:01.423
STEP: creating a pod to probe DNS 05/01/23 19:23:01.423
STEP: submitting the pod to kubernetes 05/01/23 19:23:01.424
May  1 19:23:01.503: INFO: Waiting up to 15m0s for pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0" in namespace "dns-1973" to be "running"
May  1 19:23:01.526: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.337728ms
May  1 19:23:03.545: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041819164s
May  1 19:23:05.548: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04473161s
May  1 19:23:07.569: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Running", Reason="", readiness=true. Elapsed: 6.065864779s
May  1 19:23:07.569: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0" satisfied condition "running"
STEP: retrieving the pod 05/01/23 19:23:07.569
STEP: looking for the results for each expected name from probers 05/01/23 19:23:07.59
May  1 19:23:07.645: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.672: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.770: INFO: Unable to read wheezy_udp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:07.900: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.104: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.157: INFO: Unable to read jessie_udp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.184: INFO: Unable to read jessie_tcp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.236: INFO: Unable to read jessie_udp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.266: INFO: Unable to read jessie_tcp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.303: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.326: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
May  1 19:23:08.531: INFO: Lookups using dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1973 wheezy_tcp@dns-test-service.dns-1973 wheezy_udp@dns-test-service.dns-1973.svc wheezy_tcp@dns-test-service.dns-1973.svc wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1973 jessie_tcp@dns-test-service.dns-1973 jessie_udp@dns-test-service.dns-1973.svc jessie_tcp@dns-test-service.dns-1973.svc jessie_udp@_http._tcp.dns-test-service.dns-1973.svc jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc]

May  1 19:23:14.266: INFO: DNS probes using dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0 succeeded

STEP: deleting the pod 05/01/23 19:23:14.266
STEP: deleting the test service 05/01/23 19:23:14.361
STEP: deleting the test headless service 05/01/23 19:23:14.496
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 19:23:14.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1973" for this suite. 05/01/23 19:23:14.57
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":111,"skipped":2086,"failed":0}
------------------------------
• [SLOW TEST] [13.355 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:01.244
    May  1 19:23:01.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 19:23:01.247
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:01.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:01.335
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/01/23 19:23:01.347
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1973;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1973;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +notcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_tcp@PTR;sleep 1; done
     05/01/23 19:23:01.423
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1973;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1973;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1973.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1973.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1973.svc;check="$$(dig +notcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_udp@PTR;check="$$(dig +tcp +noall +answer +search 238.213.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.213.238_tcp@PTR;sleep 1; done
     05/01/23 19:23:01.423
    STEP: creating a pod to probe DNS 05/01/23 19:23:01.423
    STEP: submitting the pod to kubernetes 05/01/23 19:23:01.424
    May  1 19:23:01.503: INFO: Waiting up to 15m0s for pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0" in namespace "dns-1973" to be "running"
    May  1 19:23:01.526: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.337728ms
    May  1 19:23:03.545: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041819164s
    May  1 19:23:05.548: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04473161s
    May  1 19:23:07.569: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0": Phase="Running", Reason="", readiness=true. Elapsed: 6.065864779s
    May  1 19:23:07.569: INFO: Pod "dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 19:23:07.569
    STEP: looking for the results for each expected name from probers 05/01/23 19:23:07.59
    May  1 19:23:07.645: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.672: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.745: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.770: INFO: Unable to read wheezy_udp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.822: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.852: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:07.900: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.104: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.130: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.157: INFO: Unable to read jessie_udp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.184: INFO: Unable to read jessie_tcp@dns-test-service.dns-1973 from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.236: INFO: Unable to read jessie_udp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.266: INFO: Unable to read jessie_tcp@dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.303: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.326: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc from pod dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0: the server could not find the requested resource (get pods dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0)
    May  1 19:23:08.531: INFO: Lookups using dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1973 wheezy_tcp@dns-test-service.dns-1973 wheezy_udp@dns-test-service.dns-1973.svc wheezy_tcp@dns-test-service.dns-1973.svc wheezy_udp@_http._tcp.dns-test-service.dns-1973.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1973.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1973 jessie_tcp@dns-test-service.dns-1973 jessie_udp@dns-test-service.dns-1973.svc jessie_tcp@dns-test-service.dns-1973.svc jessie_udp@_http._tcp.dns-test-service.dns-1973.svc jessie_tcp@_http._tcp.dns-test-service.dns-1973.svc]

    May  1 19:23:14.266: INFO: DNS probes using dns-1973/dns-test-7cd132df-1e7b-4f14-a859-a52b42eb04b0 succeeded

    STEP: deleting the pod 05/01/23 19:23:14.266
    STEP: deleting the test service 05/01/23 19:23:14.361
    STEP: deleting the test headless service 05/01/23 19:23:14.496
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 19:23:14.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1973" for this suite. 05/01/23 19:23:14.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:14.609
May  1 19:23:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:23:14.614
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:14.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:14.721
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:23:14.849
May  1 19:23:14.919: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8613" to be "running and ready"
May  1 19:23:14.938: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.637749ms
May  1 19:23:14.938: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:23:16.965: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045223778s
May  1 19:23:16.965: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:23:18.998: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.078612568s
May  1 19:23:18.998: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  1 19:23:18.998: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 05/01/23 19:23:19.048
May  1 19:23:19.114: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8613" to be "running and ready"
May  1 19:23:19.161: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 46.199645ms
May  1 19:23:19.161: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 19:23:21.179: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.064826473s
May  1 19:23:21.179: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May  1 19:23:21.179: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/01/23 19:23:21.204
STEP: delete the pod with lifecycle hook 05/01/23 19:23:21.312
May  1 19:23:21.349: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  1 19:23:21.375: INFO: Pod pod-with-poststart-http-hook still exists
May  1 19:23:23.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  1 19:23:23.395: INFO: Pod pod-with-poststart-http-hook still exists
May  1 19:23:25.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  1 19:23:25.394: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  1 19:23:25.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8613" for this suite. 05/01/23 19:23:25.427
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":112,"skipped":2099,"failed":0}
------------------------------
• [SLOW TEST] [10.849 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:14.609
    May  1 19:23:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:23:14.614
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:14.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:14.721
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:23:14.849
    May  1 19:23:14.919: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8613" to be "running and ready"
    May  1 19:23:14.938: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.637749ms
    May  1 19:23:14.938: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:23:16.965: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045223778s
    May  1 19:23:16.965: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:23:18.998: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.078612568s
    May  1 19:23:18.998: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  1 19:23:18.998: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 05/01/23 19:23:19.048
    May  1 19:23:19.114: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8613" to be "running and ready"
    May  1 19:23:19.161: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 46.199645ms
    May  1 19:23:19.161: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:23:21.179: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.064826473s
    May  1 19:23:21.179: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May  1 19:23:21.179: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/01/23 19:23:21.204
    STEP: delete the pod with lifecycle hook 05/01/23 19:23:21.312
    May  1 19:23:21.349: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  1 19:23:21.375: INFO: Pod pod-with-poststart-http-hook still exists
    May  1 19:23:23.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  1 19:23:23.395: INFO: Pod pod-with-poststart-http-hook still exists
    May  1 19:23:25.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May  1 19:23:25.394: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  1 19:23:25.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8613" for this suite. 05/01/23 19:23:25.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:25.499
May  1 19:23:25.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 19:23:25.504
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:25.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:25.618
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5529 05/01/23 19:23:25.645
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5529 05/01/23 19:23:25.734
May  1 19:23:25.794: INFO: Found 0 stateful pods, waiting for 1
May  1 19:23:35.814: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/01/23 19:23:35.901
STEP: Getting /status 05/01/23 19:23:35.937
May  1 19:23:35.959: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/01/23 19:23:35.959
May  1 19:23:36.012: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/01/23 19:23:36.012
May  1 19:23:36.023: INFO: Observed &StatefulSet event: ADDED
May  1 19:23:36.023: INFO: Found Statefulset ss in namespace statefulset-5529 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  1 19:23:36.023: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/01/23 19:23:36.023
May  1 19:23:36.023: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  1 19:23:36.053: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/01/23 19:23:36.053
May  1 19:23:36.063: INFO: Observed &StatefulSet event: ADDED
May  1 19:23:36.063: INFO: Observed Statefulset ss in namespace statefulset-5529 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  1 19:23:36.063: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 19:23:36.064: INFO: Deleting all statefulset in ns statefulset-5529
May  1 19:23:36.093: INFO: Scaling statefulset ss to 0
May  1 19:23:46.220: INFO: Waiting for statefulset status.replicas updated to 0
May  1 19:23:46.241: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 19:23:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5529" for this suite. 05/01/23 19:23:46.385
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":113,"skipped":2116,"failed":0}
------------------------------
• [SLOW TEST] [20.915 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:25.499
    May  1 19:23:25.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 19:23:25.504
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:25.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:25.618
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5529 05/01/23 19:23:25.645
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5529 05/01/23 19:23:25.734
    May  1 19:23:25.794: INFO: Found 0 stateful pods, waiting for 1
    May  1 19:23:35.814: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/01/23 19:23:35.901
    STEP: Getting /status 05/01/23 19:23:35.937
    May  1 19:23:35.959: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/01/23 19:23:35.959
    May  1 19:23:36.012: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/01/23 19:23:36.012
    May  1 19:23:36.023: INFO: Observed &StatefulSet event: ADDED
    May  1 19:23:36.023: INFO: Found Statefulset ss in namespace statefulset-5529 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  1 19:23:36.023: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/01/23 19:23:36.023
    May  1 19:23:36.023: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  1 19:23:36.053: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/01/23 19:23:36.053
    May  1 19:23:36.063: INFO: Observed &StatefulSet event: ADDED
    May  1 19:23:36.063: INFO: Observed Statefulset ss in namespace statefulset-5529 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  1 19:23:36.063: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 19:23:36.064: INFO: Deleting all statefulset in ns statefulset-5529
    May  1 19:23:36.093: INFO: Scaling statefulset ss to 0
    May  1 19:23:46.220: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 19:23:46.241: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 19:23:46.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5529" for this suite. 05/01/23 19:23:46.385
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:46.419
May  1 19:23:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:23:46.423
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:46.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:46.536
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/01/23 19:23:46.557
May  1 19:23:46.679: INFO: Waiting up to 5m0s for pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924" in namespace "emptydir-6121" to be "Succeeded or Failed"
May  1 19:23:46.711: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 31.476324ms
May  1 19:23:48.736: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056628774s
May  1 19:23:50.730: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050102985s
May  1 19:23:52.732: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052398699s
STEP: Saw pod success 05/01/23 19:23:52.732
May  1 19:23:52.732: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924" satisfied condition "Succeeded or Failed"
May  1 19:23:52.750: INFO: Trying to get logs from node 10.45.145.124 pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 container test-container: <nil>
STEP: delete the pod 05/01/23 19:23:52.846
May  1 19:23:52.909: INFO: Waiting for pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 to disappear
May  1 19:23:52.927: INFO: Pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:23:52.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6121" for this suite. 05/01/23 19:23:52.948
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":114,"skipped":2118,"failed":0}
------------------------------
• [SLOW TEST] [6.553 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:46.419
    May  1 19:23:46.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:23:46.423
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:46.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:46.536
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/01/23 19:23:46.557
    May  1 19:23:46.679: INFO: Waiting up to 5m0s for pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924" in namespace "emptydir-6121" to be "Succeeded or Failed"
    May  1 19:23:46.711: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 31.476324ms
    May  1 19:23:48.736: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056628774s
    May  1 19:23:50.730: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050102985s
    May  1 19:23:52.732: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052398699s
    STEP: Saw pod success 05/01/23 19:23:52.732
    May  1 19:23:52.732: INFO: Pod "pod-000089cc-6bac-4b7e-83ac-578d56fab924" satisfied condition "Succeeded or Failed"
    May  1 19:23:52.750: INFO: Trying to get logs from node 10.45.145.124 pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 container test-container: <nil>
    STEP: delete the pod 05/01/23 19:23:52.846
    May  1 19:23:52.909: INFO: Waiting for pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 to disappear
    May  1 19:23:52.927: INFO: Pod pod-000089cc-6bac-4b7e-83ac-578d56fab924 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:23:52.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6121" for this suite. 05/01/23 19:23:52.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:52.976
May  1 19:23:52.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:23:52.979
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:53.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:53.064
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
May  1 19:23:53.212: INFO: Waiting up to 5m0s for pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206" in namespace "svcaccounts-4455" to be "running"
May  1 19:23:53.253: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Pending", Reason="", readiness=false. Elapsed: 40.226702ms
May  1 19:23:55.306: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094155397s
May  1 19:23:57.274: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Running", Reason="", readiness=true. Elapsed: 4.06140474s
May  1 19:23:57.274: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206" satisfied condition "running"
STEP: reading a file in the container 05/01/23 19:23:57.274
May  1 19:23:57.274: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/01/23 19:23:57.804
May  1 19:23:57.804: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/01/23 19:23:58.458
May  1 19:23:58.458: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May  1 19:23:59.005: INFO: Got root ca configmap in namespace "svcaccounts-4455"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 19:23:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4455" for this suite. 05/01/23 19:23:59.039
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":115,"skipped":2133,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:52.976
    May  1 19:23:52.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:23:52.979
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:53.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:53.064
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    May  1 19:23:53.212: INFO: Waiting up to 5m0s for pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206" in namespace "svcaccounts-4455" to be "running"
    May  1 19:23:53.253: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Pending", Reason="", readiness=false. Elapsed: 40.226702ms
    May  1 19:23:55.306: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094155397s
    May  1 19:23:57.274: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206": Phase="Running", Reason="", readiness=true. Elapsed: 4.06140474s
    May  1 19:23:57.274: INFO: Pod "pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206" satisfied condition "running"
    STEP: reading a file in the container 05/01/23 19:23:57.274
    May  1 19:23:57.274: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/01/23 19:23:57.804
    May  1 19:23:57.804: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/01/23 19:23:58.458
    May  1 19:23:58.458: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4455 pod-service-account-5c12d9b0-8227-4a43-a07f-819214352206 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May  1 19:23:59.005: INFO: Got root ca configmap in namespace "svcaccounts-4455"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 19:23:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4455" for this suite. 05/01/23 19:23:59.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:23:59.071
May  1 19:23:59.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:23:59.073
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:59.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:59.168
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-77e59a4c-b77d-4745-a86a-a7370cc0b8ef 05/01/23 19:23:59.184
STEP: Creating a pod to test consume configMaps 05/01/23 19:23:59.219
May  1 19:23:59.356: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee" in namespace "configmap-1881" to be "Succeeded or Failed"
May  1 19:23:59.379: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.732317ms
May  1 19:24:01.397: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041114196s
May  1 19:24:03.407: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050342061s
May  1 19:24:05.400: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043188305s
STEP: Saw pod success 05/01/23 19:24:05.4
May  1 19:24:05.400: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee" satisfied condition "Succeeded or Failed"
May  1 19:24:05.418: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:24:05.452
May  1 19:24:05.509: INFO: Waiting for pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee to disappear
May  1 19:24:05.526: INFO: Pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:24:05.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1881" for this suite. 05/01/23 19:24:05.548
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":116,"skipped":2162,"failed":0}
------------------------------
• [SLOW TEST] [6.501 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:23:59.071
    May  1 19:23:59.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:23:59.073
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:23:59.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:23:59.168
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-77e59a4c-b77d-4745-a86a-a7370cc0b8ef 05/01/23 19:23:59.184
    STEP: Creating a pod to test consume configMaps 05/01/23 19:23:59.219
    May  1 19:23:59.356: INFO: Waiting up to 5m0s for pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee" in namespace "configmap-1881" to be "Succeeded or Failed"
    May  1 19:23:59.379: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.732317ms
    May  1 19:24:01.397: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041114196s
    May  1 19:24:03.407: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050342061s
    May  1 19:24:05.400: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043188305s
    STEP: Saw pod success 05/01/23 19:24:05.4
    May  1 19:24:05.400: INFO: Pod "pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee" satisfied condition "Succeeded or Failed"
    May  1 19:24:05.418: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:24:05.452
    May  1 19:24:05.509: INFO: Waiting for pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee to disappear
    May  1 19:24:05.526: INFO: Pod pod-configmaps-76c56412-327c-4319-8c64-244abcffb4ee no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:24:05.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1881" for this suite. 05/01/23 19:24:05.548
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:05.577
May  1 19:24:05.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 19:24:05.579
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:05.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:05.656
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May  1 19:24:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:24:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3868" for this suite. 05/01/23 19:24:06.82
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":117,"skipped":2186,"failed":0}
------------------------------
• [1.289 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:05.577
    May  1 19:24:05.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 19:24:05.579
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:05.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:05.656
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May  1 19:24:05.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:24:06.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3868" for this suite. 05/01/23 19:24:06.82
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:06.867
May  1 19:24:06.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:24:06.869
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:07.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:07.053
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 05/01/23 19:24:07.069
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/01/23 19:24:07.092
STEP: patching the secret 05/01/23 19:24:07.42
STEP: deleting the secret using a LabelSelector 05/01/23 19:24:07.468
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/01/23 19:24:07.542
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  1 19:24:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1631" for this suite. 05/01/23 19:24:07.888
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":118,"skipped":2188,"failed":0}
------------------------------
• [1.043 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:06.867
    May  1 19:24:06.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:24:06.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:07.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:07.053
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 05/01/23 19:24:07.069
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/01/23 19:24:07.092
    STEP: patching the secret 05/01/23 19:24:07.42
    STEP: deleting the secret using a LabelSelector 05/01/23 19:24:07.468
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/01/23 19:24:07.542
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:24:07.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1631" for this suite. 05/01/23 19:24:07.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:07.914
May  1 19:24:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:24:07.916
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:08.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:08.031
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:24:08.112
May  1 19:24:08.285: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4923" to be "running and ready"
May  1 19:24:08.317: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.358031ms
May  1 19:24:08.317: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:24:10.348: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063609998s
May  1 19:24:10.348: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:24:12.341: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.056770961s
May  1 19:24:12.341: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  1 19:24:12.341: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 05/01/23 19:24:12.376
May  1 19:24:12.440: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4923" to be "running and ready"
May  1 19:24:12.471: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.736115ms
May  1 19:24:12.471: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 19:24:14.492: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052478462s
May  1 19:24:14.492: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 19:24:16.491: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.051081639s
May  1 19:24:16.491: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May  1 19:24:16.491: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/01/23 19:24:16.51
STEP: delete the pod with lifecycle hook 05/01/23 19:24:16.563
May  1 19:24:16.603: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  1 19:24:16.626: INFO: Pod pod-with-poststart-exec-hook still exists
May  1 19:24:18.626: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  1 19:24:18.658: INFO: Pod pod-with-poststart-exec-hook still exists
May  1 19:24:20.627: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  1 19:24:20.646: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  1 19:24:20.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4923" for this suite. 05/01/23 19:24:20.668
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":119,"skipped":2199,"failed":0}
------------------------------
• [SLOW TEST] [12.788 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:07.914
    May  1 19:24:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:24:07.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:08.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:08.031
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:24:08.112
    May  1 19:24:08.285: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4923" to be "running and ready"
    May  1 19:24:08.317: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 32.358031ms
    May  1 19:24:08.317: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:24:10.348: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063609998s
    May  1 19:24:10.348: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:24:12.341: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.056770961s
    May  1 19:24:12.341: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  1 19:24:12.341: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 05/01/23 19:24:12.376
    May  1 19:24:12.440: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4923" to be "running and ready"
    May  1 19:24:12.471: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 30.736115ms
    May  1 19:24:12.471: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:24:14.492: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052478462s
    May  1 19:24:14.492: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:24:16.491: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.051081639s
    May  1 19:24:16.491: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May  1 19:24:16.491: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/01/23 19:24:16.51
    STEP: delete the pod with lifecycle hook 05/01/23 19:24:16.563
    May  1 19:24:16.603: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  1 19:24:16.626: INFO: Pod pod-with-poststart-exec-hook still exists
    May  1 19:24:18.626: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  1 19:24:18.658: INFO: Pod pod-with-poststart-exec-hook still exists
    May  1 19:24:20.627: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May  1 19:24:20.646: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  1 19:24:20.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4923" for this suite. 05/01/23 19:24:20.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:20.703
May  1 19:24:20.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:24:20.705
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:20.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:20.803
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-ce1f1301-5f00-4d16-a5d0-0367e4258fb4 05/01/23 19:24:20.818
STEP: Creating a pod to test consume configMaps 05/01/23 19:24:20.866
May  1 19:24:20.953: INFO: Waiting up to 5m0s for pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05" in namespace "configmap-8954" to be "Succeeded or Failed"
May  1 19:24:20.979: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010821ms
May  1 19:24:22.998: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044585327s
May  1 19:24:25.001: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048090932s
May  1 19:24:27.022: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069079597s
STEP: Saw pod success 05/01/23 19:24:27.023
May  1 19:24:27.023: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05" satisfied condition "Succeeded or Failed"
May  1 19:24:27.076: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:24:27.155
May  1 19:24:27.266: INFO: Waiting for pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 to disappear
May  1 19:24:27.293: INFO: Pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:24:27.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8954" for this suite. 05/01/23 19:24:27.332
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":120,"skipped":2207,"failed":0}
------------------------------
• [SLOW TEST] [6.652 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:20.703
    May  1 19:24:20.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:24:20.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:20.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:20.803
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-ce1f1301-5f00-4d16-a5d0-0367e4258fb4 05/01/23 19:24:20.818
    STEP: Creating a pod to test consume configMaps 05/01/23 19:24:20.866
    May  1 19:24:20.953: INFO: Waiting up to 5m0s for pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05" in namespace "configmap-8954" to be "Succeeded or Failed"
    May  1 19:24:20.979: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010821ms
    May  1 19:24:22.998: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044585327s
    May  1 19:24:25.001: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048090932s
    May  1 19:24:27.022: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.069079597s
    STEP: Saw pod success 05/01/23 19:24:27.023
    May  1 19:24:27.023: INFO: Pod "pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05" satisfied condition "Succeeded or Failed"
    May  1 19:24:27.076: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:24:27.155
    May  1 19:24:27.266: INFO: Waiting for pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 to disappear
    May  1 19:24:27.293: INFO: Pod pod-configmaps-7bfe885b-3b92-41a4-b1f8-9d0f3f17ea05 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:24:27.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8954" for this suite. 05/01/23 19:24:27.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:27.359
May  1 19:24:27.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:24:27.361
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:27.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:27.481
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:24:27.643
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:24:28.057
STEP: Deploying the webhook pod 05/01/23 19:24:28.13
STEP: Wait for the deployment to be ready 05/01/23 19:24:28.183
May  1 19:24:28.221: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May  1 19:24:30.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:24:32.297
STEP: Verifying the service has paired with the endpoint 05/01/23 19:24:32.428
May  1 19:24:33.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/01/23 19:24:33.448
STEP: create a configmap that should be updated by the webhook 05/01/23 19:24:33.512
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:24:33.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9186" for this suite. 05/01/23 19:24:33.645
STEP: Destroying namespace "webhook-9186-markers" for this suite. 05/01/23 19:24:33.673
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":121,"skipped":2218,"failed":0}
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:27.359
    May  1 19:24:27.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:24:27.361
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:27.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:27.481
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:24:27.643
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:24:28.057
    STEP: Deploying the webhook pod 05/01/23 19:24:28.13
    STEP: Wait for the deployment to be ready 05/01/23 19:24:28.183
    May  1 19:24:28.221: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    May  1 19:24:30.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:24:32.297
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:24:32.428
    May  1 19:24:33.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/01/23 19:24:33.448
    STEP: create a configmap that should be updated by the webhook 05/01/23 19:24:33.512
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:24:33.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9186" for this suite. 05/01/23 19:24:33.645
    STEP: Destroying namespace "webhook-9186-markers" for this suite. 05/01/23 19:24:33.673
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:33.912
May  1 19:24:33.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:24:33.915
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:34.015
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:24:34.146
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:24:35.086
STEP: Deploying the webhook pod 05/01/23 19:24:35.117
STEP: Wait for the deployment to be ready 05/01/23 19:24:35.188
May  1 19:24:35.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:24:37.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:24:39.325
STEP: Verifying the service has paired with the endpoint 05/01/23 19:24:39.381
May  1 19:24:40.382: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 05/01/23 19:24:40.652
STEP: Creating a configMap that should be mutated 05/01/23 19:24:40.743
STEP: Deleting the collection of validation webhooks 05/01/23 19:24:41.053
STEP: Creating a configMap that should not be mutated 05/01/23 19:24:41.338
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:24:41.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-704" for this suite. 05/01/23 19:24:41.404
STEP: Destroying namespace "webhook-704-markers" for this suite. 05/01/23 19:24:41.442
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":122,"skipped":2259,"failed":0}
------------------------------
• [SLOW TEST] [7.767 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:33.912
    May  1 19:24:33.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:24:33.915
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:34.015
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:24:34.146
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:24:35.086
    STEP: Deploying the webhook pod 05/01/23 19:24:35.117
    STEP: Wait for the deployment to be ready 05/01/23 19:24:35.188
    May  1 19:24:35.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:24:37.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 24, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:24:39.325
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:24:39.381
    May  1 19:24:40.382: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 05/01/23 19:24:40.652
    STEP: Creating a configMap that should be mutated 05/01/23 19:24:40.743
    STEP: Deleting the collection of validation webhooks 05/01/23 19:24:41.053
    STEP: Creating a configMap that should not be mutated 05/01/23 19:24:41.338
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:24:41.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-704" for this suite. 05/01/23 19:24:41.404
    STEP: Destroying namespace "webhook-704-markers" for this suite. 05/01/23 19:24:41.442
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:41.681
May  1 19:24:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:24:41.685
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:41.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:41.84
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-a4b7db83-86a7-4920-8834-613a60d20571 05/01/23 19:24:41.857
STEP: Creating a pod to test consume secrets 05/01/23 19:24:41.892
May  1 19:24:42.029: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700" in namespace "projected-9313" to be "Succeeded or Failed"
May  1 19:24:42.062: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 32.668675ms
May  1 19:24:44.122: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09247674s
May  1 19:24:46.138: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109236802s
May  1 19:24:48.081: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05156945s
STEP: Saw pod success 05/01/23 19:24:48.081
May  1 19:24:48.081: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700" satisfied condition "Succeeded or Failed"
May  1 19:24:48.100: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/01/23 19:24:48.15
May  1 19:24:48.208: INFO: Waiting for pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 to disappear
May  1 19:24:48.251: INFO: Pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 19:24:48.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9313" for this suite. 05/01/23 19:24:48.309
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":123,"skipped":2260,"failed":0}
------------------------------
• [SLOW TEST] [6.668 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:41.681
    May  1 19:24:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:24:41.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:41.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:41.84
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-a4b7db83-86a7-4920-8834-613a60d20571 05/01/23 19:24:41.857
    STEP: Creating a pod to test consume secrets 05/01/23 19:24:41.892
    May  1 19:24:42.029: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700" in namespace "projected-9313" to be "Succeeded or Failed"
    May  1 19:24:42.062: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 32.668675ms
    May  1 19:24:44.122: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09247674s
    May  1 19:24:46.138: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Pending", Reason="", readiness=false. Elapsed: 4.109236802s
    May  1 19:24:48.081: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05156945s
    STEP: Saw pod success 05/01/23 19:24:48.081
    May  1 19:24:48.081: INFO: Pod "pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700" satisfied condition "Succeeded or Failed"
    May  1 19:24:48.100: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 19:24:48.15
    May  1 19:24:48.208: INFO: Waiting for pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 to disappear
    May  1 19:24:48.251: INFO: Pod pod-projected-secrets-08a47223-e733-4a1f-8981-1a34bbb5f700 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 19:24:48.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9313" for this suite. 05/01/23 19:24:48.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:24:48.35
May  1 19:24:48.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 19:24:48.352
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:48.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:48.429
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6100 05/01/23 19:24:48.443
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 05/01/23 19:24:48.471
May  1 19:24:48.557: INFO: Found 0 stateful pods, waiting for 3
May  1 19:24:58.578: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 19:24:58.579: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  1 19:24:58.579: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  1 19:24:58.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 19:24:59.192: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 19:24:59.192: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 19:24:59.192: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/01/23 19:25:09.284
May  1 19:25:09.351: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/01/23 19:25:09.351
STEP: Updating Pods in reverse ordinal order 05/01/23 19:25:19.44
May  1 19:25:19.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 19:25:19.954: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 19:25:19.954: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 19:25:19.954: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 19:25:30.144: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
May  1 19:25:30.144: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  1 19:25:30.144: INFO: Waiting for Pod statefulset-6100/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  1 19:25:40.211: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
May  1 19:25:40.211: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  1 19:25:50.183: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
May  1 19:26:00.188: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
STEP: Rolling back to a previous revision 05/01/23 19:26:10.186
May  1 19:26:10.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 19:26:10.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 19:26:10.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 19:26:10.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 19:26:10.960: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/01/23 19:26:21.039
May  1 19:26:21.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 19:26:21.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 19:26:21.547: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 19:26:21.547: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 19:26:31.691: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
May  1 19:26:31.692: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 19:26:41.730: INFO: Deleting all statefulset in ns statefulset-6100
May  1 19:26:41.749: INFO: Scaling statefulset ss2 to 0
May  1 19:26:51.835: INFO: Waiting for statefulset status.replicas updated to 0
May  1 19:26:51.851: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 19:26:51.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6100" for this suite. 05/01/23 19:26:51.996
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":124,"skipped":2267,"failed":0}
------------------------------
• [SLOW TEST] [123.676 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:24:48.35
    May  1 19:24:48.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 19:24:48.352
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:24:48.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:24:48.429
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6100 05/01/23 19:24:48.443
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 05/01/23 19:24:48.471
    May  1 19:24:48.557: INFO: Found 0 stateful pods, waiting for 3
    May  1 19:24:58.578: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 19:24:58.579: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  1 19:24:58.579: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May  1 19:24:58.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 19:24:59.192: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 19:24:59.192: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 19:24:59.192: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/01/23 19:25:09.284
    May  1 19:25:09.351: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/01/23 19:25:09.351
    STEP: Updating Pods in reverse ordinal order 05/01/23 19:25:19.44
    May  1 19:25:19.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 19:25:19.954: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 19:25:19.954: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 19:25:19.954: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 19:25:30.144: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
    May  1 19:25:30.144: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  1 19:25:30.144: INFO: Waiting for Pod statefulset-6100/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  1 19:25:40.211: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
    May  1 19:25:40.211: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  1 19:25:50.183: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
    May  1 19:26:00.188: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
    STEP: Rolling back to a previous revision 05/01/23 19:26:10.186
    May  1 19:26:10.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 19:26:10.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 19:26:10.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 19:26:10.861: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 19:26:10.960: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/01/23 19:26:21.039
    May  1 19:26:21.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-6100 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 19:26:21.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 19:26:21.547: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 19:26:21.547: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 19:26:31.691: INFO: Waiting for StatefulSet statefulset-6100/ss2 to complete update
    May  1 19:26:31.692: INFO: Waiting for Pod statefulset-6100/ss2-0 to have revision ss2-6557876d87 update revision ss2-5d8c6ff87d
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 19:26:41.730: INFO: Deleting all statefulset in ns statefulset-6100
    May  1 19:26:41.749: INFO: Scaling statefulset ss2 to 0
    May  1 19:26:51.835: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 19:26:51.851: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 19:26:51.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6100" for this suite. 05/01/23 19:26:51.996
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:26:52.028
May  1 19:26:52.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:26:52.03
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:52.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:52.155
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8664/secret-test-c6d55cda-e8ab-4470-936c-92f5910d8090 05/01/23 19:26:52.169
STEP: Creating a pod to test consume secrets 05/01/23 19:26:52.198
May  1 19:26:52.278: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9" in namespace "secrets-8664" to be "Succeeded or Failed"
May  1 19:26:52.303: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.250308ms
May  1 19:26:54.324: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045711432s
May  1 19:26:56.322: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044126454s
May  1 19:26:58.321: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042337197s
STEP: Saw pod success 05/01/23 19:26:58.321
May  1 19:26:58.321: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9" satisfied condition "Succeeded or Failed"
May  1 19:26:58.343: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 container env-test: <nil>
STEP: delete the pod 05/01/23 19:26:58.431
May  1 19:26:58.482: INFO: Waiting for pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 to disappear
May  1 19:26:58.506: INFO: Pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  1 19:26:58.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8664" for this suite. 05/01/23 19:26:58.527
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":125,"skipped":2268,"failed":0}
------------------------------
• [SLOW TEST] [6.522 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:26:52.028
    May  1 19:26:52.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:26:52.03
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:52.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:52.155
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8664/secret-test-c6d55cda-e8ab-4470-936c-92f5910d8090 05/01/23 19:26:52.169
    STEP: Creating a pod to test consume secrets 05/01/23 19:26:52.198
    May  1 19:26:52.278: INFO: Waiting up to 5m0s for pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9" in namespace "secrets-8664" to be "Succeeded or Failed"
    May  1 19:26:52.303: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.250308ms
    May  1 19:26:54.324: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045711432s
    May  1 19:26:56.322: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044126454s
    May  1 19:26:58.321: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042337197s
    STEP: Saw pod success 05/01/23 19:26:58.321
    May  1 19:26:58.321: INFO: Pod "pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9" satisfied condition "Succeeded or Failed"
    May  1 19:26:58.343: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 container env-test: <nil>
    STEP: delete the pod 05/01/23 19:26:58.431
    May  1 19:26:58.482: INFO: Waiting for pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 to disappear
    May  1 19:26:58.506: INFO: Pod pod-configmaps-0cd5e085-0727-442b-8a92-63a7c2af34c9 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:26:58.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8664" for this suite. 05/01/23 19:26:58.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:26:58.557
May  1 19:26:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:26:58.559
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:58.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:58.647
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 05/01/23 19:26:58.661
May  1 19:26:58.661: INFO: Creating e2e-svc-a-rgqjr
May  1 19:26:58.807: INFO: Creating e2e-svc-b-8h99g
May  1 19:26:58.875: INFO: Creating e2e-svc-c-v6p2b
STEP: deleting service collection 05/01/23 19:26:59.004
May  1 19:26:59.160: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:26:59.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5843" for this suite. 05/01/23 19:26:59.215
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":126,"skipped":2320,"failed":0}
------------------------------
• [0.706 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:26:58.557
    May  1 19:26:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:26:58.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:58.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:58.647
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 05/01/23 19:26:58.661
    May  1 19:26:58.661: INFO: Creating e2e-svc-a-rgqjr
    May  1 19:26:58.807: INFO: Creating e2e-svc-b-8h99g
    May  1 19:26:58.875: INFO: Creating e2e-svc-c-v6p2b
    STEP: deleting service collection 05/01/23 19:26:59.004
    May  1 19:26:59.160: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:26:59.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5843" for this suite. 05/01/23 19:26:59.215
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:26:59.266
May  1 19:26:59.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context-test 05/01/23 19:26:59.286
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:59.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:59.433
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
May  1 19:26:59.582: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73" in namespace "security-context-test-9228" to be "Succeeded or Failed"
May  1 19:26:59.612: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 29.298859ms
May  1 19:27:01.680: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097969136s
May  1 19:27:03.656: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073704205s
May  1 19:27:05.635: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053048985s
May  1 19:27:05.636: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 19:27:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9228" for this suite. 05/01/23 19:27:05.665
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":127,"skipped":2331,"failed":0}
------------------------------
• [SLOW TEST] [6.432 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:26:59.266
    May  1 19:26:59.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context-test 05/01/23 19:26:59.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:26:59.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:26:59.433
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    May  1 19:26:59.582: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73" in namespace "security-context-test-9228" to be "Succeeded or Failed"
    May  1 19:26:59.612: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 29.298859ms
    May  1 19:27:01.680: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097969136s
    May  1 19:27:03.656: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073704205s
    May  1 19:27:05.635: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053048985s
    May  1 19:27:05.636: INFO: Pod "busybox-readonly-false-21839987-dea6-464d-9907-0a9e559a7e73" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 19:27:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9228" for this suite. 05/01/23 19:27:05.665
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:27:05.7
May  1 19:27:05.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename watch 05/01/23 19:27:05.703
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:05.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:05.802
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/01/23 19:27:05.818
STEP: creating a new configmap 05/01/23 19:27:05.863
STEP: modifying the configmap once 05/01/23 19:27:05.957
STEP: changing the label value of the configmap 05/01/23 19:27:06.038
STEP: Expecting to observe a delete notification for the watched object 05/01/23 19:27:06.087
May  1 19:27:06.087: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86378 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 19:27:06.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86386 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 19:27:06.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86387 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/01/23 19:27:06.088
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/01/23 19:27:06.135
STEP: changing the label value of the configmap back 05/01/23 19:27:16.15
STEP: modifying the configmap a third time 05/01/23 19:27:16.194
STEP: deleting the configmap 05/01/23 19:27:16.235
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/01/23 19:27:16.257
May  1 19:27:16.258: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86482 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 19:27:16.258: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86484 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 19:27:16.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86486 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  1 19:27:16.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4959" for this suite. 05/01/23 19:27:16.283
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":128,"skipped":2334,"failed":0}
------------------------------
• [SLOW TEST] [10.607 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:27:05.7
    May  1 19:27:05.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename watch 05/01/23 19:27:05.703
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:05.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:05.802
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/01/23 19:27:05.818
    STEP: creating a new configmap 05/01/23 19:27:05.863
    STEP: modifying the configmap once 05/01/23 19:27:05.957
    STEP: changing the label value of the configmap 05/01/23 19:27:06.038
    STEP: Expecting to observe a delete notification for the watched object 05/01/23 19:27:06.087
    May  1 19:27:06.087: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86378 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 19:27:06.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86386 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 19:27:06.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86387 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/01/23 19:27:06.088
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/01/23 19:27:06.135
    STEP: changing the label value of the configmap back 05/01/23 19:27:16.15
    STEP: modifying the configmap a third time 05/01/23 19:27:16.194
    STEP: deleting the configmap 05/01/23 19:27:16.235
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/01/23 19:27:16.257
    May  1 19:27:16.258: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86482 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 19:27:16.258: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86484 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 19:27:16.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4959  5e6894f4-bd9e-4f60-a514-0e05df07e92f 86486 0 2023-05-01 19:27:05 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-01 19:27:16 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  1 19:27:16.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4959" for this suite. 05/01/23 19:27:16.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:27:16.319
May  1 19:27:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:27:16.32
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:16.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:16.406
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
May  1 19:27:16.509: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f" in namespace "kubelet-test-5032" to be "running and ready"
May  1 19:27:16.534: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.095712ms
May  1 19:27:16.534: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Pending, waiting for it to be Running (with Ready = true)
May  1 19:27:18.559: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050051113s
May  1 19:27:18.559: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Pending, waiting for it to be Running (with Ready = true)
May  1 19:27:20.564: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.054735286s
May  1 19:27:20.564: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Running (Ready = true)
May  1 19:27:20.564: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  1 19:27:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5032" for this suite. 05/01/23 19:27:20.661
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":129,"skipped":2368,"failed":0}
------------------------------
• [4.369 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:27:16.319
    May  1 19:27:16.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubelet-test 05/01/23 19:27:16.32
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:16.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:16.406
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    May  1 19:27:16.509: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f" in namespace "kubelet-test-5032" to be "running and ready"
    May  1 19:27:16.534: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 25.095712ms
    May  1 19:27:16.534: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:27:18.559: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050051113s
    May  1 19:27:18.559: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:27:20.564: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.054735286s
    May  1 19:27:20.564: INFO: The phase of Pod busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f is Running (Ready = true)
    May  1 19:27:20.564: INFO: Pod "busybox-readonly-fs900cdaf0-e856-4a01-aee4-5b9d37392b3f" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  1 19:27:20.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5032" for this suite. 05/01/23 19:27:20.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:27:20.69
May  1 19:27:20.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:27:20.692
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:20.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:20.786
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 05/01/23 19:27:20.807
May  1 19:27:20.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 create -f -'
May  1 19:27:28.649: INFO: stderr: ""
May  1 19:27:28.649: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 19:27:28.649
May  1 19:27:28.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 19:27:28.862: INFO: stderr: ""
May  1 19:27:28.862: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
May  1 19:27:28.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 19:27:29.098: INFO: stderr: ""
May  1 19:27:29.098: INFO: stdout: ""
May  1 19:27:29.098: INFO: update-demo-nautilus-749d9 is created but not running
May  1 19:27:34.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 19:27:34.786: INFO: stderr: ""
May  1 19:27:34.786: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
May  1 19:27:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 19:27:35.253: INFO: stderr: ""
May  1 19:27:35.253: INFO: stdout: ""
May  1 19:27:35.253: INFO: update-demo-nautilus-749d9 is created but not running
May  1 19:27:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 19:27:40.502: INFO: stderr: ""
May  1 19:27:40.502: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
May  1 19:27:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 19:27:40.852: INFO: stderr: ""
May  1 19:27:40.852: INFO: stdout: "true"
May  1 19:27:40.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 19:27:41.159: INFO: stderr: ""
May  1 19:27:41.159: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 19:27:41.159: INFO: validating pod update-demo-nautilus-749d9
May  1 19:27:41.215: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 19:27:41.216: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 19:27:41.216: INFO: update-demo-nautilus-749d9 is verified up and running
May  1 19:27:41.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-l7wtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 19:27:41.517: INFO: stderr: ""
May  1 19:27:41.517: INFO: stdout: "true"
May  1 19:27:41.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-l7wtz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 19:27:41.775: INFO: stderr: ""
May  1 19:27:41.775: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 19:27:41.775: INFO: validating pod update-demo-nautilus-l7wtz
May  1 19:27:41.802: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 19:27:41.803: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 19:27:41.803: INFO: update-demo-nautilus-l7wtz is verified up and running
STEP: using delete to clean up resources 05/01/23 19:27:41.803
May  1 19:27:41.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 delete --grace-period=0 --force -f -'
May  1 19:27:42.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:27:42.064: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  1 19:27:42.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get rc,svc -l name=update-demo --no-headers'
May  1 19:27:42.689: INFO: stderr: "No resources found in kubectl-4321 namespace.\n"
May  1 19:27:42.690: INFO: stdout: ""
May  1 19:27:42.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  1 19:27:43.052: INFO: stderr: ""
May  1 19:27:43.052: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:27:43.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4321" for this suite. 05/01/23 19:27:43.082
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":130,"skipped":2382,"failed":0}
------------------------------
• [SLOW TEST] [22.426 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:27:20.69
    May  1 19:27:20.690: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:27:20.692
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:20.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:20.786
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 05/01/23 19:27:20.807
    May  1 19:27:20.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 create -f -'
    May  1 19:27:28.649: INFO: stderr: ""
    May  1 19:27:28.649: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 19:27:28.649
    May  1 19:27:28.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 19:27:28.862: INFO: stderr: ""
    May  1 19:27:28.862: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
    May  1 19:27:28.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 19:27:29.098: INFO: stderr: ""
    May  1 19:27:29.098: INFO: stdout: ""
    May  1 19:27:29.098: INFO: update-demo-nautilus-749d9 is created but not running
    May  1 19:27:34.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 19:27:34.786: INFO: stderr: ""
    May  1 19:27:34.786: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
    May  1 19:27:34.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 19:27:35.253: INFO: stderr: ""
    May  1 19:27:35.253: INFO: stdout: ""
    May  1 19:27:35.253: INFO: update-demo-nautilus-749d9 is created but not running
    May  1 19:27:40.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 19:27:40.502: INFO: stderr: ""
    May  1 19:27:40.502: INFO: stdout: "update-demo-nautilus-749d9 update-demo-nautilus-l7wtz "
    May  1 19:27:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 19:27:40.852: INFO: stderr: ""
    May  1 19:27:40.852: INFO: stdout: "true"
    May  1 19:27:40.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-749d9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 19:27:41.159: INFO: stderr: ""
    May  1 19:27:41.159: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 19:27:41.159: INFO: validating pod update-demo-nautilus-749d9
    May  1 19:27:41.215: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 19:27:41.216: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 19:27:41.216: INFO: update-demo-nautilus-749d9 is verified up and running
    May  1 19:27:41.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-l7wtz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 19:27:41.517: INFO: stderr: ""
    May  1 19:27:41.517: INFO: stdout: "true"
    May  1 19:27:41.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods update-demo-nautilus-l7wtz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 19:27:41.775: INFO: stderr: ""
    May  1 19:27:41.775: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 19:27:41.775: INFO: validating pod update-demo-nautilus-l7wtz
    May  1 19:27:41.802: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 19:27:41.803: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 19:27:41.803: INFO: update-demo-nautilus-l7wtz is verified up and running
    STEP: using delete to clean up resources 05/01/23 19:27:41.803
    May  1 19:27:41.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 delete --grace-period=0 --force -f -'
    May  1 19:27:42.064: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:27:42.064: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  1 19:27:42.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get rc,svc -l name=update-demo --no-headers'
    May  1 19:27:42.689: INFO: stderr: "No resources found in kubectl-4321 namespace.\n"
    May  1 19:27:42.690: INFO: stdout: ""
    May  1 19:27:42.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4321 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  1 19:27:43.052: INFO: stderr: ""
    May  1 19:27:43.052: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:27:43.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4321" for this suite. 05/01/23 19:27:43.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:27:43.118
May  1 19:27:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename aggregator 05/01/23 19:27:43.223
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:43.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:43.369
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May  1 19:27:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/01/23 19:27:43.409
May  1 19:27:45.212: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  1 19:27:47.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:49.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:51.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:53.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:55.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:57.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:27:59.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:01.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:03.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:05.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:07.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:09.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:11.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:13.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:15.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:17.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:19.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:21.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:23.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:25.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:27.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:29.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:31.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:33.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:28:35.783: INFO: Waited 333.628999ms for the sample-apiserver to be ready to handle requests.
I0501 19:28:37.192769      21 request.go:690] Waited for 1.050021975s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/helm.openshift.io/v1beta1
STEP: Read Status for v1alpha1.wardle.example.com 05/01/23 19:28:37.613
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/01/23 19:28:37.629
STEP: List APIServices 05/01/23 19:28:37.652
May  1 19:28:37.740: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
May  1 19:28:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8773" for this suite. 05/01/23 19:28:38.948
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":131,"skipped":2392,"failed":0}
------------------------------
• [SLOW TEST] [55.857 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:27:43.118
    May  1 19:27:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename aggregator 05/01/23 19:27:43.223
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:27:43.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:27:43.369
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May  1 19:27:43.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/01/23 19:27:43.409
    May  1 19:27:45.212: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May  1 19:27:47.412: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:49.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:51.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:53.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:55.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:57.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:27:59.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:01.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:03.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:05.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:07.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:09.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:11.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:13.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:15.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:17.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:19.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:21.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:23.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:25.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:27.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:29.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:31.430: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:33.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 27, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:28:35.783: INFO: Waited 333.628999ms for the sample-apiserver to be ready to handle requests.
    I0501 19:28:37.192769      21 request.go:690] Waited for 1.050021975s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/helm.openshift.io/v1beta1
    STEP: Read Status for v1alpha1.wardle.example.com 05/01/23 19:28:37.613
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/01/23 19:28:37.629
    STEP: List APIServices 05/01/23 19:28:37.652
    May  1 19:28:37.740: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    May  1 19:28:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-8773" for this suite. 05/01/23 19:28:38.948
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:28:38.976
May  1 19:28:38.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 19:28:38.981
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:28:39.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:28:39.089
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 in namespace container-probe-5339 05/01/23 19:28:39.109
May  1 19:28:39.218: INFO: Waiting up to 5m0s for pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0" in namespace "container-probe-5339" to be "not pending"
May  1 19:28:39.249: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.520021ms
May  1 19:28:41.338: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118673087s
May  1 19:28:43.269: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Running", Reason="", readiness=true. Elapsed: 4.049940566s
May  1 19:28:43.269: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0" satisfied condition "not pending"
May  1 19:28:43.269: INFO: Started pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 in namespace container-probe-5339
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:28:43.269
May  1 19:28:43.288: INFO: Initial restart count of pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 is 0
STEP: deleting the pod 05/01/23 19:32:44.345
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 19:32:44.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5339" for this suite. 05/01/23 19:32:44.416
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":132,"skipped":2394,"failed":0}
------------------------------
• [SLOW TEST] [245.468 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:28:38.976
    May  1 19:28:38.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 19:28:38.981
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:28:39.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:28:39.089
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 in namespace container-probe-5339 05/01/23 19:28:39.109
    May  1 19:28:39.218: INFO: Waiting up to 5m0s for pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0" in namespace "container-probe-5339" to be "not pending"
    May  1 19:28:39.249: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.520021ms
    May  1 19:28:41.338: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118673087s
    May  1 19:28:43.269: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0": Phase="Running", Reason="", readiness=true. Elapsed: 4.049940566s
    May  1 19:28:43.269: INFO: Pod "liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0" satisfied condition "not pending"
    May  1 19:28:43.269: INFO: Started pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 in namespace container-probe-5339
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:28:43.269
    May  1 19:28:43.288: INFO: Initial restart count of pod liveness-3bc07fb0-2833-4bf3-9a94-9446c919d3a0 is 0
    STEP: deleting the pod 05/01/23 19:32:44.345
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 19:32:44.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5339" for this suite. 05/01/23 19:32:44.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:32:44.448
May  1 19:32:44.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption 05/01/23 19:32:44.452
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:32:44.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:32:44.573
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 05/01/23 19:32:44.632
STEP: Updating PodDisruptionBudget status 05/01/23 19:32:46.684
STEP: Waiting for all pods to be running 05/01/23 19:32:46.747
May  1 19:32:46.768: INFO: running pods: 0 < 1
May  1 19:32:48.787: INFO: running pods: 0 < 1
STEP: locating a running pod 05/01/23 19:32:50.785
STEP: Waiting for the pdb to be processed 05/01/23 19:32:50.841
STEP: Patching PodDisruptionBudget status 05/01/23 19:32:50.906
STEP: Waiting for the pdb to be processed 05/01/23 19:32:50.96
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  1 19:32:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-997" for this suite. 05/01/23 19:32:50.999
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":133,"skipped":2425,"failed":0}
------------------------------
• [SLOW TEST] [6.571 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:32:44.448
    May  1 19:32:44.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption 05/01/23 19:32:44.452
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:32:44.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:32:44.573
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 05/01/23 19:32:44.632
    STEP: Updating PodDisruptionBudget status 05/01/23 19:32:46.684
    STEP: Waiting for all pods to be running 05/01/23 19:32:46.747
    May  1 19:32:46.768: INFO: running pods: 0 < 1
    May  1 19:32:48.787: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/01/23 19:32:50.785
    STEP: Waiting for the pdb to be processed 05/01/23 19:32:50.841
    STEP: Patching PodDisruptionBudget status 05/01/23 19:32:50.906
    STEP: Waiting for the pdb to be processed 05/01/23 19:32:50.96
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  1 19:32:50.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-997" for this suite. 05/01/23 19:32:50.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:32:51.023
May  1 19:32:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 19:32:51.026
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:32:51.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:32:51.104
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
May  1 19:32:51.296: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 19:32:51.316
May  1 19:32:51.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:32:51.358: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:32:52.421: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:32:52.421: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:32:53.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:32:53.542: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:32:54.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:32:54.435: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 19:32:55.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 19:32:55.411: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 05/01/23 19:32:55.496
STEP: Check that daemon pods images are updated. 05/01/23 19:32:55.558
May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-2hgkr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:56.640: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:56.640: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:57.632: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:57.632: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:58.657: INFO: Pod daemon-set-7n27v is not available
May  1 19:32:58.657: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:58.657: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:59.634: INFO: Pod daemon-set-7n27v is not available
May  1 19:32:59.634: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:32:59.634: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:00.670: INFO: Pod daemon-set-7n27v is not available
May  1 19:33:00.671: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:00.671: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:01.638: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:02.634: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:03.630: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:03.630: INFO: Pod daemon-set-x462j is not available
May  1 19:33:04.660: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:04.662: INFO: Pod daemon-set-x462j is not available
May  1 19:33:05.639: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
May  1 19:33:05.639: INFO: Pod daemon-set-x462j is not available
May  1 19:33:08.636: INFO: Pod daemon-set-wtdr9 is not available
STEP: Check that daemon pods are still running on every node of the cluster. 05/01/23 19:33:08.654
May  1 19:33:08.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:33:08.690: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 19:33:09.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:33:09.743: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 19:33:10.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:33:10.730: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 19:33:11.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 19:33:11.735: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 19:33:11.849
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4026, will wait for the garbage collector to delete the pods 05/01/23 19:33:11.849
May  1 19:33:11.936: INFO: Deleting DaemonSet.extensions daemon-set took: 23.174261ms
May  1 19:33:12.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 205.076381ms
May  1 19:33:15.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:33:15.564: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 19:33:15.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89077"},"items":null}

May  1 19:33:15.604: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 19:33:15.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4026" for this suite. 05/01/23 19:33:15.69
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":134,"skipped":2435,"failed":0}
------------------------------
• [SLOW TEST] [24.691 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:32:51.023
    May  1 19:32:51.024: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 19:32:51.026
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:32:51.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:32:51.104
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    May  1 19:32:51.296: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 19:32:51.316
    May  1 19:32:51.358: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:32:51.358: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:32:52.421: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:32:52.421: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:32:53.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:32:53.542: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:32:54.435: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:32:54.435: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 19:32:55.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 19:32:55.411: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 05/01/23 19:32:55.496
    STEP: Check that daemon pods images are updated. 05/01/23 19:32:55.558
    May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-2hgkr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:55.584: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:56.640: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:56.640: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:57.632: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:57.632: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:58.657: INFO: Pod daemon-set-7n27v is not available
    May  1 19:32:58.657: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:58.657: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:59.634: INFO: Pod daemon-set-7n27v is not available
    May  1 19:32:59.634: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:32:59.634: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:00.670: INFO: Pod daemon-set-7n27v is not available
    May  1 19:33:00.671: INFO: Wrong image for pod: daemon-set-fzpvf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:00.671: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:01.638: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:02.634: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:03.630: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:03.630: INFO: Pod daemon-set-x462j is not available
    May  1 19:33:04.660: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:04.662: INFO: Pod daemon-set-x462j is not available
    May  1 19:33:05.639: INFO: Wrong image for pod: daemon-set-sgf6q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    May  1 19:33:05.639: INFO: Pod daemon-set-x462j is not available
    May  1 19:33:08.636: INFO: Pod daemon-set-wtdr9 is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 05/01/23 19:33:08.654
    May  1 19:33:08.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:33:08.690: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 19:33:09.743: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:33:09.743: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 19:33:10.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:33:10.730: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 19:33:11.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 19:33:11.735: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 19:33:11.849
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4026, will wait for the garbage collector to delete the pods 05/01/23 19:33:11.849
    May  1 19:33:11.936: INFO: Deleting DaemonSet.extensions daemon-set took: 23.174261ms
    May  1 19:33:12.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 205.076381ms
    May  1 19:33:15.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:33:15.564: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 19:33:15.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89077"},"items":null}

    May  1 19:33:15.604: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89077"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:33:15.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4026" for this suite. 05/01/23 19:33:15.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:33:15.727
May  1 19:33:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:33:15.729
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:33:15.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:33:15.812
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
May  1 19:33:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: creating the pod 05/01/23 19:33:15.829
STEP: submitting the pod to kubernetes 05/01/23 19:33:15.829
May  1 19:33:15.899: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785" in namespace "pods-7286" to be "running and ready"
May  1 19:33:15.942: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Pending", Reason="", readiness=false. Elapsed: 36.324862ms
May  1 19:33:15.942: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:33:17.961: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054835126s
May  1 19:33:17.961: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:33:19.959: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Running", Reason="", readiness=true. Elapsed: 4.053351752s
May  1 19:33:19.959: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Running (Ready = true)
May  1 19:33:19.959: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:33:20.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7286" for this suite. 05/01/23 19:33:20.357
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":135,"skipped":2476,"failed":0}
------------------------------
• [4.653 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:33:15.727
    May  1 19:33:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:33:15.729
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:33:15.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:33:15.812
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    May  1 19:33:15.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: creating the pod 05/01/23 19:33:15.829
    STEP: submitting the pod to kubernetes 05/01/23 19:33:15.829
    May  1 19:33:15.899: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785" in namespace "pods-7286" to be "running and ready"
    May  1 19:33:15.942: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Pending", Reason="", readiness=false. Elapsed: 36.324862ms
    May  1 19:33:15.942: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:33:17.961: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054835126s
    May  1 19:33:17.961: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:33:19.959: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785": Phase="Running", Reason="", readiness=true. Elapsed: 4.053351752s
    May  1 19:33:19.959: INFO: The phase of Pod pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785 is Running (Ready = true)
    May  1 19:33:19.959: INFO: Pod "pod-exec-websocket-e5bbbe1d-fec6-46f0-bafe-f2e383856785" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:33:20.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7286" for this suite. 05/01/23 19:33:20.357
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:33:20.383
May  1 19:33:20.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 19:33:20.385
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:33:20.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:33:20.506
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 in namespace container-probe-4957 05/01/23 19:33:20.519
May  1 19:33:20.598: INFO: Waiting up to 5m0s for pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4" in namespace "container-probe-4957" to be "not pending"
May  1 19:33:20.618: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.849651ms
May  1 19:33:22.635: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037434982s
May  1 19:33:24.648: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.0498037s
May  1 19:33:24.648: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4" satisfied condition "not pending"
May  1 19:33:24.648: INFO: Started pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 in namespace container-probe-4957
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:33:24.648
May  1 19:33:24.673: INFO: Initial restart count of pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 is 0
STEP: deleting the pod 05/01/23 19:37:25.675
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 19:37:25.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4957" for this suite. 05/01/23 19:37:25.821
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":136,"skipped":2476,"failed":0}
------------------------------
• [SLOW TEST] [245.467 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:33:20.383
    May  1 19:33:20.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 19:33:20.385
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:33:20.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:33:20.506
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 in namespace container-probe-4957 05/01/23 19:33:20.519
    May  1 19:33:20.598: INFO: Waiting up to 5m0s for pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4" in namespace "container-probe-4957" to be "not pending"
    May  1 19:33:20.618: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.849651ms
    May  1 19:33:22.635: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037434982s
    May  1 19:33:24.648: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4": Phase="Running", Reason="", readiness=true. Elapsed: 4.0498037s
    May  1 19:33:24.648: INFO: Pod "busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4" satisfied condition "not pending"
    May  1 19:33:24.648: INFO: Started pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 in namespace container-probe-4957
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:33:24.648
    May  1 19:33:24.673: INFO: Initial restart count of pod busybox-8a5ae1d3-5dad-48a9-a1a8-399eef0121e4 is 0
    STEP: deleting the pod 05/01/23 19:37:25.675
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 19:37:25.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4957" for this suite. 05/01/23 19:37:25.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:37:25.855
May  1 19:37:25.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:37:25.857
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:37:25.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:37:25.984
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:37:26.116
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:37:27.466
STEP: Deploying the webhook pod 05/01/23 19:37:27.511
STEP: Wait for the deployment to be ready 05/01/23 19:37:27.569
May  1 19:37:27.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:37:29.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:37:31.785
STEP: Verifying the service has paired with the endpoint 05/01/23 19:37:31.902
May  1 19:37:32.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 05/01/23 19:37:32.952
STEP: create a pod 05/01/23 19:37:33.047
May  1 19:37:33.239: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5329" to be "running"
May  1 19:37:33.276: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.304003ms
May  1 19:37:35.293: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054652932s
May  1 19:37:37.297: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.058657309s
May  1 19:37:37.297: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/01/23 19:37:37.298
May  1 19:37:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=webhook-5329 attach --namespace=webhook-5329 to-be-attached-pod -i -c=container1'
May  1 19:37:37.596: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:37:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5329" for this suite. 05/01/23 19:37:37.643
STEP: Destroying namespace "webhook-5329-markers" for this suite. 05/01/23 19:37:37.674
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":137,"skipped":2494,"failed":0}
------------------------------
• [SLOW TEST] [12.052 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:37:25.855
    May  1 19:37:25.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:37:25.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:37:25.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:37:25.984
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:37:26.116
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:37:27.466
    STEP: Deploying the webhook pod 05/01/23 19:37:27.511
    STEP: Wait for the deployment to be ready 05/01/23 19:37:27.569
    May  1 19:37:27.625: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:37:29.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 37, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:37:31.785
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:37:31.902
    May  1 19:37:32.903: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 05/01/23 19:37:32.952
    STEP: create a pod 05/01/23 19:37:33.047
    May  1 19:37:33.239: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5329" to be "running"
    May  1 19:37:33.276: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 37.304003ms
    May  1 19:37:35.293: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054652932s
    May  1 19:37:37.297: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.058657309s
    May  1 19:37:37.297: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/01/23 19:37:37.298
    May  1 19:37:37.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=webhook-5329 attach --namespace=webhook-5329 to-be-attached-pod -i -c=container1'
    May  1 19:37:37.596: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:37:37.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5329" for this suite. 05/01/23 19:37:37.643
    STEP: Destroying namespace "webhook-5329-markers" for this suite. 05/01/23 19:37:37.674
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:37:37.923
May  1 19:37:37.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename endpointslice 05/01/23 19:37:37.925
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:37:38.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:37:38.136
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 05/01/23 19:37:43.736
STEP: referencing matching pods with named port 05/01/23 19:37:48.768
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/01/23 19:37:53.823
STEP: recreating EndpointSlices after they've been deleted 05/01/23 19:37:58.866
May  1 19:37:58.972: INFO: EndpointSlice for Service endpointslice-807/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  1 19:38:09.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-807" for this suite. 05/01/23 19:38:09.039
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":138,"skipped":2515,"failed":0}
------------------------------
• [SLOW TEST] [31.139 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:37:37.923
    May  1 19:37:37.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename endpointslice 05/01/23 19:37:37.925
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:37:38.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:37:38.136
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 05/01/23 19:37:43.736
    STEP: referencing matching pods with named port 05/01/23 19:37:48.768
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/01/23 19:37:53.823
    STEP: recreating EndpointSlices after they've been deleted 05/01/23 19:37:58.866
    May  1 19:37:58.972: INFO: EndpointSlice for Service endpointslice-807/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  1 19:38:09.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-807" for this suite. 05/01/23 19:38:09.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:38:09.063
May  1 19:38:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-runtime 05/01/23 19:38:09.066
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:38:09.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:38:09.156
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 05/01/23 19:38:09.168
W0501 19:38:09.259755      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 05/01/23 19:38:09.26
STEP: get the container status 05/01/23 19:38:15.435
STEP: the container should be terminated 05/01/23 19:38:15.454
STEP: the termination message should be set 05/01/23 19:38:15.454
May  1 19:38:15.454: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/01/23 19:38:15.454
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  1 19:38:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7015" for this suite. 05/01/23 19:38:15.578
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":139,"skipped":2532,"failed":0}
------------------------------
• [SLOW TEST] [6.606 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:38:09.063
    May  1 19:38:09.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-runtime 05/01/23 19:38:09.066
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:38:09.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:38:09.156
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 05/01/23 19:38:09.168
    W0501 19:38:09.259755      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 05/01/23 19:38:09.26
    STEP: get the container status 05/01/23 19:38:15.435
    STEP: the container should be terminated 05/01/23 19:38:15.454
    STEP: the termination message should be set 05/01/23 19:38:15.454
    May  1 19:38:15.454: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/01/23 19:38:15.454
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  1 19:38:15.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7015" for this suite. 05/01/23 19:38:15.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:38:15.678
May  1 19:38:15.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:38:15.684
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:38:15.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:38:15.795
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
May  1 19:38:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:38:35.089
May  1 19:38:35.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 create -f -'
May  1 19:38:37.584: INFO: stderr: ""
May  1 19:38:37.584: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  1 19:38:37.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 delete e2e-test-crd-publish-openapi-8801-crds test-cr'
May  1 19:38:37.945: INFO: stderr: ""
May  1 19:38:37.945: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May  1 19:38:37.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 apply -f -'
May  1 19:38:40.866: INFO: stderr: ""
May  1 19:38:40.866: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May  1 19:38:40.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 delete e2e-test-crd-publish-openapi-8801-crds test-cr'
May  1 19:38:43.023: INFO: stderr: ""
May  1 19:38:43.023: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/01/23 19:38:43.023
May  1 19:38:43.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 explain e2e-test-crd-publish-openapi-8801-crds'
May  1 19:38:44.643: INFO: stderr: ""
May  1 19:38:44.643: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8801-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:39:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-924" for this suite. 05/01/23 19:39:07.142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":140,"skipped":2556,"failed":0}
------------------------------
• [SLOW TEST] [51.496 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:38:15.678
    May  1 19:38:15.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:38:15.684
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:38:15.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:38:15.795
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    May  1 19:38:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/01/23 19:38:35.089
    May  1 19:38:35.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 create -f -'
    May  1 19:38:37.584: INFO: stderr: ""
    May  1 19:38:37.584: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  1 19:38:37.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 delete e2e-test-crd-publish-openapi-8801-crds test-cr'
    May  1 19:38:37.945: INFO: stderr: ""
    May  1 19:38:37.945: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May  1 19:38:37.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 apply -f -'
    May  1 19:38:40.866: INFO: stderr: ""
    May  1 19:38:40.866: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May  1 19:38:40.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 --namespace=crd-publish-openapi-924 delete e2e-test-crd-publish-openapi-8801-crds test-cr'
    May  1 19:38:43.023: INFO: stderr: ""
    May  1 19:38:43.023: INFO: stdout: "e2e-test-crd-publish-openapi-8801-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/01/23 19:38:43.023
    May  1 19:38:43.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-924 explain e2e-test-crd-publish-openapi-8801-crds'
    May  1 19:38:44.643: INFO: stderr: ""
    May  1 19:38:44.643: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8801-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:39:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-924" for this suite. 05/01/23 19:39:07.142
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:39:07.175
May  1 19:39:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:39:07.185
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:39:07.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:39:07.276
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-df571825-8cae-4210-b7b0-7726a5e08a74 05/01/23 19:39:07.297
STEP: Creating a pod to test consume configMaps 05/01/23 19:39:07.348
May  1 19:39:07.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901" in namespace "configmap-9487" to be "Succeeded or Failed"
May  1 19:39:07.465: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 15.48269ms
May  1 19:39:09.484: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035061647s
May  1 19:39:11.484: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034134934s
May  1 19:39:13.494: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044927636s
STEP: Saw pod success 05/01/23 19:39:13.494
May  1 19:39:13.495: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901" satisfied condition "Succeeded or Failed"
May  1 19:39:13.538: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 container configmap-volume-test: <nil>
STEP: delete the pod 05/01/23 19:39:13.713
May  1 19:39:13.822: INFO: Waiting for pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 to disappear
May  1 19:39:13.848: INFO: Pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:39:13.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9487" for this suite. 05/01/23 19:39:13.876
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":141,"skipped":2558,"failed":0}
------------------------------
• [SLOW TEST] [6.739 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:39:07.175
    May  1 19:39:07.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:39:07.185
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:39:07.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:39:07.276
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-df571825-8cae-4210-b7b0-7726a5e08a74 05/01/23 19:39:07.297
    STEP: Creating a pod to test consume configMaps 05/01/23 19:39:07.348
    May  1 19:39:07.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901" in namespace "configmap-9487" to be "Succeeded or Failed"
    May  1 19:39:07.465: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 15.48269ms
    May  1 19:39:09.484: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035061647s
    May  1 19:39:11.484: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034134934s
    May  1 19:39:13.494: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044927636s
    STEP: Saw pod success 05/01/23 19:39:13.494
    May  1 19:39:13.495: INFO: Pod "pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901" satisfied condition "Succeeded or Failed"
    May  1 19:39:13.538: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 container configmap-volume-test: <nil>
    STEP: delete the pod 05/01/23 19:39:13.713
    May  1 19:39:13.822: INFO: Waiting for pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 to disappear
    May  1 19:39:13.848: INFO: Pod pod-configmaps-9d80cdbc-d78c-47bb-b13e-0b518323b901 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:39:13.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9487" for this suite. 05/01/23 19:39:13.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:39:13.918
May  1 19:39:13.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 19:39:13.921
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:39:14.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:39:14.041
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 in namespace container-probe-6041 05/01/23 19:39:14.061
May  1 19:39:14.261: INFO: Waiting up to 5m0s for pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296" in namespace "container-probe-6041" to be "not pending"
May  1 19:39:14.334: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Pending", Reason="", readiness=false. Elapsed: 72.906047ms
May  1 19:39:16.351: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09024513s
May  1 19:39:18.358: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Running", Reason="", readiness=true. Elapsed: 4.096548809s
May  1 19:39:18.358: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296" satisfied condition "not pending"
May  1 19:39:18.358: INFO: Started pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 in namespace container-probe-6041
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:39:18.358
May  1 19:39:18.375: INFO: Initial restart count of pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 is 0
STEP: deleting the pod 05/01/23 19:43:18.999
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 19:43:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6041" for this suite. 05/01/23 19:43:19.133
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":142,"skipped":2566,"failed":0}
------------------------------
• [SLOW TEST] [245.265 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:39:13.918
    May  1 19:39:13.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 19:39:13.921
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:39:14.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:39:14.041
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 in namespace container-probe-6041 05/01/23 19:39:14.061
    May  1 19:39:14.261: INFO: Waiting up to 5m0s for pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296" in namespace "container-probe-6041" to be "not pending"
    May  1 19:39:14.334: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Pending", Reason="", readiness=false. Elapsed: 72.906047ms
    May  1 19:39:16.351: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09024513s
    May  1 19:39:18.358: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296": Phase="Running", Reason="", readiness=true. Elapsed: 4.096548809s
    May  1 19:39:18.358: INFO: Pod "test-webserver-05869dbb-352a-491a-99ee-bf8d13703296" satisfied condition "not pending"
    May  1 19:39:18.358: INFO: Started pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 in namespace container-probe-6041
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:39:18.358
    May  1 19:39:18.375: INFO: Initial restart count of pod test-webserver-05869dbb-352a-491a-99ee-bf8d13703296 is 0
    STEP: deleting the pod 05/01/23 19:43:18.999
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 19:43:19.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6041" for this suite. 05/01/23 19:43:19.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:19.19
May  1 19:43:19.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:43:19.193
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:19.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:19.3
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 05/01/23 19:43:19.32
May  1 19:43:19.410: INFO: Waiting up to 5m0s for pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238" in namespace "emptydir-9094" to be "Succeeded or Failed"
May  1 19:43:19.428: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 17.583375ms
May  1 19:43:21.445: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035517026s
May  1 19:43:23.449: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039015003s
May  1 19:43:25.479: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0693379s
STEP: Saw pod success 05/01/23 19:43:25.479
May  1 19:43:25.480: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238" satisfied condition "Succeeded or Failed"
May  1 19:43:25.497: INFO: Trying to get logs from node 10.45.145.124 pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 container test-container: <nil>
STEP: delete the pod 05/01/23 19:43:25.619
May  1 19:43:25.678: INFO: Waiting for pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 to disappear
May  1 19:43:25.702: INFO: Pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:43:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9094" for this suite. 05/01/23 19:43:25.74
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":143,"skipped":2572,"failed":0}
------------------------------
• [SLOW TEST] [6.606 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:19.19
    May  1 19:43:19.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:43:19.193
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:19.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:19.3
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/01/23 19:43:19.32
    May  1 19:43:19.410: INFO: Waiting up to 5m0s for pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238" in namespace "emptydir-9094" to be "Succeeded or Failed"
    May  1 19:43:19.428: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 17.583375ms
    May  1 19:43:21.445: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035517026s
    May  1 19:43:23.449: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039015003s
    May  1 19:43:25.479: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.0693379s
    STEP: Saw pod success 05/01/23 19:43:25.479
    May  1 19:43:25.480: INFO: Pod "pod-f3969aee-5b54-4a7e-ba57-82b918cb1238" satisfied condition "Succeeded or Failed"
    May  1 19:43:25.497: INFO: Trying to get logs from node 10.45.145.124 pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 container test-container: <nil>
    STEP: delete the pod 05/01/23 19:43:25.619
    May  1 19:43:25.678: INFO: Waiting for pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 to disappear
    May  1 19:43:25.702: INFO: Pod pod-f3969aee-5b54-4a7e-ba57-82b918cb1238 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:43:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9094" for this suite. 05/01/23 19:43:25.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:25.8
May  1 19:43:25.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 19:43:25.802
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:25.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:25.986
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 19:43:26.076
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:43:27.55
STEP: Deploying the webhook pod 05/01/23 19:43:27.592
STEP: Wait for the deployment to be ready 05/01/23 19:43:27.726
May  1 19:43:27.771: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 19:43:29.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:43:31.866
STEP: Verifying the service has paired with the endpoint 05/01/23 19:43:31.981
May  1 19:43:32.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 05/01/23 19:43:32.999
STEP: Creating a custom resource definition that should be denied by the webhook 05/01/23 19:43:33.075
May  1 19:43:33.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:43:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-35" for this suite. 05/01/23 19:43:33.325
STEP: Destroying namespace "webhook-35-markers" for this suite. 05/01/23 19:43:33.397
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":144,"skipped":2586,"failed":0}
------------------------------
• [SLOW TEST] [8.205 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:25.8
    May  1 19:43:25.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 19:43:25.802
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:25.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:25.986
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 19:43:26.076
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 19:43:27.55
    STEP: Deploying the webhook pod 05/01/23 19:43:27.592
    STEP: Wait for the deployment to be ready 05/01/23 19:43:27.726
    May  1 19:43:27.771: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 19:43:29.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:43:31.866
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:43:31.981
    May  1 19:43:32.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/01/23 19:43:32.999
    STEP: Creating a custom resource definition that should be denied by the webhook 05/01/23 19:43:33.075
    May  1 19:43:33.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:43:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-35" for this suite. 05/01/23 19:43:33.325
    STEP: Destroying namespace "webhook-35-markers" for this suite. 05/01/23 19:43:33.397
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:34.006
May  1 19:43:34.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 19:43:34.008
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:34.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:34.22
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May  1 19:43:34.257: INFO: Creating simple deployment test-new-deployment
May  1 19:43:34.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-new-deployment-845c8977d9\""}}, CollisionCount:(*int32)(nil)}
May  1 19:43:36.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 05/01/23 19:43:38.645
STEP: updating a scale subresource 05/01/23 19:43:38.683
STEP: verifying the deployment Spec.Replicas was modified 05/01/23 19:43:38.715
STEP: Patch a scale subresource 05/01/23 19:43:38.735
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 19:43:38.845: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8653  db379705-20b6-4dde-bcac-92cfdc74dd2f 93398 3 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-01 19:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00e08da68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:43:37 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-01 19:43:37 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  1 19:43:38.866: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8653  dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 93402 3 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment db379705-20b6-4dde-bcac-92cfdc74dd2f 0xc0035e4317 0xc0035e4318}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-01 19:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db379705-20b6-4dde-bcac-92cfdc74dd2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e43a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  1 19:43:38.885: INFO: Pod "test-new-deployment-845c8977d9-2d9kj" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-2d9kj test-new-deployment-845c8977d9- deployment-8653  c078ad0d-865c-41a7-8870-202c3d14a78f 93384 0 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e47a2af615381428b9e7449ab1581c9a7bc644a65bbf014ce9d3b96ef71d9267 cni.projectcalico.org/podIP:172.30.42.87/32 cni.projectcalico.org/podIPs:172.30.42.87/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.87"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.87"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 0xc0035e47d7 0xc0035e47d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfe3756c-3565-47fe-8fd5-07c3ce6a6b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h78z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.87,StartTime:2023-05-01 19:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:43:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1424b9fde0b7b9c6a8ff7e4e2d35e2d060d8a7f7ed491095595f035c55b90bab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:43:38.885: INFO: Pod "test-new-deployment-845c8977d9-rcjtc" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-rcjtc test-new-deployment-845c8977d9- deployment-8653  12410694-4e65-40ed-b27d-4ede28605beb 93405 0 2023-05-01 19:43:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 0xc0035e4a57 0xc0035e4a58}] [] [{kube-controller-manager Update v1 2023-05-01 19:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfe3756c-3565-47fe-8fd5-07c3ce6a6b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htnrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htnrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h78z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 19:43:38.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8653" for this suite. 05/01/23 19:43:38.947
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":145,"skipped":2590,"failed":0}
------------------------------
• [SLOW TEST] [5.003 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:34.006
    May  1 19:43:34.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 19:43:34.008
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:34.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:34.22
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May  1 19:43:34.257: INFO: Creating simple deployment test-new-deployment
    May  1 19:43:34.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-new-deployment-845c8977d9\""}}, CollisionCount:(*int32)(nil)}
    May  1 19:43:36.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 43, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 05/01/23 19:43:38.645
    STEP: updating a scale subresource 05/01/23 19:43:38.683
    STEP: verifying the deployment Spec.Replicas was modified 05/01/23 19:43:38.715
    STEP: Patch a scale subresource 05/01/23 19:43:38.735
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 19:43:38.845: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8653  db379705-20b6-4dde-bcac-92cfdc74dd2f 93398 3 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-01 19:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00e08da68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-01 19:43:37 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-05-01 19:43:37 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  1 19:43:38.866: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-8653  dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 93402 3 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment db379705-20b6-4dde-bcac-92cfdc74dd2f 0xc0035e4317 0xc0035e4318}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-01 19:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"db379705-20b6-4dde-bcac-92cfdc74dd2f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e43a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:43:38.885: INFO: Pod "test-new-deployment-845c8977d9-2d9kj" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-2d9kj test-new-deployment-845c8977d9- deployment-8653  c078ad0d-865c-41a7-8870-202c3d14a78f 93384 0 2023-05-01 19:43:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e47a2af615381428b9e7449ab1581c9a7bc644a65bbf014ce9d3b96ef71d9267 cni.projectcalico.org/podIP:172.30.42.87/32 cni.projectcalico.org/podIPs:172.30.42.87/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.87"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.87"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 0xc0035e47d7 0xc0035e47d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:43:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfe3756c-3565-47fe-8fd5-07c3ce6a6b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:43:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:43:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlsh8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlsh8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h78z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.87,StartTime:2023-05-01 19:43:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:43:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://1424b9fde0b7b9c6a8ff7e4e2d35e2d060d8a7f7ed491095595f035c55b90bab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:43:38.885: INFO: Pod "test-new-deployment-845c8977d9-rcjtc" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-rcjtc test-new-deployment-845c8977d9- deployment-8653  12410694-4e65-40ed-b27d-4ede28605beb 93405 0 2023-05-01 19:43:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 dfe3756c-3565-47fe-8fd5-07c3ce6a6b36 0xc0035e4a57 0xc0035e4a58}] [] [{kube-controller-manager Update v1 2023-05-01 19:43:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfe3756c-3565-47fe-8fd5-07c3ce6a6b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htnrq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htnrq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5h78z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:43:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 19:43:38.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8653" for this suite. 05/01/23 19:43:38.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:39.032
May  1 19:43:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:43:39.034
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:39.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:39.119
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 05/01/23 19:43:39.139
May  1 19:43:39.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 create -f -'
May  1 19:43:43.573: INFO: stderr: ""
May  1 19:43:43.573: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/01/23 19:43:43.573
May  1 19:43:43.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 diff -f -'
May  1 19:43:44.745: INFO: rc: 1
May  1 19:43:44.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 delete -f -'
May  1 19:43:45.020: INFO: stderr: ""
May  1 19:43:45.020: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:43:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3194" for this suite. 05/01/23 19:43:45.082
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":146,"skipped":2651,"failed":0}
------------------------------
• [SLOW TEST] [6.087 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:39.032
    May  1 19:43:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:43:39.034
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:39.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:39.119
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 05/01/23 19:43:39.139
    May  1 19:43:39.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 create -f -'
    May  1 19:43:43.573: INFO: stderr: ""
    May  1 19:43:43.573: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/01/23 19:43:43.573
    May  1 19:43:43.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 diff -f -'
    May  1 19:43:44.745: INFO: rc: 1
    May  1 19:43:44.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3194 delete -f -'
    May  1 19:43:45.020: INFO: stderr: ""
    May  1 19:43:45.020: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:43:45.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3194" for this suite. 05/01/23 19:43:45.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:45.121
May  1 19:43:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:43:45.123
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:45.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:45.278
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  1 19:43:45.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-740" for this suite. 05/01/23 19:43:45.441
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":147,"skipped":2673,"failed":0}
------------------------------
• [0.356 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:45.121
    May  1 19:43:45.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:43:45.123
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:45.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:45.278
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  1 19:43:45.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-740" for this suite. 05/01/23 19:43:45.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:43:45.511
May  1 19:43:45.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 19:43:45.513
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:45.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:45.667
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 in namespace container-probe-8709 05/01/23 19:43:45.689
May  1 19:43:45.786: INFO: Waiting up to 5m0s for pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1" in namespace "container-probe-8709" to be "not pending"
May  1 19:43:45.805: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.763112ms
May  1 19:43:47.826: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039953593s
May  1 19:43:49.824: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Running", Reason="", readiness=true. Elapsed: 4.0388038s
May  1 19:43:49.825: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1" satisfied condition "not pending"
May  1 19:43:49.825: INFO: Started pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 in namespace container-probe-8709
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:43:49.825
May  1 19:43:49.844: INFO: Initial restart count of pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 is 0
May  1 19:44:38.399: INFO: Restart count of pod container-probe-8709/busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 is now 1 (48.555512153s elapsed)
STEP: deleting the pod 05/01/23 19:44:38.399
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 19:44:38.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8709" for this suite. 05/01/23 19:44:38.473
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":148,"skipped":2719,"failed":0}
------------------------------
• [SLOW TEST] [53.007 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:43:45.511
    May  1 19:43:45.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 19:43:45.513
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:43:45.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:43:45.667
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 in namespace container-probe-8709 05/01/23 19:43:45.689
    May  1 19:43:45.786: INFO: Waiting up to 5m0s for pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1" in namespace "container-probe-8709" to be "not pending"
    May  1 19:43:45.805: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.763112ms
    May  1 19:43:47.826: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039953593s
    May  1 19:43:49.824: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1": Phase="Running", Reason="", readiness=true. Elapsed: 4.0388038s
    May  1 19:43:49.825: INFO: Pod "busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1" satisfied condition "not pending"
    May  1 19:43:49.825: INFO: Started pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 in namespace container-probe-8709
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 19:43:49.825
    May  1 19:43:49.844: INFO: Initial restart count of pod busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 is 0
    May  1 19:44:38.399: INFO: Restart count of pod container-probe-8709/busybox-856dd6bb-e9c7-4636-b68e-41396942ccc1 is now 1 (48.555512153s elapsed)
    STEP: deleting the pod 05/01/23 19:44:38.399
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 19:44:38.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8709" for this suite. 05/01/23 19:44:38.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:44:38.523
May  1 19:44:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:44:38.525
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:38.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:38.621
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:44:38.641
May  1 19:44:38.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246" in namespace "downward-api-471" to be "Succeeded or Failed"
May  1 19:44:38.792: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 18.493042ms
May  1 19:44:40.811: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037089506s
May  1 19:44:42.814: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040398842s
May  1 19:44:44.810: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036726632s
STEP: Saw pod success 05/01/23 19:44:44.81
May  1 19:44:44.811: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246" satisfied condition "Succeeded or Failed"
May  1 19:44:44.828: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 container client-container: <nil>
STEP: delete the pod 05/01/23 19:44:44.89
May  1 19:44:44.951: INFO: Waiting for pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 to disappear
May  1 19:44:44.973: INFO: Pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 19:44:44.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-471" for this suite. 05/01/23 19:44:45.003
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":149,"skipped":2729,"failed":0}
------------------------------
• [SLOW TEST] [6.511 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:44:38.523
    May  1 19:44:38.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:44:38.525
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:38.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:38.621
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:44:38.641
    May  1 19:44:38.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246" in namespace "downward-api-471" to be "Succeeded or Failed"
    May  1 19:44:38.792: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 18.493042ms
    May  1 19:44:40.811: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037089506s
    May  1 19:44:42.814: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040398842s
    May  1 19:44:44.810: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036726632s
    STEP: Saw pod success 05/01/23 19:44:44.81
    May  1 19:44:44.811: INFO: Pod "downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246" satisfied condition "Succeeded or Failed"
    May  1 19:44:44.828: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 container client-container: <nil>
    STEP: delete the pod 05/01/23 19:44:44.89
    May  1 19:44:44.951: INFO: Waiting for pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 to disappear
    May  1 19:44:44.973: INFO: Pod downwardapi-volume-87333d5f-5705-41b3-88d1-6d214c910246 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 19:44:44.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-471" for this suite. 05/01/23 19:44:45.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:44:45.044
May  1 19:44:45.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:44:45.047
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:45.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:45.127
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
May  1 19:44:45.300: INFO: created pod pod-service-account-defaultsa
May  1 19:44:45.300: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  1 19:44:45.378: INFO: created pod pod-service-account-mountsa
May  1 19:44:45.378: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  1 19:44:45.431: INFO: created pod pod-service-account-nomountsa
May  1 19:44:45.431: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  1 19:44:45.501: INFO: created pod pod-service-account-defaultsa-mountspec
May  1 19:44:45.501: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  1 19:44:45.558: INFO: created pod pod-service-account-mountsa-mountspec
May  1 19:44:45.558: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  1 19:44:45.605: INFO: created pod pod-service-account-nomountsa-mountspec
May  1 19:44:45.605: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  1 19:44:45.677: INFO: created pod pod-service-account-defaultsa-nomountspec
May  1 19:44:45.677: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  1 19:44:45.745: INFO: created pod pod-service-account-mountsa-nomountspec
May  1 19:44:45.745: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  1 19:44:45.804: INFO: created pod pod-service-account-nomountsa-nomountspec
May  1 19:44:45.804: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 19:44:45.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-160" for this suite. 05/01/23 19:44:45.888
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":150,"skipped":2744,"failed":0}
------------------------------
• [0.897 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:44:45.044
    May  1 19:44:45.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:44:45.047
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:45.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:45.127
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    May  1 19:44:45.300: INFO: created pod pod-service-account-defaultsa
    May  1 19:44:45.300: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May  1 19:44:45.378: INFO: created pod pod-service-account-mountsa
    May  1 19:44:45.378: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May  1 19:44:45.431: INFO: created pod pod-service-account-nomountsa
    May  1 19:44:45.431: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May  1 19:44:45.501: INFO: created pod pod-service-account-defaultsa-mountspec
    May  1 19:44:45.501: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May  1 19:44:45.558: INFO: created pod pod-service-account-mountsa-mountspec
    May  1 19:44:45.558: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May  1 19:44:45.605: INFO: created pod pod-service-account-nomountsa-mountspec
    May  1 19:44:45.605: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May  1 19:44:45.677: INFO: created pod pod-service-account-defaultsa-nomountspec
    May  1 19:44:45.677: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May  1 19:44:45.745: INFO: created pod pod-service-account-mountsa-nomountspec
    May  1 19:44:45.745: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May  1 19:44:45.804: INFO: created pod pod-service-account-nomountsa-nomountspec
    May  1 19:44:45.804: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 19:44:45.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-160" for this suite. 05/01/23 19:44:45.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:44:45.951
May  1 19:44:45.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:44:45.954
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:46.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:46.042
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/01/23 19:44:46.071
May  1 19:44:46.170: INFO: Waiting up to 5m0s for pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c" in namespace "emptydir-2495" to be "Succeeded or Failed"
May  1 19:44:46.190: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.377322ms
May  1 19:44:48.219: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049574019s
May  1 19:44:50.217: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046818334s
May  1 19:44:52.211: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040740186s
May  1 19:44:54.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053361678s
May  1 19:44:56.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.052688111s
STEP: Saw pod success 05/01/23 19:44:56.223
May  1 19:44:56.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c" satisfied condition "Succeeded or Failed"
May  1 19:44:56.242: INFO: Trying to get logs from node 10.45.145.124 pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c container test-container: <nil>
STEP: delete the pod 05/01/23 19:44:56.292
May  1 19:44:56.337: INFO: Waiting for pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c to disappear
May  1 19:44:56.354: INFO: Pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:44:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2495" for this suite. 05/01/23 19:44:56.378
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":151,"skipped":2790,"failed":0}
------------------------------
• [SLOW TEST] [10.461 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:44:45.951
    May  1 19:44:45.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:44:45.954
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:46.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:46.042
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/01/23 19:44:46.071
    May  1 19:44:46.170: INFO: Waiting up to 5m0s for pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c" in namespace "emptydir-2495" to be "Succeeded or Failed"
    May  1 19:44:46.190: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.377322ms
    May  1 19:44:48.219: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049574019s
    May  1 19:44:50.217: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046818334s
    May  1 19:44:52.211: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040740186s
    May  1 19:44:54.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053361678s
    May  1 19:44:56.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.052688111s
    STEP: Saw pod success 05/01/23 19:44:56.223
    May  1 19:44:56.223: INFO: Pod "pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c" satisfied condition "Succeeded or Failed"
    May  1 19:44:56.242: INFO: Trying to get logs from node 10.45.145.124 pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c container test-container: <nil>
    STEP: delete the pod 05/01/23 19:44:56.292
    May  1 19:44:56.337: INFO: Waiting for pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c to disappear
    May  1 19:44:56.354: INFO: Pod pod-051bc5e4-bb8c-4d85-88f2-b7e577665e4c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:44:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2495" for this suite. 05/01/23 19:44:56.378
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:44:56.413
May  1 19:44:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:44:56.416
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:56.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:56.491
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 05/01/23 19:44:56.553
STEP: watching for Pod to be ready 05/01/23 19:44:56.681
May  1 19:44:56.691: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions []
May  1 19:44:56.711: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
May  1 19:44:56.792: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
May  1 19:44:57.927: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
May  1 19:44:58.107: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
May  1 19:44:59.388: INFO: Found Pod pod-test in namespace pods-9775 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/01/23 19:44:59.411
STEP: getting the Pod and ensuring that it's patched 05/01/23 19:44:59.507
STEP: replacing the Pod's status Ready condition to False 05/01/23 19:44:59.531
STEP: check the Pod again to ensure its Ready conditions are False 05/01/23 19:44:59.59
STEP: deleting the Pod via a Collection with a LabelSelector 05/01/23 19:44:59.591
STEP: watching for the Pod to be deleted 05/01/23 19:44:59.63
May  1 19:44:59.640: INFO: observed event type MODIFIED
May  1 19:44:59.685: INFO: observed event type MODIFIED
May  1 19:45:02.062: INFO: observed event type MODIFIED
May  1 19:45:03.441: INFO: observed event type MODIFIED
May  1 19:45:03.468: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:45:03.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9775" for this suite. 05/01/23 19:45:03.535
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":152,"skipped":2791,"failed":0}
------------------------------
• [SLOW TEST] [7.148 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:44:56.413
    May  1 19:44:56.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:44:56.416
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:44:56.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:44:56.491
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 05/01/23 19:44:56.553
    STEP: watching for Pod to be ready 05/01/23 19:44:56.681
    May  1 19:44:56.691: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May  1 19:44:56.711: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
    May  1 19:44:56.792: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
    May  1 19:44:57.927: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
    May  1 19:44:58.107: INFO: observed Pod pod-test in namespace pods-9775 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
    May  1 19:44:59.388: INFO: Found Pod pod-test in namespace pods-9775 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 19:44:56 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/01/23 19:44:59.411
    STEP: getting the Pod and ensuring that it's patched 05/01/23 19:44:59.507
    STEP: replacing the Pod's status Ready condition to False 05/01/23 19:44:59.531
    STEP: check the Pod again to ensure its Ready conditions are False 05/01/23 19:44:59.59
    STEP: deleting the Pod via a Collection with a LabelSelector 05/01/23 19:44:59.591
    STEP: watching for the Pod to be deleted 05/01/23 19:44:59.63
    May  1 19:44:59.640: INFO: observed event type MODIFIED
    May  1 19:44:59.685: INFO: observed event type MODIFIED
    May  1 19:45:02.062: INFO: observed event type MODIFIED
    May  1 19:45:03.441: INFO: observed event type MODIFIED
    May  1 19:45:03.468: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:45:03.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9775" for this suite. 05/01/23 19:45:03.535
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:45:03.562
May  1 19:45:03.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:45:03.565
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:45:03.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:45:03.678
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-5419 05/01/23 19:45:03.707
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[] 05/01/23 19:45:03.799
May  1 19:45:03.847: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5419 05/01/23 19:45:03.848
May  1 19:45:03.932: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5419" to be "running and ready"
May  1 19:45:03.952: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.766196ms
May  1 19:45:03.952: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:45:05.972: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039380945s
May  1 19:45:05.972: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:45:07.984: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.051651074s
May  1 19:45:07.985: INFO: The phase of Pod pod1 is Running (Ready = true)
May  1 19:45:07.985: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod1:[100]] 05/01/23 19:45:08.024
May  1 19:45:08.089: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5419 05/01/23 19:45:08.089
May  1 19:45:08.147: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5419" to be "running and ready"
May  1 19:45:08.164: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.300225ms
May  1 19:45:08.164: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:45:10.182: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035117808s
May  1 19:45:10.182: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:45:12.211: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.064480654s
May  1 19:45:12.211: INFO: The phase of Pod pod2 is Running (Ready = true)
May  1 19:45:12.212: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod1:[100] pod2:[101]] 05/01/23 19:45:12.238
May  1 19:45:12.327: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/01/23 19:45:12.328
May  1 19:45:12.328: INFO: Creating new exec pod
May  1 19:45:12.396: INFO: Waiting up to 5m0s for pod "execpoddds55" in namespace "services-5419" to be "running"
May  1 19:45:12.430: INFO: Pod "execpoddds55": Phase="Pending", Reason="", readiness=false. Elapsed: 34.640551ms
May  1 19:45:14.450: INFO: Pod "execpoddds55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054511725s
May  1 19:45:16.452: INFO: Pod "execpoddds55": Phase="Running", Reason="", readiness=true. Elapsed: 4.055935993s
May  1 19:45:16.452: INFO: Pod "execpoddds55" satisfied condition "running"
May  1 19:45:17.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May  1 19:45:18.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May  1 19:45:18.091: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:45:18.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.38.213 80'
May  1 19:45:18.670: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.38.213 80\nConnection to 172.21.38.213 80 port [tcp/http] succeeded!\n"
May  1 19:45:18.670: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:45:18.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May  1 19:45:19.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May  1 19:45:19.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 19:45:19.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.38.213 81'
May  1 19:45:19.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.38.213 81\nConnection to 172.21.38.213 81 port [tcp/*] succeeded!\n"
May  1 19:45:19.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5419 05/01/23 19:45:19.731
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod2:[101]] 05/01/23 19:45:19.778
May  1 19:45:19.850: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5419 05/01/23 19:45:19.85
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[] 05/01/23 19:45:19.896
May  1 19:45:21.189: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:45:21.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5419" for this suite. 05/01/23 19:45:21.386
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":153,"skipped":2792,"failed":0}
------------------------------
• [SLOW TEST] [17.874 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:45:03.562
    May  1 19:45:03.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:45:03.565
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:45:03.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:45:03.678
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-5419 05/01/23 19:45:03.707
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[] 05/01/23 19:45:03.799
    May  1 19:45:03.847: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5419 05/01/23 19:45:03.848
    May  1 19:45:03.932: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5419" to be "running and ready"
    May  1 19:45:03.952: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.766196ms
    May  1 19:45:03.952: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:45:05.972: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039380945s
    May  1 19:45:05.972: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:45:07.984: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.051651074s
    May  1 19:45:07.985: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  1 19:45:07.985: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod1:[100]] 05/01/23 19:45:08.024
    May  1 19:45:08.089: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5419 05/01/23 19:45:08.089
    May  1 19:45:08.147: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5419" to be "running and ready"
    May  1 19:45:08.164: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.300225ms
    May  1 19:45:08.164: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:45:10.182: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035117808s
    May  1 19:45:10.182: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:45:12.211: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.064480654s
    May  1 19:45:12.211: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  1 19:45:12.212: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod1:[100] pod2:[101]] 05/01/23 19:45:12.238
    May  1 19:45:12.327: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/01/23 19:45:12.328
    May  1 19:45:12.328: INFO: Creating new exec pod
    May  1 19:45:12.396: INFO: Waiting up to 5m0s for pod "execpoddds55" in namespace "services-5419" to be "running"
    May  1 19:45:12.430: INFO: Pod "execpoddds55": Phase="Pending", Reason="", readiness=false. Elapsed: 34.640551ms
    May  1 19:45:14.450: INFO: Pod "execpoddds55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054511725s
    May  1 19:45:16.452: INFO: Pod "execpoddds55": Phase="Running", Reason="", readiness=true. Elapsed: 4.055935993s
    May  1 19:45:16.452: INFO: Pod "execpoddds55" satisfied condition "running"
    May  1 19:45:17.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    May  1 19:45:18.091: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May  1 19:45:18.091: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:45:18.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.38.213 80'
    May  1 19:45:18.670: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.38.213 80\nConnection to 172.21.38.213 80 port [tcp/http] succeeded!\n"
    May  1 19:45:18.670: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:45:18.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    May  1 19:45:19.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May  1 19:45:19.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 19:45:19.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5419 exec execpoddds55 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.38.213 81'
    May  1 19:45:19.731: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.38.213 81\nConnection to 172.21.38.213 81 port [tcp/*] succeeded!\n"
    May  1 19:45:19.731: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5419 05/01/23 19:45:19.731
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[pod2:[101]] 05/01/23 19:45:19.778
    May  1 19:45:19.850: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5419 05/01/23 19:45:19.85
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5419 to expose endpoints map[] 05/01/23 19:45:19.896
    May  1 19:45:21.189: INFO: successfully validated that service multi-endpoint-test in namespace services-5419 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:45:21.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5419" for this suite. 05/01/23 19:45:21.386
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:45:21.439
May  1 19:45:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:45:21.448
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:45:21.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:45:21.525
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  1 19:45:21.682: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 19:46:22.031: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:46:22.07
May  1 19:46:22.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption-path 05/01/23 19:46:22.073
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:22.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:22.155
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 05/01/23 19:46:22.172
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 19:46:22.172
May  1 19:46:22.345: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3232" to be "running"
May  1 19:46:22.363: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041976ms
May  1 19:46:24.382: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037031331s
May  1 19:46:26.381: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.035935461s
May  1 19:46:26.381: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 19:46:26.398
May  1 19:46:26.448: INFO: found a healthy node: 10.45.145.124
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
May  1 19:46:42.813: INFO: pods created so far: [1 1 1]
May  1 19:46:42.814: INFO: length of pods created so far: 3
May  1 19:46:48.894: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
May  1 19:46:55.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3232" for this suite. 05/01/23 19:46:55.93
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  1 19:46:56.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9283" for this suite. 05/01/23 19:46:56.248
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":154,"skipped":2801,"failed":0}
------------------------------
• [SLOW TEST] [95.078 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:45:21.439
    May  1 19:45:21.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:45:21.448
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:45:21.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:45:21.525
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  1 19:45:21.682: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 19:46:22.031: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:46:22.07
    May  1 19:46:22.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption-path 05/01/23 19:46:22.073
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:22.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:22.155
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 05/01/23 19:46:22.172
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 19:46:22.172
    May  1 19:46:22.345: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3232" to be "running"
    May  1 19:46:22.363: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041976ms
    May  1 19:46:24.382: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037031331s
    May  1 19:46:26.381: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.035935461s
    May  1 19:46:26.381: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 19:46:26.398
    May  1 19:46:26.448: INFO: found a healthy node: 10.45.145.124
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    May  1 19:46:42.813: INFO: pods created so far: [1 1 1]
    May  1 19:46:42.814: INFO: length of pods created so far: 3
    May  1 19:46:48.894: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    May  1 19:46:55.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3232" for this suite. 05/01/23 19:46:55.93
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:46:56.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9283" for this suite. 05/01/23 19:46:56.248
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:46:56.526
May  1 19:46:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:46:56.528
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:56.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:56.687
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5443-delete-me 05/01/23 19:46:56.735
STEP: Waiting for the RuntimeClass to disappear 05/01/23 19:46:56.774
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  1 19:46:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5443" for this suite. 05/01/23 19:46:56.917
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":155,"skipped":2827,"failed":0}
------------------------------
• [0.419 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:46:56.526
    May  1 19:46:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename runtimeclass 05/01/23 19:46:56.528
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:56.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:56.687
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5443-delete-me 05/01/23 19:46:56.735
    STEP: Waiting for the RuntimeClass to disappear 05/01/23 19:46:56.774
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  1 19:46:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5443" for this suite. 05/01/23 19:46:56.917
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:46:56.945
May  1 19:46:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:46:56.947
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:57.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:57.042
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 05/01/23 19:46:57.095
STEP: watching for the Service to be added 05/01/23 19:46:57.193
May  1 19:46:57.204: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May  1 19:46:57.205: INFO: Service test-service-m5w9l created
STEP: Getting /status 05/01/23 19:46:57.205
May  1 19:46:57.229: INFO: Service test-service-m5w9l has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/01/23 19:46:57.229
STEP: watching for the Service to be patched 05/01/23 19:46:57.259
May  1 19:46:57.283: INFO: observed Service test-service-m5w9l in namespace services-7207 with annotations: map[] & LoadBalancer: {[]}
May  1 19:46:57.283: INFO: Found Service test-service-m5w9l in namespace services-7207 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May  1 19:46:57.283: INFO: Service test-service-m5w9l has service status patched
STEP: updating the ServiceStatus 05/01/23 19:46:57.283
May  1 19:46:57.333: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/01/23 19:46:57.333
May  1 19:46:57.343: INFO: Observed Service test-service-m5w9l in namespace services-7207 with annotations: map[] & Conditions: {[]}
May  1 19:46:57.343: INFO: Observed event: &Service{ObjectMeta:{test-service-m5w9l  services-7207  945e2d0f-1ada-41b6-bebd-2569849916bc 95709 0 2023-05-01 19:46:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-01 19:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-01 19:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.79.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.79.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May  1 19:46:57.343: INFO: Found Service test-service-m5w9l in namespace services-7207 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  1 19:46:57.343: INFO: Service test-service-m5w9l has service status updated
STEP: patching the service 05/01/23 19:46:57.343
STEP: watching for the Service to be patched 05/01/23 19:46:57.371
May  1 19:46:57.380: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
May  1 19:46:57.381: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
May  1 19:46:57.381: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
May  1 19:46:57.381: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service:patched test-service-static:true]
May  1 19:46:57.381: INFO: Service test-service-m5w9l patched
STEP: deleting the service 05/01/23 19:46:57.382
STEP: watching for the Service to be deleted 05/01/23 19:46:57.466
May  1 19:46:57.474: INFO: Observed event: ADDED
May  1 19:46:57.474: INFO: Observed event: MODIFIED
May  1 19:46:57.474: INFO: Observed event: MODIFIED
May  1 19:46:57.475: INFO: Observed event: MODIFIED
May  1 19:46:57.475: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May  1 19:46:57.475: INFO: Service test-service-m5w9l deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:46:57.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7207" for this suite. 05/01/23 19:46:57.505
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":156,"skipped":2827,"failed":0}
------------------------------
• [0.589 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:46:56.945
    May  1 19:46:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:46:56.947
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:57.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:57.042
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 05/01/23 19:46:57.095
    STEP: watching for the Service to be added 05/01/23 19:46:57.193
    May  1 19:46:57.204: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May  1 19:46:57.205: INFO: Service test-service-m5w9l created
    STEP: Getting /status 05/01/23 19:46:57.205
    May  1 19:46:57.229: INFO: Service test-service-m5w9l has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/01/23 19:46:57.229
    STEP: watching for the Service to be patched 05/01/23 19:46:57.259
    May  1 19:46:57.283: INFO: observed Service test-service-m5w9l in namespace services-7207 with annotations: map[] & LoadBalancer: {[]}
    May  1 19:46:57.283: INFO: Found Service test-service-m5w9l in namespace services-7207 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May  1 19:46:57.283: INFO: Service test-service-m5w9l has service status patched
    STEP: updating the ServiceStatus 05/01/23 19:46:57.283
    May  1 19:46:57.333: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/01/23 19:46:57.333
    May  1 19:46:57.343: INFO: Observed Service test-service-m5w9l in namespace services-7207 with annotations: map[] & Conditions: {[]}
    May  1 19:46:57.343: INFO: Observed event: &Service{ObjectMeta:{test-service-m5w9l  services-7207  945e2d0f-1ada-41b6-bebd-2569849916bc 95709 0 2023-05-01 19:46:57 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-01 19:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-01 19:46:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.79.11,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.79.11],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May  1 19:46:57.343: INFO: Found Service test-service-m5w9l in namespace services-7207 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  1 19:46:57.343: INFO: Service test-service-m5w9l has service status updated
    STEP: patching the service 05/01/23 19:46:57.343
    STEP: watching for the Service to be patched 05/01/23 19:46:57.371
    May  1 19:46:57.380: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
    May  1 19:46:57.381: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
    May  1 19:46:57.381: INFO: observed Service test-service-m5w9l in namespace services-7207 with labels: map[test-service-static:true]
    May  1 19:46:57.381: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service:patched test-service-static:true]
    May  1 19:46:57.381: INFO: Service test-service-m5w9l patched
    STEP: deleting the service 05/01/23 19:46:57.382
    STEP: watching for the Service to be deleted 05/01/23 19:46:57.466
    May  1 19:46:57.474: INFO: Observed event: ADDED
    May  1 19:46:57.474: INFO: Observed event: MODIFIED
    May  1 19:46:57.474: INFO: Observed event: MODIFIED
    May  1 19:46:57.475: INFO: Observed event: MODIFIED
    May  1 19:46:57.475: INFO: Found Service test-service-m5w9l in namespace services-7207 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May  1 19:46:57.475: INFO: Service test-service-m5w9l deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:46:57.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7207" for this suite. 05/01/23 19:46:57.505
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:46:57.535
May  1 19:46:57.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:46:57.538
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:57.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:57.63
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-02ad75f9-96aa-443b-9c09-f20c93f28329 05/01/23 19:46:57.657
STEP: Creating a pod to test consume configMaps 05/01/23 19:46:57.731
May  1 19:46:57.832: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5" in namespace "projected-6728" to be "Succeeded or Failed"
May  1 19:46:57.856: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.782495ms
May  1 19:46:59.895: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063400548s
May  1 19:47:01.912: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079463165s
May  1 19:47:03.874: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042095972s
STEP: Saw pod success 05/01/23 19:47:03.874
May  1 19:47:03.875: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5" satisfied condition "Succeeded or Failed"
May  1 19:47:03.892: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:47:04.023
May  1 19:47:04.077: INFO: Waiting for pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 to disappear
May  1 19:47:04.095: INFO: Pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 19:47:04.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6728" for this suite. 05/01/23 19:47:04.128
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":157,"skipped":2828,"failed":0}
------------------------------
• [SLOW TEST] [6.623 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:46:57.535
    May  1 19:46:57.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:46:57.538
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:46:57.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:46:57.63
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-02ad75f9-96aa-443b-9c09-f20c93f28329 05/01/23 19:46:57.657
    STEP: Creating a pod to test consume configMaps 05/01/23 19:46:57.731
    May  1 19:46:57.832: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5" in namespace "projected-6728" to be "Succeeded or Failed"
    May  1 19:46:57.856: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.782495ms
    May  1 19:46:59.895: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063400548s
    May  1 19:47:01.912: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079463165s
    May  1 19:47:03.874: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042095972s
    STEP: Saw pod success 05/01/23 19:47:03.874
    May  1 19:47:03.875: INFO: Pod "pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5" satisfied condition "Succeeded or Failed"
    May  1 19:47:03.892: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:47:04.023
    May  1 19:47:04.077: INFO: Waiting for pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 to disappear
    May  1 19:47:04.095: INFO: Pod pod-projected-configmaps-39fce6ae-14c2-4b3a-9e94-74aa378231f5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 19:47:04.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6728" for this suite. 05/01/23 19:47:04.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:47:04.161
May  1 19:47:04.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename csistoragecapacity 05/01/23 19:47:04.163
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:04.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:04.264
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/01/23 19:47:04.281
STEP: getting /apis/storage.k8s.io 05/01/23 19:47:04.297
STEP: getting /apis/storage.k8s.io/v1 05/01/23 19:47:04.303
STEP: creating 05/01/23 19:47:04.31
STEP: watching 05/01/23 19:47:04.502
May  1 19:47:04.502: INFO: starting watch
STEP: getting 05/01/23 19:47:04.547
STEP: listing in namespace 05/01/23 19:47:04.567
STEP: listing across namespaces 05/01/23 19:47:04.609
STEP: patching 05/01/23 19:47:04.637
STEP: updating 05/01/23 19:47:04.676
May  1 19:47:04.720: INFO: waiting for watch events with expected annotations in namespace
May  1 19:47:04.721: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/01/23 19:47:04.721
STEP: deleting a collection 05/01/23 19:47:04.871
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
May  1 19:47:05.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-4288" for this suite. 05/01/23 19:47:05.128
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":158,"skipped":2836,"failed":0}
------------------------------
• [0.998 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:47:04.161
    May  1 19:47:04.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename csistoragecapacity 05/01/23 19:47:04.163
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:04.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:04.264
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/01/23 19:47:04.281
    STEP: getting /apis/storage.k8s.io 05/01/23 19:47:04.297
    STEP: getting /apis/storage.k8s.io/v1 05/01/23 19:47:04.303
    STEP: creating 05/01/23 19:47:04.31
    STEP: watching 05/01/23 19:47:04.502
    May  1 19:47:04.502: INFO: starting watch
    STEP: getting 05/01/23 19:47:04.547
    STEP: listing in namespace 05/01/23 19:47:04.567
    STEP: listing across namespaces 05/01/23 19:47:04.609
    STEP: patching 05/01/23 19:47:04.637
    STEP: updating 05/01/23 19:47:04.676
    May  1 19:47:04.720: INFO: waiting for watch events with expected annotations in namespace
    May  1 19:47:04.721: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/01/23 19:47:04.721
    STEP: deleting a collection 05/01/23 19:47:04.871
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    May  1 19:47:05.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-4288" for this suite. 05/01/23 19:47:05.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:47:05.175
May  1 19:47:05.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:47:05.177
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:05.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:05.288
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  05/01/23 19:47:05.305
May  1 19:47:05.394: INFO: Waiting up to 5m0s for pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577" in namespace "svcaccounts-9291" to be "Succeeded or Failed"
May  1 19:47:05.419: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 24.920255ms
May  1 19:47:07.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045082794s
May  1 19:47:09.444: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049550111s
May  1 19:47:11.439: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045024302s
May  1 19:47:13.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045684921s
STEP: Saw pod success 05/01/23 19:47:13.44
May  1 19:47:13.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577" satisfied condition "Succeeded or Failed"
May  1 19:47:13.459: INFO: Trying to get logs from node 10.45.145.124 pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:47:13.553
May  1 19:47:13.615: INFO: Waiting for pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 to disappear
May  1 19:47:13.631: INFO: Pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 19:47:13.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9291" for this suite. 05/01/23 19:47:13.655
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":159,"skipped":2876,"failed":0}
------------------------------
• [SLOW TEST] [8.507 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:47:05.175
    May  1 19:47:05.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 19:47:05.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:05.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:05.288
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  05/01/23 19:47:05.305
    May  1 19:47:05.394: INFO: Waiting up to 5m0s for pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577" in namespace "svcaccounts-9291" to be "Succeeded or Failed"
    May  1 19:47:05.419: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 24.920255ms
    May  1 19:47:07.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045082794s
    May  1 19:47:09.444: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049550111s
    May  1 19:47:11.439: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045024302s
    May  1 19:47:13.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045684921s
    STEP: Saw pod success 05/01/23 19:47:13.44
    May  1 19:47:13.440: INFO: Pod "test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577" satisfied condition "Succeeded or Failed"
    May  1 19:47:13.459: INFO: Trying to get logs from node 10.45.145.124 pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:47:13.553
    May  1 19:47:13.615: INFO: Waiting for pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 to disappear
    May  1 19:47:13.631: INFO: Pod test-pod-3520bfe5-45a5-4603-9aa6-d5fb32c77577 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 19:47:13.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9291" for this suite. 05/01/23 19:47:13.655
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:47:13.682
May  1 19:47:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 19:47:13.685
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:13.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:13.774
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 05/01/23 19:47:13.789
STEP: waiting for pod running 05/01/23 19:47:13.881
May  1 19:47:13.881: INFO: Waiting up to 2m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529" to be "running"
May  1 19:47:13.899: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.970744ms
May  1 19:47:15.920: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038481273s
May  1 19:47:17.930: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Running", Reason="", readiness=true. Elapsed: 4.048583783s
May  1 19:47:17.930: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" satisfied condition "running"
STEP: creating a file in subpath 05/01/23 19:47:17.93
May  1 19:47:17.947: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1529 PodName:var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:47:17.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:47:17.948: INFO: ExecWithOptions: Clientset creation
May  1 19:47:17.948: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1529/pods/var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/01/23 19:47:18.219
May  1 19:47:18.236: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1529 PodName:var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:47:18.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:47:18.237: INFO: ExecWithOptions: Clientset creation
May  1 19:47:18.237: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1529/pods/var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/01/23 19:47:18.495
May  1 19:47:19.135: INFO: Successfully updated pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e"
STEP: waiting for annotated pod running 05/01/23 19:47:19.135
May  1 19:47:19.135: INFO: Waiting up to 2m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529" to be "running"
May  1 19:47:19.156: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Running", Reason="", readiness=true. Elapsed: 20.650496ms
May  1 19:47:19.156: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" satisfied condition "running"
STEP: deleting the pod gracefully 05/01/23 19:47:19.156
May  1 19:47:19.157: INFO: Deleting pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529"
May  1 19:47:19.188: INFO: Wait up to 5m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 19:47:53.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1529" for this suite. 05/01/23 19:47:53.272
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":160,"skipped":2878,"failed":0}
------------------------------
• [SLOW TEST] [39.624 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:47:13.682
    May  1 19:47:13.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 19:47:13.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:13.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:13.774
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 05/01/23 19:47:13.789
    STEP: waiting for pod running 05/01/23 19:47:13.881
    May  1 19:47:13.881: INFO: Waiting up to 2m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529" to be "running"
    May  1 19:47:13.899: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.970744ms
    May  1 19:47:15.920: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038481273s
    May  1 19:47:17.930: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Running", Reason="", readiness=true. Elapsed: 4.048583783s
    May  1 19:47:17.930: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" satisfied condition "running"
    STEP: creating a file in subpath 05/01/23 19:47:17.93
    May  1 19:47:17.947: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1529 PodName:var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:47:17.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:47:17.948: INFO: ExecWithOptions: Clientset creation
    May  1 19:47:17.948: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1529/pods/var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/01/23 19:47:18.219
    May  1 19:47:18.236: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1529 PodName:var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:47:18.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:47:18.237: INFO: ExecWithOptions: Clientset creation
    May  1 19:47:18.237: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-1529/pods/var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/01/23 19:47:18.495
    May  1 19:47:19.135: INFO: Successfully updated pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e"
    STEP: waiting for annotated pod running 05/01/23 19:47:19.135
    May  1 19:47:19.135: INFO: Waiting up to 2m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529" to be "running"
    May  1 19:47:19.156: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e": Phase="Running", Reason="", readiness=true. Elapsed: 20.650496ms
    May  1 19:47:19.156: INFO: Pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" satisfied condition "running"
    STEP: deleting the pod gracefully 05/01/23 19:47:19.156
    May  1 19:47:19.157: INFO: Deleting pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" in namespace "var-expansion-1529"
    May  1 19:47:19.188: INFO: Wait up to 5m0s for pod "var-expansion-2ffcfe41-0c22-4e6b-9db7-c26dd66d5c4e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 19:47:53.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1529" for this suite. 05/01/23 19:47:53.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:47:53.311
May  1 19:47:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:47:53.315
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:53.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:53.389
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 05/01/23 19:47:53.406
May  1 19:47:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: rename a version 05/01/23 19:48:44.922
STEP: check the new version name is served 05/01/23 19:48:44.976
STEP: check the old version name is removed 05/01/23 19:49:05.521
STEP: check the other version is not changed 05/01/23 19:49:13.396
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:49:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3458" for this suite. 05/01/23 19:49:55.139
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":161,"skipped":2901,"failed":0}
------------------------------
• [SLOW TEST] [121.897 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:47:53.311
    May  1 19:47:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:47:53.315
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:47:53.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:47:53.389
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 05/01/23 19:47:53.406
    May  1 19:47:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: rename a version 05/01/23 19:48:44.922
    STEP: check the new version name is served 05/01/23 19:48:44.976
    STEP: check the old version name is removed 05/01/23 19:49:05.521
    STEP: check the other version is not changed 05/01/23 19:49:13.396
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:49:55.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3458" for this suite. 05/01/23 19:49:55.139
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:49:55.208
May  1 19:49:55.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 19:49:55.211
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:49:55.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:49:55.367
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 05/01/23 19:49:55.398
STEP: Ensuring ResourceQuota status is calculated 05/01/23 19:49:55.427
STEP: Creating a ResourceQuota with not best effort scope 05/01/23 19:49:57.445
STEP: Ensuring ResourceQuota status is calculated 05/01/23 19:49:57.47
STEP: Creating a best-effort pod 05/01/23 19:49:59.487
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/01/23 19:49:59.562
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/01/23 19:50:01.576
STEP: Deleting the pod 05/01/23 19:50:03.594
STEP: Ensuring resource quota status released the pod usage 05/01/23 19:50:03.631
STEP: Creating a not best-effort pod 05/01/23 19:50:05.665
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/01/23 19:50:05.729
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/01/23 19:50:07.742
STEP: Deleting the pod 05/01/23 19:50:09.757
STEP: Ensuring resource quota status released the pod usage 05/01/23 19:50:09.824
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 19:50:11.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7955" for this suite. 05/01/23 19:50:11.921
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":162,"skipped":2902,"failed":0}
------------------------------
• [SLOW TEST] [16.753 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:49:55.208
    May  1 19:49:55.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 19:49:55.211
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:49:55.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:49:55.367
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 05/01/23 19:49:55.398
    STEP: Ensuring ResourceQuota status is calculated 05/01/23 19:49:55.427
    STEP: Creating a ResourceQuota with not best effort scope 05/01/23 19:49:57.445
    STEP: Ensuring ResourceQuota status is calculated 05/01/23 19:49:57.47
    STEP: Creating a best-effort pod 05/01/23 19:49:59.487
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/01/23 19:49:59.562
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/01/23 19:50:01.576
    STEP: Deleting the pod 05/01/23 19:50:03.594
    STEP: Ensuring resource quota status released the pod usage 05/01/23 19:50:03.631
    STEP: Creating a not best-effort pod 05/01/23 19:50:05.665
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/01/23 19:50:05.729
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/01/23 19:50:07.742
    STEP: Deleting the pod 05/01/23 19:50:09.757
    STEP: Ensuring resource quota status released the pod usage 05/01/23 19:50:09.824
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 19:50:11.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7955" for this suite. 05/01/23 19:50:11.921
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:11.962
May  1 19:50:11.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:50:11.964
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:12.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:12.119
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
May  1 19:50:12.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-7093 version'
May  1 19:50:12.367: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May  1 19:50:12.367: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8+27e744f\", GitCommit:\"df8a1b9c3feb33a5ba9d3f0b80fce83a1e301859\", GitTreeState:\"clean\", BuildDate:\"2023-04-06T20:23:55Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:50:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7093" for this suite. 05/01/23 19:50:12.393
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":163,"skipped":2903,"failed":0}
------------------------------
• [0.474 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:11.962
    May  1 19:50:11.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:50:11.964
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:12.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:12.119
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    May  1 19:50:12.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-7093 version'
    May  1 19:50:12.367: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May  1 19:50:12.367: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8\", GitCommit:\"0ce7342c984110dfc93657d64df5dc3b2c0d1fe9\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:39:54Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.8+27e744f\", GitCommit:\"df8a1b9c3feb33a5ba9d3f0b80fce83a1e301859\", GitTreeState:\"clean\", BuildDate:\"2023-04-06T20:23:55Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:50:12.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7093" for this suite. 05/01/23 19:50:12.393
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:12.437
May  1 19:50:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename containers 05/01/23 19:50:12.441
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:12.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:12.642
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 05/01/23 19:50:12.674
May  1 19:50:12.801: INFO: Waiting up to 5m0s for pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423" in namespace "containers-4147" to be "Succeeded or Failed"
May  1 19:50:12.821: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007358ms
May  1 19:50:14.847: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046224834s
May  1 19:50:16.835: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033733112s
May  1 19:50:18.835: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034218014s
May  1 19:50:20.838: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03762513s
STEP: Saw pod success 05/01/23 19:50:20.839
May  1 19:50:20.839: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423" satisfied condition "Succeeded or Failed"
May  1 19:50:20.859: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:50:20.972
May  1 19:50:21.007: INFO: Waiting for pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 to disappear
May  1 19:50:21.021: INFO: Pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  1 19:50:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4147" for this suite. 05/01/23 19:50:21.042
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":164,"skipped":2907,"failed":0}
------------------------------
• [SLOW TEST] [8.653 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:12.437
    May  1 19:50:12.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename containers 05/01/23 19:50:12.441
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:12.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:12.642
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 05/01/23 19:50:12.674
    May  1 19:50:12.801: INFO: Waiting up to 5m0s for pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423" in namespace "containers-4147" to be "Succeeded or Failed"
    May  1 19:50:12.821: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007358ms
    May  1 19:50:14.847: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046224834s
    May  1 19:50:16.835: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033733112s
    May  1 19:50:18.835: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034218014s
    May  1 19:50:20.838: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.03762513s
    STEP: Saw pod success 05/01/23 19:50:20.839
    May  1 19:50:20.839: INFO: Pod "client-containers-a420bece-5b8a-401a-9c81-4772abbb0423" satisfied condition "Succeeded or Failed"
    May  1 19:50:20.859: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:50:20.972
    May  1 19:50:21.007: INFO: Waiting for pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 to disappear
    May  1 19:50:21.021: INFO: Pod client-containers-a420bece-5b8a-401a-9c81-4772abbb0423 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  1 19:50:21.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4147" for this suite. 05/01/23 19:50:21.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:21.093
May  1 19:50:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 19:50:21.094
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:21.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:21.186
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/01/23 19:50:21.208
May  1 19:50:21.307: INFO: Waiting up to 5m0s for pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67" in namespace "emptydir-856" to be "Succeeded or Failed"
May  1 19:50:21.374: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 66.186935ms
May  1 19:50:23.388: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080856527s
May  1 19:50:25.409: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101499313s
May  1 19:50:27.387: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079051977s
STEP: Saw pod success 05/01/23 19:50:27.387
May  1 19:50:27.387: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67" satisfied condition "Succeeded or Failed"
May  1 19:50:27.406: INFO: Trying to get logs from node 10.45.145.124 pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 container test-container: <nil>
STEP: delete the pod 05/01/23 19:50:27.451
May  1 19:50:27.498: INFO: Waiting for pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 to disappear
May  1 19:50:27.511: INFO: Pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 19:50:27.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-856" for this suite. 05/01/23 19:50:27.547
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":165,"skipped":2938,"failed":0}
------------------------------
• [SLOW TEST] [6.484 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:21.093
    May  1 19:50:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 19:50:21.094
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:21.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:21.186
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/01/23 19:50:21.208
    May  1 19:50:21.307: INFO: Waiting up to 5m0s for pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67" in namespace "emptydir-856" to be "Succeeded or Failed"
    May  1 19:50:21.374: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 66.186935ms
    May  1 19:50:23.388: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080856527s
    May  1 19:50:25.409: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101499313s
    May  1 19:50:27.387: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079051977s
    STEP: Saw pod success 05/01/23 19:50:27.387
    May  1 19:50:27.387: INFO: Pod "pod-9a125b29-bdf1-4c64-8198-603e357bff67" satisfied condition "Succeeded or Failed"
    May  1 19:50:27.406: INFO: Trying to get logs from node 10.45.145.124 pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 container test-container: <nil>
    STEP: delete the pod 05/01/23 19:50:27.451
    May  1 19:50:27.498: INFO: Waiting for pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 to disappear
    May  1 19:50:27.511: INFO: Pod pod-9a125b29-bdf1-4c64-8198-603e357bff67 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 19:50:27.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-856" for this suite. 05/01/23 19:50:27.547
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:27.577
May  1 19:50:27.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename watch 05/01/23 19:50:27.579
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:27.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:27.686
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/01/23 19:50:27.706
STEP: starting a background goroutine to produce watch events 05/01/23 19:50:27.725
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/01/23 19:50:27.725
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  1 19:50:31.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2136" for this suite. 05/01/23 19:50:31.556
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":166,"skipped":2940,"failed":0}
------------------------------
• [4.029 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:27.577
    May  1 19:50:27.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename watch 05/01/23 19:50:27.579
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:27.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:27.686
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/01/23 19:50:27.706
    STEP: starting a background goroutine to produce watch events 05/01/23 19:50:27.725
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/01/23 19:50:27.725
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  1 19:50:31.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2136" for this suite. 05/01/23 19:50:31.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:31.609
May  1 19:50:31.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename containers 05/01/23 19:50:31.613
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:31.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:31.693
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 05/01/23 19:50:31.713
May  1 19:50:31.800: INFO: Waiting up to 5m0s for pod "client-containers-d4c28903-1196-4806-b317-e180acb40035" in namespace "containers-755" to be "Succeeded or Failed"
May  1 19:50:31.824: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 23.772215ms
May  1 19:50:33.846: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046157361s
May  1 19:50:35.887: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087089555s
May  1 19:50:37.838: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038156967s
STEP: Saw pod success 05/01/23 19:50:37.838
May  1 19:50:37.838: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035" satisfied condition "Succeeded or Failed"
May  1 19:50:37.851: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-d4c28903-1196-4806-b317-e180acb40035 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:50:37.904
May  1 19:50:37.954: INFO: Waiting for pod client-containers-d4c28903-1196-4806-b317-e180acb40035 to disappear
May  1 19:50:37.966: INFO: Pod client-containers-d4c28903-1196-4806-b317-e180acb40035 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  1 19:50:37.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-755" for this suite. 05/01/23 19:50:37.988
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":167,"skipped":2946,"failed":0}
------------------------------
• [SLOW TEST] [6.458 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:31.609
    May  1 19:50:31.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename containers 05/01/23 19:50:31.613
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:31.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:31.693
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 05/01/23 19:50:31.713
    May  1 19:50:31.800: INFO: Waiting up to 5m0s for pod "client-containers-d4c28903-1196-4806-b317-e180acb40035" in namespace "containers-755" to be "Succeeded or Failed"
    May  1 19:50:31.824: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 23.772215ms
    May  1 19:50:33.846: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046157361s
    May  1 19:50:35.887: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087089555s
    May  1 19:50:37.838: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038156967s
    STEP: Saw pod success 05/01/23 19:50:37.838
    May  1 19:50:37.838: INFO: Pod "client-containers-d4c28903-1196-4806-b317-e180acb40035" satisfied condition "Succeeded or Failed"
    May  1 19:50:37.851: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-d4c28903-1196-4806-b317-e180acb40035 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:50:37.904
    May  1 19:50:37.954: INFO: Waiting for pod client-containers-d4c28903-1196-4806-b317-e180acb40035 to disappear
    May  1 19:50:37.966: INFO: Pod client-containers-d4c28903-1196-4806-b317-e180acb40035 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  1 19:50:37.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-755" for this suite. 05/01/23 19:50:37.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:38.069
May  1 19:50:38.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:50:38.072
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:38.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:38.207
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 05/01/23 19:50:38.242
May  1 19:50:38.421: INFO: Waiting up to 5m0s for pod "pod-2pwjw" in namespace "pods-2971" to be "running"
May  1 19:50:38.432: INFO: Pod "pod-2pwjw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044856ms
May  1 19:50:40.495: INFO: Pod "pod-2pwjw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073394544s
May  1 19:50:42.449: INFO: Pod "pod-2pwjw": Phase="Running", Reason="", readiness=true. Elapsed: 4.027978486s
May  1 19:50:42.449: INFO: Pod "pod-2pwjw" satisfied condition "running"
STEP: patching /status 05/01/23 19:50:42.449
May  1 19:50:42.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:50:42.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2971" for this suite. 05/01/23 19:50:42.535
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":168,"skipped":2964,"failed":0}
------------------------------
• [4.542 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:38.069
    May  1 19:50:38.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:50:38.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:38.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:38.207
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 05/01/23 19:50:38.242
    May  1 19:50:38.421: INFO: Waiting up to 5m0s for pod "pod-2pwjw" in namespace "pods-2971" to be "running"
    May  1 19:50:38.432: INFO: Pod "pod-2pwjw": Phase="Pending", Reason="", readiness=false. Elapsed: 11.044856ms
    May  1 19:50:40.495: INFO: Pod "pod-2pwjw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073394544s
    May  1 19:50:42.449: INFO: Pod "pod-2pwjw": Phase="Running", Reason="", readiness=true. Elapsed: 4.027978486s
    May  1 19:50:42.449: INFO: Pod "pod-2pwjw" satisfied condition "running"
    STEP: patching /status 05/01/23 19:50:42.449
    May  1 19:50:42.484: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:50:42.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2971" for this suite. 05/01/23 19:50:42.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:42.643
May  1 19:50:42.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename certificates 05/01/23 19:50:42.646
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:42.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:42.738
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/01/23 19:50:44.6
STEP: getting /apis/certificates.k8s.io 05/01/23 19:50:44.618
STEP: getting /apis/certificates.k8s.io/v1 05/01/23 19:50:44.631
STEP: creating 05/01/23 19:50:44.64
STEP: getting 05/01/23 19:50:44.734
STEP: listing 05/01/23 19:50:44.758
STEP: watching 05/01/23 19:50:44.793
May  1 19:50:44.793: INFO: starting watch
STEP: patching 05/01/23 19:50:44.8
STEP: updating 05/01/23 19:50:44.841
May  1 19:50:44.874: INFO: waiting for watch events with expected annotations
May  1 19:50:44.874: INFO: saw patched and updated annotations
STEP: getting /approval 05/01/23 19:50:44.874
STEP: patching /approval 05/01/23 19:50:44.896
STEP: updating /approval 05/01/23 19:50:44.93
STEP: getting /status 05/01/23 19:50:44.961
STEP: patching /status 05/01/23 19:50:44.985
STEP: updating /status 05/01/23 19:50:45.012
STEP: deleting 05/01/23 19:50:45.043
STEP: deleting a collection 05/01/23 19:50:45.126
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:50:45.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1994" for this suite. 05/01/23 19:50:45.25
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":169,"skipped":3061,"failed":0}
------------------------------
• [2.674 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:42.643
    May  1 19:50:42.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename certificates 05/01/23 19:50:42.646
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:42.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:42.738
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/01/23 19:50:44.6
    STEP: getting /apis/certificates.k8s.io 05/01/23 19:50:44.618
    STEP: getting /apis/certificates.k8s.io/v1 05/01/23 19:50:44.631
    STEP: creating 05/01/23 19:50:44.64
    STEP: getting 05/01/23 19:50:44.734
    STEP: listing 05/01/23 19:50:44.758
    STEP: watching 05/01/23 19:50:44.793
    May  1 19:50:44.793: INFO: starting watch
    STEP: patching 05/01/23 19:50:44.8
    STEP: updating 05/01/23 19:50:44.841
    May  1 19:50:44.874: INFO: waiting for watch events with expected annotations
    May  1 19:50:44.874: INFO: saw patched and updated annotations
    STEP: getting /approval 05/01/23 19:50:44.874
    STEP: patching /approval 05/01/23 19:50:44.896
    STEP: updating /approval 05/01/23 19:50:44.93
    STEP: getting /status 05/01/23 19:50:44.961
    STEP: patching /status 05/01/23 19:50:44.985
    STEP: updating /status 05/01/23 19:50:45.012
    STEP: deleting 05/01/23 19:50:45.043
    STEP: deleting a collection 05/01/23 19:50:45.126
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:50:45.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-1994" for this suite. 05/01/23 19:50:45.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:45.319
May  1 19:50:45.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 19:50:45.322
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:45.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:45.417
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May  1 19:50:45.449: INFO: Creating ReplicaSet my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f
W0501 19:50:45.489498      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May  1 19:50:45.510: INFO: Pod name my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Found 0 pods out of 1
May  1 19:50:50.564: INFO: Pod name my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Found 1 pods out of 1
May  1 19:50:50.564: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" is running
May  1 19:50:50.564: INFO: Waiting up to 5m0s for pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" in namespace "replicaset-4314" to be "running"
May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5": Phase="Running", Reason="", readiness=true. Elapsed: 11.13221ms
May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" satisfied condition "running"
May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:45 +0000 UTC Reason: Message:}])
May  1 19:50:50.576: INFO: Trying to dial the pod
May  1 19:50:55.631: INFO: Controller my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Got expected result from replica 1 [my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5]: "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 19:50:55.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4314" for this suite. 05/01/23 19:50:55.674
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":170,"skipped":3072,"failed":0}
------------------------------
• [SLOW TEST] [10.389 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:45.319
    May  1 19:50:45.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 19:50:45.322
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:45.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:45.417
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May  1 19:50:45.449: INFO: Creating ReplicaSet my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f
    W0501 19:50:45.489498      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May  1 19:50:45.510: INFO: Pod name my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Found 0 pods out of 1
    May  1 19:50:50.564: INFO: Pod name my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Found 1 pods out of 1
    May  1 19:50:50.564: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f" is running
    May  1 19:50:50.564: INFO: Waiting up to 5m0s for pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" in namespace "replicaset-4314" to be "running"
    May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5": Phase="Running", Reason="", readiness=true. Elapsed: 11.13221ms
    May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" satisfied condition "running"
    May  1 19:50:50.576: INFO: Pod "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-01 19:50:45 +0000 UTC Reason: Message:}])
    May  1 19:50:50.576: INFO: Trying to dial the pod
    May  1 19:50:55.631: INFO: Controller my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f: Got expected result from replica 1 [my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5]: "my-hostname-basic-77125527-8a61-4681-ab54-a0449fb06b4f-9q9t5", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 19:50:55.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4314" for this suite. 05/01/23 19:50:55.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:50:55.715
May  1 19:50:55.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:50:55.718
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:55.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:55.798
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:50:55.812
May  1 19:50:56.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1" in namespace "downward-api-8632" to be "Succeeded or Failed"
May  1 19:50:56.088: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.836559ms
May  1 19:50:58.100: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058912028s
May  1 19:51:00.102: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060103311s
May  1 19:51:02.144: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102067821s
May  1 19:51:04.111: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06959349s
STEP: Saw pod success 05/01/23 19:51:04.111
May  1 19:51:04.112: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1" satisfied condition "Succeeded or Failed"
May  1 19:51:04.126: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 container client-container: <nil>
STEP: delete the pod 05/01/23 19:51:04.157
May  1 19:51:04.186: INFO: Waiting for pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 to disappear
May  1 19:51:04.208: INFO: Pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 19:51:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8632" for this suite. 05/01/23 19:51:04.234
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":171,"skipped":3087,"failed":0}
------------------------------
• [SLOW TEST] [8.563 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:50:55.715
    May  1 19:50:55.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:50:55.718
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:50:55.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:50:55.798
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:50:55.812
    May  1 19:50:56.041: INFO: Waiting up to 5m0s for pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1" in namespace "downward-api-8632" to be "Succeeded or Failed"
    May  1 19:50:56.088: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 46.836559ms
    May  1 19:50:58.100: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058912028s
    May  1 19:51:00.102: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060103311s
    May  1 19:51:02.144: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102067821s
    May  1 19:51:04.111: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06959349s
    STEP: Saw pod success 05/01/23 19:51:04.111
    May  1 19:51:04.112: INFO: Pod "downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1" satisfied condition "Succeeded or Failed"
    May  1 19:51:04.126: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 container client-container: <nil>
    STEP: delete the pod 05/01/23 19:51:04.157
    May  1 19:51:04.186: INFO: Waiting for pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 to disappear
    May  1 19:51:04.208: INFO: Pod downwardapi-volume-411141e7-fed5-4c46-a775-261526baf7a1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 19:51:04.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8632" for this suite. 05/01/23 19:51:04.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:51:04.284
May  1 19:51:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 19:51:04.286
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:51:04.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:51:04.377
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/01/23 19:51:04.399
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:04.425
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:04.425
STEP: creating a pod to probe DNS 05/01/23 19:51:04.426
STEP: submitting the pod to kubernetes 05/01/23 19:51:04.427
May  1 19:51:04.541: INFO: Waiting up to 15m0s for pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee" in namespace "dns-7595" to be "running"
May  1 19:51:04.550: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Pending", Reason="", readiness=false. Elapsed: 9.482555ms
May  1 19:51:06.587: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046071735s
May  1 19:51:08.574: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Running", Reason="", readiness=true. Elapsed: 4.033322823s
May  1 19:51:08.574: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee" satisfied condition "running"
STEP: retrieving the pod 05/01/23 19:51:08.574
STEP: looking for the results for each expected name from probers 05/01/23 19:51:08.597
May  1 19:51:08.684: INFO: DNS probes using dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee succeeded

STEP: deleting the pod 05/01/23 19:51:08.684
STEP: changing the externalName to bar.example.com 05/01/23 19:51:08.757
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:08.872
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:08.873
STEP: creating a second pod to probe DNS 05/01/23 19:51:08.874
STEP: submitting the pod to kubernetes 05/01/23 19:51:08.874
May  1 19:51:08.974: INFO: Waiting up to 15m0s for pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc" in namespace "dns-7595" to be "running"
May  1 19:51:08.984: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039315ms
May  1 19:51:11.024: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050190672s
May  1 19:51:13.015: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040670743s
May  1 19:51:14.998: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Running", Reason="", readiness=true. Elapsed: 6.023679534s
May  1 19:51:14.998: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc" satisfied condition "running"
STEP: retrieving the pod 05/01/23 19:51:14.998
STEP: looking for the results for each expected name from probers 05/01/23 19:51:15.011
May  1 19:51:15.059: INFO: DNS probes using dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc succeeded

STEP: deleting the pod 05/01/23 19:51:15.059
STEP: changing the service to type=ClusterIP 05/01/23 19:51:15.093
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:15.159
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
 05/01/23 19:51:15.159
STEP: creating a third pod to probe DNS 05/01/23 19:51:15.16
STEP: submitting the pod to kubernetes 05/01/23 19:51:15.178
May  1 19:51:15.231: INFO: Waiting up to 15m0s for pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0" in namespace "dns-7595" to be "running"
May  1 19:51:15.244: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.596009ms
May  1 19:51:17.262: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031086897s
May  1 19:51:19.266: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Running", Reason="", readiness=true. Elapsed: 4.03564021s
May  1 19:51:19.266: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0" satisfied condition "running"
STEP: retrieving the pod 05/01/23 19:51:19.266
STEP: looking for the results for each expected name from probers 05/01/23 19:51:19.286
May  1 19:51:19.363: INFO: DNS probes using dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0 succeeded

STEP: deleting the pod 05/01/23 19:51:19.363
STEP: deleting the test externalName service 05/01/23 19:51:19.408
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 19:51:19.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7595" for this suite. 05/01/23 19:51:19.518
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":172,"skipped":3114,"failed":0}
------------------------------
• [SLOW TEST] [15.271 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:51:04.284
    May  1 19:51:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 19:51:04.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:51:04.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:51:04.377
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/01/23 19:51:04.399
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:04.425
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:04.425
    STEP: creating a pod to probe DNS 05/01/23 19:51:04.426
    STEP: submitting the pod to kubernetes 05/01/23 19:51:04.427
    May  1 19:51:04.541: INFO: Waiting up to 15m0s for pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee" in namespace "dns-7595" to be "running"
    May  1 19:51:04.550: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Pending", Reason="", readiness=false. Elapsed: 9.482555ms
    May  1 19:51:06.587: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046071735s
    May  1 19:51:08.574: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee": Phase="Running", Reason="", readiness=true. Elapsed: 4.033322823s
    May  1 19:51:08.574: INFO: Pod "dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 19:51:08.574
    STEP: looking for the results for each expected name from probers 05/01/23 19:51:08.597
    May  1 19:51:08.684: INFO: DNS probes using dns-test-eaebb5e0-b2c6-42ea-9c71-307a5d38caee succeeded

    STEP: deleting the pod 05/01/23 19:51:08.684
    STEP: changing the externalName to bar.example.com 05/01/23 19:51:08.757
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:08.872
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:08.873
    STEP: creating a second pod to probe DNS 05/01/23 19:51:08.874
    STEP: submitting the pod to kubernetes 05/01/23 19:51:08.874
    May  1 19:51:08.974: INFO: Waiting up to 15m0s for pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc" in namespace "dns-7595" to be "running"
    May  1 19:51:08.984: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039315ms
    May  1 19:51:11.024: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050190672s
    May  1 19:51:13.015: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040670743s
    May  1 19:51:14.998: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc": Phase="Running", Reason="", readiness=true. Elapsed: 6.023679534s
    May  1 19:51:14.998: INFO: Pod "dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 19:51:14.998
    STEP: looking for the results for each expected name from probers 05/01/23 19:51:15.011
    May  1 19:51:15.059: INFO: DNS probes using dns-test-24296296-9e9d-4dc4-abbc-442aa8e4adcc succeeded

    STEP: deleting the pod 05/01/23 19:51:15.059
    STEP: changing the service to type=ClusterIP 05/01/23 19:51:15.093
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:15.159
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7595.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7595.svc.cluster.local; sleep 1; done
     05/01/23 19:51:15.159
    STEP: creating a third pod to probe DNS 05/01/23 19:51:15.16
    STEP: submitting the pod to kubernetes 05/01/23 19:51:15.178
    May  1 19:51:15.231: INFO: Waiting up to 15m0s for pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0" in namespace "dns-7595" to be "running"
    May  1 19:51:15.244: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.596009ms
    May  1 19:51:17.262: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031086897s
    May  1 19:51:19.266: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0": Phase="Running", Reason="", readiness=true. Elapsed: 4.03564021s
    May  1 19:51:19.266: INFO: Pod "dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 19:51:19.266
    STEP: looking for the results for each expected name from probers 05/01/23 19:51:19.286
    May  1 19:51:19.363: INFO: DNS probes using dns-test-c81bf32d-9239-456f-a059-48c7eb1a8cb0 succeeded

    STEP: deleting the pod 05/01/23 19:51:19.363
    STEP: deleting the test externalName service 05/01/23 19:51:19.408
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 19:51:19.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7595" for this suite. 05/01/23 19:51:19.518
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:51:19.556
May  1 19:51:19.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:51:19.559
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:51:19.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:51:19.652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  1 19:51:19.783: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 19:52:20.145: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 05/01/23 19:52:20.205
May  1 19:52:20.367: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  1 19:52:20.426: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  1 19:52:20.530: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  1 19:52:20.582: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  1 19:52:20.712: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  1 19:52:20.773: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/01/23 19:52:20.773
May  1 19:52:20.774: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:20.804: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 29.938086ms
May  1 19:52:22.821: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046833841s
May  1 19:52:24.819: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045819004s
May  1 19:52:26.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058782451s
May  1 19:52:28.817: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043426528s
May  1 19:52:30.818: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044735437s
May  1 19:52:32.823: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049300138s
May  1 19:52:34.822: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.048301174s
May  1 19:52:34.822: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  1 19:52:34.822: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:34.837: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.32483ms
May  1 19:52:34.838: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  1 19:52:34.838: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:34.853: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.801425ms
May  1 19:52:34.853: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  1 19:52:34.853: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:34.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.102832ms
May  1 19:52:34.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  1 19:52:34.867: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:34.882: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.588202ms
May  1 19:52:34.882: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  1 19:52:34.882: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
May  1 19:52:34.894: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.751283ms
May  1 19:52:34.894: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/01/23 19:52:34.894
May  1 19:52:34.945: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May  1 19:52:34.963: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.973138ms
May  1 19:52:36.984: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0391911s
May  1 19:52:38.979: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033857289s
May  1 19:52:40.992: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046690704s
May  1 19:52:42.997: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.052112759s
May  1 19:52:42.997: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  1 19:52:43.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2012" for this suite. 05/01/23 19:52:43.224
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":173,"skipped":3114,"failed":0}
------------------------------
• [SLOW TEST] [83.968 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:51:19.556
    May  1 19:51:19.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption 05/01/23 19:51:19.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:51:19.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:51:19.652
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  1 19:51:19.783: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 19:52:20.145: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 05/01/23 19:52:20.205
    May  1 19:52:20.367: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  1 19:52:20.426: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  1 19:52:20.530: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  1 19:52:20.582: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  1 19:52:20.712: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  1 19:52:20.773: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/01/23 19:52:20.773
    May  1 19:52:20.774: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:20.804: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 29.938086ms
    May  1 19:52:22.821: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046833841s
    May  1 19:52:24.819: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045819004s
    May  1 19:52:26.832: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058782451s
    May  1 19:52:28.817: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.043426528s
    May  1 19:52:30.818: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.044735437s
    May  1 19:52:32.823: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.049300138s
    May  1 19:52:34.822: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.048301174s
    May  1 19:52:34.822: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  1 19:52:34.822: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:34.837: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 15.32483ms
    May  1 19:52:34.838: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  1 19:52:34.838: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:34.853: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.801425ms
    May  1 19:52:34.853: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  1 19:52:34.853: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:34.867: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.102832ms
    May  1 19:52:34.867: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  1 19:52:34.867: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:34.882: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.588202ms
    May  1 19:52:34.882: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  1 19:52:34.882: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2012" to be "running"
    May  1 19:52:34.894: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 11.751283ms
    May  1 19:52:34.894: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/01/23 19:52:34.894
    May  1 19:52:34.945: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May  1 19:52:34.963: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.973138ms
    May  1 19:52:36.984: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0391911s
    May  1 19:52:38.979: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033857289s
    May  1 19:52:40.992: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046690704s
    May  1 19:52:42.997: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.052112759s
    May  1 19:52:42.997: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:52:43.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2012" for this suite. 05/01/23 19:52:43.224
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:52:43.531
May  1 19:52:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 19:52:43.533
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:43.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:43.625
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 05/01/23 19:52:43.767
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 19:52:43.797
May  1 19:52:43.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:52:43.854: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:52:44.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:52:44.894: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:52:45.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:52:45.904: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:52:46.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:46.925: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 19:52:47.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 19:52:47.914: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/01/23 19:52:47.943
May  1 19:52:48.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:48.005: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:49.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:49.055: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:50.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:50.067: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:51.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:51.079: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:52.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:52.098: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:53.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 19:52:53.071: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
May  1 19:52:54.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 19:52:54.045: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 19:52:54.067
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2592, will wait for the garbage collector to delete the pods 05/01/23 19:52:54.068
May  1 19:52:54.177: INFO: Deleting DaemonSet.extensions daemon-set took: 38.639694ms
May  1 19:52:54.379: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.372777ms
May  1 19:52:57.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 19:52:57.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 19:52:57.522: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"99429"},"items":null}

May  1 19:52:57.536: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"99430"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 19:52:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2592" for this suite. 05/01/23 19:52:57.696
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":174,"skipped":3123,"failed":0}
------------------------------
• [SLOW TEST] [14.230 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:52:43.531
    May  1 19:52:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 19:52:43.533
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:43.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:43.625
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 05/01/23 19:52:43.767
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 19:52:43.797
    May  1 19:52:43.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:52:43.854: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:52:44.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:52:44.894: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:52:45.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:52:45.904: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:52:46.925: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:46.925: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 19:52:47.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 19:52:47.914: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/01/23 19:52:47.943
    May  1 19:52:48.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:48.005: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:49.055: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:49.055: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:50.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:50.067: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:51.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:51.079: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:52.098: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:52.098: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:53.071: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 19:52:53.071: INFO: Node 10.45.145.71 is running 0 daemon pod, expected 1
    May  1 19:52:54.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 19:52:54.045: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 19:52:54.067
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2592, will wait for the garbage collector to delete the pods 05/01/23 19:52:54.068
    May  1 19:52:54.177: INFO: Deleting DaemonSet.extensions daemon-set took: 38.639694ms
    May  1 19:52:54.379: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.372777ms
    May  1 19:52:57.504: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 19:52:57.504: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 19:52:57.522: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"99429"},"items":null}

    May  1 19:52:57.536: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"99430"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:52:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2592" for this suite. 05/01/23 19:52:57.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:52:57.766
May  1 19:52:57.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:52:57.769
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:57.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:57.915
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 05/01/23 19:52:57.941
May  1 19:52:57.942: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4510 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/01/23 19:52:58.184
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:52:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4510" for this suite. 05/01/23 19:52:58.26
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":175,"skipped":3130,"failed":0}
------------------------------
• [0.586 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:52:57.766
    May  1 19:52:57.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:52:57.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:57.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:57.915
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 05/01/23 19:52:57.941
    May  1 19:52:57.942: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-4510 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/01/23 19:52:58.184
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:52:58.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4510" for this suite. 05/01/23 19:52:58.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:52:58.358
May  1 19:52:58.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:52:58.36
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:58.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:58.492
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:52:58.529
May  1 19:52:58.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849" in namespace "projected-5855" to be "Succeeded or Failed"
May  1 19:52:58.861: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 17.023451ms
May  1 19:53:00.880: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036022153s
May  1 19:53:02.876: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03257521s
May  1 19:53:04.876: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032144154s
STEP: Saw pod success 05/01/23 19:53:04.876
May  1 19:53:04.877: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849" satisfied condition "Succeeded or Failed"
May  1 19:53:04.902: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 container client-container: <nil>
STEP: delete the pod 05/01/23 19:53:05.084
May  1 19:53:05.149: INFO: Waiting for pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 to disappear
May  1 19:53:05.171: INFO: Pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 19:53:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5855" for this suite. 05/01/23 19:53:05.216
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":176,"skipped":3166,"failed":0}
------------------------------
• [SLOW TEST] [6.904 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:52:58.358
    May  1 19:52:58.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:52:58.36
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:52:58.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:52:58.492
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:52:58.529
    May  1 19:52:58.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849" in namespace "projected-5855" to be "Succeeded or Failed"
    May  1 19:52:58.861: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 17.023451ms
    May  1 19:53:00.880: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036022153s
    May  1 19:53:02.876: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03257521s
    May  1 19:53:04.876: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032144154s
    STEP: Saw pod success 05/01/23 19:53:04.876
    May  1 19:53:04.877: INFO: Pod "downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849" satisfied condition "Succeeded or Failed"
    May  1 19:53:04.902: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 container client-container: <nil>
    STEP: delete the pod 05/01/23 19:53:05.084
    May  1 19:53:05.149: INFO: Waiting for pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 to disappear
    May  1 19:53:05.171: INFO: Pod downwardapi-volume-2c074bc3-0b5c-4db3-95ae-fa26f3c48849 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 19:53:05.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5855" for this suite. 05/01/23 19:53:05.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:53:05.268
May  1 19:53:05.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 19:53:05.272
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:05.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:05.357
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 05/01/23 19:53:05.391
May  1 19:53:05.391: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May  1 19:53:05.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:08.185: INFO: stderr: ""
May  1 19:53:08.185: INFO: stdout: "service/agnhost-replica created\n"
May  1 19:53:08.185: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May  1 19:53:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:11.471: INFO: stderr: ""
May  1 19:53:11.471: INFO: stdout: "service/agnhost-primary created\n"
May  1 19:53:11.471: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  1 19:53:11.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:13.376: INFO: stderr: ""
May  1 19:53:13.376: INFO: stdout: "service/frontend created\n"
May  1 19:53:13.377: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May  1 19:53:13.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:14.960: INFO: stderr: ""
May  1 19:53:14.960: INFO: stdout: "deployment.apps/frontend created\n"
May  1 19:53:14.960: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  1 19:53:14.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:16.306: INFO: stderr: ""
May  1 19:53:16.306: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May  1 19:53:16.306: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  1 19:53:16.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
May  1 19:53:20.851: INFO: stderr: ""
May  1 19:53:20.851: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/01/23 19:53:20.851
May  1 19:53:20.851: INFO: Waiting for all frontend pods to be Running.
May  1 19:53:20.902: INFO: Waiting for frontend to serve content.
May  1 19:53:22.031: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
May  1 19:53:27.102: INFO: Trying to add a new entry to the guestbook.
May  1 19:53:27.151: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/01/23 19:53:27.195
May  1 19:53:27.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:27.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:27.496: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/01/23 19:53:27.496
May  1 19:53:27.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:27.857: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:27.857: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/01/23 19:53:27.857
May  1 19:53:27.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:28.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:28.446: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/01/23 19:53:28.446
May  1 19:53:28.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:28.740: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:28.741: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/01/23 19:53:28.741
May  1 19:53:28.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:29.096: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:29.096: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/01/23 19:53:29.096
May  1 19:53:29.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
May  1 19:53:29.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 19:53:29.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 19:53:29.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-285" for this suite. 05/01/23 19:53:29.478
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":177,"skipped":3173,"failed":0}
------------------------------
• [SLOW TEST] [24.258 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:53:05.268
    May  1 19:53:05.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 19:53:05.272
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:05.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:05.357
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 05/01/23 19:53:05.391
    May  1 19:53:05.391: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May  1 19:53:05.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:08.185: INFO: stderr: ""
    May  1 19:53:08.185: INFO: stdout: "service/agnhost-replica created\n"
    May  1 19:53:08.185: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May  1 19:53:08.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:11.471: INFO: stderr: ""
    May  1 19:53:11.471: INFO: stdout: "service/agnhost-primary created\n"
    May  1 19:53:11.471: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May  1 19:53:11.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:13.376: INFO: stderr: ""
    May  1 19:53:13.376: INFO: stdout: "service/frontend created\n"
    May  1 19:53:13.377: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May  1 19:53:13.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:14.960: INFO: stderr: ""
    May  1 19:53:14.960: INFO: stdout: "deployment.apps/frontend created\n"
    May  1 19:53:14.960: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  1 19:53:14.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:16.306: INFO: stderr: ""
    May  1 19:53:16.306: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May  1 19:53:16.306: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May  1 19:53:16.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 create -f -'
    May  1 19:53:20.851: INFO: stderr: ""
    May  1 19:53:20.851: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/01/23 19:53:20.851
    May  1 19:53:20.851: INFO: Waiting for all frontend pods to be Running.
    May  1 19:53:20.902: INFO: Waiting for frontend to serve content.
    May  1 19:53:22.031: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
    May  1 19:53:27.102: INFO: Trying to add a new entry to the guestbook.
    May  1 19:53:27.151: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/01/23 19:53:27.195
    May  1 19:53:27.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:27.496: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:27.496: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/01/23 19:53:27.496
    May  1 19:53:27.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:27.857: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:27.857: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/01/23 19:53:27.857
    May  1 19:53:27.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:28.446: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:28.446: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/01/23 19:53:28.446
    May  1 19:53:28.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:28.740: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:28.741: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/01/23 19:53:28.741
    May  1 19:53:28.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:29.096: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:29.096: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/01/23 19:53:29.096
    May  1 19:53:29.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-285 delete --grace-period=0 --force -f -'
    May  1 19:53:29.428: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 19:53:29.428: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 19:53:29.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-285" for this suite. 05/01/23 19:53:29.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:53:29.531
May  1 19:53:29.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 19:53:29.536
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:29.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:29.703
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May  1 19:53:29.724: INFO: Creating deployment "webserver-deployment"
May  1 19:53:29.745: INFO: Waiting for observed generation 1
May  1 19:53:31.792: INFO: Waiting for all required pods to come up
May  1 19:53:31.834: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/01/23 19:53:31.834
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2vf54" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wbsg5" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nvrcl" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f98k7" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-56tg4" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sfxqk" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hk4z6" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5dwgs" in namespace "deployment-5389" to be "running"
May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v25vc" in namespace "deployment-5389" to be "running"
May  1 19:53:31.836: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5sd5g" in namespace "deployment-5389" to be "running"
May  1 19:53:31.856: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Pending", Reason="", readiness=false. Elapsed: 20.277553ms
May  1 19:53:31.857: INFO: Pod "webserver-deployment-845c8977d9-sfxqk": Phase="Pending", Reason="", readiness=false. Elapsed: 22.27837ms
May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.665791ms
May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.255965ms
May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.887142ms
May  1 19:53:31.859: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 23.826241ms
May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.117469ms
May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-2vf54": Phase="Pending", Reason="", readiness=false. Elapsed: 27.443664ms
May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-5dwgs": Phase="Pending", Reason="", readiness=false. Elapsed: 27.01309ms
May  1 19:53:31.863: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Pending", Reason="", readiness=false. Elapsed: 27.783836ms
May  1 19:53:33.877: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041452174s
May  1 19:53:33.889: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054036689s
May  1 19:53:33.902: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0669111s
May  1 19:53:33.904: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069590615s
May  1 19:53:33.907: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07199998s
May  1 19:53:33.908: INFO: Pod "webserver-deployment-845c8977d9-sfxqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.072750768s
May  1 19:53:33.908: INFO: Pod "webserver-deployment-845c8977d9-sfxqk" satisfied condition "running"
May  1 19:53:33.909: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073775327s
May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-5dwgs": Phase="Running", Reason="", readiness=true. Elapsed: 2.077299401s
May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-5dwgs" satisfied condition "running"
May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078549693s
May  1 19:53:33.914: INFO: Pod "webserver-deployment-845c8977d9-2vf54": Phase="Running", Reason="", readiness=true. Elapsed: 2.078844004s
May  1 19:53:33.914: INFO: Pod "webserver-deployment-845c8977d9-2vf54" satisfied condition "running"
May  1 19:53:35.886: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Running", Reason="", readiness=true. Elapsed: 4.050564262s
May  1 19:53:35.886: INFO: Pod "webserver-deployment-845c8977d9-hk4z6" satisfied condition "running"
May  1 19:53:35.887: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.052714694s
May  1 19:53:35.888: INFO: Pod "webserver-deployment-845c8977d9-wbsg5" satisfied condition "running"
May  1 19:53:35.902: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.067518788s
May  1 19:53:35.902: INFO: Pod "webserver-deployment-845c8977d9-nvrcl" satisfied condition "running"
May  1 19:53:35.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Running", Reason="", readiness=true. Elapsed: 4.077806348s
May  1 19:53:35.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7" satisfied condition "running"
May  1 19:53:35.915: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.079234511s
May  1 19:53:35.915: INFO: Pod "webserver-deployment-845c8977d9-5sd5g" satisfied condition "running"
May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080421973s
May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Running", Reason="", readiness=true. Elapsed: 4.081144367s
May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-56tg4" satisfied condition "running"
May  1 19:53:37.874: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Running", Reason="", readiness=true. Elapsed: 6.038872884s
May  1 19:53:37.874: INFO: Pod "webserver-deployment-845c8977d9-v25vc" satisfied condition "running"
May  1 19:53:37.874: INFO: Waiting for deployment "webserver-deployment" to complete
May  1 19:53:37.911: INFO: Updating deployment "webserver-deployment" with a non-existent image
May  1 19:53:38.030: INFO: Updating deployment webserver-deployment
May  1 19:53:38.030: INFO: Waiting for observed generation 2
May  1 19:53:40.284: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  1 19:53:40.320: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  1 19:53:40.481: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  1 19:53:40.636: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  1 19:53:40.636: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  1 19:53:40.665: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May  1 19:53:40.810: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May  1 19:53:40.810: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May  1 19:53:40.863: INFO: Updating deployment webserver-deployment
May  1 19:53:40.863: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May  1 19:53:40.929: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  1 19:53:43.047: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 19:53:43.179: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5389  8f4dc855-8bc4-497f-b61d-6fb0afba272d 100466 3 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a123a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-01 19:53:40 +0000 UTC,LastTransitionTime:2023-05-01 19:53:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-01 19:53:41 +0000 UTC,LastTransitionTime:2023-05-01 19:53:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May  1 19:53:43.208: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5389  665db086-be65-4032-bf75-323b9279f39e 100464 3 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8f4dc855-8bc4-497f-b61d-6fb0afba272d 0xc0025cdc17 0xc0025cdc18}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4dc855-8bc4-497f-b61d-6fb0afba272d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cdcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May  1 19:53:43.208: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May  1 19:53:43.208: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5389  07e77d02-18ce-4b76-b11c-152022cf958a 100449 3 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8f4dc855-8bc4-497f-b61d-6fb0afba272d 0xc0025cdd17 0xc0025cdd18}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4dc855-8bc4-497f-b61d-6fb0afba272d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cddb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May  1 19:53:43.476: INFO: Pod "webserver-deployment-69b7448995-2dr76" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2dr76 webserver-deployment-69b7448995- deployment-5389  293ca6f2-7356-4ec3-ab7b-6c15135445d0 100450 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2267 0xc003af2268}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xhzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xhzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.478: INFO: Pod "webserver-deployment-69b7448995-5lrwt" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5lrwt webserver-deployment-69b7448995- deployment-5389  b3dbf1c6-7abc-4207-9ea8-5cdf94b11633 100351 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b76389ee9e6cdf5abecbfe09a4443a3ea01c1e613f14dde39c8c36c734c09f45 cni.projectcalico.org/podIP:172.30.38.222/32 cni.projectcalico.org/podIPs:172.30.38.222/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.222"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.222"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af24b7 0xc003af24b8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6shjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6shjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.479: INFO: Pod "webserver-deployment-69b7448995-6lhl7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6lhl7 webserver-deployment-69b7448995- deployment-5389  ae7a3ed6-24d3-4e8c-aab0-97321bb968ac 100494 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2727 0xc003af2728}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgqzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgqzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.480: INFO: Pod "webserver-deployment-69b7448995-75g8c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-75g8c webserver-deployment-69b7448995- deployment-5389  8c5d73ba-b7b7-4b4c-8e78-377ceb84612a 100469 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2957 0xc003af2958}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z6gl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z6gl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.481: INFO: Pod "webserver-deployment-69b7448995-8zqjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-8zqjk webserver-deployment-69b7448995- deployment-5389  285a3aa4-17f8-49d7-baf7-e66d6b297ea2 100493 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2ba7 0xc003af2ba8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tq557,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tq557,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.482: INFO: Pod "webserver-deployment-69b7448995-9mrtq" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9mrtq webserver-deployment-69b7448995- deployment-5389  9cb9c9cd-6755-4d7e-b4ca-7e20dd21e87f 100363 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ad6b7208c0333c3a8726ebf7cafc9f09960aa57cea11122bbb843f1f46b48803 cni.projectcalico.org/podIP:172.30.38.221/32 cni.projectcalico.org/podIPs:172.30.38.221/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.221"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.221"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2e07 0xc003af2e08}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9582,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9582,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.488: INFO: Pod "webserver-deployment-69b7448995-brmh5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-brmh5 webserver-deployment-69b7448995- deployment-5389  725e2b34-813d-4574-a53f-f163742963e2 100519 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:325a2c5535dee75bc9d8c076e1e489f80e0f9ac0158aeee0c3610eb056ebda88 cni.projectcalico.org/podIP:172.30.42.85/32 cni.projectcalico.org/podIPs:172.30.42.85/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.85"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.85"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af30a7 0xc003af30a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdp6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdp6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.85,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.489: INFO: Pod "webserver-deployment-69b7448995-cfns7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cfns7 webserver-deployment-69b7448995- deployment-5389  d3c52378-7cf9-4086-9c27-611a8563fb00 100446 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3367 0xc003af3368}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lklmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lklmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.489: INFO: Pod "webserver-deployment-69b7448995-hvwvz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hvwvz webserver-deployment-69b7448995- deployment-5389  e76c4da1-5631-48f7-8f69-cb8656e079e7 100481 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3597 0xc003af3598}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz5fc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz5fc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.493: INFO: Pod "webserver-deployment-69b7448995-pvz7r" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pvz7r webserver-deployment-69b7448995- deployment-5389  09b3d77d-5653-4c85-a66b-9ac94a012f22 100525 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3467e95616e447d1d5c052677337f0e048930f1ab4aad564285d20b07e9d820b cni.projectcalico.org/podIP:172.30.244.122/32 cni.projectcalico.org/podIPs:172.30.244.122/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af37c7 0xc003af37c8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4fzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4fzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.495: INFO: Pod "webserver-deployment-69b7448995-t9xxm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-t9xxm webserver-deployment-69b7448995- deployment-5389  7eecb895-75c1-4cc2-a8a5-a1cd4965a1e6 100514 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8a969baa079128bf53bd30c2c43d4e9e7d1e2b3e37b30f066c03889b4357fb3f cni.projectcalico.org/podIP:172.30.244.81/32 cni.projectcalico.org/podIPs:172.30.244.81/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.81"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.81"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3a57 0xc003af3a58}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvm8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvm8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.81,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.497: INFO: Pod "webserver-deployment-69b7448995-tjljm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tjljm webserver-deployment-69b7448995- deployment-5389  d08b222e-86e6-4d51-a893-76889b28813d 100509 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6b54bdbc07529fa79264466dfc3e3b7cd92819e393a59c2f71163bbfbd36820d cni.projectcalico.org/podIP:172.30.244.115/32 cni.projectcalico.org/podIPs:172.30.244.115/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.115"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.115"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3d17 0xc003af3d18}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d85vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d85vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.498: INFO: Pod "webserver-deployment-69b7448995-wf9rz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wf9rz webserver-deployment-69b7448995- deployment-5389  5e624c03-d2be-40cf-bc9d-9e3e93db67c0 100373 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:87b2345a0e7c77b3e7df6d2f8780edc15c1c303aeff7b161a0f6d0e1b42b8352 cni.projectcalico.org/podIP:172.30.42.86/32 cni.projectcalico.org/podIPs:172.30.42.86/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.86"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.86"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3fc7 0xc003af3fc8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-md9kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-md9kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.499: INFO: Pod "webserver-deployment-845c8977d9-2vf54" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2vf54 webserver-deployment-845c8977d9- deployment-5389  042dd12b-d80b-4264-91b4-4de7abb8a14a 100139 0 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9eb573b2eb1eafe1d08ec1446e9c7e399b0ce2124ed12b692d2106bc3892b3ab cni.projectcalico.org/podIP:172.30.244.120/32 cni.projectcalico.org/podIPs:172.30.244.120/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.120"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.120"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610277 0xc009610278}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtwdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtwdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.120,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://17f5d9b033c4a1290569ec99822d7a7cdae0d9d950de60d5fa8d5a2cdb587b27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.500: INFO: Pod "webserver-deployment-845c8977d9-4xgvn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4xgvn webserver-deployment-845c8977d9- deployment-5389  06075cf6-5383-4ca6-a674-2d3ab0a9eb4c 100461 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096104f7 0xc0096104f8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2f8ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2f8ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.501: INFO: Pod "webserver-deployment-845c8977d9-56tg4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-56tg4 webserver-deployment-845c8977d9- deployment-5389  4285d03f-19c3-4720-9223-9cbda031bbf6 100170 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:114a14eec38cf5dde4ddb4fc43ad38512f09b205bb0f1b7d8ec0f671f46a86f8 cni.projectcalico.org/podIP:172.30.42.84/32 cni.projectcalico.org/podIPs:172.30.42.84/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.84"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.84"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610727 0xc009610728}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfl4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfl4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.84,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a7db33946e44d593ed3767d636f620a230637472d719d6fc9e420d9d21c30f80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.502: INFO: Pod "webserver-deployment-845c8977d9-5dwgs" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5dwgs webserver-deployment-845c8977d9- deployment-5389  0dbf962d-489e-46ce-9413-ee1db3c5570f 100145 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6ad056857a3eca9bb11697e01961e847f571f8747434082a83abf2688814bd36 cni.projectcalico.org/podIP:172.30.244.116/32 cni.projectcalico.org/podIPs:172.30.244.116/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.116"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.116"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096109a7 0xc0096109a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hfnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hfnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.116,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f80d7a11e23887180608695b0523dbf2be90c78d4f5d2fec74a29e47a6acf536,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.503: INFO: Pod "webserver-deployment-845c8977d9-5sd5g" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5sd5g webserver-deployment-845c8977d9- deployment-5389  216314cf-f184-4326-89e0-362d77a8bac8 100207 0 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4179ade213011339396d17e2f6a4ab820f43c183f7475bac2d9449c2462679ef cni.projectcalico.org/podIP:172.30.38.220/32 cni.projectcalico.org/podIPs:172.30.38.220/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.220"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.220"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610c37 0xc009610c38}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngpht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngpht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.220,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fa185a40c71c5024f5ff5fd43f8cfe3817d5cc2c5761da48053d0309aef20e0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.503: INFO: Pod "webserver-deployment-845c8977d9-cx6q8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cx6q8 webserver-deployment-845c8977d9- deployment-5389  299bdb8c-dd9c-4e34-8cac-2e41430ea33e 100460 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610ea7 0xc009610ea8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzb2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzb2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-dmdlw" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-dmdlw webserver-deployment-845c8977d9- deployment-5389  643748e1-fc71-436c-b1b2-01362f398d8c 100420 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096110b7 0xc0096110b8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhg6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhg6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-hk4z6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hk4z6 webserver-deployment-845c8977d9- deployment-5389  1990f425-ee8b-4cb8-8dfb-bc6c454af6e0 100200 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f254325622987544b7dd0587faa5c46aba7838c0fa2dc18678767b2f2a6b83b7 cni.projectcalico.org/podIP:172.30.38.224/32 cni.projectcalico.org/podIPs:172.30.38.224/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096112e7 0xc0096112e8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2s5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2s5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.224,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fd98e7257b9e9e8206f550f8e61443f5ab78e6f02362eef7b779c4b906ef3273,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-jvjxn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jvjxn webserver-deployment-845c8977d9- deployment-5389  7e3c7ea8-e74a-413d-878f-43b8c66c641f 100497 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611557 0xc009611558}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5zql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5zql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-mbj9c" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mbj9c webserver-deployment-845c8977d9- deployment-5389  0e2e52de-2f4a-408e-996e-973cffda1593 100516 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e672a7d7cf7842242ff25df7bff2d32c5e649481e1ae6622d0341e7ac7527b0c cni.projectcalico.org/podIP:172.30.244.121/32 cni.projectcalico.org/podIPs:172.30.244.121/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.121"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.121"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611767 0xc009611768}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gbp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gbp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-nvrcl" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nvrcl webserver-deployment-845c8977d9- deployment-5389  c2be0fb1-9610-494f-b54f-e358dc009382 100195 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:af65dfe79f24ff35bb0d0ff331c24381c2c9763794cbb3514d51720295fd2a6f cni.projectcalico.org/podIP:172.30.38.218/32 cni.projectcalico.org/podIPs:172.30.38.218/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.218"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.218"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096119d7 0xc0096119d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lp4s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lp4s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.218,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b3bc65a449b4e56133fcf936c054c95df33fb9a5cb964589a6292b0b88ea1abc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-p6ksr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-p6ksr webserver-deployment-845c8977d9- deployment-5389  84dffb31-ea59-472d-8308-872f5a378511 100490 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611c47 0xc009611c48}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-q2bbj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-q2bbj webserver-deployment-845c8977d9- deployment-5389  345e2ee4-c19d-421e-9bd6-fa945ea3b73c 100488 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611e57 0xc009611e58}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kl68m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kl68m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-sfxqk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfxqk webserver-deployment-845c8977d9- deployment-5389  52a05eca-9fb0-4d30-865b-5d2eb22c59f3 100091 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:17237de00395ff35ca4702ff8b992f4a50a38b71412d2d39325daddb2482f5ce cni.projectcalico.org/podIP:172.30.244.105/32 cni.projectcalico.org/podIPs:172.30.244.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.244.105"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003600c7 0xc0003600c8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfzh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfzh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.105,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3d5061e61122565a7915cc0d584b56f66e8481fe8dd6e0d3c23754ebc1f60467,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-t8jlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t8jlz webserver-deployment-845c8977d9- deployment-5389  fe6d6aa2-fa73-47eb-ac1f-201e96a156ac 100405 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000360567 0xc000360568}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbqdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbqdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.542: INFO: Pod "webserver-deployment-845c8977d9-vfw6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vfw6k webserver-deployment-845c8977d9- deployment-5389  9ea522a0-4689-4e54-8a75-754263dc878a 100478 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003607f7 0xc0003607f8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55jlv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55jlv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.544: INFO: Pod "webserver-deployment-845c8977d9-wbsg5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wbsg5 webserver-deployment-845c8977d9- deployment-5389  924ccb5b-4813-4fcd-aa6a-615f2a1b1dcc 100166 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:803aff29b4ce23f1d76ce749287ae70520813d50280582e2324fb4879ee56d1f cni.projectcalico.org/podIP:172.30.42.83/32 cni.projectcalico.org/podIPs:172.30.42.83/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.83"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.83"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003613d7 0xc0003613d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrqr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrqr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.83,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c6a5a8038ec398e6a737b226a2dc47fa0c16c645c196165a4754e2b383ac4f00,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.544: INFO: Pod "webserver-deployment-845c8977d9-xdlg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xdlg2 webserver-deployment-845c8977d9- deployment-5389  48364d59-6458-49ed-9e64-b035d3f10202 100473 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000361687 0xc000361688}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clzjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clzjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.546: INFO: Pod "webserver-deployment-845c8977d9-xwtlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwtlz webserver-deployment-845c8977d9- deployment-5389  ca4f00dd-1e1f-4701-8ddc-0324ac9073f2 100492 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003618a7 0xc0003618a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mb7zf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mb7zf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 19:53:43.547: INFO: Pod "webserver-deployment-845c8977d9-zzrzx" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zzrzx webserver-deployment-845c8977d9- deployment-5389  7d62ad0c-6ecb-4b7e-9f6d-1ac1f374afc8 100418 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000361cb7 0xc000361cb8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8s6tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8s6tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 19:53:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5389" for this suite. 05/01/23 19:53:43.588
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":178,"skipped":3179,"failed":0}
------------------------------
• [SLOW TEST] [14.220 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:53:29.531
    May  1 19:53:29.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 19:53:29.536
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:29.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:29.703
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May  1 19:53:29.724: INFO: Creating deployment "webserver-deployment"
    May  1 19:53:29.745: INFO: Waiting for observed generation 1
    May  1 19:53:31.792: INFO: Waiting for all required pods to come up
    May  1 19:53:31.834: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/01/23 19:53:31.834
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2vf54" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wbsg5" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-nvrcl" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-f98k7" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-56tg4" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sfxqk" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hk4z6" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5dwgs" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.835: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v25vc" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.836: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5sd5g" in namespace "deployment-5389" to be "running"
    May  1 19:53:31.856: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Pending", Reason="", readiness=false. Elapsed: 20.277553ms
    May  1 19:53:31.857: INFO: Pod "webserver-deployment-845c8977d9-sfxqk": Phase="Pending", Reason="", readiness=false. Elapsed: 22.27837ms
    May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.665791ms
    May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Pending", Reason="", readiness=false. Elapsed: 23.255965ms
    May  1 19:53:31.858: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.887142ms
    May  1 19:53:31.859: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 23.826241ms
    May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.117469ms
    May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-2vf54": Phase="Pending", Reason="", readiness=false. Elapsed: 27.443664ms
    May  1 19:53:31.862: INFO: Pod "webserver-deployment-845c8977d9-5dwgs": Phase="Pending", Reason="", readiness=false. Elapsed: 27.01309ms
    May  1 19:53:31.863: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Pending", Reason="", readiness=false. Elapsed: 27.783836ms
    May  1 19:53:33.877: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041452174s
    May  1 19:53:33.889: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054036689s
    May  1 19:53:33.902: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0669111s
    May  1 19:53:33.904: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069590615s
    May  1 19:53:33.907: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07199998s
    May  1 19:53:33.908: INFO: Pod "webserver-deployment-845c8977d9-sfxqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.072750768s
    May  1 19:53:33.908: INFO: Pod "webserver-deployment-845c8977d9-sfxqk" satisfied condition "running"
    May  1 19:53:33.909: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073775327s
    May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-5dwgs": Phase="Running", Reason="", readiness=true. Elapsed: 2.077299401s
    May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-5dwgs" satisfied condition "running"
    May  1 19:53:33.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078549693s
    May  1 19:53:33.914: INFO: Pod "webserver-deployment-845c8977d9-2vf54": Phase="Running", Reason="", readiness=true. Elapsed: 2.078844004s
    May  1 19:53:33.914: INFO: Pod "webserver-deployment-845c8977d9-2vf54" satisfied condition "running"
    May  1 19:53:35.886: INFO: Pod "webserver-deployment-845c8977d9-hk4z6": Phase="Running", Reason="", readiness=true. Elapsed: 4.050564262s
    May  1 19:53:35.886: INFO: Pod "webserver-deployment-845c8977d9-hk4z6" satisfied condition "running"
    May  1 19:53:35.887: INFO: Pod "webserver-deployment-845c8977d9-wbsg5": Phase="Running", Reason="", readiness=true. Elapsed: 4.052714694s
    May  1 19:53:35.888: INFO: Pod "webserver-deployment-845c8977d9-wbsg5" satisfied condition "running"
    May  1 19:53:35.902: INFO: Pod "webserver-deployment-845c8977d9-nvrcl": Phase="Running", Reason="", readiness=true. Elapsed: 4.067518788s
    May  1 19:53:35.902: INFO: Pod "webserver-deployment-845c8977d9-nvrcl" satisfied condition "running"
    May  1 19:53:35.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7": Phase="Running", Reason="", readiness=true. Elapsed: 4.077806348s
    May  1 19:53:35.913: INFO: Pod "webserver-deployment-845c8977d9-f98k7" satisfied condition "running"
    May  1 19:53:35.915: INFO: Pod "webserver-deployment-845c8977d9-5sd5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.079234511s
    May  1 19:53:35.915: INFO: Pod "webserver-deployment-845c8977d9-5sd5g" satisfied condition "running"
    May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080421973s
    May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-56tg4": Phase="Running", Reason="", readiness=true. Elapsed: 4.081144367s
    May  1 19:53:35.916: INFO: Pod "webserver-deployment-845c8977d9-56tg4" satisfied condition "running"
    May  1 19:53:37.874: INFO: Pod "webserver-deployment-845c8977d9-v25vc": Phase="Running", Reason="", readiness=true. Elapsed: 6.038872884s
    May  1 19:53:37.874: INFO: Pod "webserver-deployment-845c8977d9-v25vc" satisfied condition "running"
    May  1 19:53:37.874: INFO: Waiting for deployment "webserver-deployment" to complete
    May  1 19:53:37.911: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May  1 19:53:38.030: INFO: Updating deployment webserver-deployment
    May  1 19:53:38.030: INFO: Waiting for observed generation 2
    May  1 19:53:40.284: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May  1 19:53:40.320: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May  1 19:53:40.481: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  1 19:53:40.636: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May  1 19:53:40.636: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May  1 19:53:40.665: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May  1 19:53:40.810: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May  1 19:53:40.810: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May  1 19:53:40.863: INFO: Updating deployment webserver-deployment
    May  1 19:53:40.863: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May  1 19:53:40.929: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May  1 19:53:43.047: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 19:53:43.179: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5389  8f4dc855-8bc4-497f-b61d-6fb0afba272d 100466 3 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a123a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-01 19:53:40 +0000 UTC,LastTransitionTime:2023-05-01 19:53:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-05-01 19:53:41 +0000 UTC,LastTransitionTime:2023-05-01 19:53:29 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May  1 19:53:43.208: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-5389  665db086-be65-4032-bf75-323b9279f39e 100464 3 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8f4dc855-8bc4-497f-b61d-6fb0afba272d 0xc0025cdc17 0xc0025cdc18}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4dc855-8bc4-497f-b61d-6fb0afba272d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cdcb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:53:43.208: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May  1 19:53:43.208: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-5389  07e77d02-18ce-4b76-b11c-152022cf958a 100449 3 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8f4dc855-8bc4-497f-b61d-6fb0afba272d 0xc0025cdd17 0xc0025cdd18}] [] [{kube-controller-manager Update apps/v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f4dc855-8bc4-497f-b61d-6fb0afba272d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025cddb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May  1 19:53:43.476: INFO: Pod "webserver-deployment-69b7448995-2dr76" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2dr76 webserver-deployment-69b7448995- deployment-5389  293ca6f2-7356-4ec3-ab7b-6c15135445d0 100450 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2267 0xc003af2268}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xhzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xhzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.478: INFO: Pod "webserver-deployment-69b7448995-5lrwt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5lrwt webserver-deployment-69b7448995- deployment-5389  b3dbf1c6-7abc-4207-9ea8-5cdf94b11633 100351 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:b76389ee9e6cdf5abecbfe09a4443a3ea01c1e613f14dde39c8c36c734c09f45 cni.projectcalico.org/podIP:172.30.38.222/32 cni.projectcalico.org/podIPs:172.30.38.222/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.222"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.222"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af24b7 0xc003af24b8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6shjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6shjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.479: INFO: Pod "webserver-deployment-69b7448995-6lhl7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6lhl7 webserver-deployment-69b7448995- deployment-5389  ae7a3ed6-24d3-4e8c-aab0-97321bb968ac 100494 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2727 0xc003af2728}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rgqzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rgqzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.480: INFO: Pod "webserver-deployment-69b7448995-75g8c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-75g8c webserver-deployment-69b7448995- deployment-5389  8c5d73ba-b7b7-4b4c-8e78-377ceb84612a 100469 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2957 0xc003af2958}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8z6gl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8z6gl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.481: INFO: Pod "webserver-deployment-69b7448995-8zqjk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-8zqjk webserver-deployment-69b7448995- deployment-5389  285a3aa4-17f8-49d7-baf7-e66d6b297ea2 100493 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2ba7 0xc003af2ba8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tq557,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tq557,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.482: INFO: Pod "webserver-deployment-69b7448995-9mrtq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9mrtq webserver-deployment-69b7448995- deployment-5389  9cb9c9cd-6755-4d7e-b4ca-7e20dd21e87f 100363 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:ad6b7208c0333c3a8726ebf7cafc9f09960aa57cea11122bbb843f1f46b48803 cni.projectcalico.org/podIP:172.30.38.221/32 cni.projectcalico.org/podIPs:172.30.38.221/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.221"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.221"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af2e07 0xc003af2e08}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9582,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9582,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.488: INFO: Pod "webserver-deployment-69b7448995-brmh5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-brmh5 webserver-deployment-69b7448995- deployment-5389  725e2b34-813d-4574-a53f-f163742963e2 100519 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:325a2c5535dee75bc9d8c076e1e489f80e0f9ac0158aeee0c3610eb056ebda88 cni.projectcalico.org/podIP:172.30.42.85/32 cni.projectcalico.org/podIPs:172.30.42.85/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.85"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.85"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af30a7 0xc003af30a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdp6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdp6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.85,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.489: INFO: Pod "webserver-deployment-69b7448995-cfns7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cfns7 webserver-deployment-69b7448995- deployment-5389  d3c52378-7cf9-4086-9c27-611a8563fb00 100446 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3367 0xc003af3368}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lklmm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lklmm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.489: INFO: Pod "webserver-deployment-69b7448995-hvwvz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hvwvz webserver-deployment-69b7448995- deployment-5389  e76c4da1-5631-48f7-8f69-cb8656e079e7 100481 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3597 0xc003af3598}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz5fc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz5fc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.493: INFO: Pod "webserver-deployment-69b7448995-pvz7r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pvz7r webserver-deployment-69b7448995- deployment-5389  09b3d77d-5653-4c85-a66b-9ac94a012f22 100525 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:3467e95616e447d1d5c052677337f0e048930f1ab4aad564285d20b07e9d820b cni.projectcalico.org/podIP:172.30.244.122/32 cni.projectcalico.org/podIPs:172.30.244.122/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af37c7 0xc003af37c8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j4fzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j4fzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.495: INFO: Pod "webserver-deployment-69b7448995-t9xxm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-t9xxm webserver-deployment-69b7448995- deployment-5389  7eecb895-75c1-4cc2-a8a5-a1cd4965a1e6 100514 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8a969baa079128bf53bd30c2c43d4e9e7d1e2b3e37b30f066c03889b4357fb3f cni.projectcalico.org/podIP:172.30.244.81/32 cni.projectcalico.org/podIPs:172.30.244.81/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.81"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.81"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3a57 0xc003af3a58}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dvm8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dvm8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.81,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.497: INFO: Pod "webserver-deployment-69b7448995-tjljm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tjljm webserver-deployment-69b7448995- deployment-5389  d08b222e-86e6-4d51-a893-76889b28813d 100509 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6b54bdbc07529fa79264466dfc3e3b7cd92819e393a59c2f71163bbfbd36820d cni.projectcalico.org/podIP:172.30.244.115/32 cni.projectcalico.org/podIPs:172.30.244.115/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.115"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.115"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3d17 0xc003af3d18}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d85vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d85vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.498: INFO: Pod "webserver-deployment-69b7448995-wf9rz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wf9rz webserver-deployment-69b7448995- deployment-5389  5e624c03-d2be-40cf-bc9d-9e3e93db67c0 100373 0 2023-05-01 19:53:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:87b2345a0e7c77b3e7df6d2f8780edc15c1c303aeff7b161a0f6d0e1b42b8352 cni.projectcalico.org/podIP:172.30.42.86/32 cni.projectcalico.org/podIPs:172.30.42.86/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.86"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.86"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 665db086-be65-4032-bf75-323b9279f39e 0xc003af3fc7 0xc003af3fc8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665db086-be65-4032-bf75-323b9279f39e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-md9kp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-md9kp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.499: INFO: Pod "webserver-deployment-845c8977d9-2vf54" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2vf54 webserver-deployment-845c8977d9- deployment-5389  042dd12b-d80b-4264-91b4-4de7abb8a14a 100139 0 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9eb573b2eb1eafe1d08ec1446e9c7e399b0ce2124ed12b692d2106bc3892b3ab cni.projectcalico.org/podIP:172.30.244.120/32 cni.projectcalico.org/podIPs:172.30.244.120/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.120"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.120"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610277 0xc009610278}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtwdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtwdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.120,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://17f5d9b033c4a1290569ec99822d7a7cdae0d9d950de60d5fa8d5a2cdb587b27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.500: INFO: Pod "webserver-deployment-845c8977d9-4xgvn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4xgvn webserver-deployment-845c8977d9- deployment-5389  06075cf6-5383-4ca6-a674-2d3ab0a9eb4c 100461 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096104f7 0xc0096104f8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2f8ls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2f8ls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.501: INFO: Pod "webserver-deployment-845c8977d9-56tg4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-56tg4 webserver-deployment-845c8977d9- deployment-5389  4285d03f-19c3-4720-9223-9cbda031bbf6 100170 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:114a14eec38cf5dde4ddb4fc43ad38512f09b205bb0f1b7d8ec0f671f46a86f8 cni.projectcalico.org/podIP:172.30.42.84/32 cni.projectcalico.org/podIPs:172.30.42.84/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.84"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.84"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610727 0xc009610728}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfl4f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfl4f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.84,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://a7db33946e44d593ed3767d636f620a230637472d719d6fc9e420d9d21c30f80,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.502: INFO: Pod "webserver-deployment-845c8977d9-5dwgs" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5dwgs webserver-deployment-845c8977d9- deployment-5389  0dbf962d-489e-46ce-9413-ee1db3c5570f 100145 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:6ad056857a3eca9bb11697e01961e847f571f8747434082a83abf2688814bd36 cni.projectcalico.org/podIP:172.30.244.116/32 cni.projectcalico.org/podIPs:172.30.244.116/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.116"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.116"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096109a7 0xc0096109a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hfnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hfnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.116,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://f80d7a11e23887180608695b0523dbf2be90c78d4f5d2fec74a29e47a6acf536,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.503: INFO: Pod "webserver-deployment-845c8977d9-5sd5g" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5sd5g webserver-deployment-845c8977d9- deployment-5389  216314cf-f184-4326-89e0-362d77a8bac8 100207 0 2023-05-01 19:53:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4179ade213011339396d17e2f6a4ab820f43c183f7475bac2d9449c2462679ef cni.projectcalico.org/podIP:172.30.38.220/32 cni.projectcalico.org/podIPs:172.30.38.220/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.220"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.220"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610c37 0xc009610c38}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ngpht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ngpht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.220,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fa185a40c71c5024f5ff5fd43f8cfe3817d5cc2c5761da48053d0309aef20e0e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.503: INFO: Pod "webserver-deployment-845c8977d9-cx6q8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cx6q8 webserver-deployment-845c8977d9- deployment-5389  299bdb8c-dd9c-4e34-8cac-2e41430ea33e 100460 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009610ea7 0xc009610ea8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzb2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzb2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-dmdlw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-dmdlw webserver-deployment-845c8977d9- deployment-5389  643748e1-fc71-436c-b1b2-01362f398d8c 100420 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096110b7 0xc0096110b8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zhg6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zhg6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-hk4z6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hk4z6 webserver-deployment-845c8977d9- deployment-5389  1990f425-ee8b-4cb8-8dfb-bc6c454af6e0 100200 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f254325622987544b7dd0587faa5c46aba7838c0fa2dc18678767b2f2a6b83b7 cni.projectcalico.org/podIP:172.30.38.224/32 cni.projectcalico.org/podIPs:172.30.38.224/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.224"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.224"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096112e7 0xc0096112e8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q2s5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q2s5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.224,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fd98e7257b9e9e8206f550f8e61443f5ab78e6f02362eef7b779c4b906ef3273,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.510: INFO: Pod "webserver-deployment-845c8977d9-jvjxn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jvjxn webserver-deployment-845c8977d9- deployment-5389  7e3c7ea8-e74a-413d-878f-43b8c66c641f 100497 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611557 0xc009611558}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v5zql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v5zql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-mbj9c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mbj9c webserver-deployment-845c8977d9- deployment-5389  0e2e52de-2f4a-408e-996e-973cffda1593 100516 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e672a7d7cf7842242ff25df7bff2d32c5e649481e1ae6622d0341e7ac7527b0c cni.projectcalico.org/podIP:172.30.244.121/32 cni.projectcalico.org/podIPs:172.30.244.121/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.121"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.121"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611767 0xc009611768}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gbp5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gbp5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-nvrcl" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nvrcl webserver-deployment-845c8977d9- deployment-5389  c2be0fb1-9610-494f-b54f-e358dc009382 100195 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:af65dfe79f24ff35bb0d0ff331c24381c2c9763794cbb3514d51720295fd2a6f cni.projectcalico.org/podIP:172.30.38.218/32 cni.projectcalico.org/podIPs:172.30.38.218/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.218"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.218"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0096119d7 0xc0096119d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lp4s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lp4s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.218,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://b3bc65a449b4e56133fcf936c054c95df33fb9a5cb964589a6292b0b88ea1abc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.511: INFO: Pod "webserver-deployment-845c8977d9-p6ksr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-p6ksr webserver-deployment-845c8977d9- deployment-5389  84dffb31-ea59-472d-8308-872f5a378511 100490 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611c47 0xc009611c48}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds6nx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds6nx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-q2bbj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-q2bbj webserver-deployment-845c8977d9- deployment-5389  345e2ee4-c19d-421e-9bd6-fa945ea3b73c 100488 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc009611e57 0xc009611e58}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kl68m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kl68m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-sfxqk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfxqk webserver-deployment-845c8977d9- deployment-5389  52a05eca-9fb0-4d30-865b-5d2eb22c59f3 100091 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:17237de00395ff35ca4702ff8b992f4a50a38b71412d2d39325daddb2482f5ce cni.projectcalico.org/podIP:172.30.244.105/32 cni.projectcalico.org/podIPs:172.30.244.105/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.105"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.244.105"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003600c7 0xc0003600c8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfzh5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfzh5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.71,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.71,PodIP:172.30.244.105,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://3d5061e61122565a7915cc0d584b56f66e8481fe8dd6e0d3c23754ebc1f60467,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.244.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.512: INFO: Pod "webserver-deployment-845c8977d9-t8jlz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t8jlz webserver-deployment-845c8977d9- deployment-5389  fe6d6aa2-fa73-47eb-ac1f-201e96a156ac 100405 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000360567 0xc000360568}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbqdp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbqdp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.542: INFO: Pod "webserver-deployment-845c8977d9-vfw6k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vfw6k webserver-deployment-845c8977d9- deployment-5389  9ea522a0-4689-4e54-8a75-754263dc878a 100478 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003607f7 0xc0003607f8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55jlv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55jlv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.544: INFO: Pod "webserver-deployment-845c8977d9-wbsg5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wbsg5 webserver-deployment-845c8977d9- deployment-5389  924ccb5b-4813-4fcd-aa6a-615f2a1b1dcc 100166 0 2023-05-01 19:53:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:803aff29b4ce23f1d76ce749287ae70520813d50280582e2324fb4879ee56d1f cni.projectcalico.org/podIP:172.30.42.83/32 cni.projectcalico.org/podIPs:172.30.42.83/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.83"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.83"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003613d7 0xc0003613d8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 19:53:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 19:53:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 19:53:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrqr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrqr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.83,StartTime:2023-05-01 19:53:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 19:53:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://c6a5a8038ec398e6a737b226a2dc47fa0c16c645c196165a4754e2b383ac4f00,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.83,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.544: INFO: Pod "webserver-deployment-845c8977d9-xdlg2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xdlg2 webserver-deployment-845c8977d9- deployment-5389  48364d59-6458-49ed-9e64-b035d3f10202 100473 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000361687 0xc000361688}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-clzjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-clzjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.546: INFO: Pod "webserver-deployment-845c8977d9-xwtlz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xwtlz webserver-deployment-845c8977d9- deployment-5389  ca4f00dd-1e1f-4701-8ddc-0324ac9073f2 100492 0 2023-05-01 19:53:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc0003618a7 0xc0003618a8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mb7zf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mb7zf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 19:53:43.547: INFO: Pod "webserver-deployment-845c8977d9-zzrzx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zzrzx webserver-deployment-845c8977d9- deployment-5389  7d62ad0c-6ecb-4b7e-9f6d-1ac1f374afc8 100418 0 2023-05-01 19:53:40 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 07e77d02-18ce-4b76-b11c-152022cf958a 0xc000361cb7 0xc000361cb8}] [] [{kube-controller-manager Update v1 2023-05-01 19:53:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"07e77d02-18ce-4b76-b11c-152022cf958a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-01 19:53:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8s6tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8s6tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c52,c49,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zsgb6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 19:53:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:,StartTime:2023-05-01 19:53:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 19:53:43.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5389" for this suite. 05/01/23 19:53:43.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:53:43.798
May  1 19:53:43.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-webhook 05/01/23 19:53:43.801
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:43.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:43.992
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/01/23 19:53:44.057
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/01/23 19:53:51.077
STEP: Deploying the custom resource conversion webhook pod 05/01/23 19:53:51.135
STEP: Wait for the deployment to be ready 05/01/23 19:53:51.257
May  1 19:53:51.318: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
May  1 19:53:53.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:53:55.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:53:57.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 19:53:59.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 19:54:01.397
STEP: Verifying the service has paired with the endpoint 05/01/23 19:54:01.444
May  1 19:54:02.445: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May  1 19:54:02.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Creating a v1 custom resource 05/01/23 19:54:05.458
STEP: v2 custom resource should be converted 05/01/23 19:54:05.497
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:54:06.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8180" for this suite. 05/01/23 19:54:06.158
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":179,"skipped":3199,"failed":0}
------------------------------
• [SLOW TEST] [22.600 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:53:43.798
    May  1 19:53:43.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-webhook 05/01/23 19:53:43.801
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:53:43.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:53:43.992
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/01/23 19:53:44.057
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/01/23 19:53:51.077
    STEP: Deploying the custom resource conversion webhook pod 05/01/23 19:53:51.135
    STEP: Wait for the deployment to be ready 05/01/23 19:53:51.257
    May  1 19:53:51.318: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    May  1 19:53:53.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:53:55.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:53:57.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 19:53:59.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 19, 53, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 19:54:01.397
    STEP: Verifying the service has paired with the endpoint 05/01/23 19:54:01.444
    May  1 19:54:02.445: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May  1 19:54:02.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Creating a v1 custom resource 05/01/23 19:54:05.458
    STEP: v2 custom resource should be converted 05/01/23 19:54:05.497
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:54:06.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8180" for this suite. 05/01/23 19:54:06.158
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:54:06.398
May  1 19:54:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 19:54:06.401
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:06.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:06.664
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/01/23 19:54:06.68
STEP: Verify that the required pods have come up 05/01/23 19:54:06.716
May  1 19:54:06.735: INFO: Pod name sample-pod: Found 0 pods out of 3
May  1 19:54:11.753: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/01/23 19:54:11.753
May  1 19:54:11.796: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/01/23 19:54:11.796
STEP: DeleteCollection of the ReplicaSets 05/01/23 19:54:11.833
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/01/23 19:54:11.877
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 19:54:11.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6213" for this suite. 05/01/23 19:54:11.941
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":180,"skipped":3209,"failed":0}
------------------------------
• [SLOW TEST] [5.660 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:54:06.398
    May  1 19:54:06.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 19:54:06.401
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:06.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:06.664
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/01/23 19:54:06.68
    STEP: Verify that the required pods have come up 05/01/23 19:54:06.716
    May  1 19:54:06.735: INFO: Pod name sample-pod: Found 0 pods out of 3
    May  1 19:54:11.753: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/01/23 19:54:11.753
    May  1 19:54:11.796: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/01/23 19:54:11.796
    STEP: DeleteCollection of the ReplicaSets 05/01/23 19:54:11.833
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/01/23 19:54:11.877
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 19:54:11.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6213" for this suite. 05/01/23 19:54:11.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:54:12.06
May  1 19:54:12.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 19:54:12.073
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:12.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:12.213
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 05/01/23 19:54:12.252
STEP: Creating a ResourceQuota 05/01/23 19:54:17.274
STEP: Ensuring resource quota status is calculated 05/01/23 19:54:17.295
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 19:54:19.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8146" for this suite. 05/01/23 19:54:19.337
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":181,"skipped":3223,"failed":0}
------------------------------
• [SLOW TEST] [7.313 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:54:12.06
    May  1 19:54:12.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 19:54:12.073
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:12.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:12.213
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 05/01/23 19:54:12.252
    STEP: Creating a ResourceQuota 05/01/23 19:54:17.274
    STEP: Ensuring resource quota status is calculated 05/01/23 19:54:17.295
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 19:54:19.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8146" for this suite. 05/01/23 19:54:19.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:54:19.388
May  1 19:54:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:54:19.39
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:19.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:19.481
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:54:19.583
May  1 19:54:19.726: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7612" to be "running and ready"
May  1 19:54:19.751: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 24.297891ms
May  1 19:54:19.751: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:21.783: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056538486s
May  1 19:54:21.783: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:23.771: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.044727065s
May  1 19:54:23.771: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  1 19:54:23.771: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 05/01/23 19:54:23.802
May  1 19:54:23.848: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7612" to be "running and ready"
May  1 19:54:23.874: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 26.088257ms
May  1 19:54:23.874: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:25.893: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044330393s
May  1 19:54:25.893: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:27.891: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.043058214s
May  1 19:54:27.891: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May  1 19:54:27.891: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/01/23 19:54:27.93
May  1 19:54:27.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  1 19:54:27.986: INFO: Pod pod-with-prestop-exec-hook still exists
May  1 19:54:29.986: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  1 19:54:30.017: INFO: Pod pod-with-prestop-exec-hook still exists
May  1 19:54:31.987: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  1 19:54:32.011: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/01/23 19:54:32.011
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  1 19:54:32.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7612" for this suite. 05/01/23 19:54:32.118
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":182,"skipped":3306,"failed":0}
------------------------------
• [SLOW TEST] [12.784 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:54:19.388
    May  1 19:54:19.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 19:54:19.39
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:19.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:19.481
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/01/23 19:54:19.583
    May  1 19:54:19.726: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7612" to be "running and ready"
    May  1 19:54:19.751: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 24.297891ms
    May  1 19:54:19.751: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:21.783: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056538486s
    May  1 19:54:21.783: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:23.771: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.044727065s
    May  1 19:54:23.771: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  1 19:54:23.771: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 05/01/23 19:54:23.802
    May  1 19:54:23.848: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7612" to be "running and ready"
    May  1 19:54:23.874: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 26.088257ms
    May  1 19:54:23.874: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:25.893: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044330393s
    May  1 19:54:25.893: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:27.891: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.043058214s
    May  1 19:54:27.891: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May  1 19:54:27.891: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/01/23 19:54:27.93
    May  1 19:54:27.970: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  1 19:54:27.986: INFO: Pod pod-with-prestop-exec-hook still exists
    May  1 19:54:29.986: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  1 19:54:30.017: INFO: Pod pod-with-prestop-exec-hook still exists
    May  1 19:54:31.987: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May  1 19:54:32.011: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/01/23 19:54:32.011
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  1 19:54:32.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7612" for this suite. 05/01/23 19:54:32.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:54:32.174
May  1 19:54:32.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 19:54:32.177
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:32.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:32.325
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/01/23 19:54:32.358
May  1 19:54:32.437: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4204" to be "running and ready"
May  1 19:54:32.449: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.65391ms
May  1 19:54:32.449: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:34.469: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032137008s
May  1 19:54:34.470: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:36.462: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.024638101s
May  1 19:54:36.462: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May  1 19:54:36.462: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/01/23 19:54:36.481
STEP: Then the orphan pod is adopted 05/01/23 19:54:36.5
STEP: When the matched label of one of its pods change 05/01/23 19:54:37.53
May  1 19:54:37.543: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/01/23 19:54:37.673
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 19:54:38.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4204" for this suite. 05/01/23 19:54:38.736
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":183,"skipped":3315,"failed":0}
------------------------------
• [SLOW TEST] [6.599 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:54:32.174
    May  1 19:54:32.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 19:54:32.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:32.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:32.325
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/01/23 19:54:32.358
    May  1 19:54:32.437: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4204" to be "running and ready"
    May  1 19:54:32.449: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.65391ms
    May  1 19:54:32.449: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:34.469: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032137008s
    May  1 19:54:34.470: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:36.462: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.024638101s
    May  1 19:54:36.462: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May  1 19:54:36.462: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/01/23 19:54:36.481
    STEP: Then the orphan pod is adopted 05/01/23 19:54:36.5
    STEP: When the matched label of one of its pods change 05/01/23 19:54:37.53
    May  1 19:54:37.543: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/01/23 19:54:37.673
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 19:54:38.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4204" for this suite. 05/01/23 19:54:38.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:54:38.778
May  1 19:54:38.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pod-network-test 05/01/23 19:54:38.781
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:38.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:38.893
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-540 05/01/23 19:54:38.917
STEP: creating a selector 05/01/23 19:54:38.917
STEP: Creating the service pods in kubernetes 05/01/23 19:54:38.918
May  1 19:54:38.918: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  1 19:54:39.218: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-540" to be "running and ready"
May  1 19:54:39.286: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 67.675383ms
May  1 19:54:39.286: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:41.324: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105323541s
May  1 19:54:41.324: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:54:43.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.100033088s
May  1 19:54:43.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:45.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.083709016s
May  1 19:54:45.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:47.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.08278415s
May  1 19:54:47.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:49.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.082400635s
May  1 19:54:49.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:51.306: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.088112008s
May  1 19:54:51.306: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:53.314: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.095916792s
May  1 19:54:53.314: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:55.310: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.091434707s
May  1 19:54:55.310: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:57.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.080981031s
May  1 19:54:57.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:54:59.304: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.085807765s
May  1 19:54:59.304: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 19:55:01.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.106977251s
May  1 19:55:01.325: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  1 19:55:01.325: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  1 19:55:01.344: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-540" to be "running and ready"
May  1 19:55:01.361: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.029933ms
May  1 19:55:01.361: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  1 19:55:01.361: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  1 19:55:01.382: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-540" to be "running and ready"
May  1 19:55:01.417: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 34.649013ms
May  1 19:55:01.417: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  1 19:55:01.417: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/01/23 19:55:01.44
May  1 19:55:01.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-540" to be "running"
May  1 19:55:01.504: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.425346ms
May  1 19:55:03.523: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031503025s
May  1 19:55:05.520: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028452593s
May  1 19:55:05.520: INFO: Pod "test-container-pod" satisfied condition "running"
May  1 19:55:05.536: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  1 19:55:05.536: INFO: Breadth first check of 172.30.42.107 on host 10.45.145.124...
May  1 19:55:05.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.42.107&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:55:05.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:55:05.554: INFO: ExecWithOptions: Clientset creation
May  1 19:55:05.554: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.42.107%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 19:55:05.919: INFO: Waiting for responses: map[]
May  1 19:55:05.919: INFO: reached 172.30.42.107 after 0/1 tries
May  1 19:55:05.919: INFO: Breadth first check of 172.30.38.232 on host 10.45.145.126...
May  1 19:55:05.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.38.232&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:55:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:55:05.973: INFO: ExecWithOptions: Clientset creation
May  1 19:55:05.973: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.38.232%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 19:55:06.358: INFO: Waiting for responses: map[]
May  1 19:55:06.359: INFO: reached 172.30.38.232 after 0/1 tries
May  1 19:55:06.359: INFO: Breadth first check of 172.30.244.99 on host 10.45.145.71...
May  1 19:55:06.371: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.244.99&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 19:55:06.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 19:55:06.373: INFO: ExecWithOptions: Clientset creation
May  1 19:55:06.373: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.244.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 19:55:06.697: INFO: Waiting for responses: map[]
May  1 19:55:06.697: INFO: reached 172.30.244.99 after 0/1 tries
May  1 19:55:06.697: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  1 19:55:06.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-540" for this suite. 05/01/23 19:55:06.721
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":184,"skipped":3320,"failed":0}
------------------------------
• [SLOW TEST] [27.993 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:54:38.778
    May  1 19:54:38.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pod-network-test 05/01/23 19:54:38.781
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:54:38.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:54:38.893
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-540 05/01/23 19:54:38.917
    STEP: creating a selector 05/01/23 19:54:38.917
    STEP: Creating the service pods in kubernetes 05/01/23 19:54:38.918
    May  1 19:54:38.918: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  1 19:54:39.218: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-540" to be "running and ready"
    May  1 19:54:39.286: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 67.675383ms
    May  1 19:54:39.286: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:41.324: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105323541s
    May  1 19:54:41.324: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:54:43.318: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.100033088s
    May  1 19:54:43.318: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:45.302: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.083709016s
    May  1 19:54:45.302: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:47.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.08278415s
    May  1 19:54:47.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:49.301: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.082400635s
    May  1 19:54:49.301: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:51.306: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.088112008s
    May  1 19:54:51.306: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:53.314: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.095916792s
    May  1 19:54:53.314: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:55.310: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.091434707s
    May  1 19:54:55.310: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:57.299: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.080981031s
    May  1 19:54:57.299: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:54:59.304: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.085807765s
    May  1 19:54:59.304: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 19:55:01.325: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.106977251s
    May  1 19:55:01.325: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  1 19:55:01.325: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  1 19:55:01.344: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-540" to be "running and ready"
    May  1 19:55:01.361: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.029933ms
    May  1 19:55:01.361: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  1 19:55:01.361: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  1 19:55:01.382: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-540" to be "running and ready"
    May  1 19:55:01.417: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 34.649013ms
    May  1 19:55:01.417: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  1 19:55:01.417: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/01/23 19:55:01.44
    May  1 19:55:01.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-540" to be "running"
    May  1 19:55:01.504: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.425346ms
    May  1 19:55:03.523: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031503025s
    May  1 19:55:05.520: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.028452593s
    May  1 19:55:05.520: INFO: Pod "test-container-pod" satisfied condition "running"
    May  1 19:55:05.536: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  1 19:55:05.536: INFO: Breadth first check of 172.30.42.107 on host 10.45.145.124...
    May  1 19:55:05.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.42.107&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:55:05.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:55:05.554: INFO: ExecWithOptions: Clientset creation
    May  1 19:55:05.554: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.42.107%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 19:55:05.919: INFO: Waiting for responses: map[]
    May  1 19:55:05.919: INFO: reached 172.30.42.107 after 0/1 tries
    May  1 19:55:05.919: INFO: Breadth first check of 172.30.38.232 on host 10.45.145.126...
    May  1 19:55:05.971: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.38.232&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:55:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:55:05.973: INFO: ExecWithOptions: Clientset creation
    May  1 19:55:05.973: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.38.232%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 19:55:06.358: INFO: Waiting for responses: map[]
    May  1 19:55:06.359: INFO: reached 172.30.38.232 after 0/1 tries
    May  1 19:55:06.359: INFO: Breadth first check of 172.30.244.99 on host 10.45.145.71...
    May  1 19:55:06.371: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.103:9080/dial?request=hostname&protocol=http&host=172.30.244.99&port=8083&tries=1'] Namespace:pod-network-test-540 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 19:55:06.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 19:55:06.373: INFO: ExecWithOptions: Clientset creation
    May  1 19:55:06.373: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-540/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.103%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.244.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 19:55:06.697: INFO: Waiting for responses: map[]
    May  1 19:55:06.697: INFO: reached 172.30.244.99 after 0/1 tries
    May  1 19:55:06.697: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  1 19:55:06.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-540" for this suite. 05/01/23 19:55:06.721
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:55:06.772
May  1 19:55:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:55:06.775
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:55:06.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:55:06.949
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
May  1 19:55:07.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/01/23 19:55:31.81
May  1 19:55:31.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
May  1 19:55:34.671: INFO: stderr: ""
May  1 19:55:34.671: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  1 19:55:34.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 delete e2e-test-crd-publish-openapi-4398-crds test-foo'
May  1 19:55:34.900: INFO: stderr: ""
May  1 19:55:34.900: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May  1 19:55:34.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
May  1 19:55:37.246: INFO: stderr: ""
May  1 19:55:37.246: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May  1 19:55:37.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 delete e2e-test-crd-publish-openapi-4398-crds test-foo'
May  1 19:55:37.457: INFO: stderr: ""
May  1 19:55:37.457: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/01/23 19:55:37.457
May  1 19:55:37.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
May  1 19:55:38.665: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/01/23 19:55:38.665
May  1 19:55:38.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
May  1 19:55:39.844: INFO: rc: 1
May  1 19:55:39.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
May  1 19:55:44.282: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/01/23 19:55:44.282
May  1 19:55:44.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
May  1 19:55:45.697: INFO: rc: 1
May  1 19:55:45.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
May  1 19:55:46.949: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/01/23 19:55:46.949
May  1 19:55:46.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds'
May  1 19:55:47.974: INFO: stderr: ""
May  1 19:55:47.974: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/01/23 19:55:47.974
May  1 19:55:47.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.metadata'
May  1 19:55:49.047: INFO: stderr: ""
May  1 19:55:49.047: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May  1 19:55:49.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec'
May  1 19:55:49.998: INFO: stderr: ""
May  1 19:55:49.998: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May  1 19:55:49.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec.bars'
May  1 19:55:51.106: INFO: stderr: ""
May  1 19:55:51.106: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/01/23 19:55:51.107
May  1 19:55:51.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec.bars2'
May  1 19:55:52.117: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 19:56:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1212" for this suite. 05/01/23 19:56:14.48
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":185,"skipped":3324,"failed":0}
------------------------------
• [SLOW TEST] [67.733 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:55:06.772
    May  1 19:55:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 19:55:06.775
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:55:06.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:55:06.949
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    May  1 19:55:07.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/01/23 19:55:31.81
    May  1 19:55:31.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
    May  1 19:55:34.671: INFO: stderr: ""
    May  1 19:55:34.671: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  1 19:55:34.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 delete e2e-test-crd-publish-openapi-4398-crds test-foo'
    May  1 19:55:34.900: INFO: stderr: ""
    May  1 19:55:34.900: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May  1 19:55:34.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
    May  1 19:55:37.246: INFO: stderr: ""
    May  1 19:55:37.246: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May  1 19:55:37.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 delete e2e-test-crd-publish-openapi-4398-crds test-foo'
    May  1 19:55:37.457: INFO: stderr: ""
    May  1 19:55:37.457: INFO: stdout: "e2e-test-crd-publish-openapi-4398-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/01/23 19:55:37.457
    May  1 19:55:37.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
    May  1 19:55:38.665: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/01/23 19:55:38.665
    May  1 19:55:38.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
    May  1 19:55:39.844: INFO: rc: 1
    May  1 19:55:39.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
    May  1 19:55:44.282: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/01/23 19:55:44.282
    May  1 19:55:44.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 create -f -'
    May  1 19:55:45.697: INFO: rc: 1
    May  1 19:55:45.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 --namespace=crd-publish-openapi-1212 apply -f -'
    May  1 19:55:46.949: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/01/23 19:55:46.949
    May  1 19:55:46.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds'
    May  1 19:55:47.974: INFO: stderr: ""
    May  1 19:55:47.974: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/01/23 19:55:47.974
    May  1 19:55:47.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.metadata'
    May  1 19:55:49.047: INFO: stderr: ""
    May  1 19:55:49.047: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May  1 19:55:49.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec'
    May  1 19:55:49.998: INFO: stderr: ""
    May  1 19:55:49.998: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May  1 19:55:49.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec.bars'
    May  1 19:55:51.106: INFO: stderr: ""
    May  1 19:55:51.106: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4398-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/01/23 19:55:51.107
    May  1 19:55:51.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=crd-publish-openapi-1212 explain e2e-test-crd-publish-openapi-4398-crds.spec.bars2'
    May  1 19:55:52.117: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 19:56:14.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1212" for this suite. 05/01/23 19:56:14.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:14.508
May  1 19:56:14.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:56:14.51
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:14.626
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-b48628db-fc54-485f-b5ac-d01734adff70 05/01/23 19:56:14.646
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
May  1 19:56:14.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5357" for this suite. 05/01/23 19:56:14.688
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":186,"skipped":3336,"failed":0}
------------------------------
• [0.220 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:14.508
    May  1 19:56:14.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:56:14.51
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:14.626
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-b48628db-fc54-485f-b5ac-d01734adff70 05/01/23 19:56:14.646
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:56:14.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5357" for this suite. 05/01/23 19:56:14.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:14.739
May  1 19:56:14.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename namespaces 05/01/23 19:56:14.741
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:14.843
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 05/01/23 19:56:14.857
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.946
STEP: Creating a service in the namespace 05/01/23 19:56:14.958
STEP: Deleting the namespace 05/01/23 19:56:15.047
STEP: Waiting for the namespace to be removed. 05/01/23 19:56:15.079
STEP: Recreating the namespace 05/01/23 19:56:24.1
STEP: Verifying there is no service in the namespace 05/01/23 19:56:24.174
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  1 19:56:24.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4923" for this suite. 05/01/23 19:56:24.295
STEP: Destroying namespace "nsdeletetest-3011" for this suite. 05/01/23 19:56:24.345
May  1 19:56:24.363: INFO: Namespace nsdeletetest-3011 was already deleted
STEP: Destroying namespace "nsdeletetest-5066" for this suite. 05/01/23 19:56:24.363
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":187,"skipped":3358,"failed":0}
------------------------------
• [SLOW TEST] [9.655 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:14.739
    May  1 19:56:14.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename namespaces 05/01/23 19:56:14.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:14.843
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 05/01/23 19:56:14.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:14.946
    STEP: Creating a service in the namespace 05/01/23 19:56:14.958
    STEP: Deleting the namespace 05/01/23 19:56:15.047
    STEP: Waiting for the namespace to be removed. 05/01/23 19:56:15.079
    STEP: Recreating the namespace 05/01/23 19:56:24.1
    STEP: Verifying there is no service in the namespace 05/01/23 19:56:24.174
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  1 19:56:24.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4923" for this suite. 05/01/23 19:56:24.295
    STEP: Destroying namespace "nsdeletetest-3011" for this suite. 05/01/23 19:56:24.345
    May  1 19:56:24.363: INFO: Namespace nsdeletetest-3011 was already deleted
    STEP: Destroying namespace "nsdeletetest-5066" for this suite. 05/01/23 19:56:24.363
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:24.395
May  1 19:56:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 19:56:24.397
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:24.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:24.483
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-fb954732-e770-4559-8d54-b859b5db7454 05/01/23 19:56:24.521
STEP: Creating a pod to test consume configMaps 05/01/23 19:56:24.542
May  1 19:56:24.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3" in namespace "configmap-6798" to be "Succeeded or Failed"
May  1 19:56:24.642: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.062007ms
May  1 19:56:26.661: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049113839s
May  1 19:56:28.660: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047615661s
May  1 19:56:30.676: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064218055s
STEP: Saw pod success 05/01/23 19:56:30.676
May  1 19:56:30.677: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3" satisfied condition "Succeeded or Failed"
May  1 19:56:30.695: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:56:30.794
May  1 19:56:30.859: INFO: Waiting for pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 to disappear
May  1 19:56:30.878: INFO: Pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 19:56:30.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6798" for this suite. 05/01/23 19:56:30.91
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":188,"skipped":3358,"failed":0}
------------------------------
• [SLOW TEST] [6.536 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:24.395
    May  1 19:56:24.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 19:56:24.397
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:24.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:24.483
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-fb954732-e770-4559-8d54-b859b5db7454 05/01/23 19:56:24.521
    STEP: Creating a pod to test consume configMaps 05/01/23 19:56:24.542
    May  1 19:56:24.612: INFO: Waiting up to 5m0s for pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3" in namespace "configmap-6798" to be "Succeeded or Failed"
    May  1 19:56:24.642: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 30.062007ms
    May  1 19:56:26.661: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049113839s
    May  1 19:56:28.660: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047615661s
    May  1 19:56:30.676: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064218055s
    STEP: Saw pod success 05/01/23 19:56:30.676
    May  1 19:56:30.677: INFO: Pod "pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3" satisfied condition "Succeeded or Failed"
    May  1 19:56:30.695: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:56:30.794
    May  1 19:56:30.859: INFO: Waiting for pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 to disappear
    May  1 19:56:30.878: INFO: Pod pod-configmaps-70de3b87-4fe5-48a0-a908-b23e370ed4a3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 19:56:30.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6798" for this suite. 05/01/23 19:56:30.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:30.942
May  1 19:56:30.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename podtemplate 05/01/23 19:56:30.944
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.029
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/01/23 19:56:31.045
W0501 19:56:31.078646      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 05/01/23 19:56:31.078
May  1 19:56:31.172: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  1 19:56:31.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9746" for this suite. 05/01/23 19:56:31.209
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":189,"skipped":3379,"failed":0}
------------------------------
• [0.292 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:30.942
    May  1 19:56:30.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename podtemplate 05/01/23 19:56:30.944
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.029
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/01/23 19:56:31.045
    W0501 19:56:31.078646      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 05/01/23 19:56:31.078
    May  1 19:56:31.172: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  1 19:56:31.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9746" for this suite. 05/01/23 19:56:31.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:31.238
May  1 19:56:31.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename watch 05/01/23 19:56:31.24
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.373
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/01/23 19:56:31.389
STEP: modifying the configmap once 05/01/23 19:56:31.41
STEP: modifying the configmap a second time 05/01/23 19:56:31.459
STEP: deleting the configmap 05/01/23 19:56:31.571
STEP: creating a watch on configmaps from the resource version returned by the first update 05/01/23 19:56:31.642
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/01/23 19:56:31.648
May  1 19:56:31.649: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1931  e09b662f-9c87-47eb-8a29-a23cb301b58d 102957 0 2023-05-01 19:56:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-01 19:56:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 19:56:31.649: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1931  e09b662f-9c87-47eb-8a29-a23cb301b58d 102961 0 2023-05-01 19:56:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-01 19:56:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  1 19:56:31.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1931" for this suite. 05/01/23 19:56:31.734
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":190,"skipped":3400,"failed":0}
------------------------------
• [0.539 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:31.238
    May  1 19:56:31.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename watch 05/01/23 19:56:31.24
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.373
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/01/23 19:56:31.389
    STEP: modifying the configmap once 05/01/23 19:56:31.41
    STEP: modifying the configmap a second time 05/01/23 19:56:31.459
    STEP: deleting the configmap 05/01/23 19:56:31.571
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/01/23 19:56:31.642
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/01/23 19:56:31.648
    May  1 19:56:31.649: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1931  e09b662f-9c87-47eb-8a29-a23cb301b58d 102957 0 2023-05-01 19:56:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-01 19:56:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 19:56:31.649: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1931  e09b662f-9c87-47eb-8a29-a23cb301b58d 102961 0 2023-05-01 19:56:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-01 19:56:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  1 19:56:31.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1931" for this suite. 05/01/23 19:56:31.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:31.783
May  1 19:56:31.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename podtemplate 05/01/23 19:56:31.785
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.868
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/01/23 19:56:31.945
May  1 19:56:32.008: INFO: created test-podtemplate-1
May  1 19:56:32.041: INFO: created test-podtemplate-2
May  1 19:56:32.088: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/01/23 19:56:32.088
STEP: delete collection of pod templates 05/01/23 19:56:32.112
May  1 19:56:32.113: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/01/23 19:56:32.203
May  1 19:56:32.203: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  1 19:56:32.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9050" for this suite. 05/01/23 19:56:32.264
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":191,"skipped":3451,"failed":0}
------------------------------
• [0.537 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:31.783
    May  1 19:56:31.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename podtemplate 05/01/23 19:56:31.785
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:31.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:31.868
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/01/23 19:56:31.945
    May  1 19:56:32.008: INFO: created test-podtemplate-1
    May  1 19:56:32.041: INFO: created test-podtemplate-2
    May  1 19:56:32.088: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/01/23 19:56:32.088
    STEP: delete collection of pod templates 05/01/23 19:56:32.112
    May  1 19:56:32.113: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/01/23 19:56:32.203
    May  1 19:56:32.203: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  1 19:56:32.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9050" for this suite. 05/01/23 19:56:32.264
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:32.321
May  1 19:56:32.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 19:56:32.324
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:32.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:32.488
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May  1 19:56:32.658: INFO: Pod name sample-pod: Found 0 pods out of 1
May  1 19:56:37.676: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 19:56:37.677
STEP: Scaling up "test-rs" replicaset  05/01/23 19:56:37.677
May  1 19:56:37.727: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/01/23 19:56:37.727
W0501 19:56:37.773996      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  1 19:56:37.787: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
May  1 19:56:37.889: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
May  1 19:56:38.053: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
May  1 19:56:38.074: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
May  1 19:56:40.309: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 2, AvailableReplicas 2
May  1 19:56:40.511: INFO: observed Replicaset test-rs in namespace replicaset-8117 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 19:56:40.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8117" for this suite. 05/01/23 19:56:40.591
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":192,"skipped":3451,"failed":0}
------------------------------
• [SLOW TEST] [8.298 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:32.321
    May  1 19:56:32.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 19:56:32.324
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:32.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:32.488
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May  1 19:56:32.658: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  1 19:56:37.676: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 19:56:37.677
    STEP: Scaling up "test-rs" replicaset  05/01/23 19:56:37.677
    May  1 19:56:37.727: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/01/23 19:56:37.727
    W0501 19:56:37.773996      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  1 19:56:37.787: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
    May  1 19:56:37.889: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
    May  1 19:56:38.053: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
    May  1 19:56:38.074: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 1, AvailableReplicas 1
    May  1 19:56:40.309: INFO: observed ReplicaSet test-rs in namespace replicaset-8117 with ReadyReplicas 2, AvailableReplicas 2
    May  1 19:56:40.511: INFO: observed Replicaset test-rs in namespace replicaset-8117 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 19:56:40.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8117" for this suite. 05/01/23 19:56:40.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:40.622
May  1 19:56:40.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 19:56:40.625
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:40.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:40.789
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/01/23 19:56:40.863
STEP: Wait for the Deployment to create new ReplicaSet 05/01/23 19:56:40.944
STEP: delete the deployment 05/01/23 19:56:41.315
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/01/23 19:56:41.348
STEP: Gathering metrics 05/01/23 19:56:41.968
W0501 19:56:42.094111      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 19:56:42.094: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 19:56:42.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6363" for this suite. 05/01/23 19:56:42.13
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":193,"skipped":3482,"failed":0}
------------------------------
• [1.535 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:40.622
    May  1 19:56:40.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 19:56:40.625
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:40.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:40.789
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/01/23 19:56:40.863
    STEP: Wait for the Deployment to create new ReplicaSet 05/01/23 19:56:40.944
    STEP: delete the deployment 05/01/23 19:56:41.315
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/01/23 19:56:41.348
    STEP: Gathering metrics 05/01/23 19:56:41.968
    W0501 19:56:42.094111      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 19:56:42.094: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 19:56:42.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6363" for this suite. 05/01/23 19:56:42.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:42.175
May  1 19:56:42.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 19:56:42.177
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:42.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:42.33
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 05/01/23 19:56:42.372
STEP: waiting for available Endpoint 05/01/23 19:56:42.405
STEP: listing all Endpoints 05/01/23 19:56:42.413
STEP: updating the Endpoint 05/01/23 19:56:42.453
STEP: fetching the Endpoint 05/01/23 19:56:42.496
STEP: patching the Endpoint 05/01/23 19:56:42.518
STEP: fetching the Endpoint 05/01/23 19:56:42.589
STEP: deleting the Endpoint by Collection 05/01/23 19:56:42.612
STEP: waiting for Endpoint deletion 05/01/23 19:56:42.662
STEP: fetching the Endpoint 05/01/23 19:56:42.678
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 19:56:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4380" for this suite. 05/01/23 19:56:42.769
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":194,"skipped":3518,"failed":0}
------------------------------
• [0.623 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:42.175
    May  1 19:56:42.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 19:56:42.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:42.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:42.33
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 05/01/23 19:56:42.372
    STEP: waiting for available Endpoint 05/01/23 19:56:42.405
    STEP: listing all Endpoints 05/01/23 19:56:42.413
    STEP: updating the Endpoint 05/01/23 19:56:42.453
    STEP: fetching the Endpoint 05/01/23 19:56:42.496
    STEP: patching the Endpoint 05/01/23 19:56:42.518
    STEP: fetching the Endpoint 05/01/23 19:56:42.589
    STEP: deleting the Endpoint by Collection 05/01/23 19:56:42.612
    STEP: waiting for Endpoint deletion 05/01/23 19:56:42.662
    STEP: fetching the Endpoint 05/01/23 19:56:42.678
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 19:56:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4380" for this suite. 05/01/23 19:56:42.769
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:42.798
May  1 19:56:42.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 19:56:42.803
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:42.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:42.944
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 05/01/23 19:56:42.979
May  1 19:56:43.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1" in namespace "downward-api-3355" to be "Succeeded or Failed"
May  1 19:56:43.078: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.213063ms
May  1 19:56:45.099: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042272016s
May  1 19:56:47.114: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058018824s
May  1 19:56:49.103: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046672944s
May  1 19:56:51.095: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039001455s
STEP: Saw pod success 05/01/23 19:56:51.096
May  1 19:56:51.096: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1" satisfied condition "Succeeded or Failed"
May  1 19:56:51.116: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 container client-container: <nil>
STEP: delete the pod 05/01/23 19:56:51.155
May  1 19:56:51.246: INFO: Waiting for pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 to disappear
May  1 19:56:51.271: INFO: Pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 19:56:51.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3355" for this suite. 05/01/23 19:56:51.295
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":195,"skipped":3518,"failed":0}
------------------------------
• [SLOW TEST] [8.525 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:42.798
    May  1 19:56:42.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 19:56:42.803
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:42.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:42.944
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 05/01/23 19:56:42.979
    May  1 19:56:43.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1" in namespace "downward-api-3355" to be "Succeeded or Failed"
    May  1 19:56:43.078: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.213063ms
    May  1 19:56:45.099: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042272016s
    May  1 19:56:47.114: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058018824s
    May  1 19:56:49.103: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046672944s
    May  1 19:56:51.095: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.039001455s
    STEP: Saw pod success 05/01/23 19:56:51.096
    May  1 19:56:51.096: INFO: Pod "downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1" satisfied condition "Succeeded or Failed"
    May  1 19:56:51.116: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 container client-container: <nil>
    STEP: delete the pod 05/01/23 19:56:51.155
    May  1 19:56:51.246: INFO: Waiting for pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 to disappear
    May  1 19:56:51.271: INFO: Pod downwardapi-volume-3de6e998-1d3f-4aa6-a3b9-2ddbcee1c4c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 19:56:51.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3355" for this suite. 05/01/23 19:56:51.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:51.327
May  1 19:56:51.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 19:56:51.33
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:51.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:51.437
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-c322aa96-1f5f-4691-bab2-4dfa1cdb98fe 05/01/23 19:56:51.601
STEP: Creating a pod to test consume secrets 05/01/23 19:56:51.627
May  1 19:56:51.729: INFO: Waiting up to 5m0s for pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997" in namespace "secrets-3727" to be "Succeeded or Failed"
May  1 19:56:51.749: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 19.988613ms
May  1 19:56:53.775: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045743765s
May  1 19:56:55.774: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044575617s
May  1 19:56:57.782: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052986124s
STEP: Saw pod success 05/01/23 19:56:57.782
May  1 19:56:57.783: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997" satisfied condition "Succeeded or Failed"
May  1 19:56:57.803: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 19:56:57.907
May  1 19:56:57.976: INFO: Waiting for pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 to disappear
May  1 19:56:58.016: INFO: Pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 19:56:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3727" for this suite. 05/01/23 19:56:58.052
STEP: Destroying namespace "secret-namespace-475" for this suite. 05/01/23 19:56:58.083
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":196,"skipped":3525,"failed":0}
------------------------------
• [SLOW TEST] [6.786 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:51.327
    May  1 19:56:51.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 19:56:51.33
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:51.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:51.437
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-c322aa96-1f5f-4691-bab2-4dfa1cdb98fe 05/01/23 19:56:51.601
    STEP: Creating a pod to test consume secrets 05/01/23 19:56:51.627
    May  1 19:56:51.729: INFO: Waiting up to 5m0s for pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997" in namespace "secrets-3727" to be "Succeeded or Failed"
    May  1 19:56:51.749: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 19.988613ms
    May  1 19:56:53.775: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045743765s
    May  1 19:56:55.774: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044575617s
    May  1 19:56:57.782: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052986124s
    STEP: Saw pod success 05/01/23 19:56:57.782
    May  1 19:56:57.783: INFO: Pod "pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997" satisfied condition "Succeeded or Failed"
    May  1 19:56:57.803: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 19:56:57.907
    May  1 19:56:57.976: INFO: Waiting for pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 to disappear
    May  1 19:56:58.016: INFO: Pod pod-secrets-8e2114b5-a1c3-4778-b8bd-3034f3321997 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 19:56:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3727" for this suite. 05/01/23 19:56:58.052
    STEP: Destroying namespace "secret-namespace-475" for this suite. 05/01/23 19:56:58.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:56:58.127
May  1 19:56:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 19:56:58.132
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:58.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:58.292
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-379f2883-3e7b-4359-aa9b-6ea5ad4c4fd6 05/01/23 19:56:58.308
STEP: Creating a pod to test consume configMaps 05/01/23 19:56:58.355
May  1 19:56:58.439: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e" in namespace "projected-7384" to be "Succeeded or Failed"
May  1 19:56:58.466: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 27.206211ms
May  1 19:57:00.487: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048654272s
May  1 19:57:02.491: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05227489s
May  1 19:57:04.501: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061839756s
May  1 19:57:06.493: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054356573s
STEP: Saw pod success 05/01/23 19:57:06.493
May  1 19:57:06.493: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e" satisfied condition "Succeeded or Failed"
May  1 19:57:06.530: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e container agnhost-container: <nil>
STEP: delete the pod 05/01/23 19:57:06.585
May  1 19:57:06.675: INFO: Waiting for pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e to disappear
May  1 19:57:06.695: INFO: Pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 19:57:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7384" for this suite. 05/01/23 19:57:06.739
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":197,"skipped":3560,"failed":0}
------------------------------
• [SLOW TEST] [8.639 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:56:58.127
    May  1 19:56:58.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 19:56:58.132
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:56:58.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:56:58.292
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-379f2883-3e7b-4359-aa9b-6ea5ad4c4fd6 05/01/23 19:56:58.308
    STEP: Creating a pod to test consume configMaps 05/01/23 19:56:58.355
    May  1 19:56:58.439: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e" in namespace "projected-7384" to be "Succeeded or Failed"
    May  1 19:56:58.466: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 27.206211ms
    May  1 19:57:00.487: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048654272s
    May  1 19:57:02.491: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05227489s
    May  1 19:57:04.501: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061839756s
    May  1 19:57:06.493: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.054356573s
    STEP: Saw pod success 05/01/23 19:57:06.493
    May  1 19:57:06.493: INFO: Pod "pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e" satisfied condition "Succeeded or Failed"
    May  1 19:57:06.530: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 19:57:06.585
    May  1 19:57:06.675: INFO: Waiting for pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e to disappear
    May  1 19:57:06.695: INFO: Pod pod-projected-configmaps-e7405202-0161-4890-8ca0-4ec522cccb7e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 19:57:06.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7384" for this suite. 05/01/23 19:57:06.739
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:57:06.767
May  1 19:57:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename events 05/01/23 19:57:06.768
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:06.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:06.88
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/01/23 19:57:06.894
STEP: get a list of Events with a label in the current namespace 05/01/23 19:57:07.023
STEP: delete a list of events 05/01/23 19:57:07.045
May  1 19:57:07.046: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/01/23 19:57:07.25
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  1 19:57:07.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-666" for this suite. 05/01/23 19:57:07.321
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":198,"skipped":3560,"failed":0}
------------------------------
• [0.613 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:57:06.767
    May  1 19:57:06.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename events 05/01/23 19:57:06.768
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:06.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:06.88
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/01/23 19:57:06.894
    STEP: get a list of Events with a label in the current namespace 05/01/23 19:57:07.023
    STEP: delete a list of events 05/01/23 19:57:07.045
    May  1 19:57:07.046: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/01/23 19:57:07.25
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  1 19:57:07.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-666" for this suite. 05/01/23 19:57:07.321
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:57:07.383
May  1 19:57:07.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:57:07.385
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:07.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:07.535
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 05/01/23 19:57:07.683
STEP: submitting the pod to kubernetes 05/01/23 19:57:07.684
May  1 19:57:07.789: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" in namespace "pods-3481" to be "running and ready"
May  1 19:57:07.830: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 41.41198ms
May  1 19:57:07.830: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:57:09.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064902301s
May  1 19:57:09.854: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Pending, waiting for it to be Running (with Ready = true)
May  1 19:57:11.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=true. Elapsed: 4.065259518s
May  1 19:57:11.854: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Running (Ready = true)
May  1 19:57:11.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/01/23 19:57:11.873
STEP: updating the pod 05/01/23 19:57:11.891
May  1 19:57:12.523: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1"
May  1 19:57:12.523: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" in namespace "pods-3481" to be "terminated with reason DeadlineExceeded"
May  1 19:57:12.557: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=true. Elapsed: 34.369739ms
May  1 19:57:14.578: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=false. Elapsed: 2.054802081s
May  1 19:57:16.590: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.067361006s
May  1 19:57:16.590: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:57:16.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3481" for this suite. 05/01/23 19:57:16.637
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":199,"skipped":3563,"failed":0}
------------------------------
• [SLOW TEST] [9.307 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:57:07.383
    May  1 19:57:07.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:57:07.385
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:07.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:07.535
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 05/01/23 19:57:07.683
    STEP: submitting the pod to kubernetes 05/01/23 19:57:07.684
    May  1 19:57:07.789: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" in namespace "pods-3481" to be "running and ready"
    May  1 19:57:07.830: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 41.41198ms
    May  1 19:57:07.830: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:57:09.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064902301s
    May  1 19:57:09.854: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:57:11.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=true. Elapsed: 4.065259518s
    May  1 19:57:11.854: INFO: The phase of Pod pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1 is Running (Ready = true)
    May  1 19:57:11.854: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/01/23 19:57:11.873
    STEP: updating the pod 05/01/23 19:57:11.891
    May  1 19:57:12.523: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1"
    May  1 19:57:12.523: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" in namespace "pods-3481" to be "terminated with reason DeadlineExceeded"
    May  1 19:57:12.557: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=true. Elapsed: 34.369739ms
    May  1 19:57:14.578: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Running", Reason="", readiness=false. Elapsed: 2.054802081s
    May  1 19:57:16.590: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.067361006s
    May  1 19:57:16.590: INFO: Pod "pod-update-activedeadlineseconds-4636f61f-0dd5-4895-a8f4-f27e9685abf1" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:57:16.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3481" for this suite. 05/01/23 19:57:16.637
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:57:16.694
May  1 19:57:16.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 19:57:16.697
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:16.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:16.813
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
May  1 19:57:16.923: INFO: Waiting up to 5m0s for pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c" in namespace "pods-4005" to be "running and ready"
May  1 19:57:16.955: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.419438ms
May  1 19:57:16.955: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Pending, waiting for it to be Running (with Ready = true)
May  1 19:57:18.982: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05859579s
May  1 19:57:18.982: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Pending, waiting for it to be Running (with Ready = true)
May  1 19:57:20.979: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Running", Reason="", readiness=true. Elapsed: 4.056176617s
May  1 19:57:20.980: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Running (Ready = true)
May  1 19:57:20.980: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c" satisfied condition "running and ready"
May  1 19:57:21.124: INFO: Waiting up to 5m0s for pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802" in namespace "pods-4005" to be "Succeeded or Failed"
May  1 19:57:21.152: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 27.856463ms
May  1 19:57:23.204: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080395512s
May  1 19:57:25.177: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052843086s
May  1 19:57:27.202: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077750819s
May  1 19:57:29.190: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066220014s
STEP: Saw pod success 05/01/23 19:57:29.19
May  1 19:57:29.191: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802" satisfied condition "Succeeded or Failed"
May  1 19:57:29.214: INFO: Trying to get logs from node 10.45.145.124 pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 container env3cont: <nil>
STEP: delete the pod 05/01/23 19:57:29.499
May  1 19:57:29.631: INFO: Waiting for pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 to disappear
May  1 19:57:29.675: INFO: Pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 19:57:29.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4005" for this suite. 05/01/23 19:57:29.727
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":200,"skipped":3566,"failed":0}
------------------------------
• [SLOW TEST] [13.065 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:57:16.694
    May  1 19:57:16.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 19:57:16.697
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:16.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:16.813
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    May  1 19:57:16.923: INFO: Waiting up to 5m0s for pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c" in namespace "pods-4005" to be "running and ready"
    May  1 19:57:16.955: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.419438ms
    May  1 19:57:16.955: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:57:18.982: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05859579s
    May  1 19:57:18.982: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Pending, waiting for it to be Running (with Ready = true)
    May  1 19:57:20.979: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c": Phase="Running", Reason="", readiness=true. Elapsed: 4.056176617s
    May  1 19:57:20.980: INFO: The phase of Pod server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c is Running (Ready = true)
    May  1 19:57:20.980: INFO: Pod "server-envvars-c38f483a-f820-405e-823b-2c4da6b2091c" satisfied condition "running and ready"
    May  1 19:57:21.124: INFO: Waiting up to 5m0s for pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802" in namespace "pods-4005" to be "Succeeded or Failed"
    May  1 19:57:21.152: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 27.856463ms
    May  1 19:57:23.204: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080395512s
    May  1 19:57:25.177: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052843086s
    May  1 19:57:27.202: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077750819s
    May  1 19:57:29.190: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066220014s
    STEP: Saw pod success 05/01/23 19:57:29.19
    May  1 19:57:29.191: INFO: Pod "client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802" satisfied condition "Succeeded or Failed"
    May  1 19:57:29.214: INFO: Trying to get logs from node 10.45.145.124 pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 container env3cont: <nil>
    STEP: delete the pod 05/01/23 19:57:29.499
    May  1 19:57:29.631: INFO: Waiting for pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 to disappear
    May  1 19:57:29.675: INFO: Pod client-envvars-e9089dfe-7af6-4a3e-8b44-9913b82a2802 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 19:57:29.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4005" for this suite. 05/01/23 19:57:29.727
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:57:29.766
May  1 19:57:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename prestop 05/01/23 19:57:29.769
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:29.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:29.895
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-8945 05/01/23 19:57:29.914
STEP: Waiting for pods to come up. 05/01/23 19:57:30.022
May  1 19:57:30.023: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8945" to be "running"
May  1 19:57:30.054: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 31.385227ms
May  1 19:57:32.078: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054771004s
May  1 19:57:34.078: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.054667628s
May  1 19:57:34.078: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-8945 05/01/23 19:57:34.101
May  1 19:57:34.148: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8945" to be "running"
May  1 19:57:34.169: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 20.247073ms
May  1 19:57:36.209: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060002564s
May  1 19:57:38.198: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.049076888s
May  1 19:57:38.198: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/01/23 19:57:38.198
May  1 19:57:43.290: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/01/23 19:57:43.29
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
May  1 19:57:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8945" for this suite. 05/01/23 19:57:43.377
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":201,"skipped":3594,"failed":0}
------------------------------
• [SLOW TEST] [13.640 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:57:29.766
    May  1 19:57:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename prestop 05/01/23 19:57:29.769
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:29.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:29.895
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-8945 05/01/23 19:57:29.914
    STEP: Waiting for pods to come up. 05/01/23 19:57:30.022
    May  1 19:57:30.023: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-8945" to be "running"
    May  1 19:57:30.054: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 31.385227ms
    May  1 19:57:32.078: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054771004s
    May  1 19:57:34.078: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.054667628s
    May  1 19:57:34.078: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-8945 05/01/23 19:57:34.101
    May  1 19:57:34.148: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-8945" to be "running"
    May  1 19:57:34.169: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 20.247073ms
    May  1 19:57:36.209: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060002564s
    May  1 19:57:38.198: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.049076888s
    May  1 19:57:38.198: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/01/23 19:57:38.198
    May  1 19:57:43.290: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/01/23 19:57:43.29
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    May  1 19:57:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-8945" for this suite. 05/01/23 19:57:43.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 19:57:43.408
May  1 19:57:43.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 19:57:43.41
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:43.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:43.536
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 05/01/23 19:57:43.578
May  1 19:57:43.695: INFO: Waiting up to 2m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363" to be "running"
May  1 19:57:43.732: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 36.858804ms
May  1 19:57:45.770: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074187787s
May  1 19:57:47.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064426359s
May  1 19:57:49.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06481243s
May  1 19:57:51.755: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059637678s
May  1 19:57:53.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063436391s
May  1 19:57:55.799: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 12.103746654s
May  1 19:57:57.773: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 14.077707796s
May  1 19:57:59.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 16.065729314s
May  1 19:58:01.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 18.071409105s
May  1 19:58:03.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 20.056025535s
May  1 19:58:05.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 22.083406108s
May  1 19:58:07.758: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 24.062241507s
May  1 19:58:09.764: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 26.068982001s
May  1 19:58:11.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 28.064975477s
May  1 19:58:13.822: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 30.126531277s
May  1 19:58:15.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 32.066004209s
May  1 19:58:17.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 34.057030856s
May  1 19:58:19.770: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 36.074220159s
May  1 19:58:21.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 38.069793965s
May  1 19:58:23.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 40.071705558s
May  1 19:58:25.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 42.066772397s
May  1 19:58:27.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 44.057854544s
May  1 19:58:29.769: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 46.073098686s
May  1 19:58:31.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 48.063729949s
May  1 19:58:33.802: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 50.106882633s
May  1 19:58:35.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 52.055639096s
May  1 19:58:37.822: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 54.126208433s
May  1 19:58:39.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 56.064323432s
May  1 19:58:41.799: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 58.10350957s
May  1 19:58:43.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.057563353s
May  1 19:58:45.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.069696029s
May  1 19:58:47.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056648175s
May  1 19:58:49.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.065709734s
May  1 19:58:51.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.057443647s
May  1 19:58:53.750: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.05475108s
May  1 19:58:55.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.05607675s
May  1 19:58:57.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.06456035s
May  1 19:58:59.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.067026156s
May  1 19:59:01.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.061456282s
May  1 19:59:03.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.056766759s
May  1 19:59:05.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.069713937s
May  1 19:59:07.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.055849733s
May  1 19:59:09.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061534173s
May  1 19:59:11.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.057817278s
May  1 19:59:13.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.083897334s
May  1 19:59:15.754: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.058571766s
May  1 19:59:17.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.063892279s
May  1 19:59:19.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.063897739s
May  1 19:59:21.754: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.058704975s
May  1 19:59:23.773: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.077490495s
May  1 19:59:25.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.083878805s
May  1 19:59:27.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.061780984s
May  1 19:59:29.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.066572938s
May  1 19:59:31.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.05541012s
May  1 19:59:33.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.061191158s
May  1 19:59:35.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.063116834s
May  1 19:59:37.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.057574168s
May  1 19:59:39.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.071099481s
May  1 19:59:41.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.06566445s
May  1 19:59:43.781: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.085223585s
May  1 19:59:43.798: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.102844408s
STEP: updating the pod 05/01/23 19:59:43.798
May  1 19:59:44.461: INFO: Successfully updated pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949"
STEP: waiting for pod running 05/01/23 19:59:44.461
May  1 19:59:44.461: INFO: Waiting up to 2m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363" to be "running"
May  1 19:59:44.525: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 64.049053ms
May  1 19:59:46.559: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Running", Reason="", readiness=true. Elapsed: 2.097926742s
May  1 19:59:46.559: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" satisfied condition "running"
STEP: deleting the pod gracefully 05/01/23 19:59:46.559
May  1 19:59:46.559: INFO: Deleting pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363"
May  1 19:59:46.616: INFO: Wait up to 5m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 20:00:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7363" for this suite. 05/01/23 20:00:20.724
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":202,"skipped":3609,"failed":0}
------------------------------
• [SLOW TEST] [157.348 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 19:57:43.408
    May  1 19:57:43.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 19:57:43.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 19:57:43.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 19:57:43.536
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 05/01/23 19:57:43.578
    May  1 19:57:43.695: INFO: Waiting up to 2m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363" to be "running"
    May  1 19:57:43.732: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 36.858804ms
    May  1 19:57:45.770: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074187787s
    May  1 19:57:47.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064426359s
    May  1 19:57:49.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06481243s
    May  1 19:57:51.755: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 8.059637678s
    May  1 19:57:53.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063436391s
    May  1 19:57:55.799: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 12.103746654s
    May  1 19:57:57.773: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 14.077707796s
    May  1 19:57:59.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 16.065729314s
    May  1 19:58:01.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 18.071409105s
    May  1 19:58:03.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 20.056025535s
    May  1 19:58:05.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 22.083406108s
    May  1 19:58:07.758: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 24.062241507s
    May  1 19:58:09.764: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 26.068982001s
    May  1 19:58:11.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 28.064975477s
    May  1 19:58:13.822: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 30.126531277s
    May  1 19:58:15.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 32.066004209s
    May  1 19:58:17.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 34.057030856s
    May  1 19:58:19.770: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 36.074220159s
    May  1 19:58:21.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 38.069793965s
    May  1 19:58:23.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 40.071705558s
    May  1 19:58:25.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 42.066772397s
    May  1 19:58:27.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 44.057854544s
    May  1 19:58:29.769: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 46.073098686s
    May  1 19:58:31.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 48.063729949s
    May  1 19:58:33.802: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 50.106882633s
    May  1 19:58:35.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 52.055639096s
    May  1 19:58:37.822: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 54.126208433s
    May  1 19:58:39.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 56.064323432s
    May  1 19:58:41.799: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 58.10350957s
    May  1 19:58:43.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.057563353s
    May  1 19:58:45.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.069696029s
    May  1 19:58:47.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056648175s
    May  1 19:58:49.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.065709734s
    May  1 19:58:51.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.057443647s
    May  1 19:58:53.750: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.05475108s
    May  1 19:58:55.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.05607675s
    May  1 19:58:57.760: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.06456035s
    May  1 19:58:59.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.067026156s
    May  1 19:59:01.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.061456282s
    May  1 19:59:03.752: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.056766759s
    May  1 19:59:05.765: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.069713937s
    May  1 19:59:07.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.055849733s
    May  1 19:59:09.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.061534173s
    May  1 19:59:11.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.057817278s
    May  1 19:59:13.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.083897334s
    May  1 19:59:15.754: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.058571766s
    May  1 19:59:17.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.063892279s
    May  1 19:59:19.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.063897739s
    May  1 19:59:21.754: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.058704975s
    May  1 19:59:23.773: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.077490495s
    May  1 19:59:25.779: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.083878805s
    May  1 19:59:27.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.061780984s
    May  1 19:59:29.762: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.066572938s
    May  1 19:59:31.751: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.05541012s
    May  1 19:59:33.757: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.061191158s
    May  1 19:59:35.759: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.063116834s
    May  1 19:59:37.753: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.057574168s
    May  1 19:59:39.767: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.071099481s
    May  1 19:59:41.761: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.06566445s
    May  1 19:59:43.781: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.085223585s
    May  1 19:59:43.798: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.102844408s
    STEP: updating the pod 05/01/23 19:59:43.798
    May  1 19:59:44.461: INFO: Successfully updated pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949"
    STEP: waiting for pod running 05/01/23 19:59:44.461
    May  1 19:59:44.461: INFO: Waiting up to 2m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363" to be "running"
    May  1 19:59:44.525: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Pending", Reason="", readiness=false. Elapsed: 64.049053ms
    May  1 19:59:46.559: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949": Phase="Running", Reason="", readiness=true. Elapsed: 2.097926742s
    May  1 19:59:46.559: INFO: Pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" satisfied condition "running"
    STEP: deleting the pod gracefully 05/01/23 19:59:46.559
    May  1 19:59:46.559: INFO: Deleting pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" in namespace "var-expansion-7363"
    May  1 19:59:46.616: INFO: Wait up to 5m0s for pod "var-expansion-4072c336-9302-45ee-b0d2-c500a4523949" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 20:00:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7363" for this suite. 05/01/23 20:00:20.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:00:20.758
May  1 20:00:20.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 20:00:20.76
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:20.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:20.964
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/01/23 20:00:20.986
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/01/23 20:00:21.06
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/01/23 20:00:21.061
STEP: creating a pod to probe DNS 05/01/23 20:00:21.061
STEP: submitting the pod to kubernetes 05/01/23 20:00:21.061
May  1 20:00:21.181: INFO: Waiting up to 15m0s for pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641" in namespace "dns-4976" to be "running"
May  1 20:00:21.231: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Pending", Reason="", readiness=false. Elapsed: 50.245321ms
May  1 20:00:23.263: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082815532s
May  1 20:00:25.288: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Running", Reason="", readiness=true. Elapsed: 4.107772852s
May  1 20:00:25.289: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641" satisfied condition "running"
STEP: retrieving the pod 05/01/23 20:00:25.289
STEP: looking for the results for each expected name from probers 05/01/23 20:00:25.327
May  1 20:00:25.617: INFO: DNS probes using dns-4976/dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641 succeeded

STEP: deleting the pod 05/01/23 20:00:25.617
STEP: deleting the test headless service 05/01/23 20:00:25.701
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 20:00:25.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4976" for this suite. 05/01/23 20:00:25.795
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":203,"skipped":3626,"failed":0}
------------------------------
• [SLOW TEST] [5.065 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:00:20.758
    May  1 20:00:20.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 20:00:20.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:20.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:20.964
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/01/23 20:00:20.986
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/01/23 20:00:21.06
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4976.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/01/23 20:00:21.061
    STEP: creating a pod to probe DNS 05/01/23 20:00:21.061
    STEP: submitting the pod to kubernetes 05/01/23 20:00:21.061
    May  1 20:00:21.181: INFO: Waiting up to 15m0s for pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641" in namespace "dns-4976" to be "running"
    May  1 20:00:21.231: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Pending", Reason="", readiness=false. Elapsed: 50.245321ms
    May  1 20:00:23.263: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082815532s
    May  1 20:00:25.288: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641": Phase="Running", Reason="", readiness=true. Elapsed: 4.107772852s
    May  1 20:00:25.289: INFO: Pod "dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 20:00:25.289
    STEP: looking for the results for each expected name from probers 05/01/23 20:00:25.327
    May  1 20:00:25.617: INFO: DNS probes using dns-4976/dns-test-45ae4c6b-b1ef-4642-bf4e-8dc5c3093641 succeeded

    STEP: deleting the pod 05/01/23 20:00:25.617
    STEP: deleting the test headless service 05/01/23 20:00:25.701
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 20:00:25.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4976" for this suite. 05/01/23 20:00:25.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:00:25.829
May  1 20:00:25.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 20:00:25.83
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:25.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:25.926
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
May  1 20:00:26.120: INFO: Waiting up to 2m0s for pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" in namespace "var-expansion-3854" to be "container 0 failed with reason CreateContainerConfigError"
May  1 20:00:26.169: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.733088ms
May  1 20:00:28.191: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070539676s
May  1 20:00:30.193: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07291251s
May  1 20:00:30.194: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May  1 20:00:30.194: INFO: Deleting pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" in namespace "var-expansion-3854"
May  1 20:00:30.273: INFO: Wait up to 5m0s for pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 20:00:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3854" for this suite. 05/01/23 20:00:34.387
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":204,"skipped":3653,"failed":0}
------------------------------
• [SLOW TEST] [8.612 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:00:25.829
    May  1 20:00:25.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 20:00:25.83
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:25.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:25.926
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    May  1 20:00:26.120: INFO: Waiting up to 2m0s for pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" in namespace "var-expansion-3854" to be "container 0 failed with reason CreateContainerConfigError"
    May  1 20:00:26.169: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.733088ms
    May  1 20:00:28.191: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070539676s
    May  1 20:00:30.193: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07291251s
    May  1 20:00:30.194: INFO: Pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May  1 20:00:30.194: INFO: Deleting pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" in namespace "var-expansion-3854"
    May  1 20:00:30.273: INFO: Wait up to 5m0s for pod "var-expansion-1c8feb85-84f2-4e7c-8b32-d06a574808d8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 20:00:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-3854" for this suite. 05/01/23 20:00:34.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:00:34.45
May  1 20:00:34.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-watch 05/01/23 20:00:34.452
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:34.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:34.603
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May  1 20:00:34.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Creating first CR  05/01/23 20:00:37.386
May  1 20:00:37.410: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:37Z]] name:name1 resourceVersion:105619 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/01/23 20:00:47.414
May  1 20:00:47.440: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:47Z]] name:name2 resourceVersion:105691 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/01/23 20:00:57.444
May  1 20:00:57.473: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:57Z]] name:name1 resourceVersion:105735 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/01/23 20:01:07.475
May  1 20:01:07.510: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:01:07Z]] name:name2 resourceVersion:105773 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/01/23 20:01:17.511
May  1 20:01:17.545: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:57Z]] name:name1 resourceVersion:105831 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/01/23 20:01:27.545
May  1 20:01:27.583: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:01:07Z]] name:name2 resourceVersion:105884 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:01:38.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6954" for this suite. 05/01/23 20:01:38.267
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":205,"skipped":3718,"failed":0}
------------------------------
• [SLOW TEST] [63.889 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:00:34.45
    May  1 20:00:34.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-watch 05/01/23 20:00:34.452
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:00:34.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:00:34.603
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May  1 20:00:34.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Creating first CR  05/01/23 20:00:37.386
    May  1 20:00:37.410: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:37Z]] name:name1 resourceVersion:105619 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/01/23 20:00:47.414
    May  1 20:00:47.440: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:47Z]] name:name2 resourceVersion:105691 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/01/23 20:00:57.444
    May  1 20:00:57.473: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:57Z]] name:name1 resourceVersion:105735 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/01/23 20:01:07.475
    May  1 20:01:07.510: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:01:07Z]] name:name2 resourceVersion:105773 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/01/23 20:01:17.511
    May  1 20:01:17.545: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:00:57Z]] name:name1 resourceVersion:105831 uid:811bb452-b498-4822-8f70-00dff5f25b21] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/01/23 20:01:27.545
    May  1 20:01:27.583: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-01T20:00:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-01T20:01:07Z]] name:name2 resourceVersion:105884 uid:a2ef95be-7638-4f7a-89df-90b3dae0a5c9] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:01:38.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6954" for this suite. 05/01/23 20:01:38.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:01:38.34
May  1 20:01:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:01:38.342
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:38.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:38.549
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:01:38.677
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:01:39.757
STEP: Deploying the webhook pod 05/01/23 20:01:39.815
STEP: Wait for the deployment to be ready 05/01/23 20:01:39.872
May  1 20:01:39.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:01:42.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 1, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:01:44.045
STEP: Verifying the service has paired with the endpoint 05/01/23 20:01:44.182
May  1 20:01:45.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
May  1 20:01:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/01/23 20:01:45.769
STEP: Creating a custom resource that should be denied by the webhook 05/01/23 20:01:45.922
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/01/23 20:01:48.105
STEP: Updating the custom resource with disallowed data should be denied 05/01/23 20:01:48.162
STEP: Deleting the custom resource should be denied 05/01/23 20:01:48.225
STEP: Remove the offending key and value from the custom resource data 05/01/23 20:01:48.27
STEP: Deleting the updated custom resource should be successful 05/01/23 20:01:48.333
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:01:48.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4477" for this suite. 05/01/23 20:01:49.091
STEP: Destroying namespace "webhook-4477-markers" for this suite. 05/01/23 20:01:49.122
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":206,"skipped":3724,"failed":0}
------------------------------
• [SLOW TEST] [11.236 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:01:38.34
    May  1 20:01:38.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:01:38.342
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:38.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:38.549
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:01:38.677
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:01:39.757
    STEP: Deploying the webhook pod 05/01/23 20:01:39.815
    STEP: Wait for the deployment to be ready 05/01/23 20:01:39.872
    May  1 20:01:39.926: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:01:42.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 1, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 1, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:01:44.045
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:01:44.182
    May  1 20:01:45.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    May  1 20:01:45.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/01/23 20:01:45.769
    STEP: Creating a custom resource that should be denied by the webhook 05/01/23 20:01:45.922
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/01/23 20:01:48.105
    STEP: Updating the custom resource with disallowed data should be denied 05/01/23 20:01:48.162
    STEP: Deleting the custom resource should be denied 05/01/23 20:01:48.225
    STEP: Remove the offending key and value from the custom resource data 05/01/23 20:01:48.27
    STEP: Deleting the updated custom resource should be successful 05/01/23 20:01:48.333
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:01:48.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4477" for this suite. 05/01/23 20:01:49.091
    STEP: Destroying namespace "webhook-4477-markers" for this suite. 05/01/23 20:01:49.122
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:01:49.592
May  1 20:01:49.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:01:49.597
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:49.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:49.773
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:01:50.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6744" for this suite. 05/01/23 20:01:50.257
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":207,"skipped":3813,"failed":0}
------------------------------
• [0.703 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:01:49.592
    May  1 20:01:49.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:01:49.597
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:49.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:49.773
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:01:50.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6744" for this suite. 05/01/23 20:01:50.257
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:01:50.298
May  1 20:01:50.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 20:01:50.301
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:50.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:50.402
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5977 05/01/23 20:01:50.441
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-5977 05/01/23 20:01:50.467
May  1 20:01:50.567: INFO: Found 0 stateful pods, waiting for 1
May  1 20:02:00.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/01/23 20:02:00.71
STEP: updating a scale subresource 05/01/23 20:02:00.731
STEP: verifying the statefulset Spec.Replicas was modified 05/01/23 20:02:00.758
STEP: Patch a scale subresource 05/01/23 20:02:00.803
STEP: verifying the statefulset Spec.Replicas was modified 05/01/23 20:02:00.835
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 20:02:00.861: INFO: Deleting all statefulset in ns statefulset-5977
May  1 20:02:00.885: INFO: Scaling statefulset ss to 0
May  1 20:02:11.049: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:02:11.077: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 20:02:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5977" for this suite. 05/01/23 20:02:11.288
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":208,"skipped":3814,"failed":0}
------------------------------
• [SLOW TEST] [21.050 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:01:50.298
    May  1 20:01:50.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 20:01:50.301
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:01:50.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:01:50.402
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5977 05/01/23 20:01:50.441
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-5977 05/01/23 20:01:50.467
    May  1 20:01:50.567: INFO: Found 0 stateful pods, waiting for 1
    May  1 20:02:00.604: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/01/23 20:02:00.71
    STEP: updating a scale subresource 05/01/23 20:02:00.731
    STEP: verifying the statefulset Spec.Replicas was modified 05/01/23 20:02:00.758
    STEP: Patch a scale subresource 05/01/23 20:02:00.803
    STEP: verifying the statefulset Spec.Replicas was modified 05/01/23 20:02:00.835
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 20:02:00.861: INFO: Deleting all statefulset in ns statefulset-5977
    May  1 20:02:00.885: INFO: Scaling statefulset ss to 0
    May  1 20:02:11.049: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:02:11.077: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 20:02:11.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5977" for this suite. 05/01/23 20:02:11.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:02:11.351
May  1 20:02:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:02:11.353
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:11.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:11.542
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:02:11.692
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:02:13.239
STEP: Deploying the webhook pod 05/01/23 20:02:13.293
STEP: Wait for the deployment to be ready 05/01/23 20:02:13.349
May  1 20:02:13.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:02:15.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:02:17.472
STEP: Verifying the service has paired with the endpoint 05/01/23 20:02:17.52
May  1 20:02:18.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
May  1 20:02:18.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9334-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:02:19.144
STEP: Creating a custom resource while v1 is storage version 05/01/23 20:02:19.272
STEP: Patching Custom Resource Definition to set v2 as storage 05/01/23 20:02:21.589
STEP: Patching the custom resource while v2 is storage version 05/01/23 20:02:21.671
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:02:22.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7597" for this suite. 05/01/23 20:02:22.617
STEP: Destroying namespace "webhook-7597-markers" for this suite. 05/01/23 20:02:22.639
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":209,"skipped":3833,"failed":0}
------------------------------
• [SLOW TEST] [11.540 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:02:11.351
    May  1 20:02:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:02:11.353
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:11.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:11.542
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:02:11.692
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:02:13.239
    STEP: Deploying the webhook pod 05/01/23 20:02:13.293
    STEP: Wait for the deployment to be ready 05/01/23 20:02:13.349
    May  1 20:02:13.398: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:02:15.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 2, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:02:17.472
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:02:17.52
    May  1 20:02:18.520: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    May  1 20:02:18.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9334-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:02:19.144
    STEP: Creating a custom resource while v1 is storage version 05/01/23 20:02:19.272
    STEP: Patching Custom Resource Definition to set v2 as storage 05/01/23 20:02:21.589
    STEP: Patching the custom resource while v2 is storage version 05/01/23 20:02:21.671
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:02:22.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7597" for this suite. 05/01/23 20:02:22.617
    STEP: Destroying namespace "webhook-7597-markers" for this suite. 05/01/23 20:02:22.639
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:02:22.894
May  1 20:02:22.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 20:02:22.895
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:22.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:23.037
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 05/01/23 20:02:23.123
STEP: submitting the pod to kubernetes 05/01/23 20:02:23.123
May  1 20:02:23.269: INFO: Waiting up to 5m0s for pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" in namespace "pods-9585" to be "running and ready"
May  1 20:02:23.310: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Pending", Reason="", readiness=false. Elapsed: 41.32172ms
May  1 20:02:23.310: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:02:25.330: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061528428s
May  1 20:02:25.331: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:02:27.342: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Running", Reason="", readiness=true. Elapsed: 4.07301164s
May  1 20:02:27.342: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Running (Ready = true)
May  1 20:02:27.342: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/01/23 20:02:27.394
STEP: updating the pod 05/01/23 20:02:27.416
May  1 20:02:28.027: INFO: Successfully updated pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95"
May  1 20:02:28.027: INFO: Waiting up to 5m0s for pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" in namespace "pods-9585" to be "running"
May  1 20:02:28.045: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Running", Reason="", readiness=true. Elapsed: 18.189484ms
May  1 20:02:28.045: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/01/23 20:02:28.045
May  1 20:02:28.064: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 20:02:28.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9585" for this suite. 05/01/23 20:02:28.132
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":210,"skipped":3872,"failed":0}
------------------------------
• [SLOW TEST] [5.291 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:02:22.894
    May  1 20:02:22.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 20:02:22.895
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:22.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:23.037
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 05/01/23 20:02:23.123
    STEP: submitting the pod to kubernetes 05/01/23 20:02:23.123
    May  1 20:02:23.269: INFO: Waiting up to 5m0s for pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" in namespace "pods-9585" to be "running and ready"
    May  1 20:02:23.310: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Pending", Reason="", readiness=false. Elapsed: 41.32172ms
    May  1 20:02:23.310: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:02:25.330: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061528428s
    May  1 20:02:25.331: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:02:27.342: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Running", Reason="", readiness=true. Elapsed: 4.07301164s
    May  1 20:02:27.342: INFO: The phase of Pod pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95 is Running (Ready = true)
    May  1 20:02:27.342: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/01/23 20:02:27.394
    STEP: updating the pod 05/01/23 20:02:27.416
    May  1 20:02:28.027: INFO: Successfully updated pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95"
    May  1 20:02:28.027: INFO: Waiting up to 5m0s for pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" in namespace "pods-9585" to be "running"
    May  1 20:02:28.045: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95": Phase="Running", Reason="", readiness=true. Elapsed: 18.189484ms
    May  1 20:02:28.045: INFO: Pod "pod-update-8f302532-3b8e-4e10-a509-bb3a56decf95" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/01/23 20:02:28.045
    May  1 20:02:28.064: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 20:02:28.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9585" for this suite. 05/01/23 20:02:28.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:02:28.192
May  1 20:02:28.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename taint-multiple-pods 05/01/23 20:02:28.195
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:28.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:28.397
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
May  1 20:02:28.435: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 20:03:28.831: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
May  1 20:03:28.874: INFO: Starting informer...
STEP: Starting pods... 05/01/23 20:03:28.874
May  1 20:03:29.198: INFO: Pod1 is running on 10.45.145.124. Tainting Node
May  1 20:03:29.483: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5778" to be "running"
May  1 20:03:29.510: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.153181ms
May  1 20:03:31.531: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.04858736s
May  1 20:03:31.531: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May  1 20:03:31.531: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5778" to be "running"
May  1 20:03:31.552: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.75303ms
May  1 20:03:33.576: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044932056s
May  1 20:03:33.576: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May  1 20:03:33.576: INFO: Pod2 is running on 10.45.145.124. Tainting Node
STEP: Trying to apply a taint on the Node 05/01/23 20:03:33.577
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 20:03:33.663
STEP: Waiting for Pod1 and Pod2 to be deleted 05/01/23 20:03:33.682
May  1 20:03:40.953: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May  1 20:04:00.977: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 20:04:01.043
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
May  1 20:04:01.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5778" for this suite. 05/01/23 20:04:01.088
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":211,"skipped":3926,"failed":0}
------------------------------
• [SLOW TEST] [92.972 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:02:28.192
    May  1 20:02:28.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename taint-multiple-pods 05/01/23 20:02:28.195
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:02:28.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:02:28.397
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    May  1 20:02:28.435: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 20:03:28.831: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    May  1 20:03:28.874: INFO: Starting informer...
    STEP: Starting pods... 05/01/23 20:03:28.874
    May  1 20:03:29.198: INFO: Pod1 is running on 10.45.145.124. Tainting Node
    May  1 20:03:29.483: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5778" to be "running"
    May  1 20:03:29.510: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 27.153181ms
    May  1 20:03:31.531: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.04858736s
    May  1 20:03:31.531: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May  1 20:03:31.531: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5778" to be "running"
    May  1 20:03:31.552: INFO: Pod "taint-eviction-b2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.75303ms
    May  1 20:03:33.576: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.044932056s
    May  1 20:03:33.576: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May  1 20:03:33.576: INFO: Pod2 is running on 10.45.145.124. Tainting Node
    STEP: Trying to apply a taint on the Node 05/01/23 20:03:33.577
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 20:03:33.663
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/01/23 20:03:33.682
    May  1 20:03:40.953: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May  1 20:04:00.977: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/01/23 20:04:01.043
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:04:01.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5778" for this suite. 05/01/23 20:04:01.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:04:01.169
May  1 20:04:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:04:01.172
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:04:01.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:04:01.3
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/01/23 20:04:01.335
May  1 20:04:01.456: INFO: Waiting up to 5m0s for pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86" in namespace "emptydir-6774" to be "Succeeded or Failed"
May  1 20:04:01.517: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 61.151043ms
May  1 20:04:03.544: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087421746s
May  1 20:04:05.547: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091255248s
May  1 20:04:07.543: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086446553s
May  1 20:04:09.553: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096515125s
STEP: Saw pod success 05/01/23 20:04:09.553
May  1 20:04:09.553: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86" satisfied condition "Succeeded or Failed"
May  1 20:04:09.616: INFO: Trying to get logs from node 10.45.145.124 pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 container test-container: <nil>
STEP: delete the pod 05/01/23 20:04:09.871
May  1 20:04:09.959: INFO: Waiting for pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 to disappear
May  1 20:04:10.003: INFO: Pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:04:10.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6774" for this suite. 05/01/23 20:04:10.054
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":212,"skipped":3940,"failed":0}
------------------------------
• [SLOW TEST] [8.933 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:04:01.169
    May  1 20:04:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:04:01.172
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:04:01.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:04:01.3
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/01/23 20:04:01.335
    May  1 20:04:01.456: INFO: Waiting up to 5m0s for pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86" in namespace "emptydir-6774" to be "Succeeded or Failed"
    May  1 20:04:01.517: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 61.151043ms
    May  1 20:04:03.544: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087421746s
    May  1 20:04:05.547: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091255248s
    May  1 20:04:07.543: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.086446553s
    May  1 20:04:09.553: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096515125s
    STEP: Saw pod success 05/01/23 20:04:09.553
    May  1 20:04:09.553: INFO: Pod "pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86" satisfied condition "Succeeded or Failed"
    May  1 20:04:09.616: INFO: Trying to get logs from node 10.45.145.124 pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 container test-container: <nil>
    STEP: delete the pod 05/01/23 20:04:09.871
    May  1 20:04:09.959: INFO: Waiting for pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 to disappear
    May  1 20:04:10.003: INFO: Pod pod-98a12cea-ecac-42a7-ad5f-4795e09d5b86 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:04:10.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6774" for this suite. 05/01/23 20:04:10.054
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:04:10.108
May  1 20:04:10.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 20:04:10.114
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:04:10.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:04:10.302
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/01/23 20:04:10.402
STEP: delete the rc 05/01/23 20:04:15.476
STEP: wait for the rc to be deleted 05/01/23 20:04:15.571
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/01/23 20:04:20.61
STEP: Gathering metrics 05/01/23 20:04:50.68
W0501 20:04:50.732578      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 20:04:50.732: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  1 20:04:50.732: INFO: Deleting pod "simpletest.rc-27zfp" in namespace "gc-2763"
May  1 20:04:50.823: INFO: Deleting pod "simpletest.rc-29qwt" in namespace "gc-2763"
May  1 20:04:50.897: INFO: Deleting pod "simpletest.rc-2b8sf" in namespace "gc-2763"
May  1 20:04:50.985: INFO: Deleting pod "simpletest.rc-2bdg8" in namespace "gc-2763"
May  1 20:04:51.064: INFO: Deleting pod "simpletest.rc-42lrx" in namespace "gc-2763"
May  1 20:04:51.270: INFO: Deleting pod "simpletest.rc-46sp6" in namespace "gc-2763"
May  1 20:04:51.364: INFO: Deleting pod "simpletest.rc-46t7z" in namespace "gc-2763"
May  1 20:04:51.465: INFO: Deleting pod "simpletest.rc-4jwhg" in namespace "gc-2763"
May  1 20:04:51.577: INFO: Deleting pod "simpletest.rc-4jwm9" in namespace "gc-2763"
May  1 20:04:51.704: INFO: Deleting pod "simpletest.rc-5d5kx" in namespace "gc-2763"
May  1 20:04:51.811: INFO: Deleting pod "simpletest.rc-5hqqs" in namespace "gc-2763"
May  1 20:04:51.931: INFO: Deleting pod "simpletest.rc-5vlcm" in namespace "gc-2763"
May  1 20:04:52.012: INFO: Deleting pod "simpletest.rc-5wclc" in namespace "gc-2763"
May  1 20:04:52.134: INFO: Deleting pod "simpletest.rc-6489w" in namespace "gc-2763"
May  1 20:04:52.212: INFO: Deleting pod "simpletest.rc-67q5k" in namespace "gc-2763"
May  1 20:04:52.482: INFO: Deleting pod "simpletest.rc-6lrvl" in namespace "gc-2763"
May  1 20:04:52.641: INFO: Deleting pod "simpletest.rc-6mdqb" in namespace "gc-2763"
May  1 20:04:52.716: INFO: Deleting pod "simpletest.rc-6v5m2" in namespace "gc-2763"
May  1 20:04:52.856: INFO: Deleting pod "simpletest.rc-755jj" in namespace "gc-2763"
May  1 20:04:52.969: INFO: Deleting pod "simpletest.rc-78jdh" in namespace "gc-2763"
May  1 20:04:53.074: INFO: Deleting pod "simpletest.rc-7z4dv" in namespace "gc-2763"
May  1 20:04:53.178: INFO: Deleting pod "simpletest.rc-8vhhm" in namespace "gc-2763"
May  1 20:04:53.255: INFO: Deleting pod "simpletest.rc-8xhhq" in namespace "gc-2763"
May  1 20:04:53.337: INFO: Deleting pod "simpletest.rc-988fw" in namespace "gc-2763"
May  1 20:04:53.523: INFO: Deleting pod "simpletest.rc-98kkt" in namespace "gc-2763"
May  1 20:04:53.705: INFO: Deleting pod "simpletest.rc-bctf7" in namespace "gc-2763"
May  1 20:04:53.836: INFO: Deleting pod "simpletest.rc-bj29h" in namespace "gc-2763"
May  1 20:04:54.014: INFO: Deleting pod "simpletest.rc-bjw7q" in namespace "gc-2763"
May  1 20:04:54.167: INFO: Deleting pod "simpletest.rc-bwhkf" in namespace "gc-2763"
May  1 20:04:54.276: INFO: Deleting pod "simpletest.rc-c4lgp" in namespace "gc-2763"
May  1 20:04:54.421: INFO: Deleting pod "simpletest.rc-c4vjj" in namespace "gc-2763"
May  1 20:04:54.605: INFO: Deleting pod "simpletest.rc-cbfbt" in namespace "gc-2763"
May  1 20:04:54.693: INFO: Deleting pod "simpletest.rc-cjdml" in namespace "gc-2763"
May  1 20:04:54.776: INFO: Deleting pod "simpletest.rc-cl7r5" in namespace "gc-2763"
May  1 20:04:54.882: INFO: Deleting pod "simpletest.rc-clv5t" in namespace "gc-2763"
May  1 20:04:55.011: INFO: Deleting pod "simpletest.rc-cvcmg" in namespace "gc-2763"
May  1 20:04:55.115: INFO: Deleting pod "simpletest.rc-dqs8n" in namespace "gc-2763"
May  1 20:04:55.192: INFO: Deleting pod "simpletest.rc-f7g6v" in namespace "gc-2763"
May  1 20:04:55.319: INFO: Deleting pod "simpletest.rc-fccng" in namespace "gc-2763"
May  1 20:04:55.473: INFO: Deleting pod "simpletest.rc-fd4g5" in namespace "gc-2763"
May  1 20:04:55.542: INFO: Deleting pod "simpletest.rc-ffqc4" in namespace "gc-2763"
May  1 20:04:55.609: INFO: Deleting pod "simpletest.rc-fgvls" in namespace "gc-2763"
May  1 20:04:55.670: INFO: Deleting pod "simpletest.rc-fh5zg" in namespace "gc-2763"
May  1 20:04:55.739: INFO: Deleting pod "simpletest.rc-flrqd" in namespace "gc-2763"
May  1 20:04:55.793: INFO: Deleting pod "simpletest.rc-fxj79" in namespace "gc-2763"
May  1 20:04:55.855: INFO: Deleting pod "simpletest.rc-gld24" in namespace "gc-2763"
May  1 20:04:55.933: INFO: Deleting pod "simpletest.rc-hf9b5" in namespace "gc-2763"
May  1 20:04:56.008: INFO: Deleting pod "simpletest.rc-hgg98" in namespace "gc-2763"
May  1 20:04:56.104: INFO: Deleting pod "simpletest.rc-hjsrc" in namespace "gc-2763"
May  1 20:04:56.183: INFO: Deleting pod "simpletest.rc-j4mjk" in namespace "gc-2763"
May  1 20:04:56.254: INFO: Deleting pod "simpletest.rc-j8tml" in namespace "gc-2763"
May  1 20:04:56.339: INFO: Deleting pod "simpletest.rc-jb77f" in namespace "gc-2763"
May  1 20:04:56.479: INFO: Deleting pod "simpletest.rc-js896" in namespace "gc-2763"
May  1 20:04:56.620: INFO: Deleting pod "simpletest.rc-jzlxd" in namespace "gc-2763"
May  1 20:04:56.748: INFO: Deleting pod "simpletest.rc-k77m9" in namespace "gc-2763"
May  1 20:04:56.956: INFO: Deleting pod "simpletest.rc-krbwr" in namespace "gc-2763"
May  1 20:04:57.142: INFO: Deleting pod "simpletest.rc-kth66" in namespace "gc-2763"
May  1 20:04:57.286: INFO: Deleting pod "simpletest.rc-l7jxg" in namespace "gc-2763"
May  1 20:04:57.601: INFO: Deleting pod "simpletest.rc-lzbv5" in namespace "gc-2763"
May  1 20:04:57.683: INFO: Deleting pod "simpletest.rc-m6st5" in namespace "gc-2763"
May  1 20:04:57.764: INFO: Deleting pod "simpletest.rc-m7z6g" in namespace "gc-2763"
May  1 20:04:57.844: INFO: Deleting pod "simpletest.rc-mcn2f" in namespace "gc-2763"
May  1 20:04:57.955: INFO: Deleting pod "simpletest.rc-mdcwl" in namespace "gc-2763"
May  1 20:04:58.085: INFO: Deleting pod "simpletest.rc-mjqq5" in namespace "gc-2763"
May  1 20:04:58.386: INFO: Deleting pod "simpletest.rc-mtwdd" in namespace "gc-2763"
May  1 20:04:58.576: INFO: Deleting pod "simpletest.rc-mxrxc" in namespace "gc-2763"
May  1 20:04:58.759: INFO: Deleting pod "simpletest.rc-nhpc5" in namespace "gc-2763"
May  1 20:04:58.953: INFO: Deleting pod "simpletest.rc-nmf57" in namespace "gc-2763"
May  1 20:04:59.088: INFO: Deleting pod "simpletest.rc-ntbqm" in namespace "gc-2763"
May  1 20:04:59.203: INFO: Deleting pod "simpletest.rc-nv46j" in namespace "gc-2763"
May  1 20:04:59.274: INFO: Deleting pod "simpletest.rc-pc95m" in namespace "gc-2763"
May  1 20:04:59.354: INFO: Deleting pod "simpletest.rc-qc7n6" in namespace "gc-2763"
May  1 20:04:59.462: INFO: Deleting pod "simpletest.rc-qll8c" in namespace "gc-2763"
May  1 20:04:59.632: INFO: Deleting pod "simpletest.rc-qzh69" in namespace "gc-2763"
May  1 20:04:59.795: INFO: Deleting pod "simpletest.rc-r5b2m" in namespace "gc-2763"
May  1 20:04:59.875: INFO: Deleting pod "simpletest.rc-rcl4j" in namespace "gc-2763"
May  1 20:04:59.989: INFO: Deleting pod "simpletest.rc-rxqwx" in namespace "gc-2763"
May  1 20:05:00.083: INFO: Deleting pod "simpletest.rc-rzdlx" in namespace "gc-2763"
May  1 20:05:00.174: INFO: Deleting pod "simpletest.rc-spww6" in namespace "gc-2763"
May  1 20:05:00.266: INFO: Deleting pod "simpletest.rc-sr96l" in namespace "gc-2763"
May  1 20:05:00.367: INFO: Deleting pod "simpletest.rc-stsg4" in namespace "gc-2763"
May  1 20:05:00.502: INFO: Deleting pod "simpletest.rc-sv55m" in namespace "gc-2763"
May  1 20:05:00.662: INFO: Deleting pod "simpletest.rc-sw486" in namespace "gc-2763"
May  1 20:05:00.757: INFO: Deleting pod "simpletest.rc-sxs46" in namespace "gc-2763"
May  1 20:05:00.896: INFO: Deleting pod "simpletest.rc-tl94l" in namespace "gc-2763"
May  1 20:05:00.973: INFO: Deleting pod "simpletest.rc-tsphq" in namespace "gc-2763"
May  1 20:05:01.106: INFO: Deleting pod "simpletest.rc-v9v2p" in namespace "gc-2763"
May  1 20:05:01.226: INFO: Deleting pod "simpletest.rc-vljnk" in namespace "gc-2763"
May  1 20:05:01.379: INFO: Deleting pod "simpletest.rc-vmrzs" in namespace "gc-2763"
May  1 20:05:01.499: INFO: Deleting pod "simpletest.rc-vzpz2" in namespace "gc-2763"
May  1 20:05:01.565: INFO: Deleting pod "simpletest.rc-w8cjx" in namespace "gc-2763"
May  1 20:05:01.647: INFO: Deleting pod "simpletest.rc-w8p92" in namespace "gc-2763"
May  1 20:05:01.792: INFO: Deleting pod "simpletest.rc-wnvsk" in namespace "gc-2763"
May  1 20:05:01.875: INFO: Deleting pod "simpletest.rc-xcgq2" in namespace "gc-2763"
May  1 20:05:01.974: INFO: Deleting pod "simpletest.rc-xr7xg" in namespace "gc-2763"
May  1 20:05:02.074: INFO: Deleting pod "simpletest.rc-xsgcz" in namespace "gc-2763"
May  1 20:05:02.154: INFO: Deleting pod "simpletest.rc-z45zn" in namespace "gc-2763"
May  1 20:05:02.237: INFO: Deleting pod "simpletest.rc-z9bk9" in namespace "gc-2763"
May  1 20:05:02.334: INFO: Deleting pod "simpletest.rc-z9cx8" in namespace "gc-2763"
May  1 20:05:02.422: INFO: Deleting pod "simpletest.rc-z9sgw" in namespace "gc-2763"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 20:05:02.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2763" for this suite. 05/01/23 20:05:02.721
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":213,"skipped":3941,"failed":0}
------------------------------
• [SLOW TEST] [52.698 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:04:10.108
    May  1 20:04:10.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 20:04:10.114
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:04:10.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:04:10.302
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/01/23 20:04:10.402
    STEP: delete the rc 05/01/23 20:04:15.476
    STEP: wait for the rc to be deleted 05/01/23 20:04:15.571
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/01/23 20:04:20.61
    STEP: Gathering metrics 05/01/23 20:04:50.68
    W0501 20:04:50.732578      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 20:04:50.732: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  1 20:04:50.732: INFO: Deleting pod "simpletest.rc-27zfp" in namespace "gc-2763"
    May  1 20:04:50.823: INFO: Deleting pod "simpletest.rc-29qwt" in namespace "gc-2763"
    May  1 20:04:50.897: INFO: Deleting pod "simpletest.rc-2b8sf" in namespace "gc-2763"
    May  1 20:04:50.985: INFO: Deleting pod "simpletest.rc-2bdg8" in namespace "gc-2763"
    May  1 20:04:51.064: INFO: Deleting pod "simpletest.rc-42lrx" in namespace "gc-2763"
    May  1 20:04:51.270: INFO: Deleting pod "simpletest.rc-46sp6" in namespace "gc-2763"
    May  1 20:04:51.364: INFO: Deleting pod "simpletest.rc-46t7z" in namespace "gc-2763"
    May  1 20:04:51.465: INFO: Deleting pod "simpletest.rc-4jwhg" in namespace "gc-2763"
    May  1 20:04:51.577: INFO: Deleting pod "simpletest.rc-4jwm9" in namespace "gc-2763"
    May  1 20:04:51.704: INFO: Deleting pod "simpletest.rc-5d5kx" in namespace "gc-2763"
    May  1 20:04:51.811: INFO: Deleting pod "simpletest.rc-5hqqs" in namespace "gc-2763"
    May  1 20:04:51.931: INFO: Deleting pod "simpletest.rc-5vlcm" in namespace "gc-2763"
    May  1 20:04:52.012: INFO: Deleting pod "simpletest.rc-5wclc" in namespace "gc-2763"
    May  1 20:04:52.134: INFO: Deleting pod "simpletest.rc-6489w" in namespace "gc-2763"
    May  1 20:04:52.212: INFO: Deleting pod "simpletest.rc-67q5k" in namespace "gc-2763"
    May  1 20:04:52.482: INFO: Deleting pod "simpletest.rc-6lrvl" in namespace "gc-2763"
    May  1 20:04:52.641: INFO: Deleting pod "simpletest.rc-6mdqb" in namespace "gc-2763"
    May  1 20:04:52.716: INFO: Deleting pod "simpletest.rc-6v5m2" in namespace "gc-2763"
    May  1 20:04:52.856: INFO: Deleting pod "simpletest.rc-755jj" in namespace "gc-2763"
    May  1 20:04:52.969: INFO: Deleting pod "simpletest.rc-78jdh" in namespace "gc-2763"
    May  1 20:04:53.074: INFO: Deleting pod "simpletest.rc-7z4dv" in namespace "gc-2763"
    May  1 20:04:53.178: INFO: Deleting pod "simpletest.rc-8vhhm" in namespace "gc-2763"
    May  1 20:04:53.255: INFO: Deleting pod "simpletest.rc-8xhhq" in namespace "gc-2763"
    May  1 20:04:53.337: INFO: Deleting pod "simpletest.rc-988fw" in namespace "gc-2763"
    May  1 20:04:53.523: INFO: Deleting pod "simpletest.rc-98kkt" in namespace "gc-2763"
    May  1 20:04:53.705: INFO: Deleting pod "simpletest.rc-bctf7" in namespace "gc-2763"
    May  1 20:04:53.836: INFO: Deleting pod "simpletest.rc-bj29h" in namespace "gc-2763"
    May  1 20:04:54.014: INFO: Deleting pod "simpletest.rc-bjw7q" in namespace "gc-2763"
    May  1 20:04:54.167: INFO: Deleting pod "simpletest.rc-bwhkf" in namespace "gc-2763"
    May  1 20:04:54.276: INFO: Deleting pod "simpletest.rc-c4lgp" in namespace "gc-2763"
    May  1 20:04:54.421: INFO: Deleting pod "simpletest.rc-c4vjj" in namespace "gc-2763"
    May  1 20:04:54.605: INFO: Deleting pod "simpletest.rc-cbfbt" in namespace "gc-2763"
    May  1 20:04:54.693: INFO: Deleting pod "simpletest.rc-cjdml" in namespace "gc-2763"
    May  1 20:04:54.776: INFO: Deleting pod "simpletest.rc-cl7r5" in namespace "gc-2763"
    May  1 20:04:54.882: INFO: Deleting pod "simpletest.rc-clv5t" in namespace "gc-2763"
    May  1 20:04:55.011: INFO: Deleting pod "simpletest.rc-cvcmg" in namespace "gc-2763"
    May  1 20:04:55.115: INFO: Deleting pod "simpletest.rc-dqs8n" in namespace "gc-2763"
    May  1 20:04:55.192: INFO: Deleting pod "simpletest.rc-f7g6v" in namespace "gc-2763"
    May  1 20:04:55.319: INFO: Deleting pod "simpletest.rc-fccng" in namespace "gc-2763"
    May  1 20:04:55.473: INFO: Deleting pod "simpletest.rc-fd4g5" in namespace "gc-2763"
    May  1 20:04:55.542: INFO: Deleting pod "simpletest.rc-ffqc4" in namespace "gc-2763"
    May  1 20:04:55.609: INFO: Deleting pod "simpletest.rc-fgvls" in namespace "gc-2763"
    May  1 20:04:55.670: INFO: Deleting pod "simpletest.rc-fh5zg" in namespace "gc-2763"
    May  1 20:04:55.739: INFO: Deleting pod "simpletest.rc-flrqd" in namespace "gc-2763"
    May  1 20:04:55.793: INFO: Deleting pod "simpletest.rc-fxj79" in namespace "gc-2763"
    May  1 20:04:55.855: INFO: Deleting pod "simpletest.rc-gld24" in namespace "gc-2763"
    May  1 20:04:55.933: INFO: Deleting pod "simpletest.rc-hf9b5" in namespace "gc-2763"
    May  1 20:04:56.008: INFO: Deleting pod "simpletest.rc-hgg98" in namespace "gc-2763"
    May  1 20:04:56.104: INFO: Deleting pod "simpletest.rc-hjsrc" in namespace "gc-2763"
    May  1 20:04:56.183: INFO: Deleting pod "simpletest.rc-j4mjk" in namespace "gc-2763"
    May  1 20:04:56.254: INFO: Deleting pod "simpletest.rc-j8tml" in namespace "gc-2763"
    May  1 20:04:56.339: INFO: Deleting pod "simpletest.rc-jb77f" in namespace "gc-2763"
    May  1 20:04:56.479: INFO: Deleting pod "simpletest.rc-js896" in namespace "gc-2763"
    May  1 20:04:56.620: INFO: Deleting pod "simpletest.rc-jzlxd" in namespace "gc-2763"
    May  1 20:04:56.748: INFO: Deleting pod "simpletest.rc-k77m9" in namespace "gc-2763"
    May  1 20:04:56.956: INFO: Deleting pod "simpletest.rc-krbwr" in namespace "gc-2763"
    May  1 20:04:57.142: INFO: Deleting pod "simpletest.rc-kth66" in namespace "gc-2763"
    May  1 20:04:57.286: INFO: Deleting pod "simpletest.rc-l7jxg" in namespace "gc-2763"
    May  1 20:04:57.601: INFO: Deleting pod "simpletest.rc-lzbv5" in namespace "gc-2763"
    May  1 20:04:57.683: INFO: Deleting pod "simpletest.rc-m6st5" in namespace "gc-2763"
    May  1 20:04:57.764: INFO: Deleting pod "simpletest.rc-m7z6g" in namespace "gc-2763"
    May  1 20:04:57.844: INFO: Deleting pod "simpletest.rc-mcn2f" in namespace "gc-2763"
    May  1 20:04:57.955: INFO: Deleting pod "simpletest.rc-mdcwl" in namespace "gc-2763"
    May  1 20:04:58.085: INFO: Deleting pod "simpletest.rc-mjqq5" in namespace "gc-2763"
    May  1 20:04:58.386: INFO: Deleting pod "simpletest.rc-mtwdd" in namespace "gc-2763"
    May  1 20:04:58.576: INFO: Deleting pod "simpletest.rc-mxrxc" in namespace "gc-2763"
    May  1 20:04:58.759: INFO: Deleting pod "simpletest.rc-nhpc5" in namespace "gc-2763"
    May  1 20:04:58.953: INFO: Deleting pod "simpletest.rc-nmf57" in namespace "gc-2763"
    May  1 20:04:59.088: INFO: Deleting pod "simpletest.rc-ntbqm" in namespace "gc-2763"
    May  1 20:04:59.203: INFO: Deleting pod "simpletest.rc-nv46j" in namespace "gc-2763"
    May  1 20:04:59.274: INFO: Deleting pod "simpletest.rc-pc95m" in namespace "gc-2763"
    May  1 20:04:59.354: INFO: Deleting pod "simpletest.rc-qc7n6" in namespace "gc-2763"
    May  1 20:04:59.462: INFO: Deleting pod "simpletest.rc-qll8c" in namespace "gc-2763"
    May  1 20:04:59.632: INFO: Deleting pod "simpletest.rc-qzh69" in namespace "gc-2763"
    May  1 20:04:59.795: INFO: Deleting pod "simpletest.rc-r5b2m" in namespace "gc-2763"
    May  1 20:04:59.875: INFO: Deleting pod "simpletest.rc-rcl4j" in namespace "gc-2763"
    May  1 20:04:59.989: INFO: Deleting pod "simpletest.rc-rxqwx" in namespace "gc-2763"
    May  1 20:05:00.083: INFO: Deleting pod "simpletest.rc-rzdlx" in namespace "gc-2763"
    May  1 20:05:00.174: INFO: Deleting pod "simpletest.rc-spww6" in namespace "gc-2763"
    May  1 20:05:00.266: INFO: Deleting pod "simpletest.rc-sr96l" in namespace "gc-2763"
    May  1 20:05:00.367: INFO: Deleting pod "simpletest.rc-stsg4" in namespace "gc-2763"
    May  1 20:05:00.502: INFO: Deleting pod "simpletest.rc-sv55m" in namespace "gc-2763"
    May  1 20:05:00.662: INFO: Deleting pod "simpletest.rc-sw486" in namespace "gc-2763"
    May  1 20:05:00.757: INFO: Deleting pod "simpletest.rc-sxs46" in namespace "gc-2763"
    May  1 20:05:00.896: INFO: Deleting pod "simpletest.rc-tl94l" in namespace "gc-2763"
    May  1 20:05:00.973: INFO: Deleting pod "simpletest.rc-tsphq" in namespace "gc-2763"
    May  1 20:05:01.106: INFO: Deleting pod "simpletest.rc-v9v2p" in namespace "gc-2763"
    May  1 20:05:01.226: INFO: Deleting pod "simpletest.rc-vljnk" in namespace "gc-2763"
    May  1 20:05:01.379: INFO: Deleting pod "simpletest.rc-vmrzs" in namespace "gc-2763"
    May  1 20:05:01.499: INFO: Deleting pod "simpletest.rc-vzpz2" in namespace "gc-2763"
    May  1 20:05:01.565: INFO: Deleting pod "simpletest.rc-w8cjx" in namespace "gc-2763"
    May  1 20:05:01.647: INFO: Deleting pod "simpletest.rc-w8p92" in namespace "gc-2763"
    May  1 20:05:01.792: INFO: Deleting pod "simpletest.rc-wnvsk" in namespace "gc-2763"
    May  1 20:05:01.875: INFO: Deleting pod "simpletest.rc-xcgq2" in namespace "gc-2763"
    May  1 20:05:01.974: INFO: Deleting pod "simpletest.rc-xr7xg" in namespace "gc-2763"
    May  1 20:05:02.074: INFO: Deleting pod "simpletest.rc-xsgcz" in namespace "gc-2763"
    May  1 20:05:02.154: INFO: Deleting pod "simpletest.rc-z45zn" in namespace "gc-2763"
    May  1 20:05:02.237: INFO: Deleting pod "simpletest.rc-z9bk9" in namespace "gc-2763"
    May  1 20:05:02.334: INFO: Deleting pod "simpletest.rc-z9cx8" in namespace "gc-2763"
    May  1 20:05:02.422: INFO: Deleting pod "simpletest.rc-z9sgw" in namespace "gc-2763"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 20:05:02.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2763" for this suite. 05/01/23 20:05:02.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:03.104
May  1 20:05:03.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:05:03.108
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:03.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:03.317
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:05:03.426
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:05:05.52
STEP: Deploying the webhook pod 05/01/23 20:05:05.607
STEP: Wait for the deployment to be ready 05/01/23 20:05:05.683
May  1 20:05:05.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:05:07.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 20:05:09.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 20:05:11.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  1 20:05:13.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:05:15.858
STEP: Verifying the service has paired with the endpoint 05/01/23 20:05:15.937
May  1 20:05:16.942: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/01/23 20:05:16.961
STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:16.961
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/01/23 20:05:17.139
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/01/23 20:05:18.195
STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:18.196
STEP: Having no error when timeout is longer than webhook latency 05/01/23 20:05:19.362
STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:19.363
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/01/23 20:05:24.864
STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:24.864
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:05:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7767" for this suite. 05/01/23 20:05:30.068
STEP: Destroying namespace "webhook-7767-markers" for this suite. 05/01/23 20:05:30.1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":214,"skipped":3978,"failed":0}
------------------------------
• [SLOW TEST] [27.275 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:03.104
    May  1 20:05:03.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:05:03.108
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:03.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:03.317
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:05:03.426
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:05:05.52
    STEP: Deploying the webhook pod 05/01/23 20:05:05.607
    STEP: Wait for the deployment to be ready 05/01/23 20:05:05.683
    May  1 20:05:05.764: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:05:07.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 20:05:09.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 20:05:11.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May  1 20:05:13.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:05:15.858
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:05:15.937
    May  1 20:05:16.942: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/01/23 20:05:16.961
    STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:16.961
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/01/23 20:05:17.139
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/01/23 20:05:18.195
    STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:18.196
    STEP: Having no error when timeout is longer than webhook latency 05/01/23 20:05:19.362
    STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:19.363
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/01/23 20:05:24.864
    STEP: Registering slow webhook via the AdmissionRegistration API 05/01/23 20:05:24.864
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:05:30.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7767" for this suite. 05/01/23 20:05:30.068
    STEP: Destroying namespace "webhook-7767-markers" for this suite. 05/01/23 20:05:30.1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:30.382
May  1 20:05:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 20:05:30.385
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:30.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:30.547
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/01/23 20:05:30.654
May  1 20:05:30.654: INFO: Creating simple deployment test-deployment-hq7fd
May  1 20:05:30.766: INFO: deployment "test-deployment-hq7fd" doesn't have the required revision set
May  1 20:05:32.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-hq7fd-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 05/01/23 20:05:34.935
May  1 20:05:34.963: INFO: Deployment test-deployment-hq7fd has Conditions: [{Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 05/01/23 20:05:34.964
May  1 20:05:35.019: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hq7fd-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/01/23 20:05:35.019
May  1 20:05:35.041: INFO: Observed &Deployment event: ADDED
May  1 20:05:35.041: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
May  1 20:05:35.042: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.042: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  1 20:05:35.043: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hq7fd-777898ffcc" is progressing.}
May  1 20:05:35.044: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.044: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  1 20:05:35.044: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
May  1 20:05:35.046: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.047: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  1 20:05:35.047: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
May  1 20:05:35.047: INFO: Found Deployment test-deployment-hq7fd in namespace deployment-9975 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  1 20:05:35.047: INFO: Deployment test-deployment-hq7fd has an updated status
STEP: patching the Statefulset Status 05/01/23 20:05:35.047
May  1 20:05:35.048: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May  1 20:05:35.085: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/01/23 20:05:35.085
May  1 20:05:35.096: INFO: Observed &Deployment event: ADDED
May  1 20:05:35.096: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hq7fd-777898ffcc" is progressing.}
May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
May  1 20:05:35.098: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May  1 20:05:35.098: INFO: Observed &Deployment event: MODIFIED
May  1 20:05:35.098: INFO: Found deployment test-deployment-hq7fd in namespace deployment-9975 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May  1 20:05:35.098: INFO: Deployment test-deployment-hq7fd has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 20:05:35.116: INFO: Deployment "test-deployment-hq7fd":
&Deployment{ObjectMeta:{test-deployment-hq7fd  deployment-9975  63c438c0-1879-453e-863b-be91e6f63fa4 111482 1 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-01 20:05:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-01 20:05:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d89fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hq7fd-777898ffcc",LastUpdateTime:2023-05-01 20:05:35 +0000 UTC,LastTransitionTime:2023-05-01 20:05:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May  1 20:05:35.150: INFO: New ReplicaSet "test-deployment-hq7fd-777898ffcc" of Deployment "test-deployment-hq7fd":
&ReplicaSet{ObjectMeta:{test-deployment-hq7fd-777898ffcc  deployment-9975  3273a7ce-a593-4ec5-8109-ce6b68b87061 111467 1 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hq7fd 63c438c0-1879-453e-863b-be91e6f63fa4 0xc004a12b07 0xc004a12b08}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63c438c0-1879-453e-863b-be91e6f63fa4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:05:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a12bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May  1 20:05:35.179: INFO: Pod "test-deployment-hq7fd-777898ffcc-bjmfr" is available:
&Pod{ObjectMeta:{test-deployment-hq7fd-777898ffcc-bjmfr test-deployment-hq7fd-777898ffcc- deployment-9975  0e15186c-3865-4a60-b364-2f9fb45d14ff 111466 0 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:e33b9da4aa1976ac37581eb1d93ec299d6700d51ff2f59328c2b8ce7b93f11f7 cni.projectcalico.org/podIP:172.30.42.110/32 cni.projectcalico.org/podIPs:172.30.42.110/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.110"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.110"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-hq7fd-777898ffcc 3273a7ce-a593-4ec5-8109-ce6b68b87061 0xc005db9457 0xc005db9458}] [] [{kube-controller-manager Update v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3273a7ce-a593-4ec5-8109-ce6b68b87061\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:05:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldt9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldt9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c50,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2thb4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.110,StartTime:2023-05-01 20:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bc4a1753cbba285fa15cb0d05f56e293ce59b4d601b7ebed8551bb20daadc2f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 20:05:35.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9975" for this suite. 05/01/23 20:05:35.265
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":215,"skipped":3994,"failed":0}
------------------------------
• [4.948 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:30.382
    May  1 20:05:30.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 20:05:30.385
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:30.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:30.547
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/01/23 20:05:30.654
    May  1 20:05:30.654: INFO: Creating simple deployment test-deployment-hq7fd
    May  1 20:05:30.766: INFO: deployment "test-deployment-hq7fd" doesn't have the required revision set
    May  1 20:05:32.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-hq7fd-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 05/01/23 20:05:34.935
    May  1 20:05:34.963: INFO: Deployment test-deployment-hq7fd has Conditions: [{Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 05/01/23 20:05:34.964
    May  1 20:05:35.019: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 5, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 5, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-hq7fd-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/01/23 20:05:35.019
    May  1 20:05:35.041: INFO: Observed &Deployment event: ADDED
    May  1 20:05:35.041: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
    May  1 20:05:35.042: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.042: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
    May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  1 20:05:35.043: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  1 20:05:35.043: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hq7fd-777898ffcc" is progressing.}
    May  1 20:05:35.044: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.044: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  1 20:05:35.044: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
    May  1 20:05:35.046: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.047: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  1 20:05:35.047: INFO: Observed Deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
    May  1 20:05:35.047: INFO: Found Deployment test-deployment-hq7fd in namespace deployment-9975 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  1 20:05:35.047: INFO: Deployment test-deployment-hq7fd has an updated status
    STEP: patching the Statefulset Status 05/01/23 20:05:35.047
    May  1 20:05:35.048: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May  1 20:05:35.085: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/01/23 20:05:35.085
    May  1 20:05:35.096: INFO: Observed &Deployment event: ADDED
    May  1 20:05:35.096: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
    May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-hq7fd-777898ffcc"}
    May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May  1 20:05:35.097: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:30 +0000 UTC 2023-05-01 20:05:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-hq7fd-777898ffcc" is progressing.}
    May  1 20:05:35.097: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
    May  1 20:05:35.098: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:33 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-01 20:05:33 +0000 UTC 2023-05-01 20:05:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-hq7fd-777898ffcc" has successfully progressed.}
    May  1 20:05:35.098: INFO: Observed deployment test-deployment-hq7fd in namespace deployment-9975 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May  1 20:05:35.098: INFO: Observed &Deployment event: MODIFIED
    May  1 20:05:35.098: INFO: Found deployment test-deployment-hq7fd in namespace deployment-9975 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May  1 20:05:35.098: INFO: Deployment test-deployment-hq7fd has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 20:05:35.116: INFO: Deployment "test-deployment-hq7fd":
    &Deployment{ObjectMeta:{test-deployment-hq7fd  deployment-9975  63c438c0-1879-453e-863b-be91e6f63fa4 111482 1 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-01 20:05:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-01 20:05:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d89fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-hq7fd-777898ffcc",LastUpdateTime:2023-05-01 20:05:35 +0000 UTC,LastTransitionTime:2023-05-01 20:05:35 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May  1 20:05:35.150: INFO: New ReplicaSet "test-deployment-hq7fd-777898ffcc" of Deployment "test-deployment-hq7fd":
    &ReplicaSet{ObjectMeta:{test-deployment-hq7fd-777898ffcc  deployment-9975  3273a7ce-a593-4ec5-8109-ce6b68b87061 111467 1 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-hq7fd 63c438c0-1879-453e-863b-be91e6f63fa4 0xc004a12b07 0xc004a12b08}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63c438c0-1879-453e-863b-be91e6f63fa4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:05:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a12bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May  1 20:05:35.179: INFO: Pod "test-deployment-hq7fd-777898ffcc-bjmfr" is available:
    &Pod{ObjectMeta:{test-deployment-hq7fd-777898ffcc-bjmfr test-deployment-hq7fd-777898ffcc- deployment-9975  0e15186c-3865-4a60-b364-2f9fb45d14ff 111466 0 2023-05-01 20:05:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:e33b9da4aa1976ac37581eb1d93ec299d6700d51ff2f59328c2b8ce7b93f11f7 cni.projectcalico.org/podIP:172.30.42.110/32 cni.projectcalico.org/podIPs:172.30.42.110/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.110"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.110"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-hq7fd-777898ffcc 3273a7ce-a593-4ec5-8109-ce6b68b87061 0xc005db9457 0xc005db9458}] [] [{kube-controller-manager Update v1 2023-05-01 20:05:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3273a7ce-a593-4ec5-8109-ce6b68b87061\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:05:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:05:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ldt9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ldt9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c56,c50,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2thb4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:05:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.110,StartTime:2023-05-01 20:05:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:05:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://bc4a1753cbba285fa15cb0d05f56e293ce59b4d601b7ebed8551bb20daadc2f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 20:05:35.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9975" for this suite. 05/01/23 20:05:35.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:35.336
May  1 20:05:35.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 20:05:35.339
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:35.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:35.507
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6311.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6311.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/01/23 20:05:35.527
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6311.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6311.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/01/23 20:05:35.527
STEP: creating a pod to probe /etc/hosts 05/01/23 20:05:35.527
STEP: submitting the pod to kubernetes 05/01/23 20:05:35.528
May  1 20:05:35.624: INFO: Waiting up to 15m0s for pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657" in namespace "dns-6311" to be "running"
May  1 20:05:35.645: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Pending", Reason="", readiness=false. Elapsed: 20.74246ms
May  1 20:05:37.677: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052554266s
May  1 20:05:39.674: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Running", Reason="", readiness=true. Elapsed: 4.049736811s
May  1 20:05:39.674: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657" satisfied condition "running"
STEP: retrieving the pod 05/01/23 20:05:39.674
STEP: looking for the results for each expected name from probers 05/01/23 20:05:39.714
May  1 20:05:39.949: INFO: DNS probes using dns-6311/dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657 succeeded

STEP: deleting the pod 05/01/23 20:05:39.949
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 20:05:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6311" for this suite. 05/01/23 20:05:40.088
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":216,"skipped":4024,"failed":0}
------------------------------
• [4.786 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:35.336
    May  1 20:05:35.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 20:05:35.339
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:35.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:35.507
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6311.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6311.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/01/23 20:05:35.527
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6311.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6311.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/01/23 20:05:35.527
    STEP: creating a pod to probe /etc/hosts 05/01/23 20:05:35.527
    STEP: submitting the pod to kubernetes 05/01/23 20:05:35.528
    May  1 20:05:35.624: INFO: Waiting up to 15m0s for pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657" in namespace "dns-6311" to be "running"
    May  1 20:05:35.645: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Pending", Reason="", readiness=false. Elapsed: 20.74246ms
    May  1 20:05:37.677: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052554266s
    May  1 20:05:39.674: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657": Phase="Running", Reason="", readiness=true. Elapsed: 4.049736811s
    May  1 20:05:39.674: INFO: Pod "dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 20:05:39.674
    STEP: looking for the results for each expected name from probers 05/01/23 20:05:39.714
    May  1 20:05:39.949: INFO: DNS probes using dns-6311/dns-test-8d5edaa9-3e6a-435d-a578-f88c3c945657 succeeded

    STEP: deleting the pod 05/01/23 20:05:39.949
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 20:05:40.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6311" for this suite. 05/01/23 20:05:40.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:40.133
May  1 20:05:40.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 20:05:40.137
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:40.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:40.259
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 05/01/23 20:05:40.303
May  1 20:05:40.408: INFO: Waiting up to 5m0s for pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac" in namespace "var-expansion-5426" to be "Succeeded or Failed"
May  1 20:05:40.467: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 58.374389ms
May  1 20:05:42.502: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093290211s
May  1 20:05:44.515: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106762655s
May  1 20:05:46.496: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087122083s
STEP: Saw pod success 05/01/23 20:05:46.496
May  1 20:05:46.497: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac" satisfied condition "Succeeded or Failed"
May  1 20:05:46.516: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac container dapi-container: <nil>
STEP: delete the pod 05/01/23 20:05:46.61
May  1 20:05:46.665: INFO: Waiting for pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac to disappear
May  1 20:05:46.684: INFO: Pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 20:05:46.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5426" for this suite. 05/01/23 20:05:46.711
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":217,"skipped":4071,"failed":0}
------------------------------
• [SLOW TEST] [6.609 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:40.133
    May  1 20:05:40.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 20:05:40.137
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:40.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:40.259
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 05/01/23 20:05:40.303
    May  1 20:05:40.408: INFO: Waiting up to 5m0s for pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac" in namespace "var-expansion-5426" to be "Succeeded or Failed"
    May  1 20:05:40.467: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 58.374389ms
    May  1 20:05:42.502: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093290211s
    May  1 20:05:44.515: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106762655s
    May  1 20:05:46.496: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087122083s
    STEP: Saw pod success 05/01/23 20:05:46.496
    May  1 20:05:46.497: INFO: Pod "var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac" satisfied condition "Succeeded or Failed"
    May  1 20:05:46.516: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac container dapi-container: <nil>
    STEP: delete the pod 05/01/23 20:05:46.61
    May  1 20:05:46.665: INFO: Waiting for pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac to disappear
    May  1 20:05:46.684: INFO: Pod var-expansion-49539d6d-7211-45b3-9d3c-bfaba3b029ac no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 20:05:46.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5426" for this suite. 05/01/23 20:05:46.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:46.766
May  1 20:05:46.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename endpointslice 05/01/23 20:05:46.768
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:46.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:46.871
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 05/01/23 20:05:46.892
STEP: getting /apis/discovery.k8s.io 05/01/23 20:05:46.907
STEP: getting /apis/discovery.k8s.iov1 05/01/23 20:05:46.917
STEP: creating 05/01/23 20:05:46.929
STEP: getting 05/01/23 20:05:47.042
STEP: listing 05/01/23 20:05:47.073
STEP: watching 05/01/23 20:05:47.101
May  1 20:05:47.101: INFO: starting watch
STEP: cluster-wide listing 05/01/23 20:05:47.114
STEP: cluster-wide watching 05/01/23 20:05:47.159
May  1 20:05:47.160: INFO: starting watch
STEP: patching 05/01/23 20:05:47.165
STEP: updating 05/01/23 20:05:47.2
May  1 20:05:47.275: INFO: waiting for watch events with expected annotations
May  1 20:05:47.275: INFO: saw patched and updated annotations
STEP: deleting 05/01/23 20:05:47.275
STEP: deleting a collection 05/01/23 20:05:47.376
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  1 20:05:47.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3665" for this suite. 05/01/23 20:05:47.549
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":218,"skipped":4173,"failed":0}
------------------------------
• [0.809 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:46.766
    May  1 20:05:46.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename endpointslice 05/01/23 20:05:46.768
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:46.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:46.871
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 05/01/23 20:05:46.892
    STEP: getting /apis/discovery.k8s.io 05/01/23 20:05:46.907
    STEP: getting /apis/discovery.k8s.iov1 05/01/23 20:05:46.917
    STEP: creating 05/01/23 20:05:46.929
    STEP: getting 05/01/23 20:05:47.042
    STEP: listing 05/01/23 20:05:47.073
    STEP: watching 05/01/23 20:05:47.101
    May  1 20:05:47.101: INFO: starting watch
    STEP: cluster-wide listing 05/01/23 20:05:47.114
    STEP: cluster-wide watching 05/01/23 20:05:47.159
    May  1 20:05:47.160: INFO: starting watch
    STEP: patching 05/01/23 20:05:47.165
    STEP: updating 05/01/23 20:05:47.2
    May  1 20:05:47.275: INFO: waiting for watch events with expected annotations
    May  1 20:05:47.275: INFO: saw patched and updated annotations
    STEP: deleting 05/01/23 20:05:47.275
    STEP: deleting a collection 05/01/23 20:05:47.376
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  1 20:05:47.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3665" for this suite. 05/01/23 20:05:47.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:47.577
May  1 20:05:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:05:47.58
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:47.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:47.666
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
May  1 20:05:47.758: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-ff717e48-53c8-49ef-8ac7-df820623f1c5 05/01/23 20:05:47.758
STEP: Creating configMap with name cm-test-opt-upd-168a2d84-9024-4a13-a3af-2e2b2b097eef 05/01/23 20:05:47.793
STEP: Creating the pod 05/01/23 20:05:47.817
May  1 20:05:47.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb" in namespace "projected-1108" to be "running and ready"
May  1 20:05:47.954: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Pending", Reason="", readiness=false. Elapsed: 60.964228ms
May  1 20:05:47.955: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Pending, waiting for it to be Running (with Ready = true)
May  1 20:05:50.032: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138627167s
May  1 20:05:50.032: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Pending, waiting for it to be Running (with Ready = true)
May  1 20:05:51.976: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Running", Reason="", readiness=true. Elapsed: 4.082488931s
May  1 20:05:51.976: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Running (Ready = true)
May  1 20:05:51.976: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-ff717e48-53c8-49ef-8ac7-df820623f1c5 05/01/23 20:05:52.458
STEP: Updating configmap cm-test-opt-upd-168a2d84-9024-4a13-a3af-2e2b2b097eef 05/01/23 20:05:52.485
STEP: Creating configMap with name cm-test-opt-create-da065e23-499e-4bde-b815-d0fd5a6424d6 05/01/23 20:05:52.507
STEP: waiting to observe update in volume 05/01/23 20:05:52.527
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:05:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1108" for this suite. 05/01/23 20:05:54.908
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":219,"skipped":4188,"failed":0}
------------------------------
• [SLOW TEST] [7.380 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:47.577
    May  1 20:05:47.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:05:47.58
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:47.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:47.666
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    May  1 20:05:47.758: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-ff717e48-53c8-49ef-8ac7-df820623f1c5 05/01/23 20:05:47.758
    STEP: Creating configMap with name cm-test-opt-upd-168a2d84-9024-4a13-a3af-2e2b2b097eef 05/01/23 20:05:47.793
    STEP: Creating the pod 05/01/23 20:05:47.817
    May  1 20:05:47.893: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb" in namespace "projected-1108" to be "running and ready"
    May  1 20:05:47.954: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Pending", Reason="", readiness=false. Elapsed: 60.964228ms
    May  1 20:05:47.955: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:05:50.032: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138627167s
    May  1 20:05:50.032: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:05:51.976: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb": Phase="Running", Reason="", readiness=true. Elapsed: 4.082488931s
    May  1 20:05:51.976: INFO: The phase of Pod pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb is Running (Ready = true)
    May  1 20:05:51.976: INFO: Pod "pod-projected-configmaps-19b48fbf-d943-4de5-8c9a-99e051760efb" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-ff717e48-53c8-49ef-8ac7-df820623f1c5 05/01/23 20:05:52.458
    STEP: Updating configmap cm-test-opt-upd-168a2d84-9024-4a13-a3af-2e2b2b097eef 05/01/23 20:05:52.485
    STEP: Creating configMap with name cm-test-opt-create-da065e23-499e-4bde-b815-d0fd5a6424d6 05/01/23 20:05:52.507
    STEP: waiting to observe update in volume 05/01/23 20:05:52.527
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:05:54.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1108" for this suite. 05/01/23 20:05:54.908
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:05:54.96
May  1 20:05:54.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 20:05:54.965
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:55.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:55.093
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 05/01/23 20:05:55.163
May  1 20:05:55.257: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7658" to be "running and ready"
May  1 20:05:55.306: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 49.562254ms
May  1 20:05:55.307: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 20:05:57.351: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094152978s
May  1 20:05:57.351: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May  1 20:05:59.337: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.079649072s
May  1 20:05:59.337: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May  1 20:05:59.337: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 05/01/23 20:05:59.358
May  1 20:05:59.419: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7658" to be "running and ready"
May  1 20:05:59.476: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 56.092995ms
May  1 20:05:59.476: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 20:06:01.504: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084019305s
May  1 20:06:01.504: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May  1 20:06:03.501: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.081153966s
May  1 20:06:03.501: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May  1 20:06:03.501: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/01/23 20:06:03.522
May  1 20:06:03.560: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  1 20:06:03.581: INFO: Pod pod-with-prestop-http-hook still exists
May  1 20:06:05.582: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  1 20:06:05.619: INFO: Pod pod-with-prestop-http-hook still exists
May  1 20:06:07.582: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  1 20:06:07.603: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/01/23 20:06:07.603
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
May  1 20:06:07.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7658" for this suite. 05/01/23 20:06:07.672
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":220,"skipped":4188,"failed":0}
------------------------------
• [SLOW TEST] [12.738 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:05:54.96
    May  1 20:05:54.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/01/23 20:05:54.965
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:05:55.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:05:55.093
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 05/01/23 20:05:55.163
    May  1 20:05:55.257: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7658" to be "running and ready"
    May  1 20:05:55.306: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 49.562254ms
    May  1 20:05:55.307: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:05:57.351: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094152978s
    May  1 20:05:57.351: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:05:59.337: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.079649072s
    May  1 20:05:59.337: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May  1 20:05:59.337: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 05/01/23 20:05:59.358
    May  1 20:05:59.419: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7658" to be "running and ready"
    May  1 20:05:59.476: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 56.092995ms
    May  1 20:05:59.476: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:06:01.504: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084019305s
    May  1 20:06:01.504: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:06:03.501: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.081153966s
    May  1 20:06:03.501: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May  1 20:06:03.501: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/01/23 20:06:03.522
    May  1 20:06:03.560: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  1 20:06:03.581: INFO: Pod pod-with-prestop-http-hook still exists
    May  1 20:06:05.582: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  1 20:06:05.619: INFO: Pod pod-with-prestop-http-hook still exists
    May  1 20:06:07.582: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May  1 20:06:07.603: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/01/23 20:06:07.603
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    May  1 20:06:07.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7658" for this suite. 05/01/23 20:06:07.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:06:07.702
May  1 20:06:07.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:06:07.705
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:07.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:07.82
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May  1 20:06:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:06:08.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7344" for this suite. 05/01/23 20:06:08.605
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":221,"skipped":4218,"failed":0}
------------------------------
• [0.933 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:06:07.702
    May  1 20:06:07.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:06:07.705
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:07.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:07.82
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May  1 20:06:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:06:08.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7344" for this suite. 05/01/23 20:06:08.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:06:08.64
May  1 20:06:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename server-version 05/01/23 20:06:08.642
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:08.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:08.84
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/01/23 20:06:08.891
STEP: Confirm major version 05/01/23 20:06:08.911
May  1 20:06:08.911: INFO: Major version: 1
STEP: Confirm minor version 05/01/23 20:06:08.911
May  1 20:06:08.912: INFO: cleanMinorVersion: 25
May  1 20:06:08.912: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
May  1 20:06:08.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5887" for this suite. 05/01/23 20:06:09.078
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":222,"skipped":4276,"failed":0}
------------------------------
• [0.495 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:06:08.64
    May  1 20:06:08.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename server-version 05/01/23 20:06:08.642
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:08.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:08.84
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/01/23 20:06:08.891
    STEP: Confirm major version 05/01/23 20:06:08.911
    May  1 20:06:08.911: INFO: Major version: 1
    STEP: Confirm minor version 05/01/23 20:06:08.911
    May  1 20:06:08.912: INFO: cleanMinorVersion: 25
    May  1 20:06:08.912: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    May  1 20:06:08.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-5887" for this suite. 05/01/23 20:06:09.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:06:09.138
May  1 20:06:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename cronjob 05/01/23 20:06:09.14
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:09.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:09.3
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/01/23 20:06:09.342
STEP: Ensuring more than one job is running at a time 05/01/23 20:06:09.369
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/01/23 20:08:01.4
STEP: Removing cronjob 05/01/23 20:08:01.414
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  1 20:08:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2005" for this suite. 05/01/23 20:08:01.561
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":223,"skipped":4295,"failed":0}
------------------------------
• [SLOW TEST] [112.462 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:06:09.138
    May  1 20:06:09.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename cronjob 05/01/23 20:06:09.14
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:06:09.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:06:09.3
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/01/23 20:06:09.342
    STEP: Ensuring more than one job is running at a time 05/01/23 20:06:09.369
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/01/23 20:08:01.4
    STEP: Removing cronjob 05/01/23 20:08:01.414
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  1 20:08:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2005" for this suite. 05/01/23 20:08:01.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:08:01.63
May  1 20:08:01.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename deployment 05/01/23 20:08:01.635
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:08:01.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:08:01.831
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/01/23 20:08:01.894
STEP: waiting for Deployment to be created 05/01/23 20:08:01.93
STEP: waiting for all Replicas to be Ready 05/01/23 20:08:01.973
May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.205: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.205: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.218: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:02.218: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May  1 20:08:04.506: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  1 20:08:04.506: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May  1 20:08:06.028: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/01/23 20:08:06.028
W0501 20:08:06.062950      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May  1 20:08:06.072: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/01/23 20:08:06.072
May  1 20:08:06.091: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.092: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.095: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.095: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.160: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.160: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.220: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.221: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:06.266: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:06.266: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:06.418: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:06.418: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:09.520: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:09.521: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:09.639: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
STEP: listing Deployments 05/01/23 20:08:09.639
May  1 20:08:09.695: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/01/23 20:08:09.696
May  1 20:08:09.788: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/01/23 20:08:09.788
May  1 20:08:09.841: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:09.846: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:09.922: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:10.005: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:10.054: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:12.632: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:12.695: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:12.733: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:12.796: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:12.811: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May  1 20:08:16.195: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/01/23 20:08:16.273
STEP: fetching the DeploymentStatus 05/01/23 20:08:16.328
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:16.370: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
May  1 20:08:16.370: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 3
STEP: deleting the Deployment 05/01/23 20:08:16.37
May  1 20:08:16.422: INFO: observed event type MODIFIED
May  1 20:08:16.423: INFO: observed event type MODIFIED
May  1 20:08:16.423: INFO: observed event type MODIFIED
May  1 20:08:16.423: INFO: observed event type MODIFIED
May  1 20:08:16.424: INFO: observed event type MODIFIED
May  1 20:08:16.424: INFO: observed event type MODIFIED
May  1 20:08:16.425: INFO: observed event type MODIFIED
May  1 20:08:16.425: INFO: observed event type MODIFIED
May  1 20:08:16.426: INFO: observed event type MODIFIED
May  1 20:08:16.426: INFO: observed event type MODIFIED
May  1 20:08:16.426: INFO: observed event type MODIFIED
May  1 20:08:16.427: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May  1 20:08:16.453: INFO: Log out all the ReplicaSets if there is no deployment created
May  1 20:08:16.479: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2047  b76406b4-15ca-4fe0-b19d-1d6b2118c08c 113321 4 2023-05-01 20:08:06 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f817 0xc00621f818}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621f8a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May  1 20:08:16.508: INFO: pod: "test-deployment-54cc775c4b-78wnn":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-78wnn test-deployment-54cc775c4b- deployment-2047  8b41cb1c-a8d0-4f26-84b2-0910731c5b8d 113318 0 2023-05-01 20:08:06 +0000 UTC 2023-05-01 20:08:17 +0000 UTC 0xc00621fd28 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:4856190efeebbb0f724a31b0aaff255706308e51fb81bbf81a014897075ddff1 cni.projectcalico.org/podIP:172.30.42.68/32 cni.projectcalico.org/podIPs:172.30.42.68/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.68"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.68"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b76406b4-15ca-4fe0-b19d-1d6b2118c08c 0xc00621fd87 0xc00621fd88}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b76406b4-15ca-4fe0-b19d-1d6b2118c08c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64gqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64gqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.68,StartTime:2023-05-01 20:08:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://eccb369f23bd8a74147b6228d94a262247a95ee6953fd2e1d900164c05c2672e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  1 20:08:16.508: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2047  afd54148-335b-402b-b692-358b43255f66 113314 2 2023-05-01 20:08:09 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f907 0xc00621f908}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621f990 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May  1 20:08:16.542: INFO: pod: "test-deployment-7c7d8d58c8-cmx5q":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cmx5q test-deployment-7c7d8d58c8- deployment-2047  7026e3ad-c433-4f14-a4d8-4d473b63f421 113313 0 2023-05-01 20:08:12 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d2964ce81d1aeac9b6d75db5dc478d8ef63490691d64b96955115b58a158911e cni.projectcalico.org/podIP:172.30.38.230/32 cni.projectcalico.org/podIPs:172.30.38.230/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.230"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.38.230"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 afd54148-335b-402b-b692-358b43255f66 0xc0027ac4f7 0xc0027ac4f8}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afd54148-335b-402b-b692-358b43255f66\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4zl2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zl2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.230,StartTime:2023-05-01 20:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://376a830c71f1413f940f1ab43be24cfe471b26aa61c5120a13a8923410724bbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  1 20:08:16.544: INFO: pod: "test-deployment-7c7d8d58c8-vhr67":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-vhr67 test-deployment-7c7d8d58c8- deployment-2047  8b54b3f4-96a6-463d-8090-a0a26f1597e6 113251 0 2023-05-01 20:08:09 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:9f14fd19f105797754e595932979e388fb9b3365f32f43aaa047d099e0ec92ea cni.projectcalico.org/podIP:172.30.42.80/32 cni.projectcalico.org/podIPs:172.30.42.80/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.80"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.42.80"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 afd54148-335b-402b-b692-358b43255f66 0xc0027ac797 0xc0027ac798}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afd54148-335b-402b-b692-358b43255f66\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttdhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttdhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.80,StartTime:2023-05-01 20:08:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://efd3b9d7110034acf3b865f0f1512eab6ee6dabbfc7636ee3afdbf142178fb9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May  1 20:08:16.545: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2047  7b693c48-4246-4dac-9da6-cd2520c69590 113172 3 2023-05-01 20:08:01 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f9f7 0xc00621f9f8}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621fa80 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
May  1 20:08:16.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2047" for this suite. 05/01/23 20:08:16.599
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":224,"skipped":4322,"failed":0}
------------------------------
• [SLOW TEST] [15.022 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:08:01.63
    May  1 20:08:01.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename deployment 05/01/23 20:08:01.635
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:08:01.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:08:01.831
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/01/23 20:08:01.894
    STEP: waiting for Deployment to be created 05/01/23 20:08:01.93
    STEP: waiting for all Replicas to be Ready 05/01/23 20:08:01.973
    May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.048: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.205: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.205: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.218: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:02.218: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May  1 20:08:04.506: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  1 20:08:04.506: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May  1 20:08:06.028: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/01/23 20:08:06.028
    W0501 20:08:06.062950      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May  1 20:08:06.072: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/01/23 20:08:06.072
    May  1 20:08:06.091: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.092: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.093: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 0
    May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.094: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.095: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.095: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.160: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.160: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.220: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.221: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:06.266: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:06.266: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:06.418: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:06.418: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:09.520: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:09.521: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:09.639: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    STEP: listing Deployments 05/01/23 20:08:09.639
    May  1 20:08:09.695: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/01/23 20:08:09.696
    May  1 20:08:09.788: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/01/23 20:08:09.788
    May  1 20:08:09.841: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:09.846: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:09.922: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:10.005: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:10.054: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:12.632: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:12.695: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:12.733: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:12.796: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:12.811: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May  1 20:08:16.195: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/01/23 20:08:16.273
    STEP: fetching the DeploymentStatus 05/01/23 20:08:16.328
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 1
    May  1 20:08:16.367: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:16.368: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:16.370: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 2
    May  1 20:08:16.370: INFO: observed Deployment test-deployment in namespace deployment-2047 with ReadyReplicas 3
    STEP: deleting the Deployment 05/01/23 20:08:16.37
    May  1 20:08:16.422: INFO: observed event type MODIFIED
    May  1 20:08:16.423: INFO: observed event type MODIFIED
    May  1 20:08:16.423: INFO: observed event type MODIFIED
    May  1 20:08:16.423: INFO: observed event type MODIFIED
    May  1 20:08:16.424: INFO: observed event type MODIFIED
    May  1 20:08:16.424: INFO: observed event type MODIFIED
    May  1 20:08:16.425: INFO: observed event type MODIFIED
    May  1 20:08:16.425: INFO: observed event type MODIFIED
    May  1 20:08:16.426: INFO: observed event type MODIFIED
    May  1 20:08:16.426: INFO: observed event type MODIFIED
    May  1 20:08:16.426: INFO: observed event type MODIFIED
    May  1 20:08:16.427: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May  1 20:08:16.453: INFO: Log out all the ReplicaSets if there is no deployment created
    May  1 20:08:16.479: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-2047  b76406b4-15ca-4fe0-b19d-1d6b2118c08c 113321 4 2023-05-01 20:08:06 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f817 0xc00621f818}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621f8a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May  1 20:08:16.508: INFO: pod: "test-deployment-54cc775c4b-78wnn":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-78wnn test-deployment-54cc775c4b- deployment-2047  8b41cb1c-a8d0-4f26-84b2-0910731c5b8d 113318 0 2023-05-01 20:08:06 +0000 UTC 2023-05-01 20:08:17 +0000 UTC 0xc00621fd28 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:4856190efeebbb0f724a31b0aaff255706308e51fb81bbf81a014897075ddff1 cni.projectcalico.org/podIP:172.30.42.68/32 cni.projectcalico.org/podIPs:172.30.42.68/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.68"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.68"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b b76406b4-15ca-4fe0-b19d-1d6b2118c08c 0xc00621fd87 0xc00621fd88}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b76406b4-15ca-4fe0-b19d-1d6b2118c08c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-64gqp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-64gqp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.68,StartTime:2023-05-01 20:08:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://eccb369f23bd8a74147b6228d94a262247a95ee6953fd2e1d900164c05c2672e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  1 20:08:16.508: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-2047  afd54148-335b-402b-b692-358b43255f66 113314 2 2023-05-01 20:08:09 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f907 0xc00621f908}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621f990 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May  1 20:08:16.542: INFO: pod: "test-deployment-7c7d8d58c8-cmx5q":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-cmx5q test-deployment-7c7d8d58c8- deployment-2047  7026e3ad-c433-4f14-a4d8-4d473b63f421 113313 0 2023-05-01 20:08:12 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d2964ce81d1aeac9b6d75db5dc478d8ef63490691d64b96955115b58a158911e cni.projectcalico.org/podIP:172.30.38.230/32 cni.projectcalico.org/podIPs:172.30.38.230/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.230"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.38.230"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 afd54148-335b-402b-b692-358b43255f66 0xc0027ac4f7 0xc0027ac4f8}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afd54148-335b-402b-b692-358b43255f66\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4zl2s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zl2s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.126,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.126,PodIP:172.30.38.230,StartTime:2023-05-01 20:08:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://376a830c71f1413f940f1ab43be24cfe471b26aa61c5120a13a8923410724bbf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.38.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  1 20:08:16.544: INFO: pod: "test-deployment-7c7d8d58c8-vhr67":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-vhr67 test-deployment-7c7d8d58c8- deployment-2047  8b54b3f4-96a6-463d-8090-a0a26f1597e6 113251 0 2023-05-01 20:08:09 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:9f14fd19f105797754e595932979e388fb9b3365f32f43aaa047d099e0ec92ea cni.projectcalico.org/podIP:172.30.42.80/32 cni.projectcalico.org/podIPs:172.30.42.80/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.80"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.42.80"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 afd54148-335b-402b-b692-358b43255f66 0xc0027ac797 0xc0027ac798}] [] [{kube-controller-manager Update v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afd54148-335b-402b-b692-358b43255f66\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-01 20:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-05-01 20:08:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-05-01 20:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttdhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttdhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.145.124,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c39,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-t8zvb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-01 20:08:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.145.124,PodIP:172.30.42.80,StartTime:2023-05-01 20:08:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-01 20:08:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://efd3b9d7110034acf3b865f0f1512eab6ee6dabbfc7636ee3afdbf142178fb9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.42.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May  1 20:08:16.545: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-2047  7b693c48-4246-4dac-9da6-cd2520c69590 113172 3 2023-05-01 20:08:01 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5ed7a14a-707a-4df4-9ac6-35cbf48fb325 0xc00621f9f7 0xc00621f9f8}] [] [{kube-controller-manager Update apps/v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5ed7a14a-707a-4df4-9ac6-35cbf48fb325\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-01 20:08:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00621fa80 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    May  1 20:08:16.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2047" for this suite. 05/01/23 20:08:16.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:08:16.658
May  1 20:08:16.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-preemption 05/01/23 20:08:16.66
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:08:16.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:08:16.835
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
May  1 20:08:16.967: INFO: Waiting up to 1m0s for all nodes to be ready
May  1 20:09:17.463: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 05/01/23 20:09:17.521
May  1 20:09:17.692: INFO: Created pod: pod0-0-sched-preemption-low-priority
May  1 20:09:17.768: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May  1 20:09:17.946: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May  1 20:09:18.029: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May  1 20:09:18.167: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May  1 20:09:18.294: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/01/23 20:09:18.294
May  1 20:09:18.295: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:18.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 32.9076ms
May  1 20:09:20.362: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067159972s
May  1 20:09:22.349: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.054439889s
May  1 20:09:22.349: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May  1 20:09:22.349: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.371: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 21.857569ms
May  1 20:09:22.371: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May  1 20:09:22.371: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.398: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 27.265301ms
May  1 20:09:22.398: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May  1 20:09:22.398: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.416: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.800945ms
May  1 20:09:22.416: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May  1 20:09:22.416: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.439: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.696862ms
May  1 20:09:22.439: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May  1 20:09:22.439: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.461: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.21328ms
May  1 20:09:22.462: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/01/23 20:09:22.462
May  1 20:09:22.514: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9045" to be "running"
May  1 20:09:22.537: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.521506ms
May  1 20:09:24.567: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053823146s
May  1 20:09:26.557: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042813043s
May  1 20:09:28.557: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043118314s
May  1 20:09:30.566: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.051889012s
May  1 20:09:30.566: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
May  1 20:09:30.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9045" for this suite. 05/01/23 20:09:30.791
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":225,"skipped":4337,"failed":0}
------------------------------
• [SLOW TEST] [74.522 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:08:16.658
    May  1 20:08:16.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-preemption 05/01/23 20:08:16.66
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:08:16.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:08:16.835
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    May  1 20:08:16.967: INFO: Waiting up to 1m0s for all nodes to be ready
    May  1 20:09:17.463: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 05/01/23 20:09:17.521
    May  1 20:09:17.692: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May  1 20:09:17.768: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May  1 20:09:17.946: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May  1 20:09:18.029: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May  1 20:09:18.167: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May  1 20:09:18.294: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/01/23 20:09:18.294
    May  1 20:09:18.295: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:18.328: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 32.9076ms
    May  1 20:09:20.362: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067159972s
    May  1 20:09:22.349: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.054439889s
    May  1 20:09:22.349: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May  1 20:09:22.349: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.371: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 21.857569ms
    May  1 20:09:22.371: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May  1 20:09:22.371: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.398: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 27.265301ms
    May  1 20:09:22.398: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May  1 20:09:22.398: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.416: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.800945ms
    May  1 20:09:22.416: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May  1 20:09:22.416: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.439: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.696862ms
    May  1 20:09:22.439: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May  1 20:09:22.439: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.461: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 22.21328ms
    May  1 20:09:22.462: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/01/23 20:09:22.462
    May  1 20:09:22.514: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9045" to be "running"
    May  1 20:09:22.537: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 23.521506ms
    May  1 20:09:24.567: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053823146s
    May  1 20:09:26.557: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042813043s
    May  1 20:09:28.557: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043118314s
    May  1 20:09:30.566: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.051889012s
    May  1 20:09:30.566: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:09:30.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9045" for this suite. 05/01/23 20:09:30.791
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:09:31.182
May  1 20:09:31.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:09:31.186
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:09:31.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:09:31.376
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:09:31.395
May  1 20:09:31.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee" in namespace "projected-5041" to be "Succeeded or Failed"
May  1 20:09:31.566: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.542164ms
May  1 20:09:33.599: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054737182s
May  1 20:09:35.587: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043576381s
May  1 20:09:37.644: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100243509s
STEP: Saw pod success 05/01/23 20:09:37.644
May  1 20:09:37.644: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee" satisfied condition "Succeeded or Failed"
May  1 20:09:37.674: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee container client-container: <nil>
STEP: delete the pod 05/01/23 20:09:37.966
May  1 20:09:38.089: INFO: Waiting for pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee to disappear
May  1 20:09:38.130: INFO: Pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 20:09:38.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5041" for this suite. 05/01/23 20:09:38.163
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":226,"skipped":4341,"failed":0}
------------------------------
• [SLOW TEST] [7.056 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:09:31.182
    May  1 20:09:31.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:09:31.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:09:31.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:09:31.376
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:09:31.395
    May  1 20:09:31.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee" in namespace "projected-5041" to be "Succeeded or Failed"
    May  1 20:09:31.566: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.542164ms
    May  1 20:09:33.599: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054737182s
    May  1 20:09:35.587: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043576381s
    May  1 20:09:37.644: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100243509s
    STEP: Saw pod success 05/01/23 20:09:37.644
    May  1 20:09:37.644: INFO: Pod "downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee" satisfied condition "Succeeded or Failed"
    May  1 20:09:37.674: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee container client-container: <nil>
    STEP: delete the pod 05/01/23 20:09:37.966
    May  1 20:09:38.089: INFO: Waiting for pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee to disappear
    May  1 20:09:38.130: INFO: Pod downwardapi-volume-8bdd90df-b4b0-4503-89d8-02923fb9a6ee no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 20:09:38.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5041" for this suite. 05/01/23 20:09:38.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:09:38.245
May  1 20:09:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir-wrapper 05/01/23 20:09:38.248
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:09:38.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:09:38.403
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/01/23 20:09:38.429
STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:09:40.586
May  1 20:09:40.661: INFO: Pod name wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa: Found 0 pods out of 5
May  1 20:09:45.722: INFO: Pod name wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/01/23 20:09:45.722
May  1 20:09:45.723: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:45.753: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb": Phase="Pending", Reason="", readiness=false. Elapsed: 29.631325ms
May  1 20:09:47.785: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb": Phase="Running", Reason="", readiness=true. Elapsed: 2.0620394s
May  1 20:09:47.785: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb" satisfied condition "running"
May  1 20:09:47.785: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:47.808: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk": Phase="Running", Reason="", readiness=true. Elapsed: 23.357896ms
May  1 20:09:47.809: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk" satisfied condition "running"
May  1 20:09:47.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:47.853: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz": Phase="Running", Reason="", readiness=true. Elapsed: 44.618653ms
May  1 20:09:47.853: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz" satisfied condition "running"
May  1 20:09:47.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:47.913: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx": Phase="Running", Reason="", readiness=true. Elapsed: 60.046837ms
May  1 20:09:47.913: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx" satisfied condition "running"
May  1 20:09:47.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:47.944: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5": Phase="Running", Reason="", readiness=true. Elapsed: 30.735487ms
May  1 20:09:47.944: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:09:47.944
May  1 20:09:48.102: INFO: Deleting ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa took: 67.079663ms
May  1 20:09:48.303: INFO: Terminating ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa pods took: 201.168107ms
STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:09:52.959
May  1 20:09:53.056: INFO: Pod name wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad: Found 0 pods out of 5
May  1 20:09:58.110: INFO: Pod name wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/01/23 20:09:58.11
May  1 20:09:58.110: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:09:58.142: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd": Phase="Pending", Reason="", readiness=false. Elapsed: 31.825091ms
May  1 20:10:00.196: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.085829776s
May  1 20:10:00.196: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd" satisfied condition "running"
May  1 20:10:00.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:00.245: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6": Phase="Running", Reason="", readiness=true. Elapsed: 48.204158ms
May  1 20:10:00.245: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6" satisfied condition "running"
May  1 20:10:00.245: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:00.264: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx": Phase="Running", Reason="", readiness=true. Elapsed: 18.719047ms
May  1 20:10:00.264: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx" satisfied condition "running"
May  1 20:10:00.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:00.287: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j": Phase="Pending", Reason="", readiness=false. Elapsed: 23.198371ms
May  1 20:10:02.353: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.089236163s
May  1 20:10:02.353: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j" satisfied condition "running"
May  1 20:10:02.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:02.408: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5": Phase="Running", Reason="", readiness=true. Elapsed: 54.554721ms
May  1 20:10:02.408: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:10:02.408
May  1 20:10:02.575: INFO: Deleting ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad took: 51.652984ms
May  1 20:10:02.776: INFO: Terminating ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad pods took: 200.860321ms
STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:10:09.043
May  1 20:10:09.125: INFO: Pod name wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1: Found 0 pods out of 5
May  1 20:10:14.166: INFO: Pod name wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/01/23 20:10:14.166
May  1 20:10:14.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:14.191: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v": Phase="Pending", Reason="", readiness=false. Elapsed: 23.908402ms
May  1 20:10:16.225: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.058186623s
May  1 20:10:16.225: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v" satisfied condition "running"
May  1 20:10:16.225: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:16.260: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf": Phase="Running", Reason="", readiness=true. Elapsed: 34.679489ms
May  1 20:10:16.260: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf" satisfied condition "running"
May  1 20:10:16.260: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:16.288: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh": Phase="Running", Reason="", readiness=true. Elapsed: 28.306034ms
May  1 20:10:16.289: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh" satisfied condition "running"
May  1 20:10:16.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:16.331: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj": Phase="Running", Reason="", readiness=true. Elapsed: 42.649839ms
May  1 20:10:16.331: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj" satisfied condition "running"
May  1 20:10:16.331: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj" in namespace "emptydir-wrapper-8536" to be "running"
May  1 20:10:16.373: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj": Phase="Running", Reason="", readiness=true. Elapsed: 41.089025ms
May  1 20:10:16.373: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:10:16.373
May  1 20:10:16.493: INFO: Deleting ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 took: 42.13666ms
May  1 20:10:16.694: INFO: Terminating ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 pods took: 200.569077ms
STEP: Cleaning up the configMaps 05/01/23 20:10:20.595
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
May  1 20:10:21.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8536" for this suite. 05/01/23 20:10:21.94
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":227,"skipped":4368,"failed":0}
------------------------------
• [SLOW TEST] [43.721 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:09:38.245
    May  1 20:09:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir-wrapper 05/01/23 20:09:38.248
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:09:38.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:09:38.403
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/01/23 20:09:38.429
    STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:09:40.586
    May  1 20:09:40.661: INFO: Pod name wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa: Found 0 pods out of 5
    May  1 20:09:45.722: INFO: Pod name wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/01/23 20:09:45.722
    May  1 20:09:45.723: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:45.753: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb": Phase="Pending", Reason="", readiness=false. Elapsed: 29.631325ms
    May  1 20:09:47.785: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb": Phase="Running", Reason="", readiness=true. Elapsed: 2.0620394s
    May  1 20:09:47.785: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-4zgjb" satisfied condition "running"
    May  1 20:09:47.785: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:47.808: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk": Phase="Running", Reason="", readiness=true. Elapsed: 23.357896ms
    May  1 20:09:47.809: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-6t7gk" satisfied condition "running"
    May  1 20:09:47.809: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:47.853: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz": Phase="Running", Reason="", readiness=true. Elapsed: 44.618653ms
    May  1 20:09:47.853: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-9dqpz" satisfied condition "running"
    May  1 20:09:47.853: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:47.913: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx": Phase="Running", Reason="", readiness=true. Elapsed: 60.046837ms
    May  1 20:09:47.913: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-n69xx" satisfied condition "running"
    May  1 20:09:47.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:47.944: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5": Phase="Running", Reason="", readiness=true. Elapsed: 30.735487ms
    May  1 20:09:47.944: INFO: Pod "wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa-qjxp5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:09:47.944
    May  1 20:09:48.102: INFO: Deleting ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa took: 67.079663ms
    May  1 20:09:48.303: INFO: Terminating ReplicationController wrapped-volume-race-f42cf067-9e11-457d-ab77-1d2888d87aaa pods took: 201.168107ms
    STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:09:52.959
    May  1 20:09:53.056: INFO: Pod name wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad: Found 0 pods out of 5
    May  1 20:09:58.110: INFO: Pod name wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/01/23 20:09:58.11
    May  1 20:09:58.110: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:09:58.142: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd": Phase="Pending", Reason="", readiness=false. Elapsed: 31.825091ms
    May  1 20:10:00.196: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd": Phase="Running", Reason="", readiness=true. Elapsed: 2.085829776s
    May  1 20:10:00.196: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-2c9pd" satisfied condition "running"
    May  1 20:10:00.196: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:00.245: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6": Phase="Running", Reason="", readiness=true. Elapsed: 48.204158ms
    May  1 20:10:00.245: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-4bvw6" satisfied condition "running"
    May  1 20:10:00.245: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:00.264: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx": Phase="Running", Reason="", readiness=true. Elapsed: 18.719047ms
    May  1 20:10:00.264: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-clhcx" satisfied condition "running"
    May  1 20:10:00.264: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:00.287: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j": Phase="Pending", Reason="", readiness=false. Elapsed: 23.198371ms
    May  1 20:10:02.353: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.089236163s
    May  1 20:10:02.353: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-gkj9j" satisfied condition "running"
    May  1 20:10:02.353: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:02.408: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5": Phase="Running", Reason="", readiness=true. Elapsed: 54.554721ms
    May  1 20:10:02.408: INFO: Pod "wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad-t66k5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:10:02.408
    May  1 20:10:02.575: INFO: Deleting ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad took: 51.652984ms
    May  1 20:10:02.776: INFO: Terminating ReplicationController wrapped-volume-race-3f12f809-d76b-4544-a77e-e31ec8d964ad pods took: 200.860321ms
    STEP: Creating RC which spawns configmap-volume pods 05/01/23 20:10:09.043
    May  1 20:10:09.125: INFO: Pod name wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1: Found 0 pods out of 5
    May  1 20:10:14.166: INFO: Pod name wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/01/23 20:10:14.166
    May  1 20:10:14.167: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:14.191: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v": Phase="Pending", Reason="", readiness=false. Elapsed: 23.908402ms
    May  1 20:10:16.225: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.058186623s
    May  1 20:10:16.225: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-25g8v" satisfied condition "running"
    May  1 20:10:16.225: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:16.260: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf": Phase="Running", Reason="", readiness=true. Elapsed: 34.679489ms
    May  1 20:10:16.260: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-2bktf" satisfied condition "running"
    May  1 20:10:16.260: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:16.288: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh": Phase="Running", Reason="", readiness=true. Elapsed: 28.306034ms
    May  1 20:10:16.289: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-bttgh" satisfied condition "running"
    May  1 20:10:16.289: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:16.331: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj": Phase="Running", Reason="", readiness=true. Elapsed: 42.649839ms
    May  1 20:10:16.331: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-kg7rj" satisfied condition "running"
    May  1 20:10:16.331: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj" in namespace "emptydir-wrapper-8536" to be "running"
    May  1 20:10:16.373: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj": Phase="Running", Reason="", readiness=true. Elapsed: 41.089025ms
    May  1 20:10:16.373: INFO: Pod "wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1-r2jdj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 in namespace emptydir-wrapper-8536, will wait for the garbage collector to delete the pods 05/01/23 20:10:16.373
    May  1 20:10:16.493: INFO: Deleting ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 took: 42.13666ms
    May  1 20:10:16.694: INFO: Terminating ReplicationController wrapped-volume-race-e80472c7-6a98-48ee-a2e8-0d78a4b21cf1 pods took: 200.569077ms
    STEP: Cleaning up the configMaps 05/01/23 20:10:20.595
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    May  1 20:10:21.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8536" for this suite. 05/01/23 20:10:21.94
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:10:21.969
May  1 20:10:21.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 20:10:21.971
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:10:22.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:10:22.089
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
May  1 20:10:22.306: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/01/23 20:10:22.358
May  1 20:10:22.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:22.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/01/23 20:10:22.389
May  1 20:10:22.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:22.524: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:23.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:23.546: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:24.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:24.581: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:25.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  1 20:10:25.560: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/01/23 20:10:25.584
May  1 20:10:25.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  1 20:10:25.684: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May  1 20:10:26.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:26.706: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/01/23 20:10:26.706
May  1 20:10:26.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:26.760: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:27.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:27.788: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:28.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:28.786: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:29.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:29.793: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:30.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:30.803: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:31.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:31.782: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:10:32.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  1 20:10:32.783: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:10:32.815
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1524, will wait for the garbage collector to delete the pods 05/01/23 20:10:32.816
May  1 20:10:32.908: INFO: Deleting DaemonSet.extensions daemon-set took: 23.565227ms
May  1 20:10:33.008: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295372ms
May  1 20:10:36.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:10:36.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 20:10:36.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"115262"},"items":null}

May  1 20:10:36.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"115262"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 20:10:36.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1524" for this suite. 05/01/23 20:10:36.969
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":228,"skipped":4369,"failed":0}
------------------------------
• [SLOW TEST] [15.027 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:10:21.969
    May  1 20:10:21.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 20:10:21.971
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:10:22.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:10:22.089
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    May  1 20:10:22.306: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/01/23 20:10:22.358
    May  1 20:10:22.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:22.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/01/23 20:10:22.389
    May  1 20:10:22.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:22.524: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:23.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:23.546: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:24.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:24.581: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:25.560: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  1 20:10:25.560: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/01/23 20:10:25.584
    May  1 20:10:25.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  1 20:10:25.684: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May  1 20:10:26.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:26.706: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/01/23 20:10:26.706
    May  1 20:10:26.760: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:26.760: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:27.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:27.788: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:28.786: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:28.786: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:29.793: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:29.793: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:30.803: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:30.803: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:31.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:31.782: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:10:32.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  1 20:10:32.783: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:10:32.815
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1524, will wait for the garbage collector to delete the pods 05/01/23 20:10:32.816
    May  1 20:10:32.908: INFO: Deleting DaemonSet.extensions daemon-set took: 23.565227ms
    May  1 20:10:33.008: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.295372ms
    May  1 20:10:36.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:10:36.641: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 20:10:36.654: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"115262"},"items":null}

    May  1 20:10:36.670: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"115262"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:10:36.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1524" for this suite. 05/01/23 20:10:36.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:10:36.999
May  1 20:10:36.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 20:10:37.002
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:10:37.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:10:37.084
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4519 05/01/23 20:10:37.107
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 05/01/23 20:10:37.132
May  1 20:10:37.196: INFO: Found 0 stateful pods, waiting for 3
May  1 20:10:47.249: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:10:47.249: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:10:47.249: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/01/23 20:10:47.316
May  1 20:10:47.373: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/01/23 20:10:47.373
STEP: Not applying an update when the partition is greater than the number of replicas 05/01/23 20:10:57.48
STEP: Performing a canary update 05/01/23 20:10:57.48
May  1 20:10:57.539: INFO: Updating stateful set ss2
May  1 20:10:57.581: INFO: Waiting for Pod statefulset-4519/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 05/01/23 20:11:07.668
May  1 20:11:07.902: INFO: Found 1 stateful pods, waiting for 3
May  1 20:11:17.930: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:11:17.930: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:11:17.930: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/01/23 20:11:17.974
May  1 20:11:18.052: INFO: Updating stateful set ss2
May  1 20:11:18.100: INFO: Waiting for Pod statefulset-4519/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
May  1 20:11:28.251: INFO: Updating stateful set ss2
May  1 20:11:28.318: INFO: Waiting for StatefulSet statefulset-4519/ss2 to complete update
May  1 20:11:28.318: INFO: Waiting for Pod statefulset-4519/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 20:11:38.372: INFO: Deleting all statefulset in ns statefulset-4519
May  1 20:11:38.394: INFO: Scaling statefulset ss2 to 0
May  1 20:11:48.502: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:11:48.520: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 20:11:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4519" for this suite. 05/01/23 20:11:48.701
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":229,"skipped":4389,"failed":0}
------------------------------
• [SLOW TEST] [71.738 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:10:36.999
    May  1 20:10:36.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 20:10:37.002
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:10:37.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:10:37.084
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4519 05/01/23 20:10:37.107
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 05/01/23 20:10:37.132
    May  1 20:10:37.196: INFO: Found 0 stateful pods, waiting for 3
    May  1 20:10:47.249: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:10:47.249: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:10:47.249: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 05/01/23 20:10:47.316
    May  1 20:10:47.373: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/01/23 20:10:47.373
    STEP: Not applying an update when the partition is greater than the number of replicas 05/01/23 20:10:57.48
    STEP: Performing a canary update 05/01/23 20:10:57.48
    May  1 20:10:57.539: INFO: Updating stateful set ss2
    May  1 20:10:57.581: INFO: Waiting for Pod statefulset-4519/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 05/01/23 20:11:07.668
    May  1 20:11:07.902: INFO: Found 1 stateful pods, waiting for 3
    May  1 20:11:17.930: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:11:17.930: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:11:17.930: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/01/23 20:11:17.974
    May  1 20:11:18.052: INFO: Updating stateful set ss2
    May  1 20:11:18.100: INFO: Waiting for Pod statefulset-4519/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    May  1 20:11:28.251: INFO: Updating stateful set ss2
    May  1 20:11:28.318: INFO: Waiting for StatefulSet statefulset-4519/ss2 to complete update
    May  1 20:11:28.318: INFO: Waiting for Pod statefulset-4519/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 20:11:38.372: INFO: Deleting all statefulset in ns statefulset-4519
    May  1 20:11:38.394: INFO: Scaling statefulset ss2 to 0
    May  1 20:11:48.502: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:11:48.520: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 20:11:48.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4519" for this suite. 05/01/23 20:11:48.701
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:11:48.739
May  1 20:11:48.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:11:48.742
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:11:48.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:11:48.977
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-5748 05/01/23 20:11:49.051
May  1 20:11:49.209: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5748" to be "running and ready"
May  1 20:11:49.275: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 64.524935ms
May  1 20:11:49.276: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  1 20:11:51.298: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.087449982s
May  1 20:11:51.298: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  1 20:11:51.298: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  1 20:11:51.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  1 20:11:52.112: INFO: rc: 7
May  1 20:11:52.172: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  1 20:11:52.191: INFO: Pod kube-proxy-mode-detector no longer exists
May  1 20:11:52.191: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-5748 05/01/23 20:11:52.191
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5748 05/01/23 20:11:52.296
I0501 20:11:52.367977      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5748, replica count: 3
I0501 20:11:55.439923      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:11:58.440212      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:11:58.481: INFO: Creating new exec pod
May  1 20:11:58.565: INFO: Waiting up to 5m0s for pod "execpod-affinityn74mw" in namespace "services-5748" to be "running"
May  1 20:11:58.607: INFO: Pod "execpod-affinityn74mw": Phase="Pending", Reason="", readiness=false. Elapsed: 41.639774ms
May  1 20:12:00.656: INFO: Pod "execpod-affinityn74mw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090318361s
May  1 20:12:02.652: INFO: Pod "execpod-affinityn74mw": Phase="Running", Reason="", readiness=true. Elapsed: 4.086748119s
May  1 20:12:02.652: INFO: Pod "execpod-affinityn74mw" satisfied condition "running"
May  1 20:12:03.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May  1 20:12:04.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May  1 20:12:04.097: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:12:04.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.242.48 80'
May  1 20:12:04.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.242.48 80\nConnection to 172.21.242.48 80 port [tcp/http] succeeded!\n"
May  1 20:12:04.668: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:12:04.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.242.48:80/ ; done'
May  1 20:12:05.458: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
May  1 20:12:05.458: INFO: stdout: "\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9"
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
May  1 20:12:05.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.242.48:80/'
May  1 20:12:06.399: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
May  1 20:12:06.399: INFO: stdout: "affinity-clusterip-timeout-kd9f9"
May  1 20:12:26.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.242.48:80/'
May  1 20:12:27.000: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
May  1 20:12:27.000: INFO: stdout: "affinity-clusterip-timeout-c6vv5"
May  1 20:12:27.000: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5748, will wait for the garbage collector to delete the pods 05/01/23 20:12:27.082
May  1 20:12:27.197: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 38.517145ms
May  1 20:12:27.397: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.290177ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:12:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5748" for this suite. 05/01/23 20:12:31.16
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":230,"skipped":4391,"failed":0}
------------------------------
• [SLOW TEST] [42.445 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:11:48.739
    May  1 20:11:48.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:11:48.742
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:11:48.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:11:48.977
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-5748 05/01/23 20:11:49.051
    May  1 20:11:49.209: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5748" to be "running and ready"
    May  1 20:11:49.275: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 64.524935ms
    May  1 20:11:49.276: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:11:51.298: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.087449982s
    May  1 20:11:51.298: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  1 20:11:51.298: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  1 20:11:51.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  1 20:11:52.112: INFO: rc: 7
    May  1 20:11:52.172: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  1 20:11:52.191: INFO: Pod kube-proxy-mode-detector no longer exists
    May  1 20:11:52.191: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-clusterip-timeout in namespace services-5748 05/01/23 20:11:52.191
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-5748 05/01/23 20:11:52.296
    I0501 20:11:52.367977      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5748, replica count: 3
    I0501 20:11:55.439923      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:11:58.440212      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:11:58.481: INFO: Creating new exec pod
    May  1 20:11:58.565: INFO: Waiting up to 5m0s for pod "execpod-affinityn74mw" in namespace "services-5748" to be "running"
    May  1 20:11:58.607: INFO: Pod "execpod-affinityn74mw": Phase="Pending", Reason="", readiness=false. Elapsed: 41.639774ms
    May  1 20:12:00.656: INFO: Pod "execpod-affinityn74mw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090318361s
    May  1 20:12:02.652: INFO: Pod "execpod-affinityn74mw": Phase="Running", Reason="", readiness=true. Elapsed: 4.086748119s
    May  1 20:12:02.652: INFO: Pod "execpod-affinityn74mw" satisfied condition "running"
    May  1 20:12:03.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    May  1 20:12:04.097: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    May  1 20:12:04.097: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:12:04.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.242.48 80'
    May  1 20:12:04.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.242.48 80\nConnection to 172.21.242.48 80 port [tcp/http] succeeded!\n"
    May  1 20:12:04.668: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:12:04.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.242.48:80/ ; done'
    May  1 20:12:05.458: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
    May  1 20:12:05.458: INFO: stdout: "\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9\naffinity-clusterip-timeout-kd9f9"
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Received response from host: affinity-clusterip-timeout-kd9f9
    May  1 20:12:05.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.242.48:80/'
    May  1 20:12:06.399: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
    May  1 20:12:06.399: INFO: stdout: "affinity-clusterip-timeout-kd9f9"
    May  1 20:12:26.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5748 exec execpod-affinityn74mw -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.242.48:80/'
    May  1 20:12:27.000: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.242.48:80/\n"
    May  1 20:12:27.000: INFO: stdout: "affinity-clusterip-timeout-c6vv5"
    May  1 20:12:27.000: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5748, will wait for the garbage collector to delete the pods 05/01/23 20:12:27.082
    May  1 20:12:27.197: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 38.517145ms
    May  1 20:12:27.397: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.290177ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:12:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5748" for this suite. 05/01/23 20:12:31.16
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:12:31.192
May  1 20:12:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename events 05/01/23 20:12:31.195
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:31.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:31.355
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/01/23 20:12:31.37
STEP: listing events in all namespaces 05/01/23 20:12:31.435
STEP: listing events in test namespace 05/01/23 20:12:31.594
STEP: listing events with field selection filtering on source 05/01/23 20:12:31.622
STEP: listing events with field selection filtering on reportingController 05/01/23 20:12:31.642
STEP: getting the test event 05/01/23 20:12:31.681
STEP: patching the test event 05/01/23 20:12:31.709
STEP: getting the test event 05/01/23 20:12:31.781
STEP: updating the test event 05/01/23 20:12:31.807
STEP: getting the test event 05/01/23 20:12:31.859
STEP: deleting the test event 05/01/23 20:12:31.875
STEP: listing events in all namespaces 05/01/23 20:12:31.918
STEP: listing events in test namespace 05/01/23 20:12:32.077
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
May  1 20:12:32.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4777" for this suite. 05/01/23 20:12:32.156
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":231,"skipped":4400,"failed":0}
------------------------------
• [1.004 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:12:31.192
    May  1 20:12:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename events 05/01/23 20:12:31.195
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:31.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:31.355
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/01/23 20:12:31.37
    STEP: listing events in all namespaces 05/01/23 20:12:31.435
    STEP: listing events in test namespace 05/01/23 20:12:31.594
    STEP: listing events with field selection filtering on source 05/01/23 20:12:31.622
    STEP: listing events with field selection filtering on reportingController 05/01/23 20:12:31.642
    STEP: getting the test event 05/01/23 20:12:31.681
    STEP: patching the test event 05/01/23 20:12:31.709
    STEP: getting the test event 05/01/23 20:12:31.781
    STEP: updating the test event 05/01/23 20:12:31.807
    STEP: getting the test event 05/01/23 20:12:31.859
    STEP: deleting the test event 05/01/23 20:12:31.875
    STEP: listing events in all namespaces 05/01/23 20:12:31.918
    STEP: listing events in test namespace 05/01/23 20:12:32.077
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    May  1 20:12:32.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-4777" for this suite. 05/01/23 20:12:32.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:12:32.2
May  1 20:12:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename disruption 05/01/23 20:12:32.203
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:32.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:32.329
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 05/01/23 20:12:32.366
STEP: Waiting for the pdb to be processed 05/01/23 20:12:32.402
STEP: First trying to evict a pod which shouldn't be evictable 05/01/23 20:12:34.495
STEP: Waiting for all pods to be running 05/01/23 20:12:34.496
May  1 20:12:34.520: INFO: pods: 0 < 3
May  1 20:12:36.548: INFO: running pods: 0 < 3
STEP: locating a running pod 05/01/23 20:12:38.542
STEP: Updating the pdb to allow a pod to be evicted 05/01/23 20:12:38.614
STEP: Waiting for the pdb to be processed 05/01/23 20:12:38.706
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/01/23 20:12:38.74
STEP: Waiting for all pods to be running 05/01/23 20:12:38.74
STEP: Waiting for the pdb to observed all healthy pods 05/01/23 20:12:38.766
STEP: Patching the pdb to disallow a pod to be evicted 05/01/23 20:12:38.902
STEP: Waiting for the pdb to be processed 05/01/23 20:12:39.01
STEP: Waiting for all pods to be running 05/01/23 20:12:41.053
May  1 20:12:41.096: INFO: running pods: 2 < 3
STEP: locating a running pod 05/01/23 20:12:43.126
STEP: Deleting the pdb to allow a pod to be evicted 05/01/23 20:12:43.252
STEP: Waiting for the pdb to be deleted 05/01/23 20:12:43.335
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/01/23 20:12:43.367
STEP: Waiting for all pods to be running 05/01/23 20:12:43.367
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
May  1 20:12:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7437" for this suite. 05/01/23 20:12:43.492
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":232,"skipped":4411,"failed":0}
------------------------------
• [SLOW TEST] [11.331 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:12:32.2
    May  1 20:12:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename disruption 05/01/23 20:12:32.203
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:32.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:32.329
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 05/01/23 20:12:32.366
    STEP: Waiting for the pdb to be processed 05/01/23 20:12:32.402
    STEP: First trying to evict a pod which shouldn't be evictable 05/01/23 20:12:34.495
    STEP: Waiting for all pods to be running 05/01/23 20:12:34.496
    May  1 20:12:34.520: INFO: pods: 0 < 3
    May  1 20:12:36.548: INFO: running pods: 0 < 3
    STEP: locating a running pod 05/01/23 20:12:38.542
    STEP: Updating the pdb to allow a pod to be evicted 05/01/23 20:12:38.614
    STEP: Waiting for the pdb to be processed 05/01/23 20:12:38.706
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/01/23 20:12:38.74
    STEP: Waiting for all pods to be running 05/01/23 20:12:38.74
    STEP: Waiting for the pdb to observed all healthy pods 05/01/23 20:12:38.766
    STEP: Patching the pdb to disallow a pod to be evicted 05/01/23 20:12:38.902
    STEP: Waiting for the pdb to be processed 05/01/23 20:12:39.01
    STEP: Waiting for all pods to be running 05/01/23 20:12:41.053
    May  1 20:12:41.096: INFO: running pods: 2 < 3
    STEP: locating a running pod 05/01/23 20:12:43.126
    STEP: Deleting the pdb to allow a pod to be evicted 05/01/23 20:12:43.252
    STEP: Waiting for the pdb to be deleted 05/01/23 20:12:43.335
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/01/23 20:12:43.367
    STEP: Waiting for all pods to be running 05/01/23 20:12:43.367
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    May  1 20:12:43.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7437" for this suite. 05/01/23 20:12:43.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:12:43.534
May  1 20:12:43.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename cronjob 05/01/23 20:12:43.536
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:43.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:43.664
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/01/23 20:12:43.68
STEP: creating 05/01/23 20:12:43.68
STEP: getting 05/01/23 20:12:43.704
STEP: listing 05/01/23 20:12:43.729
STEP: watching 05/01/23 20:12:43.751
May  1 20:12:43.752: INFO: starting watch
STEP: cluster-wide listing 05/01/23 20:12:43.758
STEP: cluster-wide watching 05/01/23 20:12:43.791
May  1 20:12:43.791: INFO: starting watch
STEP: patching 05/01/23 20:12:43.815
STEP: updating 05/01/23 20:12:43.836
May  1 20:12:43.870: INFO: waiting for watch events with expected annotations
May  1 20:12:43.870: INFO: saw patched and updated annotations
STEP: patching /status 05/01/23 20:12:43.87
STEP: updating /status 05/01/23 20:12:43.889
STEP: get /status 05/01/23 20:12:43.92
STEP: deleting 05/01/23 20:12:43.948
STEP: deleting a collection 05/01/23 20:12:44.006
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  1 20:12:44.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2404" for this suite. 05/01/23 20:12:44.068
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":233,"skipped":4429,"failed":0}
------------------------------
• [0.558 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:12:43.534
    May  1 20:12:43.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename cronjob 05/01/23 20:12:43.536
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:43.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:43.664
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/01/23 20:12:43.68
    STEP: creating 05/01/23 20:12:43.68
    STEP: getting 05/01/23 20:12:43.704
    STEP: listing 05/01/23 20:12:43.729
    STEP: watching 05/01/23 20:12:43.751
    May  1 20:12:43.752: INFO: starting watch
    STEP: cluster-wide listing 05/01/23 20:12:43.758
    STEP: cluster-wide watching 05/01/23 20:12:43.791
    May  1 20:12:43.791: INFO: starting watch
    STEP: patching 05/01/23 20:12:43.815
    STEP: updating 05/01/23 20:12:43.836
    May  1 20:12:43.870: INFO: waiting for watch events with expected annotations
    May  1 20:12:43.870: INFO: saw patched and updated annotations
    STEP: patching /status 05/01/23 20:12:43.87
    STEP: updating /status 05/01/23 20:12:43.889
    STEP: get /status 05/01/23 20:12:43.92
    STEP: deleting 05/01/23 20:12:43.948
    STEP: deleting a collection 05/01/23 20:12:44.006
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  1 20:12:44.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2404" for this suite. 05/01/23 20:12:44.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:12:44.096
May  1 20:12:44.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:12:44.099
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:44.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:44.193
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1857 05/01/23 20:12:44.213
STEP: creating replication controller nodeport-test in namespace services-1857 05/01/23 20:12:44.311
I0501 20:12:44.339199      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1857, replica count: 2
I0501 20:12:47.390986      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:12:50.392402      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:12:50.392: INFO: Creating new exec pod
May  1 20:12:50.467: INFO: Waiting up to 5m0s for pod "execpodqqrtz" in namespace "services-1857" to be "running"
May  1 20:12:50.487: INFO: Pod "execpodqqrtz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.481868ms
May  1 20:12:52.507: INFO: Pod "execpodqqrtz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040346323s
May  1 20:12:54.517: INFO: Pod "execpodqqrtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.050017708s
May  1 20:12:54.517: INFO: Pod "execpodqqrtz" satisfied condition "running"
May  1 20:12:55.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  1 20:12:56.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  1 20:12:56.695: INFO: stdout: ""
May  1 20:12:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May  1 20:12:59.460: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May  1 20:12:59.460: INFO: stdout: "nodeport-test-pjcct"
May  1 20:12:59.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.5.21 80'
May  1 20:13:00.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.5.21 80\nConnection to 172.21.5.21 80 port [tcp/http] succeeded!\n"
May  1 20:13:00.102: INFO: stdout: ""
May  1 20:13:01.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.5.21 80'
May  1 20:13:01.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.5.21 80\nConnection to 172.21.5.21 80 port [tcp/http] succeeded!\n"
May  1 20:13:01.598: INFO: stdout: "nodeport-test-55bzq"
May  1 20:13:01.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30248'
May  1 20:13:02.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30248\nConnection to 10.45.145.71 30248 port [tcp/*] succeeded!\n"
May  1 20:13:02.222: INFO: stdout: ""
May  1 20:13:03.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30248'
May  1 20:13:03.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30248\nConnection to 10.45.145.71 30248 port [tcp/*] succeeded!\n"
May  1 20:13:03.877: INFO: stdout: "nodeport-test-pjcct"
May  1 20:13:03.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30248'
May  1 20:13:04.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30248\nConnection to 10.45.145.126 30248 port [tcp/*] succeeded!\n"
May  1 20:13:04.330: INFO: stdout: "nodeport-test-pjcct"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:13:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1857" for this suite. 05/01/23 20:13:04.364
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":234,"skipped":4437,"failed":0}
------------------------------
• [SLOW TEST] [20.307 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:12:44.096
    May  1 20:12:44.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:12:44.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:12:44.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:12:44.193
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1857 05/01/23 20:12:44.213
    STEP: creating replication controller nodeport-test in namespace services-1857 05/01/23 20:12:44.311
    I0501 20:12:44.339199      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1857, replica count: 2
    I0501 20:12:47.390986      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:12:50.392402      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:12:50.392: INFO: Creating new exec pod
    May  1 20:12:50.467: INFO: Waiting up to 5m0s for pod "execpodqqrtz" in namespace "services-1857" to be "running"
    May  1 20:12:50.487: INFO: Pod "execpodqqrtz": Phase="Pending", Reason="", readiness=false. Elapsed: 20.481868ms
    May  1 20:12:52.507: INFO: Pod "execpodqqrtz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040346323s
    May  1 20:12:54.517: INFO: Pod "execpodqqrtz": Phase="Running", Reason="", readiness=true. Elapsed: 4.050017708s
    May  1 20:12:54.517: INFO: Pod "execpodqqrtz" satisfied condition "running"
    May  1 20:12:55.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  1 20:12:56.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  1 20:12:56.695: INFO: stdout: ""
    May  1 20:12:57.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    May  1 20:12:59.460: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May  1 20:12:59.460: INFO: stdout: "nodeport-test-pjcct"
    May  1 20:12:59.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.5.21 80'
    May  1 20:13:00.102: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.5.21 80\nConnection to 172.21.5.21 80 port [tcp/http] succeeded!\n"
    May  1 20:13:00.102: INFO: stdout: ""
    May  1 20:13:01.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.5.21 80'
    May  1 20:13:01.598: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.5.21 80\nConnection to 172.21.5.21 80 port [tcp/http] succeeded!\n"
    May  1 20:13:01.598: INFO: stdout: "nodeport-test-55bzq"
    May  1 20:13:01.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30248'
    May  1 20:13:02.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30248\nConnection to 10.45.145.71 30248 port [tcp/*] succeeded!\n"
    May  1 20:13:02.222: INFO: stdout: ""
    May  1 20:13:03.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30248'
    May  1 20:13:03.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30248\nConnection to 10.45.145.71 30248 port [tcp/*] succeeded!\n"
    May  1 20:13:03.877: INFO: stdout: "nodeport-test-pjcct"
    May  1 20:13:03.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-1857 exec execpodqqrtz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30248'
    May  1 20:13:04.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30248\nConnection to 10.45.145.126 30248 port [tcp/*] succeeded!\n"
    May  1 20:13:04.330: INFO: stdout: "nodeport-test-pjcct"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:13:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1857" for this suite. 05/01/23 20:13:04.364
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:04.403
May  1 20:13:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:13:04.406
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:04.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:04.591
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:13:04.64
May  1 20:13:04.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  1 20:13:04.898: INFO: stderr: ""
May  1 20:13:04.898: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/01/23 20:13:04.898
May  1 20:13:04.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May  1 20:13:06.683: INFO: stderr: ""
May  1 20:13:06.683: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:13:06.684
May  1 20:13:06.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 delete pods e2e-test-httpd-pod'
May  1 20:13:10.736: INFO: stderr: ""
May  1 20:13:10.736: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:13:10.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5430" for this suite. 05/01/23 20:13:10.808
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":235,"skipped":4441,"failed":0}
------------------------------
• [SLOW TEST] [6.449 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:04.403
    May  1 20:13:04.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:13:04.406
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:04.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:04.591
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:13:04.64
    May  1 20:13:04.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  1 20:13:04.898: INFO: stderr: ""
    May  1 20:13:04.898: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/01/23 20:13:04.898
    May  1 20:13:04.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    May  1 20:13:06.683: INFO: stderr: ""
    May  1 20:13:06.683: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:13:06.684
    May  1 20:13:06.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-5430 delete pods e2e-test-httpd-pod'
    May  1 20:13:10.736: INFO: stderr: ""
    May  1 20:13:10.736: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:13:10.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5430" for this suite. 05/01/23 20:13:10.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:10.854
May  1 20:13:10.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:13:10.856
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:10.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:11.007
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 05/01/23 20:13:11.038
STEP: fetching the ConfigMap 05/01/23 20:13:11.077
STEP: patching the ConfigMap 05/01/23 20:13:11.102
STEP: listing all ConfigMaps in all namespaces with a label selector 05/01/23 20:13:11.167
STEP: deleting the ConfigMap by collection with a label selector 05/01/23 20:13:11.374
STEP: listing all ConfigMaps in test namespace 05/01/23 20:13:11.456
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:13:11.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8245" for this suite. 05/01/23 20:13:11.51
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":236,"skipped":4471,"failed":0}
------------------------------
• [0.727 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:10.854
    May  1 20:13:10.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:13:10.856
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:10.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:11.007
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 05/01/23 20:13:11.038
    STEP: fetching the ConfigMap 05/01/23 20:13:11.077
    STEP: patching the ConfigMap 05/01/23 20:13:11.102
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/01/23 20:13:11.167
    STEP: deleting the ConfigMap by collection with a label selector 05/01/23 20:13:11.374
    STEP: listing all ConfigMaps in test namespace 05/01/23 20:13:11.456
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:13:11.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8245" for this suite. 05/01/23 20:13:11.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:11.592
May  1 20:13:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:13:11.595
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:11.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:11.705
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-8318/configmap-test-a88ba2d8-c569-4c86-81e5-012096075d67 05/01/23 20:13:11.724
STEP: Creating a pod to test consume configMaps 05/01/23 20:13:11.756
May  1 20:13:11.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069" in namespace "configmap-8318" to be "Succeeded or Failed"
May  1 20:13:11.912: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 55.960566ms
May  1 20:13:13.937: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080641064s
May  1 20:13:15.934: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077920723s
May  1 20:13:17.947: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090012459s
STEP: Saw pod success 05/01/23 20:13:17.947
May  1 20:13:17.947: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069" satisfied condition "Succeeded or Failed"
May  1 20:13:17.966: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 container env-test: <nil>
STEP: delete the pod 05/01/23 20:13:18.092
May  1 20:13:18.141: INFO: Waiting for pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 to disappear
May  1 20:13:18.169: INFO: Pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:13:18.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8318" for this suite. 05/01/23 20:13:18.195
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":237,"skipped":4535,"failed":0}
------------------------------
• [SLOW TEST] [6.624 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:11.592
    May  1 20:13:11.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:13:11.595
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:11.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:11.705
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-8318/configmap-test-a88ba2d8-c569-4c86-81e5-012096075d67 05/01/23 20:13:11.724
    STEP: Creating a pod to test consume configMaps 05/01/23 20:13:11.756
    May  1 20:13:11.856: INFO: Waiting up to 5m0s for pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069" in namespace "configmap-8318" to be "Succeeded or Failed"
    May  1 20:13:11.912: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 55.960566ms
    May  1 20:13:13.937: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080641064s
    May  1 20:13:15.934: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077920723s
    May  1 20:13:17.947: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090012459s
    STEP: Saw pod success 05/01/23 20:13:17.947
    May  1 20:13:17.947: INFO: Pod "pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069" satisfied condition "Succeeded or Failed"
    May  1 20:13:17.966: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 container env-test: <nil>
    STEP: delete the pod 05/01/23 20:13:18.092
    May  1 20:13:18.141: INFO: Waiting for pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 to disappear
    May  1 20:13:18.169: INFO: Pod pod-configmaps-54e4f4b7-64cf-4649-bb40-db2a0bec2069 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:13:18.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8318" for this suite. 05/01/23 20:13:18.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:18.227
May  1 20:13:18.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:13:18.229
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:18.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:18.349
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-39e83c97-a154-4c5e-90e7-e219a9ef199c 05/01/23 20:13:18.376
STEP: Creating a pod to test consume configMaps 05/01/23 20:13:18.397
May  1 20:13:18.502: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25" in namespace "projected-1435" to be "Succeeded or Failed"
May  1 20:13:18.539: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.860612ms
May  1 20:13:20.566: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063671901s
May  1 20:13:22.562: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059340217s
May  1 20:13:24.559: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056650027s
STEP: Saw pod success 05/01/23 20:13:24.559
May  1 20:13:24.559: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25" satisfied condition "Succeeded or Failed"
May  1 20:13:24.578: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:13:24.625
May  1 20:13:24.702: INFO: Waiting for pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 to disappear
May  1 20:13:24.722: INFO: Pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:13:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1435" for this suite. 05/01/23 20:13:24.746
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":238,"skipped":4555,"failed":0}
------------------------------
• [SLOW TEST] [6.547 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:18.227
    May  1 20:13:18.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:13:18.229
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:18.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:18.349
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-39e83c97-a154-4c5e-90e7-e219a9ef199c 05/01/23 20:13:18.376
    STEP: Creating a pod to test consume configMaps 05/01/23 20:13:18.397
    May  1 20:13:18.502: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25" in namespace "projected-1435" to be "Succeeded or Failed"
    May  1 20:13:18.539: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.860612ms
    May  1 20:13:20.566: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063671901s
    May  1 20:13:22.562: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059340217s
    May  1 20:13:24.559: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056650027s
    STEP: Saw pod success 05/01/23 20:13:24.559
    May  1 20:13:24.559: INFO: Pod "pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25" satisfied condition "Succeeded or Failed"
    May  1 20:13:24.578: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:13:24.625
    May  1 20:13:24.702: INFO: Waiting for pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 to disappear
    May  1 20:13:24.722: INFO: Pod pod-projected-configmaps-bf19476e-f7cf-4c9c-a0ec-9e12b262ab25 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:13:24.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1435" for this suite. 05/01/23 20:13:24.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:24.779
May  1 20:13:24.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:13:24.781
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:24.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:24.948
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
May  1 20:13:25.092: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-7967fcd5-e57c-4dd7-bca6-575162caeedd 05/01/23 20:13:25.092
STEP: Creating configMap with name cm-test-opt-upd-4a8c3186-6950-4002-a3db-1f7b9f9ef0d5 05/01/23 20:13:25.126
STEP: Creating the pod 05/01/23 20:13:25.148
May  1 20:13:25.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8" in namespace "configmap-6944" to be "running and ready"
May  1 20:13:25.306: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.250925ms
May  1 20:13:25.306: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:13:27.348: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067824131s
May  1 20:13:27.348: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:13:29.331: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.05104817s
May  1 20:13:29.331: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Running (Ready = true)
May  1 20:13:29.331: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-7967fcd5-e57c-4dd7-bca6-575162caeedd 05/01/23 20:13:29.514
STEP: Updating configmap cm-test-opt-upd-4a8c3186-6950-4002-a3db-1f7b9f9ef0d5 05/01/23 20:13:29.547
STEP: Creating configMap with name cm-test-opt-create-78f7fa88-7156-41f6-942d-5f3a273cb654 05/01/23 20:13:29.57
STEP: waiting to observe update in volume 05/01/23 20:13:29.608
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:13:32.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6944" for this suite. 05/01/23 20:13:32.207
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":239,"skipped":4577,"failed":0}
------------------------------
• [SLOW TEST] [7.497 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:24.779
    May  1 20:13:24.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:13:24.781
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:24.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:24.948
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    May  1 20:13:25.092: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-7967fcd5-e57c-4dd7-bca6-575162caeedd 05/01/23 20:13:25.092
    STEP: Creating configMap with name cm-test-opt-upd-4a8c3186-6950-4002-a3db-1f7b9f9ef0d5 05/01/23 20:13:25.126
    STEP: Creating the pod 05/01/23 20:13:25.148
    May  1 20:13:25.280: INFO: Waiting up to 5m0s for pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8" in namespace "configmap-6944" to be "running and ready"
    May  1 20:13:25.306: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.250925ms
    May  1 20:13:25.306: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:13:27.348: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067824131s
    May  1 20:13:27.348: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:13:29.331: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.05104817s
    May  1 20:13:29.331: INFO: The phase of Pod pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8 is Running (Ready = true)
    May  1 20:13:29.331: INFO: Pod "pod-configmaps-24603215-15e4-4e56-9614-640a6e61f2a8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-7967fcd5-e57c-4dd7-bca6-575162caeedd 05/01/23 20:13:29.514
    STEP: Updating configmap cm-test-opt-upd-4a8c3186-6950-4002-a3db-1f7b9f9ef0d5 05/01/23 20:13:29.547
    STEP: Creating configMap with name cm-test-opt-create-78f7fa88-7156-41f6-942d-5f3a273cb654 05/01/23 20:13:29.57
    STEP: waiting to observe update in volume 05/01/23 20:13:29.608
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:13:32.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6944" for this suite. 05/01/23 20:13:32.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:13:32.294
May  1 20:13:32.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-runtime 05/01/23 20:13:32.299
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:32.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:32.444
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/01/23 20:13:32.578
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/01/23 20:13:50.211
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/01/23 20:13:50.232
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/01/23 20:13:50.275
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/01/23 20:13:50.275
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/01/23 20:13:50.41
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/01/23 20:13:54.541
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/01/23 20:13:57.674
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/01/23 20:13:57.791
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/01/23 20:13:57.791
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/01/23 20:13:58.157
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/01/23 20:13:59.232
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/01/23 20:14:04.489
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/01/23 20:14:04.535
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/01/23 20:14:04.536
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  1 20:14:04.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2664" for this suite. 05/01/23 20:14:04.875
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":240,"skipped":4622,"failed":0}
------------------------------
• [SLOW TEST] [32.610 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:13:32.294
    May  1 20:13:32.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-runtime 05/01/23 20:13:32.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:13:32.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:13:32.444
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/01/23 20:13:32.578
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/01/23 20:13:50.211
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/01/23 20:13:50.232
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/01/23 20:13:50.275
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/01/23 20:13:50.275
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/01/23 20:13:50.41
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/01/23 20:13:54.541
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/01/23 20:13:57.674
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/01/23 20:13:57.791
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/01/23 20:13:57.791
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/01/23 20:13:58.157
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/01/23 20:13:59.232
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/01/23 20:14:04.489
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/01/23 20:14:04.535
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/01/23 20:14:04.536
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  1 20:14:04.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2664" for this suite. 05/01/23 20:14:04.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:14:04.917
May  1 20:14:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 20:14:04.919
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:05.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:05.025
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 05/01/23 20:14:05.057
W0501 20:14:05.084598      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 05/01/23 20:14:05.085
STEP: delete a job 05/01/23 20:14:09.129
STEP: deleting Job.batch foo in namespace job-4947, will wait for the garbage collector to delete the pods 05/01/23 20:14:09.129
May  1 20:14:09.227: INFO: Deleting Job.batch foo took: 28.141182ms
May  1 20:14:09.427: INFO: Terminating Job.batch foo pods took: 200.411716ms
STEP: Ensuring job was deleted 05/01/23 20:14:43.428
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 20:14:43.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4947" for this suite. 05/01/23 20:14:43.535
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":241,"skipped":4638,"failed":0}
------------------------------
• [SLOW TEST] [38.676 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:14:04.917
    May  1 20:14:04.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 20:14:04.919
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:05.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:05.025
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 05/01/23 20:14:05.057
    W0501 20:14:05.084598      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 05/01/23 20:14:05.085
    STEP: delete a job 05/01/23 20:14:09.129
    STEP: deleting Job.batch foo in namespace job-4947, will wait for the garbage collector to delete the pods 05/01/23 20:14:09.129
    May  1 20:14:09.227: INFO: Deleting Job.batch foo took: 28.141182ms
    May  1 20:14:09.427: INFO: Terminating Job.batch foo pods took: 200.411716ms
    STEP: Ensuring job was deleted 05/01/23 20:14:43.428
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 20:14:43.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4947" for this suite. 05/01/23 20:14:43.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:14:43.598
May  1 20:14:43.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:14:43.601
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:43.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:43.696
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-d0fbca65-0624-4c6e-8cd1-a7604e11cad4 05/01/23 20:14:43.732
STEP: Creating secret with name secret-projected-all-test-volume-f37f3410-3605-4767-9087-99e0b17dca76 05/01/23 20:14:43.777
STEP: Creating a pod to test Check all projections for projected volume plugin 05/01/23 20:14:43.795
May  1 20:14:43.975: INFO: Waiting up to 5m0s for pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147" in namespace "projected-7232" to be "Succeeded or Failed"
May  1 20:14:44.009: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 33.67149ms
May  1 20:14:46.031: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056327751s
May  1 20:14:48.060: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08453765s
May  1 20:14:50.042: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066855923s
STEP: Saw pod success 05/01/23 20:14:50.042
May  1 20:14:50.042: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147" satisfied condition "Succeeded or Failed"
May  1 20:14:50.067: INFO: Trying to get logs from node 10.45.145.124 pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 container projected-all-volume-test: <nil>
STEP: delete the pod 05/01/23 20:14:50.12
May  1 20:14:50.169: INFO: Waiting for pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 to disappear
May  1 20:14:50.211: INFO: Pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
May  1 20:14:50.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7232" for this suite. 05/01/23 20:14:50.235
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":242,"skipped":4646,"failed":0}
------------------------------
• [SLOW TEST] [6.662 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:14:43.598
    May  1 20:14:43.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:14:43.601
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:43.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:43.696
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-d0fbca65-0624-4c6e-8cd1-a7604e11cad4 05/01/23 20:14:43.732
    STEP: Creating secret with name secret-projected-all-test-volume-f37f3410-3605-4767-9087-99e0b17dca76 05/01/23 20:14:43.777
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/01/23 20:14:43.795
    May  1 20:14:43.975: INFO: Waiting up to 5m0s for pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147" in namespace "projected-7232" to be "Succeeded or Failed"
    May  1 20:14:44.009: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 33.67149ms
    May  1 20:14:46.031: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056327751s
    May  1 20:14:48.060: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08453765s
    May  1 20:14:50.042: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066855923s
    STEP: Saw pod success 05/01/23 20:14:50.042
    May  1 20:14:50.042: INFO: Pod "projected-volume-0002193a-659a-4644-ad63-18f08037e147" satisfied condition "Succeeded or Failed"
    May  1 20:14:50.067: INFO: Trying to get logs from node 10.45.145.124 pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 container projected-all-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:14:50.12
    May  1 20:14:50.169: INFO: Waiting for pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 to disappear
    May  1 20:14:50.211: INFO: Pod projected-volume-0002193a-659a-4644-ad63-18f08037e147 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    May  1 20:14:50.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7232" for this suite. 05/01/23 20:14:50.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:14:50.261
May  1 20:14:50.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 20:14:50.266
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:50.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:50.337
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-80c16b85-8d41-4f08-a980-b280d9274e1c 05/01/23 20:14:50.356
STEP: Creating a pod to test consume secrets 05/01/23 20:14:50.413
May  1 20:14:50.555: INFO: Waiting up to 5m0s for pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca" in namespace "secrets-7942" to be "Succeeded or Failed"
May  1 20:14:50.589: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 33.365244ms
May  1 20:14:52.616: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060823705s
May  1 20:14:54.609: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05343187s
May  1 20:14:56.620: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064065257s
STEP: Saw pod success 05/01/23 20:14:56.62
May  1 20:14:56.620: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca" satisfied condition "Succeeded or Failed"
May  1 20:14:56.638: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:14:56.68
May  1 20:14:56.741: INFO: Waiting for pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca to disappear
May  1 20:14:56.791: INFO: Pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 20:14:56.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7942" for this suite. 05/01/23 20:14:56.889
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":243,"skipped":4653,"failed":0}
------------------------------
• [SLOW TEST] [6.667 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:14:50.261
    May  1 20:14:50.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 20:14:50.266
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:50.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:50.337
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-80c16b85-8d41-4f08-a980-b280d9274e1c 05/01/23 20:14:50.356
    STEP: Creating a pod to test consume secrets 05/01/23 20:14:50.413
    May  1 20:14:50.555: INFO: Waiting up to 5m0s for pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca" in namespace "secrets-7942" to be "Succeeded or Failed"
    May  1 20:14:50.589: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 33.365244ms
    May  1 20:14:52.616: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060823705s
    May  1 20:14:54.609: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05343187s
    May  1 20:14:56.620: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064065257s
    STEP: Saw pod success 05/01/23 20:14:56.62
    May  1 20:14:56.620: INFO: Pod "pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca" satisfied condition "Succeeded or Failed"
    May  1 20:14:56.638: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:14:56.68
    May  1 20:14:56.741: INFO: Waiting for pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca to disappear
    May  1 20:14:56.791: INFO: Pod pod-secrets-39ffef88-0c51-42e9-83a7-a9df61a36bca no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 20:14:56.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7942" for this suite. 05/01/23 20:14:56.889
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:14:56.93
May  1 20:14:56.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename subpath 05/01/23 20:14:56.933
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:57.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:57.049
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/01/23 20:14:57.069
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-fpbq 05/01/23 20:14:57.173
STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:14:57.174
May  1 20:14:57.265: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fpbq" in namespace "subpath-2112" to be "Succeeded or Failed"
May  1 20:14:57.309: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 44.164803ms
May  1 20:14:59.330: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06472594s
May  1 20:15:01.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.071182988s
May  1 20:15:03.344: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.078447431s
May  1 20:15:05.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.070903715s
May  1 20:15:07.339: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.07432349s
May  1 20:15:09.342: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.076893752s
May  1 20:15:11.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.07054254s
May  1 20:15:13.353: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.087630454s
May  1 20:15:15.345: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.079809595s
May  1 20:15:17.338: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.072431771s
May  1 20:15:19.332: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 22.066898871s
May  1 20:15:21.339: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=false. Elapsed: 24.074139917s
May  1 20:15:23.333: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.067752027s
STEP: Saw pod success 05/01/23 20:15:23.333
May  1 20:15:23.333: INFO: Pod "pod-subpath-test-projected-fpbq" satisfied condition "Succeeded or Failed"
May  1 20:15:23.385: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-projected-fpbq container test-container-subpath-projected-fpbq: <nil>
STEP: delete the pod 05/01/23 20:15:23.466
May  1 20:15:23.521: INFO: Waiting for pod pod-subpath-test-projected-fpbq to disappear
May  1 20:15:23.546: INFO: Pod pod-subpath-test-projected-fpbq no longer exists
STEP: Deleting pod pod-subpath-test-projected-fpbq 05/01/23 20:15:23.546
May  1 20:15:23.547: INFO: Deleting pod "pod-subpath-test-projected-fpbq" in namespace "subpath-2112"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  1 20:15:23.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2112" for this suite. 05/01/23 20:15:23.588
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":244,"skipped":4655,"failed":0}
------------------------------
• [SLOW TEST] [26.698 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:14:56.93
    May  1 20:14:56.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename subpath 05/01/23 20:14:56.933
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:14:57.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:14:57.049
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/01/23 20:14:57.069
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-fpbq 05/01/23 20:14:57.173
    STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:14:57.174
    May  1 20:14:57.265: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fpbq" in namespace "subpath-2112" to be "Succeeded or Failed"
    May  1 20:14:57.309: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 44.164803ms
    May  1 20:14:59.330: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06472594s
    May  1 20:15:01.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.071182988s
    May  1 20:15:03.344: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.078447431s
    May  1 20:15:05.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.070903715s
    May  1 20:15:07.339: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.07432349s
    May  1 20:15:09.342: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.076893752s
    May  1 20:15:11.336: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.07054254s
    May  1 20:15:13.353: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.087630454s
    May  1 20:15:15.345: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.079809595s
    May  1 20:15:17.338: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.072431771s
    May  1 20:15:19.332: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=true. Elapsed: 22.066898871s
    May  1 20:15:21.339: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Running", Reason="", readiness=false. Elapsed: 24.074139917s
    May  1 20:15:23.333: INFO: Pod "pod-subpath-test-projected-fpbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.067752027s
    STEP: Saw pod success 05/01/23 20:15:23.333
    May  1 20:15:23.333: INFO: Pod "pod-subpath-test-projected-fpbq" satisfied condition "Succeeded or Failed"
    May  1 20:15:23.385: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-projected-fpbq container test-container-subpath-projected-fpbq: <nil>
    STEP: delete the pod 05/01/23 20:15:23.466
    May  1 20:15:23.521: INFO: Waiting for pod pod-subpath-test-projected-fpbq to disappear
    May  1 20:15:23.546: INFO: Pod pod-subpath-test-projected-fpbq no longer exists
    STEP: Deleting pod pod-subpath-test-projected-fpbq 05/01/23 20:15:23.546
    May  1 20:15:23.547: INFO: Deleting pod "pod-subpath-test-projected-fpbq" in namespace "subpath-2112"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  1 20:15:23.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2112" for this suite. 05/01/23 20:15:23.588
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:15:23.628
May  1 20:15:23.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:15:23.632
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:23.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:23.774
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-6753 05/01/23 20:15:23.808
STEP: creating service affinity-clusterip in namespace services-6753 05/01/23 20:15:23.808
STEP: creating replication controller affinity-clusterip in namespace services-6753 05/01/23 20:15:23.9
I0501 20:15:23.942364      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6753, replica count: 3
I0501 20:15:26.994283      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:15:27.027: INFO: Creating new exec pod
May  1 20:15:27.093: INFO: Waiting up to 5m0s for pod "execpod-affinitynsf5c" in namespace "services-6753" to be "running"
May  1 20:15:27.113: INFO: Pod "execpod-affinitynsf5c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.125787ms
May  1 20:15:29.139: INFO: Pod "execpod-affinitynsf5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045778967s
May  1 20:15:31.137: INFO: Pod "execpod-affinitynsf5c": Phase="Running", Reason="", readiness=true. Elapsed: 4.043630371s
May  1 20:15:31.137: INFO: Pod "execpod-affinitynsf5c" satisfied condition "running"
May  1 20:15:32.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May  1 20:15:32.657: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May  1 20:15:32.657: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:15:32.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.73.231 80'
May  1 20:15:33.648: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.73.231 80\nConnection to 172.21.73.231 80 port [tcp/http] succeeded!\n"
May  1 20:15:33.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:15:33.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.73.231:80/ ; done'
May  1 20:15:34.431: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n"
May  1 20:15:34.431: INFO: stdout: "\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr"
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
May  1 20:15:34.431: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6753, will wait for the garbage collector to delete the pods 05/01/23 20:15:34.499
May  1 20:15:34.639: INFO: Deleting ReplicationController affinity-clusterip took: 70.840109ms
May  1 20:15:34.842: INFO: Terminating ReplicationController affinity-clusterip pods took: 202.933495ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:15:38.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6753" for this suite. 05/01/23 20:15:38.084
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":245,"skipped":4655,"failed":0}
------------------------------
• [SLOW TEST] [14.483 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:15:23.628
    May  1 20:15:23.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:15:23.632
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:23.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:23.774
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-6753 05/01/23 20:15:23.808
    STEP: creating service affinity-clusterip in namespace services-6753 05/01/23 20:15:23.808
    STEP: creating replication controller affinity-clusterip in namespace services-6753 05/01/23 20:15:23.9
    I0501 20:15:23.942364      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6753, replica count: 3
    I0501 20:15:26.994283      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:15:27.027: INFO: Creating new exec pod
    May  1 20:15:27.093: INFO: Waiting up to 5m0s for pod "execpod-affinitynsf5c" in namespace "services-6753" to be "running"
    May  1 20:15:27.113: INFO: Pod "execpod-affinitynsf5c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.125787ms
    May  1 20:15:29.139: INFO: Pod "execpod-affinitynsf5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045778967s
    May  1 20:15:31.137: INFO: Pod "execpod-affinitynsf5c": Phase="Running", Reason="", readiness=true. Elapsed: 4.043630371s
    May  1 20:15:31.137: INFO: Pod "execpod-affinitynsf5c" satisfied condition "running"
    May  1 20:15:32.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    May  1 20:15:32.657: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May  1 20:15:32.657: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:15:32.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.73.231 80'
    May  1 20:15:33.648: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.73.231 80\nConnection to 172.21.73.231 80 port [tcp/http] succeeded!\n"
    May  1 20:15:33.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:15:33.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6753 exec execpod-affinitynsf5c -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.73.231:80/ ; done'
    May  1 20:15:34.431: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.73.231:80/\n"
    May  1 20:15:34.431: INFO: stdout: "\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr\naffinity-clusterip-hkqzr"
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Received response from host: affinity-clusterip-hkqzr
    May  1 20:15:34.431: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6753, will wait for the garbage collector to delete the pods 05/01/23 20:15:34.499
    May  1 20:15:34.639: INFO: Deleting ReplicationController affinity-clusterip took: 70.840109ms
    May  1 20:15:34.842: INFO: Terminating ReplicationController affinity-clusterip pods took: 202.933495ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:15:38.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6753" for this suite. 05/01/23 20:15:38.084
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:15:38.117
May  1 20:15:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:15:38.119
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:38.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:38.293
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6803 05/01/23 20:15:38.316
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/01/23 20:15:38.405
STEP: creating service externalsvc in namespace services-6803 05/01/23 20:15:38.405
STEP: creating replication controller externalsvc in namespace services-6803 05/01/23 20:15:38.453
I0501 20:15:38.488595      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6803, replica count: 2
I0501 20:15:41.552565      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:15:44.554599      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/01/23 20:15:44.575
May  1 20:15:44.665: INFO: Creating new exec pod
May  1 20:15:44.736: INFO: Waiting up to 5m0s for pod "execpodrcb6s" in namespace "services-6803" to be "running"
May  1 20:15:44.760: INFO: Pod "execpodrcb6s": Phase="Pending", Reason="", readiness=false. Elapsed: 23.654294ms
May  1 20:15:46.783: INFO: Pod "execpodrcb6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046354513s
May  1 20:15:48.779: INFO: Pod "execpodrcb6s": Phase="Running", Reason="", readiness=true. Elapsed: 4.042692531s
May  1 20:15:48.779: INFO: Pod "execpodrcb6s" satisfied condition "running"
May  1 20:15:48.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6803 exec execpodrcb6s -- /bin/sh -x -c nslookup nodeport-service.services-6803.svc.cluster.local'
May  1 20:15:49.274: INFO: stderr: "+ nslookup nodeport-service.services-6803.svc.cluster.local\n"
May  1 20:15:49.275: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6803.svc.cluster.local\tcanonical name = externalsvc.services-6803.svc.cluster.local.\nName:\texternalsvc.services-6803.svc.cluster.local\nAddress: 172.21.238.191\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6803, will wait for the garbage collector to delete the pods 05/01/23 20:15:49.275
May  1 20:15:49.379: INFO: Deleting ReplicationController externalsvc took: 35.739381ms
May  1 20:15:49.580: INFO: Terminating ReplicationController externalsvc pods took: 201.025046ms
May  1 20:15:53.118: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:15:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6803" for this suite. 05/01/23 20:15:53.251
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":246,"skipped":4677,"failed":0}
------------------------------
• [SLOW TEST] [15.167 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:15:38.117
    May  1 20:15:38.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:15:38.119
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:38.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:38.293
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-6803 05/01/23 20:15:38.316
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/01/23 20:15:38.405
    STEP: creating service externalsvc in namespace services-6803 05/01/23 20:15:38.405
    STEP: creating replication controller externalsvc in namespace services-6803 05/01/23 20:15:38.453
    I0501 20:15:38.488595      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6803, replica count: 2
    I0501 20:15:41.552565      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:15:44.554599      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/01/23 20:15:44.575
    May  1 20:15:44.665: INFO: Creating new exec pod
    May  1 20:15:44.736: INFO: Waiting up to 5m0s for pod "execpodrcb6s" in namespace "services-6803" to be "running"
    May  1 20:15:44.760: INFO: Pod "execpodrcb6s": Phase="Pending", Reason="", readiness=false. Elapsed: 23.654294ms
    May  1 20:15:46.783: INFO: Pod "execpodrcb6s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046354513s
    May  1 20:15:48.779: INFO: Pod "execpodrcb6s": Phase="Running", Reason="", readiness=true. Elapsed: 4.042692531s
    May  1 20:15:48.779: INFO: Pod "execpodrcb6s" satisfied condition "running"
    May  1 20:15:48.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6803 exec execpodrcb6s -- /bin/sh -x -c nslookup nodeport-service.services-6803.svc.cluster.local'
    May  1 20:15:49.274: INFO: stderr: "+ nslookup nodeport-service.services-6803.svc.cluster.local\n"
    May  1 20:15:49.275: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6803.svc.cluster.local\tcanonical name = externalsvc.services-6803.svc.cluster.local.\nName:\texternalsvc.services-6803.svc.cluster.local\nAddress: 172.21.238.191\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6803, will wait for the garbage collector to delete the pods 05/01/23 20:15:49.275
    May  1 20:15:49.379: INFO: Deleting ReplicationController externalsvc took: 35.739381ms
    May  1 20:15:49.580: INFO: Terminating ReplicationController externalsvc pods took: 201.025046ms
    May  1 20:15:53.118: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:15:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6803" for this suite. 05/01/23 20:15:53.251
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:15:53.286
May  1 20:15:53.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:15:53.288
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:53.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:53.396
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/01/23 20:15:53.455
May  1 20:15:53.537: INFO: Waiting up to 5m0s for pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910" in namespace "emptydir-932" to be "Succeeded or Failed"
May  1 20:15:53.583: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 46.611839ms
May  1 20:15:55.610: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072677612s
May  1 20:15:57.605: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06841803s
May  1 20:15:59.614: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077347703s
STEP: Saw pod success 05/01/23 20:15:59.614
May  1 20:15:59.615: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910" satisfied condition "Succeeded or Failed"
May  1 20:15:59.649: INFO: Trying to get logs from node 10.45.145.124 pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 container test-container: <nil>
STEP: delete the pod 05/01/23 20:15:59.699
May  1 20:15:59.757: INFO: Waiting for pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 to disappear
May  1 20:15:59.775: INFO: Pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:15:59.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-932" for this suite. 05/01/23 20:15:59.8
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4678,"failed":0}
------------------------------
• [SLOW TEST] [6.542 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:15:53.286
    May  1 20:15:53.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:15:53.288
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:53.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:53.396
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/01/23 20:15:53.455
    May  1 20:15:53.537: INFO: Waiting up to 5m0s for pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910" in namespace "emptydir-932" to be "Succeeded or Failed"
    May  1 20:15:53.583: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 46.611839ms
    May  1 20:15:55.610: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072677612s
    May  1 20:15:57.605: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06841803s
    May  1 20:15:59.614: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077347703s
    STEP: Saw pod success 05/01/23 20:15:59.614
    May  1 20:15:59.615: INFO: Pod "pod-c80659ab-ca2e-4173-bf30-e456382a3910" satisfied condition "Succeeded or Failed"
    May  1 20:15:59.649: INFO: Trying to get logs from node 10.45.145.124 pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 container test-container: <nil>
    STEP: delete the pod 05/01/23 20:15:59.699
    May  1 20:15:59.757: INFO: Waiting for pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 to disappear
    May  1 20:15:59.775: INFO: Pod pod-c80659ab-ca2e-4173-bf30-e456382a3910 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:15:59.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-932" for this suite. 05/01/23 20:15:59.8
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:15:59.831
May  1 20:15:59.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename endpointslice 05/01/23 20:15:59.833
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:59.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:59.918
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
May  1 20:16:00.043: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
May  1 20:16:00.043: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  1 20:16:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3483" for this suite. 05/01/23 20:16:00.206
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":248,"skipped":4679,"failed":0}
------------------------------
• [0.424 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:15:59.831
    May  1 20:15:59.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename endpointslice 05/01/23 20:15:59.833
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:15:59.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:15:59.918
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    May  1 20:16:00.043: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    May  1 20:16:00.043: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  1 20:16:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-3483" for this suite. 05/01/23 20:16:00.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:00.254
May  1 20:16:00.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename podtemplate 05/01/23 20:16:00.26
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:00.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:00.402
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
May  1 20:16:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6073" for this suite. 05/01/23 20:16:00.725
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":249,"skipped":4691,"failed":0}
------------------------------
• [0.501 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:00.254
    May  1 20:16:00.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename podtemplate 05/01/23 20:16:00.26
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:00.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:00.402
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    May  1 20:16:00.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6073" for this suite. 05/01/23 20:16:00.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:00.759
May  1 20:16:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename lease-test 05/01/23 20:16:00.76
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:00.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:00.866
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
May  1 20:16:01.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2911" for this suite. 05/01/23 20:16:01.299
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":250,"skipped":4720,"failed":0}
------------------------------
• [0.580 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:00.759
    May  1 20:16:00.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename lease-test 05/01/23 20:16:00.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:00.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:00.866
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    May  1 20:16:01.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2911" for this suite. 05/01/23 20:16:01.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:01.343
May  1 20:16:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 20:16:01.346
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:01.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:01.48
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 05/01/23 20:16:01.529
May  1 20:16:01.614: INFO: created test-pod-1
May  1 20:16:01.678: INFO: created test-pod-2
May  1 20:16:01.725: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/01/23 20:16:01.725
May  1 20:16:01.725: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5214' to be running and ready
May  1 20:16:01.801: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:01.802: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:01.802: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:01.802: INFO: 0 / 3 pods in namespace 'pods-5214' are running and ready (0 seconds elapsed)
May  1 20:16:01.802: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
May  1 20:16:01.802: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
May  1 20:16:01.802: INFO: test-pod-1  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:01.802: INFO: test-pod-2  10.45.145.124  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:01.802: INFO: test-pod-3  10.45.145.124  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:01.802: INFO: 
May  1 20:16:03.892: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:03.892: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:03.892: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:03.892: INFO: 0 / 3 pods in namespace 'pods-5214' are running and ready (2 seconds elapsed)
May  1 20:16:03.892: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
May  1 20:16:03.892: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
May  1 20:16:03.892: INFO: test-pod-1  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:03.892: INFO: test-pod-2  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:03.892: INFO: test-pod-3  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:03.892: INFO: 
May  1 20:16:05.887: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May  1 20:16:05.887: INFO: 2 / 3 pods in namespace 'pods-5214' are running and ready (4 seconds elapsed)
May  1 20:16:05.887: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
May  1 20:16:05.887: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
May  1 20:16:05.887: INFO: test-pod-3  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
May  1 20:16:05.887: INFO: 
May  1 20:16:08.002: INFO: 3 / 3 pods in namespace 'pods-5214' are running and ready (6 seconds elapsed)
May  1 20:16:08.002: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/01/23 20:16:08.113
May  1 20:16:08.137: INFO: Pod quantity 3 is different from expected quantity 0
May  1 20:16:09.169: INFO: Pod quantity 3 is different from expected quantity 0
May  1 20:16:10.173: INFO: Pod quantity 3 is different from expected quantity 0
May  1 20:16:11.176: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 20:16:12.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5214" for this suite. 05/01/23 20:16:12.191
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":251,"skipped":4731,"failed":0}
------------------------------
• [SLOW TEST] [10.875 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:01.343
    May  1 20:16:01.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 20:16:01.346
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:01.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:01.48
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 05/01/23 20:16:01.529
    May  1 20:16:01.614: INFO: created test-pod-1
    May  1 20:16:01.678: INFO: created test-pod-2
    May  1 20:16:01.725: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/01/23 20:16:01.725
    May  1 20:16:01.725: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5214' to be running and ready
    May  1 20:16:01.801: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:01.802: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:01.802: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:01.802: INFO: 0 / 3 pods in namespace 'pods-5214' are running and ready (0 seconds elapsed)
    May  1 20:16:01.802: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
    May  1 20:16:01.802: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    May  1 20:16:01.802: INFO: test-pod-1  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:01.802: INFO: test-pod-2  10.45.145.124  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:01.802: INFO: test-pod-3  10.45.145.124  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:01.802: INFO: 
    May  1 20:16:03.892: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:03.892: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:03.892: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:03.892: INFO: 0 / 3 pods in namespace 'pods-5214' are running and ready (2 seconds elapsed)
    May  1 20:16:03.892: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
    May  1 20:16:03.892: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    May  1 20:16:03.892: INFO: test-pod-1  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:03.892: INFO: test-pod-2  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:03.892: INFO: test-pod-3  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:03.892: INFO: 
    May  1 20:16:05.887: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May  1 20:16:05.887: INFO: 2 / 3 pods in namespace 'pods-5214' are running and ready (4 seconds elapsed)
    May  1 20:16:05.887: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
    May  1 20:16:05.887: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    May  1 20:16:05.887: INFO: test-pod-3  10.45.145.124  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:16:01 +0000 UTC  }]
    May  1 20:16:05.887: INFO: 
    May  1 20:16:08.002: INFO: 3 / 3 pods in namespace 'pods-5214' are running and ready (6 seconds elapsed)
    May  1 20:16:08.002: INFO: expected 0 pod replicas in namespace 'pods-5214', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/01/23 20:16:08.113
    May  1 20:16:08.137: INFO: Pod quantity 3 is different from expected quantity 0
    May  1 20:16:09.169: INFO: Pod quantity 3 is different from expected quantity 0
    May  1 20:16:10.173: INFO: Pod quantity 3 is different from expected quantity 0
    May  1 20:16:11.176: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 20:16:12.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5214" for this suite. 05/01/23 20:16:12.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:12.222
May  1 20:16:12.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 20:16:12.226
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:12.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:12.404
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 05/01/23 20:16:12.493
STEP: Creating a ResourceQuota 05/01/23 20:16:17.542
STEP: Ensuring resource quota status is calculated 05/01/23 20:16:17.583
STEP: Creating a Service 05/01/23 20:16:19.611
STEP: Creating a NodePort Service 05/01/23 20:16:19.694
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/01/23 20:16:19.78
STEP: Ensuring resource quota status captures service creation 05/01/23 20:16:19.881
STEP: Deleting Services 05/01/23 20:16:21.907
STEP: Ensuring resource quota status released usage 05/01/23 20:16:22.04
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 20:16:24.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7831" for this suite. 05/01/23 20:16:24.124
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":252,"skipped":4739,"failed":0}
------------------------------
• [SLOW TEST] [11.932 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:12.222
    May  1 20:16:12.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 20:16:12.226
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:12.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:12.404
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 05/01/23 20:16:12.493
    STEP: Creating a ResourceQuota 05/01/23 20:16:17.542
    STEP: Ensuring resource quota status is calculated 05/01/23 20:16:17.583
    STEP: Creating a Service 05/01/23 20:16:19.611
    STEP: Creating a NodePort Service 05/01/23 20:16:19.694
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/01/23 20:16:19.78
    STEP: Ensuring resource quota status captures service creation 05/01/23 20:16:19.881
    STEP: Deleting Services 05/01/23 20:16:21.907
    STEP: Ensuring resource quota status released usage 05/01/23 20:16:22.04
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 20:16:24.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7831" for this suite. 05/01/23 20:16:24.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:24.16
May  1 20:16:24.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:16:24.165
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:24.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:24.302
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-bd24149f-9e10-4596-a946-5a5d929dc854 05/01/23 20:16:24.321
STEP: Creating a pod to test consume configMaps 05/01/23 20:16:24.349
May  1 20:16:24.423: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8" in namespace "projected-3796" to be "Succeeded or Failed"
May  1 20:16:24.441: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.65511ms
May  1 20:16:26.464: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.041323141s
May  1 20:16:28.463: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=false. Elapsed: 4.039997113s
May  1 20:16:30.466: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=false. Elapsed: 6.04237235s
May  1 20:16:32.460: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036476111s
STEP: Saw pod success 05/01/23 20:16:32.46
May  1 20:16:32.460: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8" satisfied condition "Succeeded or Failed"
May  1 20:16:32.477: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:16:32.513
May  1 20:16:32.582: INFO: Waiting for pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 to disappear
May  1 20:16:32.607: INFO: Pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:16:32.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3796" for this suite. 05/01/23 20:16:32.629
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":253,"skipped":4762,"failed":0}
------------------------------
• [SLOW TEST] [8.495 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:24.16
    May  1 20:16:24.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:16:24.165
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:24.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:24.302
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-bd24149f-9e10-4596-a946-5a5d929dc854 05/01/23 20:16:24.321
    STEP: Creating a pod to test consume configMaps 05/01/23 20:16:24.349
    May  1 20:16:24.423: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8" in namespace "projected-3796" to be "Succeeded or Failed"
    May  1 20:16:24.441: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.65511ms
    May  1 20:16:26.464: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=true. Elapsed: 2.041323141s
    May  1 20:16:28.463: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=false. Elapsed: 4.039997113s
    May  1 20:16:30.466: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Running", Reason="", readiness=false. Elapsed: 6.04237235s
    May  1 20:16:32.460: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036476111s
    STEP: Saw pod success 05/01/23 20:16:32.46
    May  1 20:16:32.460: INFO: Pod "pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8" satisfied condition "Succeeded or Failed"
    May  1 20:16:32.477: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:16:32.513
    May  1 20:16:32.582: INFO: Waiting for pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 to disappear
    May  1 20:16:32.607: INFO: Pod pod-projected-configmaps-4aafa863-622d-4431-a9a1-0fc06ce65ba8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:16:32.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3796" for this suite. 05/01/23 20:16:32.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:32.66
May  1 20:16:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename namespaces 05/01/23 20:16:32.661
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:32.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:32.758
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 05/01/23 20:16:32.786
May  1 20:16:32.810: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/01/23 20:16:32.81
May  1 20:16:32.846: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/01/23 20:16:32.846
May  1 20:16:32.904: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  1 20:16:32.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1130" for this suite. 05/01/23 20:16:32.96
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":254,"skipped":4792,"failed":0}
------------------------------
• [0.332 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:32.66
    May  1 20:16:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename namespaces 05/01/23 20:16:32.661
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:32.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:32.758
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 05/01/23 20:16:32.786
    May  1 20:16:32.810: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/01/23 20:16:32.81
    May  1 20:16:32.846: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/01/23 20:16:32.846
    May  1 20:16:32.904: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:16:32.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1130" for this suite. 05/01/23 20:16:32.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:32.997
May  1 20:16:32.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 20:16:32.998
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:33.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:33.111
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-b793eac7-6661-4f0a-89c1-4961ddfbf3d1 05/01/23 20:16:33.161
STEP: Creating a pod to test consume secrets 05/01/23 20:16:33.191
May  1 20:16:33.286: INFO: Waiting up to 5m0s for pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b" in namespace "secrets-307" to be "Succeeded or Failed"
May  1 20:16:33.311: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.606752ms
May  1 20:16:35.331: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045227197s
May  1 20:16:37.329: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042887304s
May  1 20:16:39.349: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062501038s
STEP: Saw pod success 05/01/23 20:16:39.349
May  1 20:16:39.349: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b" satisfied condition "Succeeded or Failed"
May  1 20:16:39.397: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:16:39.595
May  1 20:16:39.703: INFO: Waiting for pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b to disappear
May  1 20:16:39.731: INFO: Pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 20:16:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-307" for this suite. 05/01/23 20:16:39.758
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":255,"skipped":4834,"failed":0}
------------------------------
• [SLOW TEST] [6.811 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:32.997
    May  1 20:16:32.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 20:16:32.998
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:33.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:33.111
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-b793eac7-6661-4f0a-89c1-4961ddfbf3d1 05/01/23 20:16:33.161
    STEP: Creating a pod to test consume secrets 05/01/23 20:16:33.191
    May  1 20:16:33.286: INFO: Waiting up to 5m0s for pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b" in namespace "secrets-307" to be "Succeeded or Failed"
    May  1 20:16:33.311: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.606752ms
    May  1 20:16:35.331: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045227197s
    May  1 20:16:37.329: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042887304s
    May  1 20:16:39.349: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062501038s
    STEP: Saw pod success 05/01/23 20:16:39.349
    May  1 20:16:39.349: INFO: Pod "pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b" satisfied condition "Succeeded or Failed"
    May  1 20:16:39.397: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:16:39.595
    May  1 20:16:39.703: INFO: Waiting for pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b to disappear
    May  1 20:16:39.731: INFO: Pod pod-secrets-c728aadb-3b78-415d-98d4-57c444da4a2b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 20:16:39.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-307" for this suite. 05/01/23 20:16:39.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:39.809
May  1 20:16:39.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:16:39.814
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:39.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:40.007
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-c329dff2-f5d9-49c4-b4a6-1b76ba931eaf 05/01/23 20:16:40.065
STEP: Creating a pod to test consume secrets 05/01/23 20:16:40.141
May  1 20:16:40.213: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70" in namespace "projected-861" to be "Succeeded or Failed"
May  1 20:16:40.233: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 19.803081ms
May  1 20:16:42.261: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04761495s
May  1 20:16:44.274: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060861678s
May  1 20:16:46.254: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040925568s
STEP: Saw pod success 05/01/23 20:16:46.255
May  1 20:16:46.255: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70" satisfied condition "Succeeded or Failed"
May  1 20:16:46.288: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:16:46.423
May  1 20:16:46.539: INFO: Waiting for pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 to disappear
May  1 20:16:46.558: INFO: Pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 20:16:46.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-861" for this suite. 05/01/23 20:16:46.59
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":256,"skipped":4841,"failed":0}
------------------------------
• [SLOW TEST] [6.817 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:39.809
    May  1 20:16:39.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:16:39.814
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:39.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:40.007
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-c329dff2-f5d9-49c4-b4a6-1b76ba931eaf 05/01/23 20:16:40.065
    STEP: Creating a pod to test consume secrets 05/01/23 20:16:40.141
    May  1 20:16:40.213: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70" in namespace "projected-861" to be "Succeeded or Failed"
    May  1 20:16:40.233: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 19.803081ms
    May  1 20:16:42.261: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04761495s
    May  1 20:16:44.274: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060861678s
    May  1 20:16:46.254: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040925568s
    STEP: Saw pod success 05/01/23 20:16:46.255
    May  1 20:16:46.255: INFO: Pod "pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70" satisfied condition "Succeeded or Failed"
    May  1 20:16:46.288: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:16:46.423
    May  1 20:16:46.539: INFO: Waiting for pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 to disappear
    May  1 20:16:46.558: INFO: Pod pod-projected-secrets-749fa9b9-a1ff-4662-827e-86a936040a70 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 20:16:46.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-861" for this suite. 05/01/23 20:16:46.59
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:16:46.626
May  1 20:16:46.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename subpath 05/01/23 20:16:46.63
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:46.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:46.739
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/01/23 20:16:46.765
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-rg7g 05/01/23 20:16:46.824
STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:16:46.825
May  1 20:16:46.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rg7g" in namespace "subpath-664" to be "Succeeded or Failed"
May  1 20:16:46.922: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Pending", Reason="", readiness=false. Elapsed: 21.796012ms
May  1 20:16:48.969: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068959447s
May  1 20:16:50.943: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 4.043075129s
May  1 20:16:52.947: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 6.046187247s
May  1 20:16:54.951: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 8.05023026s
May  1 20:16:56.944: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 10.043698567s
May  1 20:16:58.947: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 12.046480426s
May  1 20:17:00.954: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 14.05391149s
May  1 20:17:02.942: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 16.041212834s
May  1 20:17:04.961: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 18.060558946s
May  1 20:17:06.943: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 20.043005718s
May  1 20:17:08.950: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 22.049280855s
May  1 20:17:10.991: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=false. Elapsed: 24.090197203s
May  1 20:17:12.944: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.043335805s
STEP: Saw pod success 05/01/23 20:17:12.944
May  1 20:17:12.944: INFO: Pod "pod-subpath-test-configmap-rg7g" satisfied condition "Succeeded or Failed"
May  1 20:17:12.986: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-configmap-rg7g container test-container-subpath-configmap-rg7g: <nil>
STEP: delete the pod 05/01/23 20:17:13.052
May  1 20:17:13.130: INFO: Waiting for pod pod-subpath-test-configmap-rg7g to disappear
May  1 20:17:13.156: INFO: Pod pod-subpath-test-configmap-rg7g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rg7g 05/01/23 20:17:13.156
May  1 20:17:13.156: INFO: Deleting pod "pod-subpath-test-configmap-rg7g" in namespace "subpath-664"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  1 20:17:13.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-664" for this suite. 05/01/23 20:17:13.215
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":257,"skipped":4842,"failed":0}
------------------------------
• [SLOW TEST] [26.625 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:16:46.626
    May  1 20:16:46.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename subpath 05/01/23 20:16:46.63
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:16:46.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:16:46.739
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/01/23 20:16:46.765
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-rg7g 05/01/23 20:16:46.824
    STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:16:46.825
    May  1 20:16:46.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rg7g" in namespace "subpath-664" to be "Succeeded or Failed"
    May  1 20:16:46.922: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Pending", Reason="", readiness=false. Elapsed: 21.796012ms
    May  1 20:16:48.969: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068959447s
    May  1 20:16:50.943: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 4.043075129s
    May  1 20:16:52.947: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 6.046187247s
    May  1 20:16:54.951: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 8.05023026s
    May  1 20:16:56.944: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 10.043698567s
    May  1 20:16:58.947: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 12.046480426s
    May  1 20:17:00.954: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 14.05391149s
    May  1 20:17:02.942: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 16.041212834s
    May  1 20:17:04.961: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 18.060558946s
    May  1 20:17:06.943: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 20.043005718s
    May  1 20:17:08.950: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=true. Elapsed: 22.049280855s
    May  1 20:17:10.991: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Running", Reason="", readiness=false. Elapsed: 24.090197203s
    May  1 20:17:12.944: INFO: Pod "pod-subpath-test-configmap-rg7g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.043335805s
    STEP: Saw pod success 05/01/23 20:17:12.944
    May  1 20:17:12.944: INFO: Pod "pod-subpath-test-configmap-rg7g" satisfied condition "Succeeded or Failed"
    May  1 20:17:12.986: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-configmap-rg7g container test-container-subpath-configmap-rg7g: <nil>
    STEP: delete the pod 05/01/23 20:17:13.052
    May  1 20:17:13.130: INFO: Waiting for pod pod-subpath-test-configmap-rg7g to disappear
    May  1 20:17:13.156: INFO: Pod pod-subpath-test-configmap-rg7g no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-rg7g 05/01/23 20:17:13.156
    May  1 20:17:13.156: INFO: Deleting pod "pod-subpath-test-configmap-rg7g" in namespace "subpath-664"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  1 20:17:13.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-664" for this suite. 05/01/23 20:17:13.215
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:13.252
May  1 20:17:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:17:13.254
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:13.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:13.356
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:17:13.447
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:17:15.356
STEP: Deploying the webhook pod 05/01/23 20:17:15.412
STEP: Wait for the deployment to be ready 05/01/23 20:17:15.461
May  1 20:17:15.493: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May  1 20:17:17.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:17:19.658
STEP: Verifying the service has paired with the endpoint 05/01/23 20:17:19.715
May  1 20:17:20.716: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
May  1 20:17:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-871-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:17:21.29
STEP: Creating a custom resource that should be mutated by the webhook 05/01/23 20:17:21.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:17:24.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7270" for this suite. 05/01/23 20:17:24.16
STEP: Destroying namespace "webhook-7270-markers" for this suite. 05/01/23 20:17:24.229
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":258,"skipped":4843,"failed":0}
------------------------------
• [SLOW TEST] [11.200 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:13.252
    May  1 20:17:13.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:17:13.254
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:13.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:13.356
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:17:13.447
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:17:15.356
    STEP: Deploying the webhook pod 05/01/23 20:17:15.412
    STEP: Wait for the deployment to be ready 05/01/23 20:17:15.461
    May  1 20:17:15.493: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    May  1 20:17:17.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 17, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:17:19.658
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:17:19.715
    May  1 20:17:20.716: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    May  1 20:17:20.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-871-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:17:21.29
    STEP: Creating a custom resource that should be mutated by the webhook 05/01/23 20:17:21.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:17:24.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7270" for this suite. 05/01/23 20:17:24.16
    STEP: Destroying namespace "webhook-7270-markers" for this suite. 05/01/23 20:17:24.229
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:24.455
May  1 20:17:24.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 20:17:24.456
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:24.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:24.627
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/01/23 20:17:24.645
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_tcp@PTR;sleep 1; done
 05/01/23 20:17:24.848
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_tcp@PTR;sleep 1; done
 05/01/23 20:17:24.848
STEP: creating a pod to probe DNS 05/01/23 20:17:24.855
STEP: submitting the pod to kubernetes 05/01/23 20:17:24.856
May  1 20:17:25.038: INFO: Waiting up to 15m0s for pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f" in namespace "dns-6851" to be "running"
May  1 20:17:25.077: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.657925ms
May  1 20:17:27.107: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06864381s
May  1 20:17:29.130: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Running", Reason="", readiness=true. Elapsed: 4.092222366s
May  1 20:17:29.130: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f" satisfied condition "running"
STEP: retrieving the pod 05/01/23 20:17:29.13
STEP: looking for the results for each expected name from probers 05/01/23 20:17:29.154
May  1 20:17:29.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.251: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.322: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.444: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.679: INFO: Unable to read jessie_udp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.755: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.832: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:29.918: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
May  1 20:17:30.236: INFO: Lookups using dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f failed for: [wheezy_udp@dns-test-service.dns-6851.svc.cluster.local wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local jessie_udp@dns-test-service.dns-6851.svc.cluster.local jessie_tcp@dns-test-service.dns-6851.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local]

May  1 20:17:36.051: INFO: DNS probes using dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f succeeded

STEP: deleting the pod 05/01/23 20:17:36.051
STEP: deleting the test service 05/01/23 20:17:36.139
STEP: deleting the test headless service 05/01/23 20:17:36.239
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 20:17:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6851" for this suite. 05/01/23 20:17:36.327
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":259,"skipped":4852,"failed":0}
------------------------------
• [SLOW TEST] [11.910 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:24.455
    May  1 20:17:24.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 20:17:24.456
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:24.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:24.627
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/01/23 20:17:24.645
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_tcp@PTR;sleep 1; done
     05/01/23 20:17:24.848
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.14.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.14.237_tcp@PTR;sleep 1; done
     05/01/23 20:17:24.848
    STEP: creating a pod to probe DNS 05/01/23 20:17:24.855
    STEP: submitting the pod to kubernetes 05/01/23 20:17:24.856
    May  1 20:17:25.038: INFO: Waiting up to 15m0s for pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f" in namespace "dns-6851" to be "running"
    May  1 20:17:25.077: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.657925ms
    May  1 20:17:27.107: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06864381s
    May  1 20:17:29.130: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f": Phase="Running", Reason="", readiness=true. Elapsed: 4.092222366s
    May  1 20:17:29.130: INFO: Pod "dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 20:17:29.13
    STEP: looking for the results for each expected name from probers 05/01/23 20:17:29.154
    May  1 20:17:29.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.251: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.322: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.444: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.679: INFO: Unable to read jessie_udp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.755: INFO: Unable to read jessie_tcp@dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.832: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:29.918: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local from pod dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f: the server could not find the requested resource (get pods dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f)
    May  1 20:17:30.236: INFO: Lookups using dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f failed for: [wheezy_udp@dns-test-service.dns-6851.svc.cluster.local wheezy_tcp@dns-test-service.dns-6851.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local jessie_udp@dns-test-service.dns-6851.svc.cluster.local jessie_tcp@dns-test-service.dns-6851.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6851.svc.cluster.local]

    May  1 20:17:36.051: INFO: DNS probes using dns-6851/dns-test-8f0e6b32-b8ef-437c-af5e-9b8215fde07f succeeded

    STEP: deleting the pod 05/01/23 20:17:36.051
    STEP: deleting the test service 05/01/23 20:17:36.139
    STEP: deleting the test headless service 05/01/23 20:17:36.239
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 20:17:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6851" for this suite. 05/01/23 20:17:36.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:36.368
May  1 20:17:36.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 20:17:36.37
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:36.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:36.478
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
May  1 20:17:36.531: INFO: Got root ca configmap in namespace "svcaccounts-3336"
May  1 20:17:36.575: INFO: Deleted root ca configmap in namespace "svcaccounts-3336"
STEP: waiting for a new root ca configmap created 05/01/23 20:17:37.085
May  1 20:17:37.103: INFO: Recreated root ca configmap in namespace "svcaccounts-3336"
May  1 20:17:37.121: INFO: Updated root ca configmap in namespace "svcaccounts-3336"
STEP: waiting for the root ca configmap reconciled 05/01/23 20:17:37.622
May  1 20:17:37.640: INFO: Reconciled root ca configmap in namespace "svcaccounts-3336"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 20:17:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3336" for this suite. 05/01/23 20:17:37.7
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":260,"skipped":4858,"failed":0}
------------------------------
• [1.367 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:36.368
    May  1 20:17:36.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 20:17:36.37
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:36.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:36.478
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    May  1 20:17:36.531: INFO: Got root ca configmap in namespace "svcaccounts-3336"
    May  1 20:17:36.575: INFO: Deleted root ca configmap in namespace "svcaccounts-3336"
    STEP: waiting for a new root ca configmap created 05/01/23 20:17:37.085
    May  1 20:17:37.103: INFO: Recreated root ca configmap in namespace "svcaccounts-3336"
    May  1 20:17:37.121: INFO: Updated root ca configmap in namespace "svcaccounts-3336"
    STEP: waiting for the root ca configmap reconciled 05/01/23 20:17:37.622
    May  1 20:17:37.640: INFO: Reconciled root ca configmap in namespace "svcaccounts-3336"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 20:17:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3336" for this suite. 05/01/23 20:17:37.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:37.744
May  1 20:17:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubelet-test 05/01/23 20:17:37.746
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:37.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:37.892
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May  1 20:17:38.033: INFO: Waiting up to 5m0s for pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd" in namespace "kubelet-test-6126" to be "running and ready"
May  1 20:17:38.065: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.243368ms
May  1 20:17:38.065: INFO: The phase of Pod busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd is Pending, waiting for it to be Running (with Ready = true)
May  1 20:17:40.096: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.063658125s
May  1 20:17:40.096: INFO: The phase of Pod busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd is Running (Ready = true)
May  1 20:17:40.096: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
May  1 20:17:40.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6126" for this suite. 05/01/23 20:17:40.311
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":261,"skipped":4896,"failed":0}
------------------------------
• [2.677 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:37.744
    May  1 20:17:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubelet-test 05/01/23 20:17:37.746
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:37.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:37.892
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May  1 20:17:38.033: INFO: Waiting up to 5m0s for pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd" in namespace "kubelet-test-6126" to be "running and ready"
    May  1 20:17:38.065: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd": Phase="Pending", Reason="", readiness=false. Elapsed: 32.243368ms
    May  1 20:17:38.065: INFO: The phase of Pod busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:17:40.096: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd": Phase="Running", Reason="", readiness=true. Elapsed: 2.063658125s
    May  1 20:17:40.096: INFO: The phase of Pod busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd is Running (Ready = true)
    May  1 20:17:40.096: INFO: Pod "busybox-scheduling-21e94522-3343-4980-ac88-9d723698afdd" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    May  1 20:17:40.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6126" for this suite. 05/01/23 20:17:40.311
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:40.425
May  1 20:17:40.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 20:17:40.427
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:40.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:40.598
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:17:40.682
May  1 20:17:40.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018" in namespace "downward-api-1002" to be "Succeeded or Failed"
May  1 20:17:40.865: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 41.578357ms
May  1 20:17:42.913: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089438336s
May  1 20:17:44.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064414334s
May  1 20:17:46.905: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082366763s
May  1 20:17:48.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064908144s
STEP: Saw pod success 05/01/23 20:17:48.888
May  1 20:17:48.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018" satisfied condition "Succeeded or Failed"
May  1 20:17:48.908: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 container client-container: <nil>
STEP: delete the pod 05/01/23 20:17:48.995
May  1 20:17:49.079: INFO: Waiting for pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 to disappear
May  1 20:17:49.122: INFO: Pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 20:17:49.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1002" for this suite. 05/01/23 20:17:49.149
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":262,"skipped":4900,"failed":0}
------------------------------
• [SLOW TEST] [8.775 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:40.425
    May  1 20:17:40.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 20:17:40.427
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:40.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:40.598
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:17:40.682
    May  1 20:17:40.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018" in namespace "downward-api-1002" to be "Succeeded or Failed"
    May  1 20:17:40.865: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 41.578357ms
    May  1 20:17:42.913: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089438336s
    May  1 20:17:44.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064414334s
    May  1 20:17:46.905: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082366763s
    May  1 20:17:48.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064908144s
    STEP: Saw pod success 05/01/23 20:17:48.888
    May  1 20:17:48.888: INFO: Pod "downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018" satisfied condition "Succeeded or Failed"
    May  1 20:17:48.908: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 container client-container: <nil>
    STEP: delete the pod 05/01/23 20:17:48.995
    May  1 20:17:49.079: INFO: Waiting for pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 to disappear
    May  1 20:17:49.122: INFO: Pod downwardapi-volume-206e2f2e-7c54-4802-872e-208e33f1b018 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 20:17:49.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1002" for this suite. 05/01/23 20:17:49.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:17:49.206
May  1 20:17:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 20:17:49.207
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:49.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:49.39
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3402 05/01/23 20:17:49.441
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 05/01/23 20:17:49.491
STEP: Creating pod with conflicting port in namespace statefulset-3402 05/01/23 20:17:49.534
STEP: Waiting until pod test-pod will start running in namespace statefulset-3402 05/01/23 20:17:49.654
May  1 20:17:49.655: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3402" to be "running"
May  1 20:17:49.682: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 27.057863ms
May  1 20:17:51.705: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050304501s
May  1 20:17:53.701: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.045672404s
May  1 20:17:53.701: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3402 05/01/23 20:17:53.701
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3402 05/01/23 20:17:53.728
May  1 20:17:53.854: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Pending. Waiting for statefulset controller to delete.
May  1 20:17:53.948: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Failed. Waiting for statefulset controller to delete.
May  1 20:17:53.973: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Failed. Waiting for statefulset controller to delete.
May  1 20:17:53.981: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3402
STEP: Removing pod with conflicting port in namespace statefulset-3402 05/01/23 20:17:53.981
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3402 and will be in running state 05/01/23 20:17:54.037
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 20:17:58.154: INFO: Deleting all statefulset in ns statefulset-3402
May  1 20:17:58.193: INFO: Scaling statefulset ss to 0
May  1 20:18:08.354: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:18:08.373: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 20:18:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3402" for this suite. 05/01/23 20:18:08.501
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":263,"skipped":4929,"failed":0}
------------------------------
• [SLOW TEST] [19.321 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:17:49.206
    May  1 20:17:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 20:17:49.207
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:17:49.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:17:49.39
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3402 05/01/23 20:17:49.441
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 05/01/23 20:17:49.491
    STEP: Creating pod with conflicting port in namespace statefulset-3402 05/01/23 20:17:49.534
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3402 05/01/23 20:17:49.654
    May  1 20:17:49.655: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3402" to be "running"
    May  1 20:17:49.682: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 27.057863ms
    May  1 20:17:51.705: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050304501s
    May  1 20:17:53.701: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.045672404s
    May  1 20:17:53.701: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3402 05/01/23 20:17:53.701
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3402 05/01/23 20:17:53.728
    May  1 20:17:53.854: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Pending. Waiting for statefulset controller to delete.
    May  1 20:17:53.948: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Failed. Waiting for statefulset controller to delete.
    May  1 20:17:53.973: INFO: Observed stateful pod in namespace: statefulset-3402, name: ss-0, uid: aa1d4cc2-033e-4a22-a085-0a44e18361a8, status phase: Failed. Waiting for statefulset controller to delete.
    May  1 20:17:53.981: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3402
    STEP: Removing pod with conflicting port in namespace statefulset-3402 05/01/23 20:17:53.981
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3402 and will be in running state 05/01/23 20:17:54.037
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 20:17:58.154: INFO: Deleting all statefulset in ns statefulset-3402
    May  1 20:17:58.193: INFO: Scaling statefulset ss to 0
    May  1 20:18:08.354: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:18:08.373: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 20:18:08.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3402" for this suite. 05/01/23 20:18:08.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:18:08.528
May  1 20:18:08.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:18:08.53
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:08.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:08.643
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 05/01/23 20:18:08.659
May  1 20:18:08.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 create -f -'
May  1 20:18:11.665: INFO: stderr: ""
May  1 20:18:11.665: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:11.665
May  1 20:18:11.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:11.927: INFO: stderr: ""
May  1 20:18:11.927: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
May  1 20:18:11.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:12.356: INFO: stderr: ""
May  1 20:18:12.356: INFO: stdout: ""
May  1 20:18:12.356: INFO: update-demo-nautilus-898q7 is created but not running
May  1 20:18:17.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:17.642: INFO: stderr: ""
May  1 20:18:17.642: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
May  1 20:18:17.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:17.896: INFO: stderr: ""
May  1 20:18:17.897: INFO: stdout: "true"
May  1 20:18:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:18.178: INFO: stderr: ""
May  1 20:18:18.178: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:18.178: INFO: validating pod update-demo-nautilus-898q7
May  1 20:18:18.335: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:18.335: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:18.335: INFO: update-demo-nautilus-898q7 is verified up and running
May  1 20:18:18.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:18.556: INFO: stderr: ""
May  1 20:18:18.556: INFO: stdout: "true"
May  1 20:18:18.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:18.804: INFO: stderr: ""
May  1 20:18:18.804: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:18.804: INFO: validating pod update-demo-nautilus-g7nsd
May  1 20:18:18.981: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:18.981: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:18.981: INFO: update-demo-nautilus-g7nsd is verified up and running
STEP: scaling down the replication controller 05/01/23 20:18:18.981
May  1 20:18:18.993: INFO: scanned /root for discovery docs: <nil>
May  1 20:18:18.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May  1 20:18:20.456: INFO: stderr: ""
May  1 20:18:20.456: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:20.456
May  1 20:18:20.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:20.826: INFO: stderr: ""
May  1 20:18:20.826: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
STEP: Replicas for name=update-demo: expected=1 actual=2 05/01/23 20:18:20.826
May  1 20:18:25.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:26.038: INFO: stderr: ""
May  1 20:18:26.038: INFO: stdout: "update-demo-nautilus-g7nsd "
May  1 20:18:26.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:26.279: INFO: stderr: ""
May  1 20:18:26.279: INFO: stdout: "true"
May  1 20:18:26.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:26.469: INFO: stderr: ""
May  1 20:18:26.469: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:26.469: INFO: validating pod update-demo-nautilus-g7nsd
May  1 20:18:26.499: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:26.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:26.500: INFO: update-demo-nautilus-g7nsd is verified up and running
STEP: scaling up the replication controller 05/01/23 20:18:26.5
May  1 20:18:26.511: INFO: scanned /root for discovery docs: <nil>
May  1 20:18:26.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May  1 20:18:27.827: INFO: stderr: ""
May  1 20:18:27.827: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:27.827
May  1 20:18:27.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:28.192: INFO: stderr: ""
May  1 20:18:28.192: INFO: stdout: "update-demo-nautilus-g7nsd update-demo-nautilus-wpllz "
May  1 20:18:28.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:28.426: INFO: stderr: ""
May  1 20:18:28.426: INFO: stdout: "true"
May  1 20:18:28.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:28.587: INFO: stderr: ""
May  1 20:18:28.588: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:28.588: INFO: validating pod update-demo-nautilus-g7nsd
May  1 20:18:28.620: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:28.620: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:28.620: INFO: update-demo-nautilus-g7nsd is verified up and running
May  1 20:18:28.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:28.840: INFO: stderr: ""
May  1 20:18:28.840: INFO: stdout: ""
May  1 20:18:28.840: INFO: update-demo-nautilus-wpllz is created but not running
May  1 20:18:33.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May  1 20:18:34.044: INFO: stderr: ""
May  1 20:18:34.044: INFO: stdout: "update-demo-nautilus-g7nsd update-demo-nautilus-wpllz "
May  1 20:18:34.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:34.230: INFO: stderr: ""
May  1 20:18:34.231: INFO: stdout: "true"
May  1 20:18:34.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:34.462: INFO: stderr: ""
May  1 20:18:34.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:34.462: INFO: validating pod update-demo-nautilus-g7nsd
May  1 20:18:34.492: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:34.492: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:34.492: INFO: update-demo-nautilus-g7nsd is verified up and running
May  1 20:18:34.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May  1 20:18:34.675: INFO: stderr: ""
May  1 20:18:34.675: INFO: stdout: "true"
May  1 20:18:34.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May  1 20:18:34.875: INFO: stderr: ""
May  1 20:18:34.876: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
May  1 20:18:34.876: INFO: validating pod update-demo-nautilus-wpllz
May  1 20:18:34.912: INFO: got data: {
  "image": "nautilus.jpg"
}

May  1 20:18:34.912: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  1 20:18:34.912: INFO: update-demo-nautilus-wpllz is verified up and running
STEP: using delete to clean up resources 05/01/23 20:18:34.912
May  1 20:18:34.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 delete --grace-period=0 --force -f -'
May  1 20:18:35.108: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 20:18:35.109: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  1 20:18:35.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get rc,svc -l name=update-demo --no-headers'
May  1 20:18:35.412: INFO: stderr: "No resources found in kubectl-1415 namespace.\n"
May  1 20:18:35.412: INFO: stdout: ""
May  1 20:18:35.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  1 20:18:35.855: INFO: stderr: ""
May  1 20:18:35.855: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:18:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1415" for this suite. 05/01/23 20:18:35.912
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":264,"skipped":4935,"failed":0}
------------------------------
• [SLOW TEST] [27.427 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:18:08.528
    May  1 20:18:08.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:18:08.53
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:08.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:08.643
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 05/01/23 20:18:08.659
    May  1 20:18:08.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 create -f -'
    May  1 20:18:11.665: INFO: stderr: ""
    May  1 20:18:11.665: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:11.665
    May  1 20:18:11.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:11.927: INFO: stderr: ""
    May  1 20:18:11.927: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
    May  1 20:18:11.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:12.356: INFO: stderr: ""
    May  1 20:18:12.356: INFO: stdout: ""
    May  1 20:18:12.356: INFO: update-demo-nautilus-898q7 is created but not running
    May  1 20:18:17.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:17.642: INFO: stderr: ""
    May  1 20:18:17.642: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
    May  1 20:18:17.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:17.896: INFO: stderr: ""
    May  1 20:18:17.897: INFO: stdout: "true"
    May  1 20:18:17.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-898q7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:18.178: INFO: stderr: ""
    May  1 20:18:18.178: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:18.178: INFO: validating pod update-demo-nautilus-898q7
    May  1 20:18:18.335: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:18.335: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:18.335: INFO: update-demo-nautilus-898q7 is verified up and running
    May  1 20:18:18.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:18.556: INFO: stderr: ""
    May  1 20:18:18.556: INFO: stdout: "true"
    May  1 20:18:18.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:18.804: INFO: stderr: ""
    May  1 20:18:18.804: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:18.804: INFO: validating pod update-demo-nautilus-g7nsd
    May  1 20:18:18.981: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:18.981: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:18.981: INFO: update-demo-nautilus-g7nsd is verified up and running
    STEP: scaling down the replication controller 05/01/23 20:18:18.981
    May  1 20:18:18.993: INFO: scanned /root for discovery docs: <nil>
    May  1 20:18:18.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May  1 20:18:20.456: INFO: stderr: ""
    May  1 20:18:20.456: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:20.456
    May  1 20:18:20.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:20.826: INFO: stderr: ""
    May  1 20:18:20.826: INFO: stdout: "update-demo-nautilus-898q7 update-demo-nautilus-g7nsd "
    STEP: Replicas for name=update-demo: expected=1 actual=2 05/01/23 20:18:20.826
    May  1 20:18:25.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:26.038: INFO: stderr: ""
    May  1 20:18:26.038: INFO: stdout: "update-demo-nautilus-g7nsd "
    May  1 20:18:26.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:26.279: INFO: stderr: ""
    May  1 20:18:26.279: INFO: stdout: "true"
    May  1 20:18:26.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:26.469: INFO: stderr: ""
    May  1 20:18:26.469: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:26.469: INFO: validating pod update-demo-nautilus-g7nsd
    May  1 20:18:26.499: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:26.500: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:26.500: INFO: update-demo-nautilus-g7nsd is verified up and running
    STEP: scaling up the replication controller 05/01/23 20:18:26.5
    May  1 20:18:26.511: INFO: scanned /root for discovery docs: <nil>
    May  1 20:18:26.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May  1 20:18:27.827: INFO: stderr: ""
    May  1 20:18:27.827: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/01/23 20:18:27.827
    May  1 20:18:27.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:28.192: INFO: stderr: ""
    May  1 20:18:28.192: INFO: stdout: "update-demo-nautilus-g7nsd update-demo-nautilus-wpllz "
    May  1 20:18:28.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:28.426: INFO: stderr: ""
    May  1 20:18:28.426: INFO: stdout: "true"
    May  1 20:18:28.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:28.587: INFO: stderr: ""
    May  1 20:18:28.588: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:28.588: INFO: validating pod update-demo-nautilus-g7nsd
    May  1 20:18:28.620: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:28.620: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:28.620: INFO: update-demo-nautilus-g7nsd is verified up and running
    May  1 20:18:28.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:28.840: INFO: stderr: ""
    May  1 20:18:28.840: INFO: stdout: ""
    May  1 20:18:28.840: INFO: update-demo-nautilus-wpllz is created but not running
    May  1 20:18:33.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May  1 20:18:34.044: INFO: stderr: ""
    May  1 20:18:34.044: INFO: stdout: "update-demo-nautilus-g7nsd update-demo-nautilus-wpllz "
    May  1 20:18:34.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:34.230: INFO: stderr: ""
    May  1 20:18:34.231: INFO: stdout: "true"
    May  1 20:18:34.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-g7nsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:34.462: INFO: stderr: ""
    May  1 20:18:34.462: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:34.462: INFO: validating pod update-demo-nautilus-g7nsd
    May  1 20:18:34.492: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:34.492: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:34.492: INFO: update-demo-nautilus-g7nsd is verified up and running
    May  1 20:18:34.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May  1 20:18:34.675: INFO: stderr: ""
    May  1 20:18:34.675: INFO: stdout: "true"
    May  1 20:18:34.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods update-demo-nautilus-wpllz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May  1 20:18:34.875: INFO: stderr: ""
    May  1 20:18:34.876: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    May  1 20:18:34.876: INFO: validating pod update-demo-nautilus-wpllz
    May  1 20:18:34.912: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May  1 20:18:34.912: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May  1 20:18:34.912: INFO: update-demo-nautilus-wpllz is verified up and running
    STEP: using delete to clean up resources 05/01/23 20:18:34.912
    May  1 20:18:34.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 delete --grace-period=0 --force -f -'
    May  1 20:18:35.108: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 20:18:35.109: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May  1 20:18:35.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get rc,svc -l name=update-demo --no-headers'
    May  1 20:18:35.412: INFO: stderr: "No resources found in kubectl-1415 namespace.\n"
    May  1 20:18:35.412: INFO: stdout: ""
    May  1 20:18:35.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1415 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  1 20:18:35.855: INFO: stderr: ""
    May  1 20:18:35.855: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:18:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1415" for this suite. 05/01/23 20:18:35.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:18:35.959
May  1 20:18:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:18:35.962
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:36.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:36.085
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7506 05/01/23 20:18:36.13
STEP: changing the ExternalName service to type=ClusterIP 05/01/23 20:18:36.163
STEP: creating replication controller externalname-service in namespace services-7506 05/01/23 20:18:36.3
I0501 20:18:36.339013      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7506, replica count: 2
I0501 20:18:39.406728      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:18:39.406: INFO: Creating new exec pod
May  1 20:18:39.509: INFO: Waiting up to 5m0s for pod "execpodh64jf" in namespace "services-7506" to be "running"
May  1 20:18:39.529: INFO: Pod "execpodh64jf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038342ms
May  1 20:18:41.602: INFO: Pod "execpodh64jf": Phase="Running", Reason="", readiness=true. Elapsed: 2.093294018s
May  1 20:18:41.603: INFO: Pod "execpodh64jf" satisfied condition "running"
May  1 20:18:42.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  1 20:18:43.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  1 20:18:43.938: INFO: stdout: ""
May  1 20:18:44.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  1 20:18:45.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  1 20:18:45.504: INFO: stdout: ""
May  1 20:18:45.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May  1 20:18:46.503: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May  1 20:18:46.503: INFO: stdout: "externalname-service-54mdm"
May  1 20:18:46.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.86.231 80'
May  1 20:18:47.001: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 172.21.86.231 80\nConnection to 172.21.86.231 80 port [tcp/http] succeeded!\n"
May  1 20:18:47.001: INFO: stdout: "externalname-service-fbr2m"
May  1 20:18:47.001: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:18:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7506" for this suite. 05/01/23 20:18:47.139
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":265,"skipped":4958,"failed":0}
------------------------------
• [SLOW TEST] [11.248 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:18:35.959
    May  1 20:18:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:18:35.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:36.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:36.085
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-7506 05/01/23 20:18:36.13
    STEP: changing the ExternalName service to type=ClusterIP 05/01/23 20:18:36.163
    STEP: creating replication controller externalname-service in namespace services-7506 05/01/23 20:18:36.3
    I0501 20:18:36.339013      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7506, replica count: 2
    I0501 20:18:39.406728      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:18:39.406: INFO: Creating new exec pod
    May  1 20:18:39.509: INFO: Waiting up to 5m0s for pod "execpodh64jf" in namespace "services-7506" to be "running"
    May  1 20:18:39.529: INFO: Pod "execpodh64jf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.038342ms
    May  1 20:18:41.602: INFO: Pod "execpodh64jf": Phase="Running", Reason="", readiness=true. Elapsed: 2.093294018s
    May  1 20:18:41.603: INFO: Pod "execpodh64jf" satisfied condition "running"
    May  1 20:18:42.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  1 20:18:43.938: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  1 20:18:43.938: INFO: stdout: ""
    May  1 20:18:44.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  1 20:18:45.504: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  1 20:18:45.504: INFO: stdout: ""
    May  1 20:18:45.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    May  1 20:18:46.503: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May  1 20:18:46.503: INFO: stdout: "externalname-service-54mdm"
    May  1 20:18:46.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-7506 exec execpodh64jf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.86.231 80'
    May  1 20:18:47.001: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 172.21.86.231 80\nConnection to 172.21.86.231 80 port [tcp/http] succeeded!\n"
    May  1 20:18:47.001: INFO: stdout: "externalname-service-fbr2m"
    May  1 20:18:47.001: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:18:47.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7506" for this suite. 05/01/23 20:18:47.139
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:18:47.213
May  1 20:18:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:18:47.217
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:47.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:47.382
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:18:47.419
May  1 20:18:47.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May  1 20:18:47.883: INFO: stderr: ""
May  1 20:18:47.883: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/01/23 20:18:47.883
STEP: verifying the pod e2e-test-httpd-pod was created 05/01/23 20:18:52.935
May  1 20:18:52.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 get pod e2e-test-httpd-pod -o json'
May  1 20:18:53.069: INFO: stderr: ""
May  1 20:18:53.069: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"6cd138dd653602edb8d71c7464160ec096f71c0408a2654c11ff95aff8ad0440\",\n            \"cni.projectcalico.org/podIP\": \"172.30.42.102/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.42.102/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.42.102\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.42.102\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-05-01T20:18:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3138\",\n        \"resourceVersion\": \"121903\",\n        \"uid\": \"fe4b79d9-8845-4191-bcd3-27786d921fcd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tbfwv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-x2wfd\"\n            }\n        ],\n        \"nodeName\": \"10.45.145.124\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c61,c20\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tbfwv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://c2f2ef9abf49d825d4bb0aaac27d3acaa69f0c81a0defb797d86f752f23b7ba4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-01T20:18:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.145.124\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.42.102\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.42.102\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-01T20:18:47Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/01/23 20:18:53.07
May  1 20:18:53.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 replace -f -'
May  1 20:18:59.535: INFO: stderr: ""
May  1 20:18:59.535: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/01/23 20:18:59.536
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
May  1 20:18:59.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 delete pods e2e-test-httpd-pod'
May  1 20:19:02.630: INFO: stderr: ""
May  1 20:19:02.630: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:19:02.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3138" for this suite. 05/01/23 20:19:02.677
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":266,"skipped":4987,"failed":0}
------------------------------
• [SLOW TEST] [15.494 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:18:47.213
    May  1 20:18:47.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:18:47.217
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:18:47.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:18:47.382
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 05/01/23 20:18:47.419
    May  1 20:18:47.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May  1 20:18:47.883: INFO: stderr: ""
    May  1 20:18:47.883: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/01/23 20:18:47.883
    STEP: verifying the pod e2e-test-httpd-pod was created 05/01/23 20:18:52.935
    May  1 20:18:52.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 get pod e2e-test-httpd-pod -o json'
    May  1 20:18:53.069: INFO: stderr: ""
    May  1 20:18:53.069: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"6cd138dd653602edb8d71c7464160ec096f71c0408a2654c11ff95aff8ad0440\",\n            \"cni.projectcalico.org/podIP\": \"172.30.42.102/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.42.102/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.42.102\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.42.102\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-05-01T20:18:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3138\",\n        \"resourceVersion\": \"121903\",\n        \"uid\": \"fe4b79d9-8845-4191-bcd3-27786d921fcd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tbfwv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-x2wfd\"\n            }\n        ],\n        \"nodeName\": \"10.45.145.124\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c61,c20\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tbfwv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-01T20:18:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://c2f2ef9abf49d825d4bb0aaac27d3acaa69f0c81a0defb797d86f752f23b7ba4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-01T20:18:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.145.124\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.42.102\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.42.102\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-01T20:18:47Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/01/23 20:18:53.07
    May  1 20:18:53.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 replace -f -'
    May  1 20:18:59.535: INFO: stderr: ""
    May  1 20:18:59.535: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 05/01/23 20:18:59.536
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    May  1 20:18:59.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-3138 delete pods e2e-test-httpd-pod'
    May  1 20:19:02.630: INFO: stderr: ""
    May  1 20:19:02.630: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:19:02.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3138" for this suite. 05/01/23 20:19:02.677
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:19:02.708
May  1 20:19:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename namespaces 05/01/23 20:19:02.712
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:02.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:02.836
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 05/01/23 20:19:02.867
STEP: patching the Namespace 05/01/23 20:19:02.966
STEP: get the Namespace and ensuring it has the label 05/01/23 20:19:02.995
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
May  1 20:19:03.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9237" for this suite. 05/01/23 20:19:03.065
STEP: Destroying namespace "nspatchtest-7e5d579a-cfbd-44e4-93d9-caad722f551e-2963" for this suite. 05/01/23 20:19:03.092
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":267,"skipped":4988,"failed":0}
------------------------------
• [0.413 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:19:02.708
    May  1 20:19:02.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename namespaces 05/01/23 20:19:02.712
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:02.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:02.836
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 05/01/23 20:19:02.867
    STEP: patching the Namespace 05/01/23 20:19:02.966
    STEP: get the Namespace and ensuring it has the label 05/01/23 20:19:02.995
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:19:03.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9237" for this suite. 05/01/23 20:19:03.065
    STEP: Destroying namespace "nspatchtest-7e5d579a-cfbd-44e4-93d9-caad722f551e-2963" for this suite. 05/01/23 20:19:03.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:19:03.13
May  1 20:19:03.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename subpath 05/01/23 20:19:03.134
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:03.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:03.235
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/01/23 20:19:03.281
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-lw2m 05/01/23 20:19:03.343
STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:19:03.344
May  1 20:19:03.463: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lw2m" in namespace "subpath-7793" to be "Succeeded or Failed"
May  1 20:19:03.484: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 20.968566ms
May  1 20:19:05.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052405456s
May  1 20:19:07.509: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 4.046208851s
May  1 20:19:09.506: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 6.043223136s
May  1 20:19:11.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 8.053282881s
May  1 20:19:13.512: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 10.048775379s
May  1 20:19:15.511: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 12.04778656s
May  1 20:19:17.513: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 14.049871709s
May  1 20:19:19.522: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 16.058497658s
May  1 20:19:21.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 18.05312758s
May  1 20:19:23.520: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 20.056524384s
May  1 20:19:25.539: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 22.075893542s
May  1 20:19:27.501: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=false. Elapsed: 24.038335542s
May  1 20:19:29.503: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039512167s
STEP: Saw pod success 05/01/23 20:19:29.503
May  1 20:19:29.503: INFO: Pod "pod-subpath-test-downwardapi-lw2m" satisfied condition "Succeeded or Failed"
May  1 20:19:29.529: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-downwardapi-lw2m container test-container-subpath-downwardapi-lw2m: <nil>
STEP: delete the pod 05/01/23 20:19:29.601
May  1 20:19:29.661: INFO: Waiting for pod pod-subpath-test-downwardapi-lw2m to disappear
May  1 20:19:29.681: INFO: Pod pod-subpath-test-downwardapi-lw2m no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lw2m 05/01/23 20:19:29.681
May  1 20:19:29.681: INFO: Deleting pod "pod-subpath-test-downwardapi-lw2m" in namespace "subpath-7793"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  1 20:19:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7793" for this suite. 05/01/23 20:19:29.735
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":268,"skipped":5005,"failed":0}
------------------------------
• [SLOW TEST] [26.636 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:19:03.13
    May  1 20:19:03.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename subpath 05/01/23 20:19:03.134
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:03.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:03.235
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/01/23 20:19:03.281
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-lw2m 05/01/23 20:19:03.343
    STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:19:03.344
    May  1 20:19:03.463: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lw2m" in namespace "subpath-7793" to be "Succeeded or Failed"
    May  1 20:19:03.484: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 20.968566ms
    May  1 20:19:05.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052405456s
    May  1 20:19:07.509: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 4.046208851s
    May  1 20:19:09.506: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 6.043223136s
    May  1 20:19:11.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 8.053282881s
    May  1 20:19:13.512: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 10.048775379s
    May  1 20:19:15.511: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 12.04778656s
    May  1 20:19:17.513: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 14.049871709s
    May  1 20:19:19.522: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 16.058497658s
    May  1 20:19:21.516: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 18.05312758s
    May  1 20:19:23.520: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 20.056524384s
    May  1 20:19:25.539: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=true. Elapsed: 22.075893542s
    May  1 20:19:27.501: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Running", Reason="", readiness=false. Elapsed: 24.038335542s
    May  1 20:19:29.503: INFO: Pod "pod-subpath-test-downwardapi-lw2m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.039512167s
    STEP: Saw pod success 05/01/23 20:19:29.503
    May  1 20:19:29.503: INFO: Pod "pod-subpath-test-downwardapi-lw2m" satisfied condition "Succeeded or Failed"
    May  1 20:19:29.529: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-downwardapi-lw2m container test-container-subpath-downwardapi-lw2m: <nil>
    STEP: delete the pod 05/01/23 20:19:29.601
    May  1 20:19:29.661: INFO: Waiting for pod pod-subpath-test-downwardapi-lw2m to disappear
    May  1 20:19:29.681: INFO: Pod pod-subpath-test-downwardapi-lw2m no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-lw2m 05/01/23 20:19:29.681
    May  1 20:19:29.681: INFO: Deleting pod "pod-subpath-test-downwardapi-lw2m" in namespace "subpath-7793"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  1 20:19:29.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7793" for this suite. 05/01/23 20:19:29.735
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:19:29.767
May  1 20:19:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename job 05/01/23 20:19:29.77
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:29.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:29.893
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 05/01/23 20:19:29.911
STEP: Ensuring job reaches completions 05/01/23 20:19:29.966
STEP: Ensuring pods with index for job exist 05/01/23 20:19:43.981
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
May  1 20:19:44.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9934" for this suite. 05/01/23 20:19:44.039
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":269,"skipped":5005,"failed":0}
------------------------------
• [SLOW TEST] [14.311 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:19:29.767
    May  1 20:19:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename job 05/01/23 20:19:29.77
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:29.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:29.893
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 05/01/23 20:19:29.911
    STEP: Ensuring job reaches completions 05/01/23 20:19:29.966
    STEP: Ensuring pods with index for job exist 05/01/23 20:19:43.981
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    May  1 20:19:44.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9934" for this suite. 05/01/23 20:19:44.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:19:44.083
May  1 20:19:44.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context-test 05/01/23 20:19:44.085
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:44.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:44.216
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
May  1 20:19:44.354: INFO: Waiting up to 5m0s for pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7" in namespace "security-context-test-2880" to be "Succeeded or Failed"
May  1 20:19:44.372: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.697837ms
May  1 20:19:46.396: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042562557s
May  1 20:19:48.393: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039772321s
May  1 20:19:50.391: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037708653s
May  1 20:19:50.391: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 20:19:50.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2880" for this suite. 05/01/23 20:19:50.419
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":270,"skipped":5023,"failed":0}
------------------------------
• [SLOW TEST] [6.364 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:19:44.083
    May  1 20:19:44.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context-test 05/01/23 20:19:44.085
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:44.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:44.216
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    May  1 20:19:44.354: INFO: Waiting up to 5m0s for pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7" in namespace "security-context-test-2880" to be "Succeeded or Failed"
    May  1 20:19:44.372: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.697837ms
    May  1 20:19:46.396: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042562557s
    May  1 20:19:48.393: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039772321s
    May  1 20:19:50.391: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037708653s
    May  1 20:19:50.391: INFO: Pod "busybox-user-65534-daea283e-9027-44db-aa5d-43ec1d96a9f7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 20:19:50.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2880" for this suite. 05/01/23 20:19:50.419
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:19:50.448
May  1 20:19:50.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:19:50.45
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:50.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:50.546
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 05/01/23 20:19:50.579
May  1 20:19:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: mark a version not serverd 05/01/23 20:20:38.157
STEP: check the unserved version gets removed 05/01/23 20:20:38.23
STEP: check the other version is not changed 05/01/23 20:20:59.608
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:21:39.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4816" for this suite. 05/01/23 20:21:39.504
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":271,"skipped":5025,"failed":0}
------------------------------
• [SLOW TEST] [109.104 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:19:50.448
    May  1 20:19:50.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:19:50.45
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:19:50.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:19:50.546
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 05/01/23 20:19:50.579
    May  1 20:19:50.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: mark a version not serverd 05/01/23 20:20:38.157
    STEP: check the unserved version gets removed 05/01/23 20:20:38.23
    STEP: check the other version is not changed 05/01/23 20:20:59.608
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:21:39.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4816" for this suite. 05/01/23 20:21:39.504
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:21:39.556
May  1 20:21:39.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename cronjob 05/01/23 20:21:39.559
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:21:39.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:21:39.649
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/01/23 20:21:39.673
STEP: Ensuring no jobs are scheduled 05/01/23 20:21:39.699
STEP: Ensuring no job exists by listing jobs explicitly 05/01/23 20:26:39.76
STEP: Removing cronjob 05/01/23 20:26:39.781
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  1 20:26:39.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6898" for this suite. 05/01/23 20:26:39.856
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":272,"skipped":5029,"failed":0}
------------------------------
• [SLOW TEST] [300.328 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:21:39.556
    May  1 20:21:39.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename cronjob 05/01/23 20:21:39.559
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:21:39.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:21:39.649
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/01/23 20:21:39.673
    STEP: Ensuring no jobs are scheduled 05/01/23 20:21:39.699
    STEP: Ensuring no job exists by listing jobs explicitly 05/01/23 20:26:39.76
    STEP: Removing cronjob 05/01/23 20:26:39.781
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  1 20:26:39.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-6898" for this suite. 05/01/23 20:26:39.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:26:39.897
May  1 20:26:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename controllerrevisions 05/01/23 20:26:39.899
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:39.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:39.97
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-d6rvb-daemon-set" 05/01/23 20:26:40.173
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:26:40.209
May  1 20:26:40.245: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
May  1 20:26:40.245: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:26:41.286: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
May  1 20:26:41.286: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:26:42.297: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
May  1 20:26:42.297: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:26:43.288: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 2
May  1 20:26:43.288: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 20:26:44.305: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 3
May  1 20:26:44.305: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d6rvb-daemon-set
STEP: Confirm DaemonSet "e2e-d6rvb-daemon-set" successfully created with "daemonset-name=e2e-d6rvb-daemon-set" label 05/01/23 20:26:44.328
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d6rvb-daemon-set" 05/01/23 20:26:44.387
May  1 20:26:44.413: INFO: Located ControllerRevision: "e2e-d6rvb-daemon-set-76dd6497cf"
STEP: Patching ControllerRevision "e2e-d6rvb-daemon-set-76dd6497cf" 05/01/23 20:26:44.436
May  1 20:26:44.465: INFO: e2e-d6rvb-daemon-set-76dd6497cf has been patched
STEP: Create a new ControllerRevision 05/01/23 20:26:44.465
May  1 20:26:44.490: INFO: Created ControllerRevision: e2e-d6rvb-daemon-set-678cd6d9b7
STEP: Confirm that there are two ControllerRevisions 05/01/23 20:26:44.49
May  1 20:26:44.490: INFO: Requesting list of ControllerRevisions to confirm quantity
May  1 20:26:44.509: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-d6rvb-daemon-set-76dd6497cf" 05/01/23 20:26:44.509
STEP: Confirm that there is only one ControllerRevision 05/01/23 20:26:44.536
May  1 20:26:44.537: INFO: Requesting list of ControllerRevisions to confirm quantity
May  1 20:26:44.550: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-d6rvb-daemon-set-678cd6d9b7" 05/01/23 20:26:44.571
May  1 20:26:44.602: INFO: e2e-d6rvb-daemon-set-678cd6d9b7 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/01/23 20:26:44.602
W0501 20:26:44.645174      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/01/23 20:26:44.645
May  1 20:26:44.645: INFO: Requesting list of ControllerRevisions to confirm quantity
May  1 20:26:45.672: INFO: Requesting list of ControllerRevisions to confirm quantity
May  1 20:26:45.697: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d6rvb-daemon-set-678cd6d9b7=updated" 05/01/23 20:26:45.697
STEP: Confirm that there is only one ControllerRevision 05/01/23 20:26:45.752
May  1 20:26:45.753: INFO: Requesting list of ControllerRevisions to confirm quantity
May  1 20:26:45.796: INFO: Found 1 ControllerRevisions
May  1 20:26:45.810: INFO: ControllerRevision "e2e-d6rvb-daemon-set-566bb88dd6" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-d6rvb-daemon-set" 05/01/23 20:26:45.842
STEP: deleting DaemonSet.extensions e2e-d6rvb-daemon-set in namespace controllerrevisions-5844, will wait for the garbage collector to delete the pods 05/01/23 20:26:45.843
May  1 20:26:46.047: INFO: Deleting DaemonSet.extensions e2e-d6rvb-daemon-set took: 83.810901ms
May  1 20:26:46.348: INFO: Terminating DaemonSet.extensions e2e-d6rvb-daemon-set pods took: 301.046482ms
May  1 20:26:48.871: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
May  1 20:26:48.871: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d6rvb-daemon-set
May  1 20:26:48.890: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125238"},"items":null}

May  1 20:26:48.916: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125238"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
May  1 20:26:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-5844" for this suite. 05/01/23 20:26:49.06
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":273,"skipped":5049,"failed":0}
------------------------------
• [SLOW TEST] [9.205 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:26:39.897
    May  1 20:26:39.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename controllerrevisions 05/01/23 20:26:39.899
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:39.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:39.97
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-d6rvb-daemon-set" 05/01/23 20:26:40.173
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:26:40.209
    May  1 20:26:40.245: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
    May  1 20:26:40.245: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:26:41.286: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
    May  1 20:26:41.286: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:26:42.297: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
    May  1 20:26:42.297: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:26:43.288: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 2
    May  1 20:26:43.288: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 20:26:44.305: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 3
    May  1 20:26:44.305: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-d6rvb-daemon-set
    STEP: Confirm DaemonSet "e2e-d6rvb-daemon-set" successfully created with "daemonset-name=e2e-d6rvb-daemon-set" label 05/01/23 20:26:44.328
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-d6rvb-daemon-set" 05/01/23 20:26:44.387
    May  1 20:26:44.413: INFO: Located ControllerRevision: "e2e-d6rvb-daemon-set-76dd6497cf"
    STEP: Patching ControllerRevision "e2e-d6rvb-daemon-set-76dd6497cf" 05/01/23 20:26:44.436
    May  1 20:26:44.465: INFO: e2e-d6rvb-daemon-set-76dd6497cf has been patched
    STEP: Create a new ControllerRevision 05/01/23 20:26:44.465
    May  1 20:26:44.490: INFO: Created ControllerRevision: e2e-d6rvb-daemon-set-678cd6d9b7
    STEP: Confirm that there are two ControllerRevisions 05/01/23 20:26:44.49
    May  1 20:26:44.490: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  1 20:26:44.509: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-d6rvb-daemon-set-76dd6497cf" 05/01/23 20:26:44.509
    STEP: Confirm that there is only one ControllerRevision 05/01/23 20:26:44.536
    May  1 20:26:44.537: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  1 20:26:44.550: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-d6rvb-daemon-set-678cd6d9b7" 05/01/23 20:26:44.571
    May  1 20:26:44.602: INFO: e2e-d6rvb-daemon-set-678cd6d9b7 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/01/23 20:26:44.602
    W0501 20:26:44.645174      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/01/23 20:26:44.645
    May  1 20:26:44.645: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  1 20:26:45.672: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  1 20:26:45.697: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-d6rvb-daemon-set-678cd6d9b7=updated" 05/01/23 20:26:45.697
    STEP: Confirm that there is only one ControllerRevision 05/01/23 20:26:45.752
    May  1 20:26:45.753: INFO: Requesting list of ControllerRevisions to confirm quantity
    May  1 20:26:45.796: INFO: Found 1 ControllerRevisions
    May  1 20:26:45.810: INFO: ControllerRevision "e2e-d6rvb-daemon-set-566bb88dd6" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-d6rvb-daemon-set" 05/01/23 20:26:45.842
    STEP: deleting DaemonSet.extensions e2e-d6rvb-daemon-set in namespace controllerrevisions-5844, will wait for the garbage collector to delete the pods 05/01/23 20:26:45.843
    May  1 20:26:46.047: INFO: Deleting DaemonSet.extensions e2e-d6rvb-daemon-set took: 83.810901ms
    May  1 20:26:46.348: INFO: Terminating DaemonSet.extensions e2e-d6rvb-daemon-set pods took: 301.046482ms
    May  1 20:26:48.871: INFO: Number of nodes with available pods controlled by daemonset e2e-d6rvb-daemon-set: 0
    May  1 20:26:48.871: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-d6rvb-daemon-set
    May  1 20:26:48.890: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"125238"},"items":null}

    May  1 20:26:48.916: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"125238"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:26:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-5844" for this suite. 05/01/23 20:26:49.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:26:49.115
May  1 20:26:49.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:26:49.117
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:49.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:49.226
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:26:49.272
May  1 20:26:49.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5" in namespace "projected-995" to be "Succeeded or Failed"
May  1 20:26:49.409: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.969741ms
May  1 20:26:51.432: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044085826s
May  1 20:26:53.425: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037301601s
May  1 20:26:55.424: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036364461s
STEP: Saw pod success 05/01/23 20:26:55.424
May  1 20:26:55.425: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5" satisfied condition "Succeeded or Failed"
May  1 20:26:55.442: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 container client-container: <nil>
STEP: delete the pod 05/01/23 20:26:55.529
May  1 20:26:55.561: INFO: Waiting for pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 to disappear
May  1 20:26:55.581: INFO: Pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 20:26:55.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-995" for this suite. 05/01/23 20:26:55.637
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":274,"skipped":5069,"failed":0}
------------------------------
• [SLOW TEST] [6.575 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:26:49.115
    May  1 20:26:49.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:26:49.117
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:49.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:49.226
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:26:49.272
    May  1 20:26:49.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5" in namespace "projected-995" to be "Succeeded or Failed"
    May  1 20:26:49.409: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.969741ms
    May  1 20:26:51.432: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044085826s
    May  1 20:26:53.425: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037301601s
    May  1 20:26:55.424: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036364461s
    STEP: Saw pod success 05/01/23 20:26:55.424
    May  1 20:26:55.425: INFO: Pod "downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5" satisfied condition "Succeeded or Failed"
    May  1 20:26:55.442: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 container client-container: <nil>
    STEP: delete the pod 05/01/23 20:26:55.529
    May  1 20:26:55.561: INFO: Waiting for pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 to disappear
    May  1 20:26:55.581: INFO: Pod downwardapi-volume-05628fb1-4bf9-42b0-9df5-6afaf4d5a5d5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 20:26:55.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-995" for this suite. 05/01/23 20:26:55.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:26:55.692
May  1 20:26:55.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:26:55.695
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:55.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:55.781
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
May  1 20:26:55.847: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-57551a6e-cfa0-46e9-85b3-8fd53c3ce307 05/01/23 20:26:55.847
STEP: Creating the pod 05/01/23 20:26:55.889
May  1 20:26:55.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f" in namespace "configmap-6775" to be "running and ready"
May  1 20:26:55.995: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.689804ms
May  1 20:26:55.995: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Pending, waiting for it to be Running (with Ready = true)
May  1 20:26:58.023: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056983173s
May  1 20:26:58.024: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Pending, waiting for it to be Running (with Ready = true)
May  1 20:27:00.009: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.04293155s
May  1 20:27:00.009: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Running (Ready = true)
May  1 20:27:00.010: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-57551a6e-cfa0-46e9-85b3-8fd53c3ce307 05/01/23 20:27:00.068
STEP: waiting to observe update in volume 05/01/23 20:27:00.092
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:28:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6775" for this suite. 05/01/23 20:28:10.46
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":275,"skipped":5084,"failed":0}
------------------------------
• [SLOW TEST] [74.814 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:26:55.692
    May  1 20:26:55.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:26:55.695
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:26:55.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:26:55.781
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    May  1 20:26:55.847: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-57551a6e-cfa0-46e9-85b3-8fd53c3ce307 05/01/23 20:26:55.847
    STEP: Creating the pod 05/01/23 20:26:55.889
    May  1 20:26:55.966: INFO: Waiting up to 5m0s for pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f" in namespace "configmap-6775" to be "running and ready"
    May  1 20:26:55.995: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.689804ms
    May  1 20:26:55.995: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:26:58.023: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056983173s
    May  1 20:26:58.024: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:27:00.009: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f": Phase="Running", Reason="", readiness=true. Elapsed: 4.04293155s
    May  1 20:27:00.009: INFO: The phase of Pod pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f is Running (Ready = true)
    May  1 20:27:00.010: INFO: Pod "pod-configmaps-39e8f39f-68c4-4081-9e83-2c974cf96a4f" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-57551a6e-cfa0-46e9-85b3-8fd53c3ce307 05/01/23 20:27:00.068
    STEP: waiting to observe update in volume 05/01/23 20:27:00.092
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:28:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6775" for this suite. 05/01/23 20:28:10.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:28:10.511
May  1 20:28:10.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:28:10.514
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:10.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:10.635
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 05/01/23 20:28:10.654
May  1 20:28:10.771: INFO: Waiting up to 5m0s for pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933" in namespace "emptydir-9821" to be "Succeeded or Failed"
May  1 20:28:10.785: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 14.40106ms
May  1 20:28:12.817: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046341462s
May  1 20:28:14.802: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031323561s
May  1 20:28:16.805: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034664039s
STEP: Saw pod success 05/01/23 20:28:16.805
May  1 20:28:16.806: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933" satisfied condition "Succeeded or Failed"
May  1 20:28:16.822: INFO: Trying to get logs from node 10.45.145.124 pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 container test-container: <nil>
STEP: delete the pod 05/01/23 20:28:16.85
May  1 20:28:16.892: INFO: Waiting for pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 to disappear
May  1 20:28:16.907: INFO: Pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:28:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9821" for this suite. 05/01/23 20:28:16.93
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":276,"skipped":5118,"failed":0}
------------------------------
• [SLOW TEST] [6.485 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:28:10.511
    May  1 20:28:10.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:28:10.514
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:10.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:10.635
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 05/01/23 20:28:10.654
    May  1 20:28:10.771: INFO: Waiting up to 5m0s for pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933" in namespace "emptydir-9821" to be "Succeeded or Failed"
    May  1 20:28:10.785: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 14.40106ms
    May  1 20:28:12.817: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046341462s
    May  1 20:28:14.802: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031323561s
    May  1 20:28:16.805: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034664039s
    STEP: Saw pod success 05/01/23 20:28:16.805
    May  1 20:28:16.806: INFO: Pod "pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933" satisfied condition "Succeeded or Failed"
    May  1 20:28:16.822: INFO: Trying to get logs from node 10.45.145.124 pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 container test-container: <nil>
    STEP: delete the pod 05/01/23 20:28:16.85
    May  1 20:28:16.892: INFO: Waiting for pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 to disappear
    May  1 20:28:16.907: INFO: Pod pod-4cf7fd84-5c4a-423e-b5a1-0d9bb880e933 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:28:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9821" for this suite. 05/01/23 20:28:16.93
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:28:16.997
May  1 20:28:16.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 20:28:17
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:17.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:17.13
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/01/23 20:28:17.157
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local;sleep 1; done
 05/01/23 20:28:17.184
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local;sleep 1; done
 05/01/23 20:28:17.184
STEP: creating a pod to probe DNS 05/01/23 20:28:17.185
STEP: submitting the pod to kubernetes 05/01/23 20:28:17.186
May  1 20:28:17.267: INFO: Waiting up to 15m0s for pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868" in namespace "dns-2961" to be "running"
May  1 20:28:17.288: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 21.054564ms
May  1 20:28:19.317: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049702763s
May  1 20:28:21.308: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041055608s
May  1 20:28:23.315: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Running", Reason="", readiness=true. Elapsed: 6.047226667s
May  1 20:28:23.315: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868" satisfied condition "running"
STEP: retrieving the pod 05/01/23 20:28:23.315
STEP: looking for the results for each expected name from probers 05/01/23 20:28:23.335
May  1 20:28:23.551: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.622: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.650: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.679: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.729: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.752: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
May  1 20:28:23.752: INFO: Lookups using dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local]

May  1 20:28:29.076: INFO: DNS probes using dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868 succeeded

STEP: deleting the pod 05/01/23 20:28:29.076
STEP: deleting the test headless service 05/01/23 20:28:29.114
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 20:28:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2961" for this suite. 05/01/23 20:28:29.212
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":277,"skipped":5118,"failed":0}
------------------------------
• [SLOW TEST] [12.257 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:28:16.997
    May  1 20:28:16.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 20:28:17
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:17.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:17.13
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/01/23 20:28:17.157
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local;sleep 1; done
     05/01/23 20:28:17.184
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2961.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local;sleep 1; done
     05/01/23 20:28:17.184
    STEP: creating a pod to probe DNS 05/01/23 20:28:17.185
    STEP: submitting the pod to kubernetes 05/01/23 20:28:17.186
    May  1 20:28:17.267: INFO: Waiting up to 15m0s for pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868" in namespace "dns-2961" to be "running"
    May  1 20:28:17.288: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 21.054564ms
    May  1 20:28:19.317: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049702763s
    May  1 20:28:21.308: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041055608s
    May  1 20:28:23.315: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868": Phase="Running", Reason="", readiness=true. Elapsed: 6.047226667s
    May  1 20:28:23.315: INFO: Pod "dns-test-54f9943b-087e-45e6-bee5-e80fa905c868" satisfied condition "running"
    STEP: retrieving the pod 05/01/23 20:28:23.315
    STEP: looking for the results for each expected name from probers 05/01/23 20:28:23.335
    May  1 20:28:23.551: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.622: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.650: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.679: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.729: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.752: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local from pod dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868: the server could not find the requested resource (get pods dns-test-54f9943b-087e-45e6-bee5-e80fa905c868)
    May  1 20:28:23.752: INFO: Lookups using dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2961.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2961.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2961.svc.cluster.local jessie_udp@dns-test-service-2.dns-2961.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2961.svc.cluster.local]

    May  1 20:28:29.076: INFO: DNS probes using dns-2961/dns-test-54f9943b-087e-45e6-bee5-e80fa905c868 succeeded

    STEP: deleting the pod 05/01/23 20:28:29.076
    STEP: deleting the test headless service 05/01/23 20:28:29.114
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 20:28:29.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2961" for this suite. 05/01/23 20:28:29.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:28:29.258
May  1 20:28:29.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:28:29.261
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:29.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:29.405
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-1c88494f-6099-4769-8a1e-1ba30eb9c4fe 05/01/23 20:28:29.43
STEP: Creating a pod to test consume secrets 05/01/23 20:28:29.457
May  1 20:28:29.594: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d" in namespace "projected-6832" to be "Succeeded or Failed"
May  1 20:28:29.641: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 46.686602ms
May  1 20:28:31.656: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061965983s
May  1 20:28:33.658: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063888652s
May  1 20:28:35.678: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083899109s
STEP: Saw pod success 05/01/23 20:28:35.678
May  1 20:28:35.679: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d" satisfied condition "Succeeded or Failed"
May  1 20:28:35.696: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d container projected-secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:28:35.738
May  1 20:28:35.775: INFO: Waiting for pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d to disappear
May  1 20:28:35.803: INFO: Pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 20:28:35.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6832" for this suite. 05/01/23 20:28:35.836
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":278,"skipped":5146,"failed":0}
------------------------------
• [SLOW TEST] [6.624 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:28:29.258
    May  1 20:28:29.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:28:29.261
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:29.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:29.405
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-1c88494f-6099-4769-8a1e-1ba30eb9c4fe 05/01/23 20:28:29.43
    STEP: Creating a pod to test consume secrets 05/01/23 20:28:29.457
    May  1 20:28:29.594: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d" in namespace "projected-6832" to be "Succeeded or Failed"
    May  1 20:28:29.641: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 46.686602ms
    May  1 20:28:31.656: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061965983s
    May  1 20:28:33.658: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063888652s
    May  1 20:28:35.678: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.083899109s
    STEP: Saw pod success 05/01/23 20:28:35.678
    May  1 20:28:35.679: INFO: Pod "pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d" satisfied condition "Succeeded or Failed"
    May  1 20:28:35.696: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:28:35.738
    May  1 20:28:35.775: INFO: Waiting for pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d to disappear
    May  1 20:28:35.803: INFO: Pod pod-projected-secrets-1b100d64-d69a-4a00-9569-4083f903c43d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 20:28:35.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6832" for this suite. 05/01/23 20:28:35.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:28:35.939
May  1 20:28:35.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename hostport 05/01/23 20:28:35.941
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:36.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:36.092
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/01/23 20:28:36.17
May  1 20:28:36.288: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4194" to be "running and ready"
May  1 20:28:36.316: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.508512ms
May  1 20:28:36.316: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:38.345: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057284636s
May  1 20:28:38.345: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:40.338: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.050052412s
May  1 20:28:40.338: INFO: The phase of Pod pod1 is Running (Ready = true)
May  1 20:28:40.338: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.45.145.126 on the node which pod1 resides and expect scheduled 05/01/23 20:28:40.338
May  1 20:28:40.410: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4194" to be "running and ready"
May  1 20:28:40.482: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 71.860408ms
May  1 20:28:40.482: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:42.610: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1995356s
May  1 20:28:42.610: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:44.517: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106694297s
May  1 20:28:44.517: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:46.496: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.086003649s
May  1 20:28:46.497: INFO: The phase of Pod pod2 is Running (Ready = true)
May  1 20:28:46.497: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.45.145.126 but use UDP protocol on the node which pod2 resides 05/01/23 20:28:46.497
May  1 20:28:46.548: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4194" to be "running and ready"
May  1 20:28:46.575: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.970146ms
May  1 20:28:46.576: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:48.601: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052194268s
May  1 20:28:48.601: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:50.597: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.048176734s
May  1 20:28:50.597: INFO: The phase of Pod pod3 is Running (Ready = true)
May  1 20:28:50.597: INFO: Pod "pod3" satisfied condition "running and ready"
May  1 20:28:50.666: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4194" to be "running and ready"
May  1 20:28:50.679: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.1026ms
May  1 20:28:50.679: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May  1 20:28:52.699: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.032960807s
May  1 20:28:52.699: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May  1 20:28:52.699: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/01/23 20:28:52.72
May  1 20:28:52.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.45.145.126 http://127.0.0.1:54323/hostname] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:28:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:28:52.722: INFO: ExecWithOptions: Clientset creation
May  1 20:28:52.722: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.45.145.126+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.145.126, port: 54323 05/01/23 20:28:53.183
May  1 20:28:53.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.45.145.126:54323/hostname] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:28:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:28:53.186: INFO: ExecWithOptions: Clientset creation
May  1 20:28:53.187: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.45.145.126%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.145.126, port: 54323 UDP 05/01/23 20:28:53.667
May  1 20:28:53.668: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.45.145.126 54323] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:28:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:28:53.669: INFO: ExecWithOptions: Clientset creation
May  1 20:28:53.669: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.45.145.126+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
May  1 20:28:59.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-4194" for this suite. 05/01/23 20:28:59.083
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":279,"skipped":5152,"failed":0}
------------------------------
• [SLOW TEST] [23.180 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:28:35.939
    May  1 20:28:35.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename hostport 05/01/23 20:28:35.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:36.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:36.092
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/01/23 20:28:36.17
    May  1 20:28:36.288: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-4194" to be "running and ready"
    May  1 20:28:36.316: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.508512ms
    May  1 20:28:36.316: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:38.345: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057284636s
    May  1 20:28:38.345: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:40.338: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.050052412s
    May  1 20:28:40.338: INFO: The phase of Pod pod1 is Running (Ready = true)
    May  1 20:28:40.338: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.45.145.126 on the node which pod1 resides and expect scheduled 05/01/23 20:28:40.338
    May  1 20:28:40.410: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-4194" to be "running and ready"
    May  1 20:28:40.482: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 71.860408ms
    May  1 20:28:40.482: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:42.610: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1995356s
    May  1 20:28:42.610: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:44.517: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106694297s
    May  1 20:28:44.517: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:46.496: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 6.086003649s
    May  1 20:28:46.497: INFO: The phase of Pod pod2 is Running (Ready = true)
    May  1 20:28:46.497: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.45.145.126 but use UDP protocol on the node which pod2 resides 05/01/23 20:28:46.497
    May  1 20:28:46.548: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-4194" to be "running and ready"
    May  1 20:28:46.575: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 26.970146ms
    May  1 20:28:46.576: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:48.601: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052194268s
    May  1 20:28:48.601: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:50.597: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.048176734s
    May  1 20:28:50.597: INFO: The phase of Pod pod3 is Running (Ready = true)
    May  1 20:28:50.597: INFO: Pod "pod3" satisfied condition "running and ready"
    May  1 20:28:50.666: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-4194" to be "running and ready"
    May  1 20:28:50.679: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 13.1026ms
    May  1 20:28:50.679: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:28:52.699: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.032960807s
    May  1 20:28:52.699: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May  1 20:28:52.699: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/01/23 20:28:52.72
    May  1 20:28:52.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.45.145.126 http://127.0.0.1:54323/hostname] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:28:52.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:28:52.722: INFO: ExecWithOptions: Clientset creation
    May  1 20:28:52.722: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.45.145.126+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.145.126, port: 54323 05/01/23 20:28:53.183
    May  1 20:28:53.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.45.145.126:54323/hostname] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:28:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:28:53.186: INFO: ExecWithOptions: Clientset creation
    May  1 20:28:53.187: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.45.145.126%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.145.126, port: 54323 UDP 05/01/23 20:28:53.667
    May  1 20:28:53.668: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.45.145.126 54323] Namespace:hostport-4194 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:28:53.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:28:53.669: INFO: ExecWithOptions: Clientset creation
    May  1 20:28:53.669: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-4194/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.45.145.126+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    May  1 20:28:59.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-4194" for this suite. 05/01/23 20:28:59.083
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:28:59.121
May  1 20:28:59.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename containers 05/01/23 20:28:59.123
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:59.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:59.217
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 05/01/23 20:28:59.245
May  1 20:28:59.363: INFO: Waiting up to 5m0s for pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27" in namespace "containers-2833" to be "Succeeded or Failed"
May  1 20:28:59.380: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 16.384237ms
May  1 20:29:01.397: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033808102s
May  1 20:29:03.399: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035165904s
May  1 20:29:05.398: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034061667s
STEP: Saw pod success 05/01/23 20:29:05.398
May  1 20:29:05.398: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27" satisfied condition "Succeeded or Failed"
May  1 20:29:05.412: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:29:05.458
May  1 20:29:05.502: INFO: Waiting for pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 to disappear
May  1 20:29:05.515: INFO: Pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
May  1 20:29:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2833" for this suite. 05/01/23 20:29:05.532
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":280,"skipped":5154,"failed":0}
------------------------------
• [SLOW TEST] [6.446 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:28:59.121
    May  1 20:28:59.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename containers 05/01/23 20:28:59.123
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:28:59.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:28:59.217
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 05/01/23 20:28:59.245
    May  1 20:28:59.363: INFO: Waiting up to 5m0s for pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27" in namespace "containers-2833" to be "Succeeded or Failed"
    May  1 20:28:59.380: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 16.384237ms
    May  1 20:29:01.397: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033808102s
    May  1 20:29:03.399: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035165904s
    May  1 20:29:05.398: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034061667s
    STEP: Saw pod success 05/01/23 20:29:05.398
    May  1 20:29:05.398: INFO: Pod "client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27" satisfied condition "Succeeded or Failed"
    May  1 20:29:05.412: INFO: Trying to get logs from node 10.45.145.124 pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:29:05.458
    May  1 20:29:05.502: INFO: Waiting for pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 to disappear
    May  1 20:29:05.515: INFO: Pod client-containers-8d8c16d9-b3a7-4484-a806-76b9bbf78d27 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    May  1 20:29:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-2833" for this suite. 05/01/23 20:29:05.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:29:05.567
May  1 20:29:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context-test 05/01/23 20:29:05.575
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:29:05.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:29:05.662
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
May  1 20:29:05.758: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb" in namespace "security-context-test-2571" to be "Succeeded or Failed"
May  1 20:29:05.772: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.98505ms
May  1 20:29:07.787: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028467439s
May  1 20:29:09.795: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036713929s
May  1 20:29:11.789: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030946505s
May  1 20:29:13.793: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034674629s
May  1 20:29:13.793: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 20:29:13.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2571" for this suite. 05/01/23 20:29:13.846
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5162,"failed":0}
------------------------------
• [SLOW TEST] [8.317 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:29:05.567
    May  1 20:29:05.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context-test 05/01/23 20:29:05.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:29:05.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:29:05.662
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    May  1 20:29:05.758: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb" in namespace "security-context-test-2571" to be "Succeeded or Failed"
    May  1 20:29:05.772: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.98505ms
    May  1 20:29:07.787: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028467439s
    May  1 20:29:09.795: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036713929s
    May  1 20:29:11.789: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030946505s
    May  1 20:29:13.793: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.034674629s
    May  1 20:29:13.793: INFO: Pod "alpine-nnp-false-b9926458-007e-472c-8099-606887573ddb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 20:29:13.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2571" for this suite. 05/01/23 20:29:13.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:29:13.887
May  1 20:29:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename statefulset 05/01/23 20:29:13.89
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:29:14.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:29:14.05
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9219 05/01/23 20:29:14.076
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-9219 05/01/23 20:29:14.278
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9219 05/01/23 20:29:14.406
May  1 20:29:14.430: INFO: Found 0 stateful pods, waiting for 1
May  1 20:29:24.445: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/01/23 20:29:24.445
May  1 20:29:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 20:29:25.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 20:29:25.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 20:29:25.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 20:29:25.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  1 20:29:35.070: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  1 20:29:35.070: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:29:35.180: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May  1 20:29:35.180: INFO: ss-0  10.45.145.124  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
May  1 20:29:35.180: INFO: 
May  1 20:29:35.180: INFO: StatefulSet ss has not reached scale 3, at 1
May  1 20:29:36.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.971246703s
May  1 20:29:37.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956538054s
May  1 20:29:38.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.923652084s
May  1 20:29:39.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896573036s
May  1 20:29:40.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.858688478s
May  1 20:29:41.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.841063081s
May  1 20:29:42.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.823523586s
May  1 20:29:43.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.806552362s
May  1 20:29:44.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 768.783166ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9219 05/01/23 20:29:45.409
May  1 20:29:45.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 20:29:45.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May  1 20:29:45.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 20:29:45.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 20:29:45.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 20:29:46.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  1 20:29:46.604: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 20:29:46.604: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 20:29:46.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May  1 20:29:47.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  1 20:29:47.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May  1 20:29:47.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May  1 20:29:47.219: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:29:47.219: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  1 20:29:47.219: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/01/23 20:29:47.219
May  1 20:29:47.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 20:29:47.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 20:29:47.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 20:29:47.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 20:29:47.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 20:29:48.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 20:29:48.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 20:29:48.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 20:29:48.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May  1 20:29:48.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May  1 20:29:48.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May  1 20:29:48.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May  1 20:29:48.735: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:29:48.809: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May  1 20:29:58.860: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  1 20:29:58.860: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  1 20:29:58.860: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  1 20:29:58.980: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May  1 20:29:58.980: INFO: ss-0  10.45.145.124  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
May  1 20:29:58.980: INFO: ss-1  10.45.145.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
May  1 20:29:58.980: INFO: ss-2  10.45.145.71   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
May  1 20:29:58.981: INFO: 
May  1 20:29:58.981: INFO: StatefulSet ss has not reached scale 0, at 3
May  1 20:30:00.047: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May  1 20:30:00.047: INFO: ss-0  10.45.145.124  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
May  1 20:30:00.047: INFO: ss-1  10.45.145.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
May  1 20:30:00.047: INFO: ss-2  10.45.145.71   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
May  1 20:30:00.047: INFO: 
May  1 20:30:00.047: INFO: StatefulSet ss has not reached scale 0, at 3
May  1 20:30:01.068: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
May  1 20:30:01.068: INFO: ss-1  10.45.145.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
May  1 20:30:01.071: INFO: 
May  1 20:30:01.071: INFO: StatefulSet ss has not reached scale 0, at 1
May  1 20:30:02.091: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.878343883s
May  1 20:30:03.138: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.858282569s
May  1 20:30:04.162: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.811395059s
May  1 20:30:05.184: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.787787767s
May  1 20:30:06.207: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.765313183s
May  1 20:30:07.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.742377344s
May  1 20:30:08.243: INFO: Verifying statefulset ss doesn't scale past 0 for another 723.14025ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9219 05/01/23 20:30:09.243
May  1 20:30:09.259: INFO: Scaling statefulset ss to 0
May  1 20:30:09.343: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
May  1 20:30:09.370: INFO: Deleting all statefulset in ns statefulset-9219
May  1 20:30:09.390: INFO: Scaling statefulset ss to 0
May  1 20:30:09.474: INFO: Waiting for statefulset status.replicas updated to 0
May  1 20:30:09.498: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
May  1 20:30:09.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9219" for this suite. 05/01/23 20:30:09.63
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":282,"skipped":5170,"failed":0}
------------------------------
• [SLOW TEST] [55.807 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:29:13.887
    May  1 20:29:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename statefulset 05/01/23 20:29:13.89
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:29:14.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:29:14.05
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9219 05/01/23 20:29:14.076
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-9219 05/01/23 20:29:14.278
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9219 05/01/23 20:29:14.406
    May  1 20:29:14.430: INFO: Found 0 stateful pods, waiting for 1
    May  1 20:29:24.445: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/01/23 20:29:24.445
    May  1 20:29:24.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 20:29:25.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 20:29:25.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 20:29:25.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 20:29:25.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May  1 20:29:35.070: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  1 20:29:35.070: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:29:35.180: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    May  1 20:29:35.180: INFO: ss-0  10.45.145.124  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:25 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
    May  1 20:29:35.180: INFO: 
    May  1 20:29:35.180: INFO: StatefulSet ss has not reached scale 3, at 1
    May  1 20:29:36.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.971246703s
    May  1 20:29:37.230: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956538054s
    May  1 20:29:38.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.923652084s
    May  1 20:29:39.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.896573036s
    May  1 20:29:40.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.858688478s
    May  1 20:29:41.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.841063081s
    May  1 20:29:42.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.823523586s
    May  1 20:29:43.383: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.806552362s
    May  1 20:29:44.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 768.783166ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9219 05/01/23 20:29:45.409
    May  1 20:29:45.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 20:29:45.963: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May  1 20:29:45.963: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 20:29:45.963: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 20:29:45.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 20:29:46.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  1 20:29:46.604: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 20:29:46.604: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 20:29:46.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May  1 20:29:47.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May  1 20:29:47.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May  1 20:29:47.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May  1 20:29:47.219: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:29:47.219: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May  1 20:29:47.219: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/01/23 20:29:47.219
    May  1 20:29:47.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 20:29:47.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 20:29:47.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 20:29:47.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 20:29:47.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 20:29:48.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 20:29:48.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 20:29:48.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 20:29:48.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=statefulset-9219 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May  1 20:29:48.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May  1 20:29:48.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May  1 20:29:48.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May  1 20:29:48.735: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:29:48.809: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    May  1 20:29:58.860: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May  1 20:29:58.860: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May  1 20:29:58.860: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May  1 20:29:58.980: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    May  1 20:29:58.980: INFO: ss-0  10.45.145.124  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
    May  1 20:29:58.980: INFO: ss-1  10.45.145.126  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
    May  1 20:29:58.980: INFO: ss-2  10.45.145.71   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
    May  1 20:29:58.981: INFO: 
    May  1 20:29:58.981: INFO: StatefulSet ss has not reached scale 0, at 3
    May  1 20:30:00.047: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    May  1 20:30:00.047: INFO: ss-0  10.45.145.124  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:14 +0000 UTC  }]
    May  1 20:30:00.047: INFO: ss-1  10.45.145.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
    May  1 20:30:00.047: INFO: ss-2  10.45.145.71   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:49 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
    May  1 20:30:00.047: INFO: 
    May  1 20:30:00.047: INFO: StatefulSet ss has not reached scale 0, at 3
    May  1 20:30:01.068: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    May  1 20:30:01.068: INFO: ss-1  10.45.145.126  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-01 20:29:35 +0000 UTC  }]
    May  1 20:30:01.071: INFO: 
    May  1 20:30:01.071: INFO: StatefulSet ss has not reached scale 0, at 1
    May  1 20:30:02.091: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.878343883s
    May  1 20:30:03.138: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.858282569s
    May  1 20:30:04.162: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.811395059s
    May  1 20:30:05.184: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.787787767s
    May  1 20:30:06.207: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.765313183s
    May  1 20:30:07.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.742377344s
    May  1 20:30:08.243: INFO: Verifying statefulset ss doesn't scale past 0 for another 723.14025ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9219 05/01/23 20:30:09.243
    May  1 20:30:09.259: INFO: Scaling statefulset ss to 0
    May  1 20:30:09.343: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    May  1 20:30:09.370: INFO: Deleting all statefulset in ns statefulset-9219
    May  1 20:30:09.390: INFO: Scaling statefulset ss to 0
    May  1 20:30:09.474: INFO: Waiting for statefulset status.replicas updated to 0
    May  1 20:30:09.498: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    May  1 20:30:09.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9219" for this suite. 05/01/23 20:30:09.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:09.695
May  1 20:30:09.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:30:09.697
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:09.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:09.779
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:30:09.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9222" for this suite. 05/01/23 20:30:09.893
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":283,"skipped":5182,"failed":0}
------------------------------
• [0.328 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:09.695
    May  1 20:30:09.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:30:09.697
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:09.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:09.779
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:30:09.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9222" for this suite. 05/01/23 20:30:09.893
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:10.043
May  1 20:30:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:30:10.045
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:10.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:10.133
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-0eb97d19-646a-4345-80c4-3de10599ad45 05/01/23 20:30:10.154
STEP: Creating a pod to test consume configMaps 05/01/23 20:30:10.237
May  1 20:30:10.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520" in namespace "projected-1785" to be "Succeeded or Failed"
May  1 20:30:10.381: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 16.471143ms
May  1 20:30:12.414: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049639987s
May  1 20:30:14.406: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041796054s
May  1 20:30:16.398: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033703494s
STEP: Saw pod success 05/01/23 20:30:16.398
May  1 20:30:16.399: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520" satisfied condition "Succeeded or Failed"
May  1 20:30:16.418: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:30:16.448
May  1 20:30:16.491: INFO: Waiting for pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 to disappear
May  1 20:30:16.505: INFO: Pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:30:16.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1785" for this suite. 05/01/23 20:30:16.544
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":284,"skipped":5217,"failed":0}
------------------------------
• [SLOW TEST] [6.533 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:10.043
    May  1 20:30:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:30:10.045
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:10.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:10.133
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-0eb97d19-646a-4345-80c4-3de10599ad45 05/01/23 20:30:10.154
    STEP: Creating a pod to test consume configMaps 05/01/23 20:30:10.237
    May  1 20:30:10.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520" in namespace "projected-1785" to be "Succeeded or Failed"
    May  1 20:30:10.381: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 16.471143ms
    May  1 20:30:12.414: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049639987s
    May  1 20:30:14.406: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041796054s
    May  1 20:30:16.398: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033703494s
    STEP: Saw pod success 05/01/23 20:30:16.398
    May  1 20:30:16.399: INFO: Pod "pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520" satisfied condition "Succeeded or Failed"
    May  1 20:30:16.418: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:30:16.448
    May  1 20:30:16.491: INFO: Waiting for pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 to disappear
    May  1 20:30:16.505: INFO: Pod pod-projected-configmaps-5d4cbc8f-6007-42da-af3b-4f8571499520 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:30:16.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1785" for this suite. 05/01/23 20:30:16.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:16.581
May  1 20:30:16.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-pred 05/01/23 20:30:16.583
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:16.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:16.681
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  1 20:30:16.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  1 20:30:16.754: INFO: Waiting for terminating namespaces to be deleted...
May  1 20:30:16.807: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.124 before test
May  1 20:30:16.856: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:30:16.856: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container calico-typha ready: true, restart count 1
May  1 20:30:16.856: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:30:16.856: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:30:16.856: INFO: 	Container pause ready: true, restart count 0
May  1 20:30:16.856: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:30:16.856: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container tuned ready: true, restart count 0
May  1 20:30:16.856: INFO: dns-default-mmw2q from openshift-dns started at 2023-05-01 20:04:01 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container dns ready: true, restart count 0
May  1 20:30:16.856: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.856: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.856: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:30:16.857: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:30:16.857: INFO: ingress-canary-t55qt from openshift-ingress-canary started at 2023-05-01 20:04:01 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:30:16.857: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.857: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.857: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:30:16.857: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:30:16.857: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:30:16.857: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.857: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:30:16.857: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:30:16.857: INFO: collect-profiles-28049535-cmpbk from openshift-operator-lifecycle-manager started at 2023-05-01 20:15:00 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.857: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 20:30:16.857: INFO: collect-profiles-28049550-vjnr9 from openshift-operator-lifecycle-manager started at 2023-05-01 20:30:00 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.858: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 20:30:16.858: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.858: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  1 20:30:16.858: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.858: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:30:16.858: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:30:16.858: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.126 before test
May  1 20:30:16.917: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:30:16.917: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 20:30:16.917: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:30:16.917: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:30:16.917: INFO: 	Container pause ready: true, restart count 0
May  1 20:30:16.917: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:30:16.917: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.917: INFO: 	Container vpn ready: true, restart count 0
May  1 20:30:16.918: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container tuned ready: true, restart count 0
May  1 20:30:16.918: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container console ready: true, restart count 0
May  1 20:30:16.918: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container download-server ready: true, restart count 0
May  1 20:30:16.918: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container dns ready: true, restart count 0
May  1 20:30:16.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.918: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:30:16.918: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container registry ready: true, restart count 0
May  1 20:30:16.918: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:30:16.918: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container pvc-permissions ready: false, restart count 0
May  1 20:30:16.918: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:30:16.918: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.918: INFO: 	Container router ready: true, restart count 0
May  1 20:30:16.918: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container alertmanager ready: true, restart count 1
May  1 20:30:16.919: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  1 20:30:16.919: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:30:16.919: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May  1 20:30:16.919: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 20:30:16.919: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.919: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container prometheus ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 20:30:16.920: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container prometheus-operator ready: true, restart count 0
May  1 20:30:16.920: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.920: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 20:30:16.920: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container reload ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container telemeter-client ready: true, restart count 0
May  1 20:30:16.920: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:30:16.920: INFO: 	Container thanos-query ready: true, restart count 0
May  1 20:30:16.920: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.920: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:30:16.921: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:30:16.921: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.921: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 20:30:16.921: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.921: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:30:16.921: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:30:16.921: INFO: packageserver-596c56d895-5gw4x from openshift-operator-lifecycle-manager started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container packageserver ready: true, restart count 0
May  1 20:30:16.921: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container e2e ready: true, restart count 0
May  1 20:30:16.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:30:16.921: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:30:16.921: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:30:16.921: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.921: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  1 20:30:16.921: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.71 before test
May  1 20:30:16.987: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.987: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  1 20:30:16.987: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.987: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:30:16.987: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.987: INFO: 	Container calico-typha ready: true, restart count 0
May  1 20:30:16.987: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.987: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:30:16.987: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.987: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:30:16.988: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-f9zmq from ibm-system started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:30:16.988: INFO: 	Container pause ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
May  1 20:30:16.988: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
May  1 20:30:16.988: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  1 20:30:16.988: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:30:16.988: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May  1 20:30:16.988: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May  1 20:30:16.988: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container tuned ready: true, restart count 0
May  1 20:30:16.988: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.988: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May  1 20:30:16.989: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May  1 20:30:16.989: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container cluster-storage-operator ready: true, restart count 1
May  1 20:30:16.989: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 20:30:16.989: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 20:30:16.989: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
May  1 20:30:16.989: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container webhook ready: true, restart count 0
May  1 20:30:16.989: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container webhook ready: true, restart count 0
May  1 20:30:16.989: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container console-operator ready: true, restart count 1
May  1 20:30:16.989: INFO: 	Container conversion-webhook-server ready: true, restart count 2
May  1 20:30:16.989: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container console ready: true, restart count 0
May  1 20:30:16.989: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container download-server ready: true, restart count 0
May  1 20:30:16.989: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container dns-operator ready: true, restart count 0
May  1 20:30:16.989: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.989: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.989: INFO: 	Container dns ready: true, restart count 0
May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.990: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:30:16.990: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May  1 20:30:16.990: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:30:16.990: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:30:16.990: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container ingress-operator ready: true, restart count 0
May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.990: INFO: router-default-dc48bc679-rtqn9 from openshift-ingress started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container router ready: true, restart count 0
May  1 20:30:16.990: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container insights-operator ready: true, restart count 1
May  1 20:30:16.990: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.990: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
May  1 20:30:16.990: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.990: INFO: 	Container migrator ready: true, restart count 0
May  1 20:30:16.990: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:30:16.991: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:30:16.991: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container marketplace-operator ready: true, restart count 0
May  1 20:30:16.991: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:30:16.991: INFO: redhat-operators-zb9j9 from openshift-marketplace started at 2023-05-01 19:34:11 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:30:16.991: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 20:03:39 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container alertmanager ready: true, restart count 1
May  1 20:30:16.991: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:30:16.991: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.991: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.991: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:30:16.991: INFO: prometheus-adapter-d8df9dbf9-wc62l from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.991: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 20:30:16.992: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 20:03:38 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container prometheus ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 20:30:16.992: INFO: prometheus-operator-admission-webhook-98cbdbf8f-xhqsl from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 20:30:16.992: INFO: thanos-querier-6dcb8b776-mh4qn from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (6 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container thanos-query ready: true, restart count 0
May  1 20:30:16.992: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:30:16.992: INFO: multus-admission-controller-6b76fd464f-8hb9z from openshift-multus started at 2023-05-01 20:03:34 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.992: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 20:30:16.992: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.992: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:30:16.992: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:30:16.993: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:30:16.993: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container check-endpoints ready: true, restart count 0
May  1 20:30:16.993: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:30:16.993: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container network-operator ready: true, restart count 1
May  1 20:30:16.993: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container catalog-operator ready: true, restart count 0
May  1 20:30:16.993: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container olm-operator ready: true, restart count 0
May  1 20:30:16.993: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container package-server-manager ready: true, restart count 0
May  1 20:30:16.993: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container packageserver ready: true, restart count 0
May  1 20:30:16.993: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container metrics ready: true, restart count 2
May  1 20:30:16.993: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container push-gateway ready: true, restart count 0
May  1 20:30:16.993: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container service-ca-operator ready: true, restart count 1
May  1 20:30:16.993: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.993: INFO: 	Container service-ca-controller ready: true, restart count 0
May  1 20:30:16.994: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:30:16.994: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:30:16.994: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:30:16.994: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
May  1 20:30:16.994: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 20:30:16.994
May  1 20:30:17.091: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7933" to be "running"
May  1 20:30:17.117: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 25.523786ms
May  1 20:30:19.144: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052481526s
May  1 20:30:21.132: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.040235063s
May  1 20:30:21.132: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 20:30:21.149
STEP: Trying to apply a random label on the found node. 05/01/23 20:30:21.184
STEP: verifying the node has the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e 42 05/01/23 20:30:21.228
STEP: Trying to relaunch the pod, now with labels. 05/01/23 20:30:21.249
May  1 20:30:21.312: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7933" to be "not pending"
May  1 20:30:21.326: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 13.695217ms
May  1 20:30:23.340: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02706211s
May  1 20:30:25.349: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.03684825s
May  1 20:30:25.350: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e off the node 10.45.145.124 05/01/23 20:30:25.365
STEP: verifying the node doesn't have the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e 05/01/23 20:30:25.434
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  1 20:30:25.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7933" for this suite. 05/01/23 20:30:25.489
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":285,"skipped":5229,"failed":0}
------------------------------
• [SLOW TEST] [8.962 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:16.581
    May  1 20:30:16.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-pred 05/01/23 20:30:16.583
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:16.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:16.681
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  1 20:30:16.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  1 20:30:16.754: INFO: Waiting for terminating namespaces to be deleted...
    May  1 20:30:16.807: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.124 before test
    May  1 20:30:16.856: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:30:16.856: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container calico-typha ready: true, restart count 1
    May  1 20:30:16.856: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:30:16.856: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:30:16.856: INFO: 	Container pause ready: true, restart count 0
    May  1 20:30:16.856: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:30:16.856: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:30:16.856: INFO: dns-default-mmw2q from openshift-dns started at 2023-05-01 20:04:01 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container dns ready: true, restart count 0
    May  1 20:30:16.856: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.856: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.856: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:30:16.857: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:30:16.857: INFO: ingress-canary-t55qt from openshift-ingress-canary started at 2023-05-01 20:04:01 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:30:16.857: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.857: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.857: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:30:16.857: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:30:16.857: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:30:16.857: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.857: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:30:16.857: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:30:16.857: INFO: collect-profiles-28049535-cmpbk from openshift-operator-lifecycle-manager started at 2023-05-01 20:15:00 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.857: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 20:30:16.857: INFO: collect-profiles-28049550-vjnr9 from openshift-operator-lifecycle-manager started at 2023-05-01 20:30:00 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.858: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 20:30:16.858: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.858: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  1 20:30:16.858: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.858: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:30:16.858: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:30:16.858: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.126 before test
    May  1 20:30:16.917: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:30:16.917: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 20:30:16.917: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:30:16.917: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:30:16.917: INFO: 	Container pause ready: true, restart count 0
    May  1 20:30:16.917: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:30:16.917: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.917: INFO: 	Container vpn ready: true, restart count 0
    May  1 20:30:16.918: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:30:16.918: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container console ready: true, restart count 0
    May  1 20:30:16.918: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container download-server ready: true, restart count 0
    May  1 20:30:16.918: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container dns ready: true, restart count 0
    May  1 20:30:16.918: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.918: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:30:16.918: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container registry ready: true, restart count 0
    May  1 20:30:16.918: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:30:16.918: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container pvc-permissions ready: false, restart count 0
    May  1 20:30:16.918: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:30:16.918: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.918: INFO: 	Container router ready: true, restart count 0
    May  1 20:30:16.918: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 20:30:16.919: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  1 20:30:16.919: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:30:16.919: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May  1 20:30:16.919: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 20:30:16.919: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.919: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:30:16.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container prometheus ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 20:30:16.920: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  1 20:30:16.920: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.920: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 20:30:16.920: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container reload ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container telemeter-client ready: true, restart count 0
    May  1 20:30:16.920: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:30:16.920: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 20:30:16.920: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.920: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:30:16.921: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:30:16.921: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.921: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 20:30:16.921: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.921: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:30:16.921: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:30:16.921: INFO: packageserver-596c56d895-5gw4x from openshift-operator-lifecycle-manager started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container packageserver ready: true, restart count 0
    May  1 20:30:16.921: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container e2e ready: true, restart count 0
    May  1 20:30:16.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:30:16.921: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:30:16.921: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:30:16.921: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.921: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    May  1 20:30:16.921: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.71 before test
    May  1 20:30:16.987: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.987: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  1 20:30:16.987: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.987: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:30:16.987: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.987: INFO: 	Container calico-typha ready: true, restart count 0
    May  1 20:30:16.987: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.987: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:30:16.987: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.987: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:30:16.988: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-f9zmq from ibm-system started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:30:16.988: INFO: 	Container pause ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    May  1 20:30:16.988: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:30:16.988: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    May  1 20:30:16.988: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    May  1 20:30:16.988: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:30:16.988: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.988: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    May  1 20:30:16.989: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    May  1 20:30:16.989: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    May  1 20:30:16.989: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 20:30:16.989: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 20:30:16.989: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    May  1 20:30:16.989: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container webhook ready: true, restart count 0
    May  1 20:30:16.989: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container webhook ready: true, restart count 0
    May  1 20:30:16.989: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container console-operator ready: true, restart count 1
    May  1 20:30:16.989: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    May  1 20:30:16.989: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container console ready: true, restart count 0
    May  1 20:30:16.989: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container download-server ready: true, restart count 0
    May  1 20:30:16.989: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container dns-operator ready: true, restart count 0
    May  1 20:30:16.989: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.989: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.989: INFO: 	Container dns ready: true, restart count 0
    May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.990: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:30:16.990: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    May  1 20:30:16.990: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:30:16.990: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:30:16.990: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container ingress-operator ready: true, restart count 0
    May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.990: INFO: router-default-dc48bc679-rtqn9 from openshift-ingress started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container router ready: true, restart count 0
    May  1 20:30:16.990: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container insights-operator ready: true, restart count 1
    May  1 20:30:16.990: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:30:16.990: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.990: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    May  1 20:30:16.990: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.990: INFO: 	Container migrator ready: true, restart count 0
    May  1 20:30:16.990: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:30:16.991: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:30:16.991: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container marketplace-operator ready: true, restart count 0
    May  1 20:30:16.991: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:30:16.991: INFO: redhat-operators-zb9j9 from openshift-marketplace started at 2023-05-01 19:34:11 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:30:16.991: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 20:03:39 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 20:30:16.991: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:30:16.991: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.991: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.991: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:30:16.991: INFO: prometheus-adapter-d8df9dbf9-wc62l from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.991: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 20:30:16.992: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 20:03:38 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container prometheus ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 20:30:16.992: INFO: prometheus-operator-admission-webhook-98cbdbf8f-xhqsl from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 20:30:16.992: INFO: thanos-querier-6dcb8b776-mh4qn from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (6 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 20:30:16.992: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:30:16.992: INFO: multus-admission-controller-6b76fd464f-8hb9z from openshift-multus started at 2023-05-01 20:03:34 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.992: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 20:30:16.992: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.992: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:30:16.992: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:30:16.993: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:30:16.993: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container check-endpoints ready: true, restart count 0
    May  1 20:30:16.993: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:30:16.993: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container network-operator ready: true, restart count 1
    May  1 20:30:16.993: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container catalog-operator ready: true, restart count 0
    May  1 20:30:16.993: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container olm-operator ready: true, restart count 0
    May  1 20:30:16.993: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container package-server-manager ready: true, restart count 0
    May  1 20:30:16.993: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container packageserver ready: true, restart count 0
    May  1 20:30:16.993: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container metrics ready: true, restart count 2
    May  1 20:30:16.993: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container push-gateway ready: true, restart count 0
    May  1 20:30:16.993: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container service-ca-operator ready: true, restart count 1
    May  1 20:30:16.993: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.993: INFO: 	Container service-ca-controller ready: true, restart count 0
    May  1 20:30:16.994: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:30:16.994: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:30:16.994: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:30:16.994: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
    May  1 20:30:16.994: INFO: 	Container tigera-operator ready: true, restart count 4
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 20:30:16.994
    May  1 20:30:17.091: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7933" to be "running"
    May  1 20:30:17.117: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 25.523786ms
    May  1 20:30:19.144: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052481526s
    May  1 20:30:21.132: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.040235063s
    May  1 20:30:21.132: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 20:30:21.149
    STEP: Trying to apply a random label on the found node. 05/01/23 20:30:21.184
    STEP: verifying the node has the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e 42 05/01/23 20:30:21.228
    STEP: Trying to relaunch the pod, now with labels. 05/01/23 20:30:21.249
    May  1 20:30:21.312: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7933" to be "not pending"
    May  1 20:30:21.326: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 13.695217ms
    May  1 20:30:23.340: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02706211s
    May  1 20:30:25.349: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.03684825s
    May  1 20:30:25.350: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e off the node 10.45.145.124 05/01/23 20:30:25.365
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-783912ec-ee04-4b91-88fd-d3919c38d12e 05/01/23 20:30:25.434
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:30:25.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7933" for this suite. 05/01/23 20:30:25.489
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:25.545
May  1 20:30:25.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:30:25.547
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:25.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:25.655
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-9e16e578-c9f5-43d0-9bb3-4c38b9fd7348 05/01/23 20:30:25.673
STEP: Creating a pod to test consume configMaps 05/01/23 20:30:25.769
May  1 20:30:25.878: INFO: Waiting up to 5m0s for pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca" in namespace "configmap-5650" to be "Succeeded or Failed"
May  1 20:30:25.896: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Pending", Reason="", readiness=false. Elapsed: 18.566591ms
May  1 20:30:27.941: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063642628s
May  1 20:30:29.916: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Running", Reason="", readiness=false. Elapsed: 4.038052833s
May  1 20:30:31.925: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047456732s
STEP: Saw pod success 05/01/23 20:30:31.925
May  1 20:30:31.926: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca" satisfied condition "Succeeded or Failed"
May  1 20:30:31.949: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:30:32.113
May  1 20:30:32.152: INFO: Waiting for pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca to disappear
May  1 20:30:32.177: INFO: Pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:30:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5650" for this suite. 05/01/23 20:30:32.198
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":286,"skipped":5229,"failed":0}
------------------------------
• [SLOW TEST] [6.747 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:25.545
    May  1 20:30:25.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:30:25.547
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:25.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:25.655
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-9e16e578-c9f5-43d0-9bb3-4c38b9fd7348 05/01/23 20:30:25.673
    STEP: Creating a pod to test consume configMaps 05/01/23 20:30:25.769
    May  1 20:30:25.878: INFO: Waiting up to 5m0s for pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca" in namespace "configmap-5650" to be "Succeeded or Failed"
    May  1 20:30:25.896: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Pending", Reason="", readiness=false. Elapsed: 18.566591ms
    May  1 20:30:27.941: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063642628s
    May  1 20:30:29.916: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Running", Reason="", readiness=false. Elapsed: 4.038052833s
    May  1 20:30:31.925: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047456732s
    STEP: Saw pod success 05/01/23 20:30:31.925
    May  1 20:30:31.926: INFO: Pod "pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca" satisfied condition "Succeeded or Failed"
    May  1 20:30:31.949: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:30:32.113
    May  1 20:30:32.152: INFO: Waiting for pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca to disappear
    May  1 20:30:32.177: INFO: Pod pod-configmaps-b48900ed-ae1e-4679-8eb7-16ed543e83ca no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:30:32.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5650" for this suite. 05/01/23 20:30:32.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:32.297
May  1 20:30:32.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename runtimeclass 05/01/23 20:30:32.299
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:32.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:32.487
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May  1 20:30:32.677: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2350 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  1 20:30:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2350" for this suite. 05/01/23 20:30:32.765
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":287,"skipped":5265,"failed":0}
------------------------------
• [0.509 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:32.297
    May  1 20:30:32.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename runtimeclass 05/01/23 20:30:32.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:32.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:32.487
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May  1 20:30:32.677: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2350 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  1 20:30:32.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2350" for this suite. 05/01/23 20:30:32.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:32.808
May  1 20:30:32.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename dns 05/01/23 20:30:32.809
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:32.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:32.908
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/01/23 20:30:32.927
May  1 20:30:33.060: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5108  f1a3fb0b-5009-4bbb-9578-8513d2bf5a42 127761 0 2023-05-01 20:30:32 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-05-01 20:30:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp7mf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp7mf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May  1 20:30:33.061: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5108" to be "running and ready"
May  1 20:30:33.093: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 31.789636ms
May  1 20:30:33.093: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  1 20:30:35.109: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048127945s
May  1 20:30:35.109: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May  1 20:30:37.108: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.04724231s
May  1 20:30:37.108: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May  1 20:30:37.108: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/01/23 20:30:37.108
May  1 20:30:37.109: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5108 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:30:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:30:37.110: INFO: ExecWithOptions: Clientset creation
May  1 20:30:37.110: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5108/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/01/23 20:30:37.457
May  1 20:30:37.457: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5108 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:30:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:30:37.458: INFO: ExecWithOptions: Clientset creation
May  1 20:30:37.458: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5108/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 20:30:37.796: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
May  1 20:30:37.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5108" for this suite. 05/01/23 20:30:37.874
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":288,"skipped":5294,"failed":0}
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:32.808
    May  1 20:30:32.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename dns 05/01/23 20:30:32.809
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:32.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:32.908
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/01/23 20:30:32.927
    May  1 20:30:33.060: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5108  f1a3fb0b-5009-4bbb-9578-8513d2bf5a42 127761 0 2023-05-01 20:30:32 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-05-01 20:30:32 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bp7mf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bp7mf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May  1 20:30:33.061: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5108" to be "running and ready"
    May  1 20:30:33.093: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 31.789636ms
    May  1 20:30:33.093: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:30:35.109: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048127945s
    May  1 20:30:35.109: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:30:37.108: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.04724231s
    May  1 20:30:37.108: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May  1 20:30:37.108: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/01/23 20:30:37.108
    May  1 20:30:37.109: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5108 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:30:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:30:37.110: INFO: ExecWithOptions: Clientset creation
    May  1 20:30:37.110: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5108/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/01/23 20:30:37.457
    May  1 20:30:37.457: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5108 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:30:37.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:30:37.458: INFO: ExecWithOptions: Clientset creation
    May  1 20:30:37.458: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-5108/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 20:30:37.796: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    May  1 20:30:37.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5108" for this suite. 05/01/23 20:30:37.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:37.934
May  1 20:30:37.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 20:30:37.937
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:38.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:38.024
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
May  1 20:30:38.269: INFO: Create a RollingUpdate DaemonSet
May  1 20:30:38.299: INFO: Check that daemon pods launch on every node of the cluster
May  1 20:30:38.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:30:38.334: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:30:39.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:30:39.372: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:30:40.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:30:40.380: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:30:41.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 20:30:41.398: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 20:30:42.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 20:30:42.369: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May  1 20:30:42.369: INFO: Update the DaemonSet to trigger a rollout
May  1 20:30:42.483: INFO: Updating DaemonSet daemon-set
May  1 20:30:46.604: INFO: Roll back the DaemonSet before rollout is complete
May  1 20:30:46.679: INFO: Updating DaemonSet daemon-set
May  1 20:30:46.680: INFO: Make sure DaemonSet rollback is complete
May  1 20:30:46.694: INFO: Wrong image for pod: daemon-set-sj5s6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May  1 20:30:46.694: INFO: Pod daemon-set-sj5s6 is not available
May  1 20:30:52.753: INFO: Pod daemon-set-wbtjc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:30:52.821
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2859, will wait for the garbage collector to delete the pods 05/01/23 20:30:52.822
May  1 20:30:52.927: INFO: Deleting DaemonSet.extensions daemon-set took: 33.736707ms
May  1 20:30:53.128: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.15616ms
May  1 20:30:57.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:30:57.158: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 20:30:57.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"128180"},"items":null}

May  1 20:30:57.203: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"128180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 20:30:57.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2859" for this suite. 05/01/23 20:30:57.3
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":289,"skipped":5304,"failed":0}
------------------------------
• [SLOW TEST] [19.411 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:37.934
    May  1 20:30:37.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 20:30:37.937
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:38.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:38.024
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    May  1 20:30:38.269: INFO: Create a RollingUpdate DaemonSet
    May  1 20:30:38.299: INFO: Check that daemon pods launch on every node of the cluster
    May  1 20:30:38.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:30:38.334: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:30:39.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:30:39.372: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:30:40.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:30:40.380: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:30:41.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 20:30:41.398: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 20:30:42.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 20:30:42.369: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    May  1 20:30:42.369: INFO: Update the DaemonSet to trigger a rollout
    May  1 20:30:42.483: INFO: Updating DaemonSet daemon-set
    May  1 20:30:46.604: INFO: Roll back the DaemonSet before rollout is complete
    May  1 20:30:46.679: INFO: Updating DaemonSet daemon-set
    May  1 20:30:46.680: INFO: Make sure DaemonSet rollback is complete
    May  1 20:30:46.694: INFO: Wrong image for pod: daemon-set-sj5s6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    May  1 20:30:46.694: INFO: Pod daemon-set-sj5s6 is not available
    May  1 20:30:52.753: INFO: Pod daemon-set-wbtjc is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:30:52.821
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2859, will wait for the garbage collector to delete the pods 05/01/23 20:30:52.822
    May  1 20:30:52.927: INFO: Deleting DaemonSet.extensions daemon-set took: 33.736707ms
    May  1 20:30:53.128: INFO: Terminating DaemonSet.extensions daemon-set pods took: 201.15616ms
    May  1 20:30:57.158: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:30:57.158: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 20:30:57.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"128180"},"items":null}

    May  1 20:30:57.203: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"128180"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:30:57.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2859" for this suite. 05/01/23 20:30:57.3
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:30:57.345
May  1 20:30:57.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:30:57.349
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:57.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:57.418
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:30:57.439
May  1 20:30:57.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb" in namespace "projected-6726" to be "Succeeded or Failed"
May  1 20:30:57.592: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 36.806139ms
May  1 20:30:59.611: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056192902s
May  1 20:31:01.609: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053551606s
May  1 20:31:03.611: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056105908s
STEP: Saw pod success 05/01/23 20:31:03.611
May  1 20:31:03.612: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb" satisfied condition "Succeeded or Failed"
May  1 20:31:03.637: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb container client-container: <nil>
STEP: delete the pod 05/01/23 20:31:03.669
May  1 20:31:03.718: INFO: Waiting for pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb to disappear
May  1 20:31:03.733: INFO: Pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 20:31:03.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6726" for this suite. 05/01/23 20:31:03.758
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":290,"skipped":5305,"failed":0}
------------------------------
• [SLOW TEST] [6.445 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:30:57.345
    May  1 20:30:57.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:30:57.349
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:30:57.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:30:57.418
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:30:57.439
    May  1 20:30:57.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb" in namespace "projected-6726" to be "Succeeded or Failed"
    May  1 20:30:57.592: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 36.806139ms
    May  1 20:30:59.611: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056192902s
    May  1 20:31:01.609: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053551606s
    May  1 20:31:03.611: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056105908s
    STEP: Saw pod success 05/01/23 20:31:03.611
    May  1 20:31:03.612: INFO: Pod "downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb" satisfied condition "Succeeded or Failed"
    May  1 20:31:03.637: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb container client-container: <nil>
    STEP: delete the pod 05/01/23 20:31:03.669
    May  1 20:31:03.718: INFO: Waiting for pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb to disappear
    May  1 20:31:03.733: INFO: Pod downwardapi-volume-bbbaa8e4-c854-4c23-a985-406fa6dd37cb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 20:31:03.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6726" for this suite. 05/01/23 20:31:03.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:31:03.792
May  1 20:31:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:31:03.793
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:03.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:03.902
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:31:04.114
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:31:05.713
STEP: Deploying the webhook pod 05/01/23 20:31:05.76
STEP: Wait for the deployment to be ready 05/01/23 20:31:05.814
May  1 20:31:05.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:31:07.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:31:09.923
STEP: Verifying the service has paired with the endpoint 05/01/23 20:31:09.96
May  1 20:31:10.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/01/23 20:31:10.981
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/01/23 20:31:11.153
STEP: Creating a dummy validating-webhook-configuration object 05/01/23 20:31:11.26
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/01/23 20:31:11.318
STEP: Creating a dummy mutating-webhook-configuration object 05/01/23 20:31:11.37
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/01/23 20:31:11.422
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:31:11.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5627" for this suite. 05/01/23 20:31:11.554
STEP: Destroying namespace "webhook-5627-markers" for this suite. 05/01/23 20:31:11.593
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":291,"skipped":5326,"failed":0}
------------------------------
• [SLOW TEST] [8.061 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:31:03.792
    May  1 20:31:03.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:31:03.793
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:03.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:03.902
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:31:04.114
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:31:05.713
    STEP: Deploying the webhook pod 05/01/23 20:31:05.76
    STEP: Wait for the deployment to be ready 05/01/23 20:31:05.814
    May  1 20:31:05.857: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:31:07.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 31, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:31:09.923
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:31:09.96
    May  1 20:31:10.962: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/01/23 20:31:10.981
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/01/23 20:31:11.153
    STEP: Creating a dummy validating-webhook-configuration object 05/01/23 20:31:11.26
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/01/23 20:31:11.318
    STEP: Creating a dummy mutating-webhook-configuration object 05/01/23 20:31:11.37
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/01/23 20:31:11.422
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:31:11.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5627" for this suite. 05/01/23 20:31:11.554
    STEP: Destroying namespace "webhook-5627-markers" for this suite. 05/01/23 20:31:11.593
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:31:11.855
May  1 20:31:11.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename svcaccounts 05/01/23 20:31:11.859
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:11.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:11.996
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
May  1 20:31:12.191: INFO: created pod
May  1 20:31:12.194: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1606" to be "Succeeded or Failed"
May  1 20:31:12.239: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 45.22775ms
May  1 20:31:14.267: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072428311s
May  1 20:31:16.256: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06136471s
May  1 20:31:18.272: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077759478s
May  1 20:31:20.260: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065420614s
STEP: Saw pod success 05/01/23 20:31:20.26
May  1 20:31:20.261: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May  1 20:31:50.262: INFO: polling logs
May  1 20:31:50.300: INFO: Pod logs: 
I0501 20:31:14.423793       1 log.go:195] OK: Got token
I0501 20:31:14.424906       1 log.go:195] validating with in-cluster discovery
I0501 20:31:14.428511       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0501 20:31:14.430372       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682973672, NotBefore:1682973072, IssuedAt:1682973072, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e0a41970-7754-4e0d-9756-978cbf8dc8fc"}}}
I0501 20:31:14.477183       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0501 20:31:14.501810       1 log.go:195] OK: Validated signature on JWT
I0501 20:31:14.502111       1 log.go:195] OK: Got valid claims from token!
I0501 20:31:14.502212       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682973672, NotBefore:1682973072, IssuedAt:1682973072, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e0a41970-7754-4e0d-9756-978cbf8dc8fc"}}}

May  1 20:31:50.300: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
May  1 20:31:50.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1606" for this suite. 05/01/23 20:31:50.366
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":292,"skipped":5337,"failed":0}
------------------------------
• [SLOW TEST] [38.548 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:31:11.855
    May  1 20:31:11.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename svcaccounts 05/01/23 20:31:11.859
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:11.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:11.996
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    May  1 20:31:12.191: INFO: created pod
    May  1 20:31:12.194: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1606" to be "Succeeded or Failed"
    May  1 20:31:12.239: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 45.22775ms
    May  1 20:31:14.267: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072428311s
    May  1 20:31:16.256: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06136471s
    May  1 20:31:18.272: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077759478s
    May  1 20:31:20.260: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065420614s
    STEP: Saw pod success 05/01/23 20:31:20.26
    May  1 20:31:20.261: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May  1 20:31:50.262: INFO: polling logs
    May  1 20:31:50.300: INFO: Pod logs: 
    I0501 20:31:14.423793       1 log.go:195] OK: Got token
    I0501 20:31:14.424906       1 log.go:195] validating with in-cluster discovery
    I0501 20:31:14.428511       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0501 20:31:14.430372       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682973672, NotBefore:1682973072, IssuedAt:1682973072, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e0a41970-7754-4e0d-9756-978cbf8dc8fc"}}}
    I0501 20:31:14.477183       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0501 20:31:14.501810       1 log.go:195] OK: Validated signature on JWT
    I0501 20:31:14.502111       1 log.go:195] OK: Got valid claims from token!
    I0501 20:31:14.502212       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-1606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682973672, NotBefore:1682973072, IssuedAt:1682973072, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e0a41970-7754-4e0d-9756-978cbf8dc8fc"}}}

    May  1 20:31:50.300: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    May  1 20:31:50.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1606" for this suite. 05/01/23 20:31:50.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:31:50.408
May  1 20:31:50.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 20:31:50.41
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:50.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:50.541
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 05/01/23 20:31:50.592
STEP: Getting a ResourceQuota 05/01/23 20:31:50.613
STEP: Updating a ResourceQuota 05/01/23 20:31:50.653
STEP: Verifying a ResourceQuota was modified 05/01/23 20:31:50.687
STEP: Deleting a ResourceQuota 05/01/23 20:31:50.702
STEP: Verifying the deleted ResourceQuota 05/01/23 20:31:50.734
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 20:31:50.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8807" for this suite. 05/01/23 20:31:50.804
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":293,"skipped":5355,"failed":0}
------------------------------
• [0.440 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:31:50.408
    May  1 20:31:50.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 20:31:50.41
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:50.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:50.541
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 05/01/23 20:31:50.592
    STEP: Getting a ResourceQuota 05/01/23 20:31:50.613
    STEP: Updating a ResourceQuota 05/01/23 20:31:50.653
    STEP: Verifying a ResourceQuota was modified 05/01/23 20:31:50.687
    STEP: Deleting a ResourceQuota 05/01/23 20:31:50.702
    STEP: Verifying the deleted ResourceQuota 05/01/23 20:31:50.734
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 20:31:50.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8807" for this suite. 05/01/23 20:31:50.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:31:50.861
May  1 20:31:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:31:50.864
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:50.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:50.959
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6934 05/01/23 20:31:50.984
May  1 20:31:51.128: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6934" to be "running and ready"
May  1 20:31:51.141: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.151147ms
May  1 20:31:51.141: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May  1 20:31:53.153: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.024718193s
May  1 20:31:53.153: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
May  1 20:31:53.153: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
May  1 20:31:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May  1 20:31:53.792: INFO: rc: 7
May  1 20:31:53.828: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May  1 20:31:53.843: INFO: Pod kube-proxy-mode-detector no longer exists
May  1 20:31:53.843: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-6934 05/01/23 20:31:53.843
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6934 05/01/23 20:31:53.907
I0501 20:31:53.938945      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6934, replica count: 3
I0501 20:31:56.990057      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:31:59.992611      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:32:00.071: INFO: Creating new exec pod
May  1 20:32:00.127: INFO: Waiting up to 5m0s for pod "execpod-affinitymtnrx" in namespace "services-6934" to be "running"
May  1 20:32:00.140: INFO: Pod "execpod-affinitymtnrx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.83262ms
May  1 20:32:02.160: INFO: Pod "execpod-affinitymtnrx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032838729s
May  1 20:32:04.154: INFO: Pod "execpod-affinitymtnrx": Phase="Running", Reason="", readiness=true. Elapsed: 4.027561778s
May  1 20:32:04.154: INFO: Pod "execpod-affinitymtnrx" satisfied condition "running"
May  1 20:32:05.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May  1 20:32:05.739: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May  1 20:32:05.739: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:32:05.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.159 80'
May  1 20:32:06.554: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.218.159 80\nConnection to 172.21.218.159 80 port [tcp/http] succeeded!\n"
May  1 20:32:06.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:32:06.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31932'
May  1 20:32:07.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31932\nConnection to 10.45.145.126 31932 port [tcp/*] succeeded!\n"
May  1 20:32:07.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:32:07.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 31932'
May  1 20:32:07.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 31932\nConnection to 10.45.145.71 31932 port [tcp/*] succeeded!\n"
May  1 20:32:07.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:32:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:31932/ ; done'
May  1 20:32:08.707: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
May  1 20:32:08.707: INFO: stdout: "\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv"
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
May  1 20:32:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.45.145.124:31932/'
May  1 20:32:09.470: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
May  1 20:32:09.470: INFO: stdout: "affinity-nodeport-timeout-fs8tv"
May  1 20:32:29.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.45.145.124:31932/'
May  1 20:32:29.968: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
May  1 20:32:29.968: INFO: stdout: "affinity-nodeport-timeout-5l7vm"
May  1 20:32:29.968: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6934, will wait for the garbage collector to delete the pods 05/01/23 20:32:29.997
May  1 20:32:30.098: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 33.771332ms
May  1 20:32:30.299: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.526736ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:32:34.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6934" for this suite. 05/01/23 20:32:34.144
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":294,"skipped":5363,"failed":0}
------------------------------
• [SLOW TEST] [43.362 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:31:50.861
    May  1 20:31:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:31:50.864
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:31:50.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:31:50.959
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6934 05/01/23 20:31:50.984
    May  1 20:31:51.128: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6934" to be "running and ready"
    May  1 20:31:51.141: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 13.151147ms
    May  1 20:31:51.141: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:31:53.153: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.024718193s
    May  1 20:31:53.153: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    May  1 20:31:53.153: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    May  1 20:31:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    May  1 20:31:53.792: INFO: rc: 7
    May  1 20:31:53.828: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    May  1 20:31:53.843: INFO: Pod kube-proxy-mode-detector no longer exists
    May  1 20:31:53.843: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-nodeport-timeout in namespace services-6934 05/01/23 20:31:53.843
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6934 05/01/23 20:31:53.907
    I0501 20:31:53.938945      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6934, replica count: 3
    I0501 20:31:56.990057      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:31:59.992611      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:32:00.071: INFO: Creating new exec pod
    May  1 20:32:00.127: INFO: Waiting up to 5m0s for pod "execpod-affinitymtnrx" in namespace "services-6934" to be "running"
    May  1 20:32:00.140: INFO: Pod "execpod-affinitymtnrx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.83262ms
    May  1 20:32:02.160: INFO: Pod "execpod-affinitymtnrx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032838729s
    May  1 20:32:04.154: INFO: Pod "execpod-affinitymtnrx": Phase="Running", Reason="", readiness=true. Elapsed: 4.027561778s
    May  1 20:32:04.154: INFO: Pod "execpod-affinitymtnrx" satisfied condition "running"
    May  1 20:32:05.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    May  1 20:32:05.739: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    May  1 20:32:05.739: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:32:05.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.159 80'
    May  1 20:32:06.554: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.218.159 80\nConnection to 172.21.218.159 80 port [tcp/http] succeeded!\n"
    May  1 20:32:06.554: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:32:06.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 31932'
    May  1 20:32:07.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 31932\nConnection to 10.45.145.126 31932 port [tcp/*] succeeded!\n"
    May  1 20:32:07.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:32:07.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 31932'
    May  1 20:32:07.862: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 31932\nConnection to 10.45.145.71 31932 port [tcp/*] succeeded!\n"
    May  1 20:32:07.862: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:32:07.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:31932/ ; done'
    May  1 20:32:08.707: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
    May  1 20:32:08.707: INFO: stdout: "\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv\naffinity-nodeport-timeout-fs8tv"
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.707: INFO: Received response from host: affinity-nodeport-timeout-fs8tv
    May  1 20:32:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.45.145.124:31932/'
    May  1 20:32:09.470: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
    May  1 20:32:09.470: INFO: stdout: "affinity-nodeport-timeout-fs8tv"
    May  1 20:32:29.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-6934 exec execpod-affinitymtnrx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.45.145.124:31932/'
    May  1 20:32:29.968: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.45.145.124:31932/\n"
    May  1 20:32:29.968: INFO: stdout: "affinity-nodeport-timeout-5l7vm"
    May  1 20:32:29.968: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6934, will wait for the garbage collector to delete the pods 05/01/23 20:32:29.997
    May  1 20:32:30.098: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 33.771332ms
    May  1 20:32:30.299: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.526736ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:32:34.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6934" for this suite. 05/01/23 20:32:34.144
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:32:34.227
May  1 20:32:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 20:32:34.23
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:34.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:34.384
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 05/01/23 20:32:34.432
May  1 20:32:34.574: INFO: Waiting up to 5m0s for pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156" in namespace "downward-api-3353" to be "Succeeded or Failed"
May  1 20:32:34.587: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 12.51977ms
May  1 20:32:36.611: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037428919s
May  1 20:32:38.600: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026327745s
May  1 20:32:40.614: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039873586s
STEP: Saw pod success 05/01/23 20:32:40.615
May  1 20:32:40.615: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156" satisfied condition "Succeeded or Failed"
May  1 20:32:40.656: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 container dapi-container: <nil>
STEP: delete the pod 05/01/23 20:32:40.907
May  1 20:32:40.955: INFO: Waiting for pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 to disappear
May  1 20:32:40.970: INFO: Pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
May  1 20:32:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3353" for this suite. 05/01/23 20:32:40.989
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":295,"skipped":5374,"failed":0}
------------------------------
• [SLOW TEST] [6.832 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:32:34.227
    May  1 20:32:34.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 20:32:34.23
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:34.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:34.384
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 05/01/23 20:32:34.432
    May  1 20:32:34.574: INFO: Waiting up to 5m0s for pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156" in namespace "downward-api-3353" to be "Succeeded or Failed"
    May  1 20:32:34.587: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 12.51977ms
    May  1 20:32:36.611: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037428919s
    May  1 20:32:38.600: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026327745s
    May  1 20:32:40.614: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039873586s
    STEP: Saw pod success 05/01/23 20:32:40.615
    May  1 20:32:40.615: INFO: Pod "downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156" satisfied condition "Succeeded or Failed"
    May  1 20:32:40.656: INFO: Trying to get logs from node 10.45.145.124 pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 container dapi-container: <nil>
    STEP: delete the pod 05/01/23 20:32:40.907
    May  1 20:32:40.955: INFO: Waiting for pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 to disappear
    May  1 20:32:40.970: INFO: Pod downward-api-5916e7f8-8a5c-4d98-a0ac-577818c37156 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    May  1 20:32:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3353" for this suite. 05/01/23 20:32:40.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:32:41.066
May  1 20:32:41.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:32:41.069
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:41.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:41.2
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-0c7affdf-2103-44bf-9a4e-042e475000c3 05/01/23 20:32:41.228
STEP: Creating a pod to test consume secrets 05/01/23 20:32:41.278
May  1 20:32:41.491: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c" in namespace "projected-7234" to be "Succeeded or Failed"
May  1 20:32:41.514: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.864284ms
May  1 20:32:43.525: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034182006s
May  1 20:32:45.529: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038150592s
May  1 20:32:47.537: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046305912s
STEP: Saw pod success 05/01/23 20:32:47.537
May  1 20:32:47.537: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c" satisfied condition "Succeeded or Failed"
May  1 20:32:47.552: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c container projected-secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:32:47.615
May  1 20:32:47.657: INFO: Waiting for pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c to disappear
May  1 20:32:47.690: INFO: Pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 20:32:47.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7234" for this suite. 05/01/23 20:32:47.709
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":296,"skipped":5423,"failed":0}
------------------------------
• [SLOW TEST] [6.696 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:32:41.066
    May  1 20:32:41.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:32:41.069
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:41.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:41.2
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-0c7affdf-2103-44bf-9a4e-042e475000c3 05/01/23 20:32:41.228
    STEP: Creating a pod to test consume secrets 05/01/23 20:32:41.278
    May  1 20:32:41.491: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c" in namespace "projected-7234" to be "Succeeded or Failed"
    May  1 20:32:41.514: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.864284ms
    May  1 20:32:43.525: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034182006s
    May  1 20:32:45.529: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038150592s
    May  1 20:32:47.537: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046305912s
    STEP: Saw pod success 05/01/23 20:32:47.537
    May  1 20:32:47.537: INFO: Pod "pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c" satisfied condition "Succeeded or Failed"
    May  1 20:32:47.552: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:32:47.615
    May  1 20:32:47.657: INFO: Waiting for pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c to disappear
    May  1 20:32:47.690: INFO: Pod pod-projected-secrets-7e26e935-73aa-4b65-8fbe-0597d6c4db3c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 20:32:47.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7234" for this suite. 05/01/23 20:32:47.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:32:47.766
May  1 20:32:47.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 20:32:47.768
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:47.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:47.973
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-b04b6342-4e55-4297-9bd3-a06f4f6b80de 05/01/23 20:32:47.991
STEP: Creating a pod to test consume secrets 05/01/23 20:32:48.041
May  1 20:32:48.138: INFO: Waiting up to 5m0s for pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3" in namespace "secrets-5013" to be "Succeeded or Failed"
May  1 20:32:48.150: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.118501ms
May  1 20:32:50.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030115457s
May  1 20:32:52.169: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031399917s
May  1 20:32:54.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030040418s
STEP: Saw pod success 05/01/23 20:32:54.168
May  1 20:32:54.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3" satisfied condition "Succeeded or Failed"
May  1 20:32:54.181: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:32:54.214
May  1 20:32:54.250: INFO: Waiting for pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 to disappear
May  1 20:32:54.265: INFO: Pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 20:32:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5013" for this suite. 05/01/23 20:32:54.287
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":297,"skipped":5458,"failed":0}
------------------------------
• [SLOW TEST] [6.562 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:32:47.766
    May  1 20:32:47.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 20:32:47.768
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:47.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:47.973
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-b04b6342-4e55-4297-9bd3-a06f4f6b80de 05/01/23 20:32:47.991
    STEP: Creating a pod to test consume secrets 05/01/23 20:32:48.041
    May  1 20:32:48.138: INFO: Waiting up to 5m0s for pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3" in namespace "secrets-5013" to be "Succeeded or Failed"
    May  1 20:32:48.150: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.118501ms
    May  1 20:32:50.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030115457s
    May  1 20:32:52.169: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031399917s
    May  1 20:32:54.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030040418s
    STEP: Saw pod success 05/01/23 20:32:54.168
    May  1 20:32:54.168: INFO: Pod "pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3" satisfied condition "Succeeded or Failed"
    May  1 20:32:54.181: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:32:54.214
    May  1 20:32:54.250: INFO: Waiting for pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 to disappear
    May  1 20:32:54.265: INFO: Pod pod-secrets-91283a41-a05f-434b-94d6-30810d791bc3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 20:32:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5013" for this suite. 05/01/23 20:32:54.287
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:32:54.328
May  1 20:32:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename endpointslice 05/01/23 20:32:54.33
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:54.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:54.408
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
May  1 20:32:54.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7099" for this suite. 05/01/23 20:32:54.768
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":298,"skipped":5459,"failed":0}
------------------------------
• [0.471 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:32:54.328
    May  1 20:32:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename endpointslice 05/01/23 20:32:54.33
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:54.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:54.408
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    May  1 20:32:54.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7099" for this suite. 05/01/23 20:32:54.768
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:32:54.8
May  1 20:32:54.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:32:54.802
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:54.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:54.928
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:32:54.966
May  1 20:32:55.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28" in namespace "projected-3654" to be "Succeeded or Failed"
May  1 20:32:55.116: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 35.710171ms
May  1 20:32:57.137: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056583513s
May  1 20:32:59.155: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075130164s
May  1 20:33:01.134: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054166547s
May  1 20:33:03.151: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070983411s
STEP: Saw pod success 05/01/23 20:33:03.151
May  1 20:33:03.152: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28" satisfied condition "Succeeded or Failed"
May  1 20:33:03.183: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 container client-container: <nil>
STEP: delete the pod 05/01/23 20:33:03.22
May  1 20:33:03.284: INFO: Waiting for pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 to disappear
May  1 20:33:03.299: INFO: Pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 20:33:03.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3654" for this suite. 05/01/23 20:33:03.317
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":299,"skipped":5462,"failed":0}
------------------------------
• [SLOW TEST] [8.546 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:32:54.8
    May  1 20:32:54.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:32:54.802
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:32:54.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:32:54.928
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:32:54.966
    May  1 20:32:55.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28" in namespace "projected-3654" to be "Succeeded or Failed"
    May  1 20:32:55.116: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 35.710171ms
    May  1 20:32:57.137: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056583513s
    May  1 20:32:59.155: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075130164s
    May  1 20:33:01.134: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054166547s
    May  1 20:33:03.151: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070983411s
    STEP: Saw pod success 05/01/23 20:33:03.151
    May  1 20:33:03.152: INFO: Pod "downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28" satisfied condition "Succeeded or Failed"
    May  1 20:33:03.183: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 container client-container: <nil>
    STEP: delete the pod 05/01/23 20:33:03.22
    May  1 20:33:03.284: INFO: Waiting for pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 to disappear
    May  1 20:33:03.299: INFO: Pod downwardapi-volume-e17358ef-6137-4557-bdc2-d86c2ee39d28 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 20:33:03.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3654" for this suite. 05/01/23 20:33:03.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:03.364
May  1 20:33:03.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename ingress 05/01/23 20:33:03.367
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:03.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:03.46
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/01/23 20:33:03.476
STEP: getting /apis/networking.k8s.io 05/01/23 20:33:03.522
STEP: getting /apis/networking.k8s.iov1 05/01/23 20:33:03.538
STEP: creating 05/01/23 20:33:03.544
STEP: getting 05/01/23 20:33:03.621
STEP: listing 05/01/23 20:33:03.638
STEP: watching 05/01/23 20:33:03.662
May  1 20:33:03.662: INFO: starting watch
STEP: cluster-wide listing 05/01/23 20:33:03.667
STEP: cluster-wide watching 05/01/23 20:33:03.678
May  1 20:33:03.678: INFO: starting watch
STEP: patching 05/01/23 20:33:03.684
STEP: updating 05/01/23 20:33:03.703
May  1 20:33:03.730: INFO: waiting for watch events with expected annotations
May  1 20:33:03.730: INFO: saw patched and updated annotations
STEP: patching /status 05/01/23 20:33:03.73
STEP: updating /status 05/01/23 20:33:03.745
STEP: get /status 05/01/23 20:33:03.77
STEP: deleting 05/01/23 20:33:03.784
STEP: deleting a collection 05/01/23 20:33:03.828
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
May  1 20:33:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9512" for this suite. 05/01/23 20:33:03.901
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":300,"skipped":5497,"failed":0}
------------------------------
• [0.578 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:03.364
    May  1 20:33:03.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename ingress 05/01/23 20:33:03.367
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:03.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:03.46
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/01/23 20:33:03.476
    STEP: getting /apis/networking.k8s.io 05/01/23 20:33:03.522
    STEP: getting /apis/networking.k8s.iov1 05/01/23 20:33:03.538
    STEP: creating 05/01/23 20:33:03.544
    STEP: getting 05/01/23 20:33:03.621
    STEP: listing 05/01/23 20:33:03.638
    STEP: watching 05/01/23 20:33:03.662
    May  1 20:33:03.662: INFO: starting watch
    STEP: cluster-wide listing 05/01/23 20:33:03.667
    STEP: cluster-wide watching 05/01/23 20:33:03.678
    May  1 20:33:03.678: INFO: starting watch
    STEP: patching 05/01/23 20:33:03.684
    STEP: updating 05/01/23 20:33:03.703
    May  1 20:33:03.730: INFO: waiting for watch events with expected annotations
    May  1 20:33:03.730: INFO: saw patched and updated annotations
    STEP: patching /status 05/01/23 20:33:03.73
    STEP: updating /status 05/01/23 20:33:03.745
    STEP: get /status 05/01/23 20:33:03.77
    STEP: deleting 05/01/23 20:33:03.784
    STEP: deleting a collection 05/01/23 20:33:03.828
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    May  1 20:33:03.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-9512" for this suite. 05/01/23 20:33:03.901
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:03.943
May  1 20:33:03.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:33:03.946
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:04.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:04.08
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May  1 20:33:04.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:33:12.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2371" for this suite. 05/01/23 20:33:12.369
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":301,"skipped":5502,"failed":0}
------------------------------
• [SLOW TEST] [8.591 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:03.943
    May  1 20:33:03.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:33:03.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:04.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:04.08
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May  1 20:33:04.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:33:12.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2371" for this suite. 05/01/23 20:33:12.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:12.541
May  1 20:33:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename proxy 05/01/23 20:33:12.543
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:12.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:12.777
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May  1 20:33:12.801: INFO: Creating pod...
May  1 20:33:13.025: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6314" to be "running"
May  1 20:33:13.100: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 75.64837ms
May  1 20:33:15.134: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109335228s
May  1 20:33:17.112: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.087675996s
May  1 20:33:17.113: INFO: Pod "agnhost" satisfied condition "running"
May  1 20:33:17.113: INFO: Creating service...
May  1 20:33:17.166: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/DELETE
May  1 20:33:17.212: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  1 20:33:17.212: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/GET
May  1 20:33:17.263: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  1 20:33:17.263: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/HEAD
May  1 20:33:17.286: INFO: http.Client request:HEAD | StatusCode:200
May  1 20:33:17.286: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/OPTIONS
May  1 20:33:17.331: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  1 20:33:17.331: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/PATCH
May  1 20:33:17.362: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  1 20:33:17.362: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/POST
May  1 20:33:17.391: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  1 20:33:17.391: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/PUT
May  1 20:33:17.414: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May  1 20:33:17.414: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/DELETE
May  1 20:33:17.466: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May  1 20:33:17.466: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/GET
May  1 20:33:17.537: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May  1 20:33:17.537: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/HEAD
May  1 20:33:17.633: INFO: http.Client request:HEAD | StatusCode:200
May  1 20:33:17.633: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/OPTIONS
May  1 20:33:17.708: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May  1 20:33:17.708: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/PATCH
May  1 20:33:17.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May  1 20:33:17.735: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/POST
May  1 20:33:17.826: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May  1 20:33:17.826: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/PUT
May  1 20:33:17.862: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  1 20:33:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6314" for this suite. 05/01/23 20:33:17.895
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":302,"skipped":5567,"failed":0}
------------------------------
• [SLOW TEST] [5.421 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:12.541
    May  1 20:33:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename proxy 05/01/23 20:33:12.543
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:12.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:12.777
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May  1 20:33:12.801: INFO: Creating pod...
    May  1 20:33:13.025: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6314" to be "running"
    May  1 20:33:13.100: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 75.64837ms
    May  1 20:33:15.134: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109335228s
    May  1 20:33:17.112: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.087675996s
    May  1 20:33:17.113: INFO: Pod "agnhost" satisfied condition "running"
    May  1 20:33:17.113: INFO: Creating service...
    May  1 20:33:17.166: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/DELETE
    May  1 20:33:17.212: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  1 20:33:17.212: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/GET
    May  1 20:33:17.263: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  1 20:33:17.263: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/HEAD
    May  1 20:33:17.286: INFO: http.Client request:HEAD | StatusCode:200
    May  1 20:33:17.286: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/OPTIONS
    May  1 20:33:17.331: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  1 20:33:17.331: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/PATCH
    May  1 20:33:17.362: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  1 20:33:17.362: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/POST
    May  1 20:33:17.391: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  1 20:33:17.391: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/pods/agnhost/proxy/some/path/with/PUT
    May  1 20:33:17.414: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May  1 20:33:17.414: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/DELETE
    May  1 20:33:17.466: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May  1 20:33:17.466: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/GET
    May  1 20:33:17.537: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May  1 20:33:17.537: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/HEAD
    May  1 20:33:17.633: INFO: http.Client request:HEAD | StatusCode:200
    May  1 20:33:17.633: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/OPTIONS
    May  1 20:33:17.708: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May  1 20:33:17.708: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/PATCH
    May  1 20:33:17.735: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May  1 20:33:17.735: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/POST
    May  1 20:33:17.826: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May  1 20:33:17.826: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-6314/services/test-service/proxy/some/path/with/PUT
    May  1 20:33:17.862: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  1 20:33:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6314" for this suite. 05/01/23 20:33:17.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:17.971
May  1 20:33:17.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 20:33:17.974
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:18.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:18.115
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/01/23 20:33:18.259
STEP: Wait for the Deployment to create new ReplicaSet 05/01/23 20:33:18.287
STEP: delete the deployment 05/01/23 20:33:18.344
STEP: wait for all rs to be garbage collected 05/01/23 20:33:18.367
STEP: expected 0 rs, got 1 rs 05/01/23 20:33:18.427
STEP: Gathering metrics 05/01/23 20:33:19.046
W0501 20:33:19.099300      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 20:33:19.099: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 20:33:19.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2082" for this suite. 05/01/23 20:33:19.148
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":303,"skipped":5577,"failed":0}
------------------------------
• [1.253 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:17.971
    May  1 20:33:17.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 20:33:17.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:18.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:18.115
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/01/23 20:33:18.259
    STEP: Wait for the Deployment to create new ReplicaSet 05/01/23 20:33:18.287
    STEP: delete the deployment 05/01/23 20:33:18.344
    STEP: wait for all rs to be garbage collected 05/01/23 20:33:18.367
    STEP: expected 0 rs, got 1 rs 05/01/23 20:33:18.427
    STEP: Gathering metrics 05/01/23 20:33:19.046
    W0501 20:33:19.099300      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 20:33:19.099: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 20:33:19.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2082" for this suite. 05/01/23 20:33:19.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:19.232
May  1 20:33:19.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 20:33:19.236
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:19.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:19.419
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 05/01/23 20:33:19.848
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:33:19.875
May  1 20:33:19.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:33:19.929: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:33:20.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:33:20.998: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:33:21.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:33:21.990: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:33:22.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May  1 20:33:22.971: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:33:23.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 20:33:23.979: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 05/01/23 20:33:24.017
May  1 20:33:24.051: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/01/23 20:33:24.051
May  1 20:33:24.127: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/01/23 20:33:24.128
May  1 20:33:24.145: INFO: Observed &DaemonSet event: ADDED
May  1 20:33:24.149: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.150: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.151: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.152: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.152: INFO: Found daemon set daemon-set in namespace daemonsets-7096 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  1 20:33:24.152: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/01/23 20:33:24.152
STEP: watching for the daemon set status to be patched 05/01/23 20:33:24.196
May  1 20:33:24.213: INFO: Observed &DaemonSet event: ADDED
May  1 20:33:24.216: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.217: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.218: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.218: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.219: INFO: Observed daemon set daemon-set in namespace daemonsets-7096 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May  1 20:33:24.219: INFO: Observed &DaemonSet event: MODIFIED
May  1 20:33:24.219: INFO: Found daemon set daemon-set in namespace daemonsets-7096 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May  1 20:33:24.219: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:33:24.242
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7096, will wait for the garbage collector to delete the pods 05/01/23 20:33:24.243
May  1 20:33:24.355: INFO: Deleting DaemonSet.extensions daemon-set took: 39.215526ms
May  1 20:33:24.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.883002ms
May  1 20:33:27.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:33:27.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May  1 20:33:27.493: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"130271"},"items":null}

May  1 20:33:27.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"130273"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 20:33:27.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7096" for this suite. 05/01/23 20:33:27.62
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":304,"skipped":5582,"failed":0}
------------------------------
• [SLOW TEST] [8.425 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:19.232
    May  1 20:33:19.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 20:33:19.236
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:19.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:19.419
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 05/01/23 20:33:19.848
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:33:19.875
    May  1 20:33:19.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:33:19.929: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:33:20.998: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:33:20.998: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:33:21.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:33:21.990: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:33:22.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May  1 20:33:22.971: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:33:23.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 20:33:23.979: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 05/01/23 20:33:24.017
    May  1 20:33:24.051: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/01/23 20:33:24.051
    May  1 20:33:24.127: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/01/23 20:33:24.128
    May  1 20:33:24.145: INFO: Observed &DaemonSet event: ADDED
    May  1 20:33:24.149: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.150: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.151: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.152: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.152: INFO: Found daemon set daemon-set in namespace daemonsets-7096 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  1 20:33:24.152: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/01/23 20:33:24.152
    STEP: watching for the daemon set status to be patched 05/01/23 20:33:24.196
    May  1 20:33:24.213: INFO: Observed &DaemonSet event: ADDED
    May  1 20:33:24.216: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.217: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.218: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.218: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.219: INFO: Observed daemon set daemon-set in namespace daemonsets-7096 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May  1 20:33:24.219: INFO: Observed &DaemonSet event: MODIFIED
    May  1 20:33:24.219: INFO: Found daemon set daemon-set in namespace daemonsets-7096 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May  1 20:33:24.219: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 05/01/23 20:33:24.242
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7096, will wait for the garbage collector to delete the pods 05/01/23 20:33:24.243
    May  1 20:33:24.355: INFO: Deleting DaemonSet.extensions daemon-set took: 39.215526ms
    May  1 20:33:24.456: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.883002ms
    May  1 20:33:27.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:33:27.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May  1 20:33:27.493: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"130271"},"items":null}

    May  1 20:33:27.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"130273"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:33:27.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7096" for this suite. 05/01/23 20:33:27.62
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:27.66
May  1 20:33:27.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename runtimeclass 05/01/23 20:33:27.663
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:27.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:27.813
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/01/23 20:33:27.834
STEP: getting /apis/node.k8s.io 05/01/23 20:33:27.868
STEP: getting /apis/node.k8s.io/v1 05/01/23 20:33:27.876
STEP: creating 05/01/23 20:33:27.89
STEP: watching 05/01/23 20:33:28.01
May  1 20:33:28.011: INFO: starting watch
STEP: getting 05/01/23 20:33:28.065
STEP: listing 05/01/23 20:33:28.089
STEP: patching 05/01/23 20:33:28.116
STEP: updating 05/01/23 20:33:28.142
May  1 20:33:28.166: INFO: waiting for watch events with expected annotations
STEP: deleting 05/01/23 20:33:28.167
STEP: deleting a collection 05/01/23 20:33:28.25
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
May  1 20:33:28.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9954" for this suite. 05/01/23 20:33:28.374
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":305,"skipped":5584,"failed":0}
------------------------------
• [0.770 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:27.66
    May  1 20:33:27.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename runtimeclass 05/01/23 20:33:27.663
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:27.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:27.813
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/01/23 20:33:27.834
    STEP: getting /apis/node.k8s.io 05/01/23 20:33:27.868
    STEP: getting /apis/node.k8s.io/v1 05/01/23 20:33:27.876
    STEP: creating 05/01/23 20:33:27.89
    STEP: watching 05/01/23 20:33:28.01
    May  1 20:33:28.011: INFO: starting watch
    STEP: getting 05/01/23 20:33:28.065
    STEP: listing 05/01/23 20:33:28.089
    STEP: patching 05/01/23 20:33:28.116
    STEP: updating 05/01/23 20:33:28.142
    May  1 20:33:28.166: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/01/23 20:33:28.167
    STEP: deleting a collection 05/01/23 20:33:28.25
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    May  1 20:33:28.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9954" for this suite. 05/01/23 20:33:28.374
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:28.439
May  1 20:33:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context 05/01/23 20:33:28.441
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:28.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:28.593
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/01/23 20:33:28.612
May  1 20:33:28.843: INFO: Waiting up to 5m0s for pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde" in namespace "security-context-3855" to be "Succeeded or Failed"
May  1 20:33:28.859: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 16.558763ms
May  1 20:33:30.877: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034151288s
May  1 20:33:32.878: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035559734s
May  1 20:33:34.874: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031029086s
STEP: Saw pod success 05/01/23 20:33:34.874
May  1 20:33:34.874: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde" satisfied condition "Succeeded or Failed"
May  1 20:33:34.887: INFO: Trying to get logs from node 10.45.145.124 pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde container test-container: <nil>
STEP: delete the pod 05/01/23 20:33:34.94
May  1 20:33:34.980: INFO: Waiting for pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde to disappear
May  1 20:33:34.992: INFO: Pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 20:33:34.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3855" for this suite. 05/01/23 20:33:35.019
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":306,"skipped":5588,"failed":0}
------------------------------
• [SLOW TEST] [6.614 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:28.439
    May  1 20:33:28.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context 05/01/23 20:33:28.441
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:28.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:28.593
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/01/23 20:33:28.612
    May  1 20:33:28.843: INFO: Waiting up to 5m0s for pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde" in namespace "security-context-3855" to be "Succeeded or Failed"
    May  1 20:33:28.859: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 16.558763ms
    May  1 20:33:30.877: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034151288s
    May  1 20:33:32.878: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035559734s
    May  1 20:33:34.874: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031029086s
    STEP: Saw pod success 05/01/23 20:33:34.874
    May  1 20:33:34.874: INFO: Pod "security-context-ea89475a-0ec7-4f19-b368-16adfe873dde" satisfied condition "Succeeded or Failed"
    May  1 20:33:34.887: INFO: Trying to get logs from node 10.45.145.124 pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde container test-container: <nil>
    STEP: delete the pod 05/01/23 20:33:34.94
    May  1 20:33:34.980: INFO: Waiting for pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde to disappear
    May  1 20:33:34.992: INFO: Pod security-context-ea89475a-0ec7-4f19-b368-16adfe873dde no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 20:33:34.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-3855" for this suite. 05/01/23 20:33:35.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:35.054
May  1 20:33:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename watch 05/01/23 20:33:35.056
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:35.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:35.15
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/01/23 20:33:35.172
STEP: creating a watch on configmaps with label B 05/01/23 20:33:35.19
STEP: creating a watch on configmaps with label A or B 05/01/23 20:33:35.198
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.206
May  1 20:33:35.232: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130472 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:35.232: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130472 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.233
May  1 20:33:35.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130476 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:35.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130476 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/01/23 20:33:35.334
May  1 20:33:35.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130482 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:35.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130482 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.395
May  1 20:33:35.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130485 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:35.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130485 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/01/23 20:33:35.434
May  1 20:33:35.458: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130486 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:35.458: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130486 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/01/23 20:33:45.459
May  1 20:33:45.509: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130591 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:33:45.510: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130591 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  1 20:33:55.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8452" for this suite. 05/01/23 20:33:55.536
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":307,"skipped":5605,"failed":0}
------------------------------
• [SLOW TEST] [20.516 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:35.054
    May  1 20:33:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename watch 05/01/23 20:33:35.056
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:35.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:35.15
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/01/23 20:33:35.172
    STEP: creating a watch on configmaps with label B 05/01/23 20:33:35.19
    STEP: creating a watch on configmaps with label A or B 05/01/23 20:33:35.198
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.206
    May  1 20:33:35.232: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130472 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:35.232: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130472 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.233
    May  1 20:33:35.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130476 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:35.334: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130476 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/01/23 20:33:35.334
    May  1 20:33:35.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130482 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:35.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130482 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/01/23 20:33:35.395
    May  1 20:33:35.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130485 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:35.434: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8452  ff177f6b-ad04-4532-b556-2442d1c71dc8 130485 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/01/23 20:33:35.434
    May  1 20:33:35.458: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130486 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:35.458: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130486 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/01/23 20:33:45.459
    May  1 20:33:45.509: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130591 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:33:45.510: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8452  334645b0-de66-4654-9d8b-c8c0c6b976c7 130591 0 2023-05-01 20:33:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-01 20:33:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  1 20:33:55.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8452" for this suite. 05/01/23 20:33:55.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:33:55.571
May  1 20:33:55.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:33:55.614
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:55.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:55.707
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 05/01/23 20:33:55.731
May  1 20:33:55.856: INFO: Waiting up to 5m0s for pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece" in namespace "emptydir-5189" to be "Succeeded or Failed"
May  1 20:33:55.890: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 33.693224ms
May  1 20:33:57.909: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05290257s
May  1 20:33:59.907: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050345264s
May  1 20:34:01.911: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054885368s
STEP: Saw pod success 05/01/23 20:34:01.911
May  1 20:34:01.912: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece" satisfied condition "Succeeded or Failed"
May  1 20:34:01.950: INFO: Trying to get logs from node 10.45.145.124 pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece container test-container: <nil>
STEP: delete the pod 05/01/23 20:34:02.041
May  1 20:34:02.081: INFO: Waiting for pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece to disappear
May  1 20:34:02.100: INFO: Pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:34:02.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5189" for this suite. 05/01/23 20:34:02.15
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":308,"skipped":5627,"failed":0}
------------------------------
• [SLOW TEST] [6.631 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:33:55.571
    May  1 20:33:55.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:33:55.614
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:33:55.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:33:55.707
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/01/23 20:33:55.731
    May  1 20:33:55.856: INFO: Waiting up to 5m0s for pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece" in namespace "emptydir-5189" to be "Succeeded or Failed"
    May  1 20:33:55.890: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 33.693224ms
    May  1 20:33:57.909: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05290257s
    May  1 20:33:59.907: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050345264s
    May  1 20:34:01.911: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054885368s
    STEP: Saw pod success 05/01/23 20:34:01.911
    May  1 20:34:01.912: INFO: Pod "pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece" satisfied condition "Succeeded or Failed"
    May  1 20:34:01.950: INFO: Trying to get logs from node 10.45.145.124 pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece container test-container: <nil>
    STEP: delete the pod 05/01/23 20:34:02.041
    May  1 20:34:02.081: INFO: Waiting for pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece to disappear
    May  1 20:34:02.100: INFO: Pod pod-1f7c16b8-d8f9-4f5e-ad23-24ce23431ece no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:34:02.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5189" for this suite. 05/01/23 20:34:02.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:34:02.225
May  1 20:34:02.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:34:02.228
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:34:02.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:34:02.319
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-06ca6e1e-7178-47cd-b39e-1bdf1615e0fe 05/01/23 20:34:02.355
STEP: Creating a pod to test consume secrets 05/01/23 20:34:02.441
May  1 20:34:02.645: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a" in namespace "projected-8476" to be "Succeeded or Failed"
May  1 20:34:02.680: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Pending", Reason="", readiness=false. Elapsed: 33.697471ms
May  1 20:34:04.699: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Running", Reason="", readiness=true. Elapsed: 2.052491632s
May  1 20:34:06.695: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Running", Reason="", readiness=false. Elapsed: 4.048610533s
May  1 20:34:08.702: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055601728s
STEP: Saw pod success 05/01/23 20:34:08.702
May  1 20:34:08.703: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a" satisfied condition "Succeeded or Failed"
May  1 20:34:08.724: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:34:08.758
May  1 20:34:08.810: INFO: Waiting for pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a to disappear
May  1 20:34:08.822: INFO: Pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 20:34:08.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8476" for this suite. 05/01/23 20:34:08.844
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":309,"skipped":5664,"failed":0}
------------------------------
• [SLOW TEST] [6.696 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:34:02.225
    May  1 20:34:02.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:34:02.228
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:34:02.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:34:02.319
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-06ca6e1e-7178-47cd-b39e-1bdf1615e0fe 05/01/23 20:34:02.355
    STEP: Creating a pod to test consume secrets 05/01/23 20:34:02.441
    May  1 20:34:02.645: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a" in namespace "projected-8476" to be "Succeeded or Failed"
    May  1 20:34:02.680: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Pending", Reason="", readiness=false. Elapsed: 33.697471ms
    May  1 20:34:04.699: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Running", Reason="", readiness=true. Elapsed: 2.052491632s
    May  1 20:34:06.695: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Running", Reason="", readiness=false. Elapsed: 4.048610533s
    May  1 20:34:08.702: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055601728s
    STEP: Saw pod success 05/01/23 20:34:08.702
    May  1 20:34:08.703: INFO: Pod "pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a" satisfied condition "Succeeded or Failed"
    May  1 20:34:08.724: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:34:08.758
    May  1 20:34:08.810: INFO: Waiting for pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a to disappear
    May  1 20:34:08.822: INFO: Pod pod-projected-secrets-80f65681-ebe7-4998-a1ce-1e162c45732a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 20:34:08.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8476" for this suite. 05/01/23 20:34:08.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:34:08.923
May  1 20:34:08.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sched-pred 05/01/23 20:34:08.926
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:34:09.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:34:09.087
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
May  1 20:34:09.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  1 20:34:09.187: INFO: Waiting for terminating namespaces to be deleted...
May  1 20:34:09.231: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.124 before test
May  1 20:34:09.281: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:34:09.281: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container calico-typha ready: true, restart count 1
May  1 20:34:09.281: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:34:09.281: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:34:09.281: INFO: 	Container pause ready: true, restart count 0
May  1 20:34:09.281: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:34:09.281: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container tuned ready: true, restart count 0
May  1 20:34:09.281: INFO: dns-default-mmw2q from openshift-dns started at 2023-05-01 20:04:01 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container dns ready: true, restart count 0
May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.281: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:34:09.281: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:34:09.281: INFO: ingress-canary-t55qt from openshift-ingress-canary started at 2023-05-01 20:04:01 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:34:09.281: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.281: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.281: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:34:09.281: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:34:09.282: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:34:09.282: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.282: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:34:09.282: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:34:09.282: INFO: collect-profiles-28049535-cmpbk from openshift-operator-lifecycle-manager started at 2023-05-01 20:15:00 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 20:34:09.282: INFO: collect-profiles-28049550-vjnr9 from openshift-operator-lifecycle-manager started at 2023-05-01 20:30:00 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container collect-profiles ready: false, restart count 0
May  1 20:34:09.282: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  1 20:34:09.282: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.282: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:34:09.282: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:34:09.282: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.126 before test
May  1 20:34:09.382: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.382: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:34:09.382: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.382: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 20:34:09.382: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.382: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:34:09.382: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.382: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:34:09.382: INFO: 	Container pause ready: true, restart count 0
May  1 20:34:09.383: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:34:09.383: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container vpn ready: true, restart count 0
May  1 20:34:09.383: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container tuned ready: true, restart count 0
May  1 20:34:09.383: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container console ready: true, restart count 0
May  1 20:34:09.383: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container download-server ready: true, restart count 0
May  1 20:34:09.383: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container dns ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:34:09.383: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container registry ready: true, restart count 0
May  1 20:34:09.383: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:34:09.383: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container pvc-permissions ready: false, restart count 0
May  1 20:34:09.383: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:34:09.383: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container router ready: true, restart count 0
May  1 20:34:09.383: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container alertmanager ready: true, restart count 1
May  1 20:34:09.383: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-state-metrics ready: true, restart count 0
May  1 20:34:09.383: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:34:09.383: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May  1 20:34:09.383: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 20:34:09.383: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container prometheus ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 20:34:09.383: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.383: INFO: 	Container prometheus-operator ready: true, restart count 0
May  1 20:34:09.383: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.383: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 20:34:09.384: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container reload ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container telemeter-client ready: true, restart count 0
May  1 20:34:09.384: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container thanos-query ready: true, restart count 0
May  1 20:34:09.384: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:34:09.384: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:34:09.384: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 20:34:09.384: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:34:09.384: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:34:09.384: INFO: packageserver-596c56d895-5gw4x from openshift-operator-lifecycle-manager started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container packageserver ready: true, restart count 0
May  1 20:34:09.384: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container e2e ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:34:09.384: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:34:09.384: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:34:09.384: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.384: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  1 20:34:09.384: INFO: 
Logging pods the apiserver thinks is on node 10.45.145.71 before test
May  1 20:34:09.450: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  1 20:34:09.450: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container calico-node ready: true, restart count 0
May  1 20:34:09.450: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container calico-typha ready: true, restart count 0
May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
May  1 20:34:09.450: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-f9zmq from ibm-system started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.450: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
May  1 20:34:09.450: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.455: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  1 20:34:09.455: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.455: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  1 20:34:09.455: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.455: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
May  1 20:34:09.455: INFO: 	Container pause ready: true, restart count 0
May  1 20:34:09.456: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.456: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
May  1 20:34:09.456: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
May  1 20:34:09.456: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.456: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  1 20:34:09.456: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.456: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
May  1 20:34:09.456: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.456: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
May  1 20:34:09.456: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.456: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
May  1 20:34:09.456: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container tuned ready: true, restart count 0
May  1 20:34:09.457: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container cluster-samples-operator ready: true, restart count 0
May  1 20:34:09.457: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
May  1 20:34:09.457: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container cluster-storage-operator ready: true, restart count 1
May  1 20:34:09.457: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 20:34:09.457: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container snapshot-controller ready: true, restart count 0
May  1 20:34:09.457: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.457: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
May  1 20:34:09.458: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.458: INFO: 	Container webhook ready: true, restart count 0
May  1 20:34:09.458: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.458: INFO: 	Container webhook ready: true, restart count 0
May  1 20:34:09.458: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.458: INFO: 	Container console-operator ready: true, restart count 1
May  1 20:34:09.458: INFO: 	Container conversion-webhook-server ready: true, restart count 2
May  1 20:34:09.458: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.458: INFO: 	Container console ready: true, restart count 0
May  1 20:34:09.459: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.459: INFO: 	Container download-server ready: true, restart count 0
May  1 20:34:09.459: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.459: INFO: 	Container dns-operator ready: true, restart count 0
May  1 20:34:09.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.459: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.459: INFO: 	Container dns ready: true, restart count 0
May  1 20:34:09.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.459: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.459: INFO: 	Container dns-node-resolver ready: true, restart count 0
May  1 20:34:09.459: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.459: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
May  1 20:34:09.460: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.460: INFO: 	Container node-ca ready: true, restart count 0
May  1 20:34:09.460: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.460: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May  1 20:34:09.460: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.460: INFO: 	Container ingress-operator ready: true, restart count 0
May  1 20:34:09.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.460: INFO: router-default-dc48bc679-rtqn9 from openshift-ingress started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.460: INFO: 	Container router ready: true, restart count 0
May  1 20:34:09.460: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.460: INFO: 	Container insights-operator ready: true, restart count 1
May  1 20:34:09.461: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.461: INFO: 	Container kube-proxy ready: true, restart count 0
May  1 20:34:09.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.461: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.461: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
May  1 20:34:09.461: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.461: INFO: 	Container migrator ready: true, restart count 0
May  1 20:34:09.461: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.461: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:34:09.461: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:34:09.462: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.462: INFO: 	Container marketplace-operator ready: true, restart count 0
May  1 20:34:09.462: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:34:09.462: INFO: redhat-operators-zb9j9 from openshift-marketplace started at 2023-05-01 19:34:11 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
May  1 20:34:09.462: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 20:03:39 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.462: INFO: 	Container alertmanager ready: true, restart count 1
May  1 20:34:09.462: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May  1 20:34:09.462: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May  1 20:34:09.463: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:34:09.463: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.463: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.463: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.463: INFO: 	Container node-exporter ready: true, restart count 0
May  1 20:34:09.463: INFO: prometheus-adapter-d8df9dbf9-wc62l from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.463: INFO: 	Container prometheus-adapter ready: true, restart count 0
May  1 20:34:09.463: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 20:03:38 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.464: INFO: 	Container config-reloader ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container prometheus ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container prometheus-proxy ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container thanos-sidecar ready: true, restart count 0
May  1 20:34:09.464: INFO: prometheus-operator-admission-webhook-98cbdbf8f-xhqsl from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.464: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May  1 20:34:09.464: INFO: thanos-querier-6dcb8b776-mh4qn from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (6 container statuses recorded)
May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May  1 20:34:09.465: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May  1 20:34:09.465: INFO: 	Container oauth-proxy ready: true, restart count 0
May  1 20:34:09.465: INFO: 	Container prom-label-proxy ready: true, restart count 0
May  1 20:34:09.465: INFO: 	Container thanos-query ready: true, restart count 0
May  1 20:34:09.465: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.465: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May  1 20:34:09.465: INFO: multus-admission-controller-6b76fd464f-8hb9z from openshift-multus started at 2023-05-01 20:03:34 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.465: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.465: INFO: 	Container multus-admission-controller ready: true, restart count 0
May  1 20:34:09.465: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.465: INFO: 	Container kube-multus ready: true, restart count 0
May  1 20:34:09.466: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.466: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May  1 20:34:09.466: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May  1 20:34:09.466: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.466: INFO: 	Container check-endpoints ready: true, restart count 0
May  1 20:34:09.466: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.466: INFO: 	Container network-check-target-container ready: true, restart count 0
May  1 20:34:09.466: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.466: INFO: 	Container network-operator ready: true, restart count 1
May  1 20:34:09.466: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.466: INFO: 	Container catalog-operator ready: true, restart count 0
May  1 20:34:09.466: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container olm-operator ready: true, restart count 0
May  1 20:34:09.467: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container package-server-manager ready: true, restart count 0
May  1 20:34:09.467: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container packageserver ready: true, restart count 0
May  1 20:34:09.467: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container metrics ready: true, restart count 2
May  1 20:34:09.467: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container push-gateway ready: true, restart count 0
May  1 20:34:09.467: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.467: INFO: 	Container service-ca-operator ready: true, restart count 1
May  1 20:34:09.468: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.468: INFO: 	Container service-ca-controller ready: true, restart count 0
May  1 20:34:09.468: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
May  1 20:34:09.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  1 20:34:09.468: INFO: 	Container systemd-logs ready: true, restart count 0
May  1 20:34:09.468: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
May  1 20:34:09.468: INFO: 	Container tigera-operator ready: true, restart count 4
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 20:34:09.468
May  1 20:34:09.558: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4026" to be "running"
May  1 20:34:09.569: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.289285ms
May  1 20:34:11.598: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040241707s
May  1 20:34:13.589: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.030845286s
May  1 20:34:13.589: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 20:34:13.601
STEP: Trying to apply a random label on the found node. 05/01/23 20:34:13.742
STEP: verifying the node has the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 95 05/01/23 20:34:13.78
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/01/23 20:34:13.795
May  1 20:34:13.872: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4026" to be "not pending"
May  1 20:34:13.887: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.660473ms
May  1 20:34:15.931: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059044028s
May  1 20:34:17.898: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.025929926s
May  1 20:34:17.898: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.45.145.124 on the node which pod4 resides and expect not scheduled 05/01/23 20:34:17.898
May  1 20:34:17.971: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4026" to be "not pending"
May  1 20:34:17.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.849315ms
May  1 20:34:20.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036033526s
May  1 20:34:21.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027692549s
May  1 20:34:24.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02836064s
May  1 20:34:26.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039874036s
May  1 20:34:28.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030208249s
May  1 20:34:29.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025276486s
May  1 20:34:31.994: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023306129s
May  1 20:34:34.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032541914s
May  1 20:34:35.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02730035s
May  1 20:34:37.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025419018s
May  1 20:34:40.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029036591s
May  1 20:34:41.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026423379s
May  1 20:34:44.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.028814442s
May  1 20:34:45.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02731005s
May  1 20:34:47.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02646208s
May  1 20:34:49.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027114658s
May  1 20:34:52.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029642189s
May  1 20:34:54.017: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.046280418s
May  1 20:34:56.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02996438s
May  1 20:34:58.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.047753515s
May  1 20:35:00.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.03742652s
May  1 20:35:02.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033799457s
May  1 20:35:04.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042767219s
May  1 20:35:06.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.056624662s
May  1 20:35:07.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.027627654s
May  1 20:35:09.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.025957096s
May  1 20:35:12.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03066949s
May  1 20:35:14.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.035249398s
May  1 20:35:16.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.029320013s
May  1 20:35:18.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.044786659s
May  1 20:35:20.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030002967s
May  1 20:35:22.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056999866s
May  1 20:35:24.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.033137543s
May  1 20:35:25.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024980202s
May  1 20:35:27.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027173287s
May  1 20:35:30.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029947405s
May  1 20:35:32.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030111343s
May  1 20:35:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025959074s
May  1 20:35:36.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.032435329s
May  1 20:35:38.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.031890998s
May  1 20:35:40.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029199548s
May  1 20:35:41.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026563825s
May  1 20:35:43.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026805654s
May  1 20:35:45.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024694811s
May  1 20:35:47.995: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023919425s
May  1 20:35:49.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027825701s
May  1 20:35:52.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.029867595s
May  1 20:35:54.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031248799s
May  1 20:35:56.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.040125859s
May  1 20:35:58.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.035026452s
May  1 20:36:00.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.028900477s
May  1 20:36:02.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.053266964s
May  1 20:36:04.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02934905s
May  1 20:36:05.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.027114419s
May  1 20:36:08.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029646334s
May  1 20:36:10.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.035685639s
May  1 20:36:11.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025473997s
May  1 20:36:14.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.034686404s
May  1 20:36:16.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.032486289s
May  1 20:36:18.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.031113793s
May  1 20:36:19.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.028216561s
May  1 20:36:21.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.027277423s
May  1 20:36:23.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.026871511s
May  1 20:36:26.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.030617725s
May  1 20:36:28.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.031549094s
May  1 20:36:30.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.037102325s
May  1 20:36:32.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.049316392s
May  1 20:36:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.02535708s
May  1 20:36:36.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.033045333s
May  1 20:36:37.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.025727038s
May  1 20:36:40.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.032393652s
May  1 20:36:41.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.027360584s
May  1 20:36:44.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.030365126s
May  1 20:36:46.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.03545358s
May  1 20:36:47.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025231345s
May  1 20:36:50.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.040861391s
May  1 20:36:52.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.034274723s
May  1 20:36:54.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.03432066s
May  1 20:36:56.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.032675402s
May  1 20:36:58.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.087155568s
May  1 20:37:00.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.032009513s
May  1 20:37:02.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.033762022s
May  1 20:37:04.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.051839273s
May  1 20:37:06.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.031471076s
May  1 20:37:07.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025784731s
May  1 20:37:10.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.031072358s
May  1 20:37:12.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.030276075s
May  1 20:37:14.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.038560824s
May  1 20:37:16.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.03910757s
May  1 20:37:18.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.029027653s
May  1 20:37:19.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.02690645s
May  1 20:37:22.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.034386556s
May  1 20:37:23.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025823671s
May  1 20:37:25.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.027922378s
May  1 20:37:28.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.036888606s
May  1 20:37:30.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.035111441s
May  1 20:37:31.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.027347418s
May  1 20:37:34.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.034086373s
May  1 20:37:36.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.030133685s
May  1 20:37:37.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.0280548s
May  1 20:37:39.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.026792421s
May  1 20:37:42.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.031755785s
May  1 20:37:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.059565328s
May  1 20:37:45.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.025698644s
May  1 20:37:48.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.029873187s
May  1 20:37:49.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.02668748s
May  1 20:37:51.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.026305245s
May  1 20:37:53.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.027081395s
May  1 20:37:55.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.026810783s
May  1 20:37:57.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.025139368s
May  1 20:38:00.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.03344842s
May  1 20:38:02.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.036732499s
May  1 20:38:03.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.02792655s
May  1 20:38:06.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.030420874s
May  1 20:38:08.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.032484093s
May  1 20:38:10.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.03994842s
May  1 20:38:12.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031791911s
May  1 20:38:14.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.05227988s
May  1 20:38:16.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.029815641s
May  1 20:38:17.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.026873408s
May  1 20:38:19.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.027400141s
May  1 20:38:21.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.027974497s
May  1 20:38:23.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.027917019s
May  1 20:38:26.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.028954233s
May  1 20:38:27.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.025276802s
May  1 20:38:30.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.033701633s
May  1 20:38:31.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.026009134s
May  1 20:38:34.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.030780352s
May  1 20:38:36.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.02898209s
May  1 20:38:38.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.035288601s
May  1 20:38:39.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024777306s
May  1 20:38:42.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.029907411s
May  1 20:38:43.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.026069073s
May  1 20:38:46.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.028702557s
May  1 20:38:48.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.030927254s
May  1 20:38:50.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029199694s
May  1 20:38:51.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02749161s
May  1 20:38:54.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.029387139s
May  1 20:38:55.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.028061987s
May  1 20:38:58.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.043544421s
May  1 20:39:00.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.032736138s
May  1 20:39:02.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.028640872s
May  1 20:39:04.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.031870475s
May  1 20:39:06.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.034261499s
May  1 20:39:08.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.03022639s
May  1 20:39:10.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.034307401s
May  1 20:39:11.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.027235586s
May  1 20:39:14.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.031415328s
May  1 20:39:15.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.027299223s
May  1 20:39:18.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.031287621s
May  1 20:39:18.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063620528s
STEP: removing the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 off the node 10.45.145.124 05/01/23 20:39:18.035
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 05/01/23 20:39:18.092
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
May  1 20:39:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4026" for this suite. 05/01/23 20:39:18.15
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":310,"skipped":5670,"failed":0}
------------------------------
• [SLOW TEST] [309.290 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:34:08.923
    May  1 20:34:08.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sched-pred 05/01/23 20:34:08.926
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:34:09.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:34:09.087
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    May  1 20:34:09.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May  1 20:34:09.187: INFO: Waiting for terminating namespaces to be deleted...
    May  1 20:34:09.231: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.124 before test
    May  1 20:34:09.281: INFO: calico-node-slcg9 from calico-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:34:09.281: INFO: calico-typha-75ff8c8c66-6z5c6 from calico-system started at 2023-05-01 16:54:24 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container calico-typha ready: true, restart count 1
    May  1 20:34:09.281: INFO: ibm-keepalived-watcher-ncf96 from kube-system started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:34:09.281: INFO: ibm-master-proxy-static-10.45.145.124 from kube-system started at 2023-05-01 16:54:15 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:34:09.281: INFO: 	Container pause ready: true, restart count 0
    May  1 20:34:09.281: INFO: ibmcloud-block-storage-driver-cmlmj from kube-system started at 2023-05-01 16:54:27 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:34:09.281: INFO: tuned-lpz6l from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:34:09.281: INFO: dns-default-mmw2q from openshift-dns started at 2023-05-01 20:04:01 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container dns ready: true, restart count 0
    May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.281: INFO: node-resolver-krvqc from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:34:09.281: INFO: node-ca-xjld6 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:34:09.281: INFO: ingress-canary-t55qt from openshift-ingress-canary started at 2023-05-01 20:04:01 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:34:09.281: INFO: openshift-kube-proxy-xbgzr from openshift-kube-proxy started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.281: INFO: node-exporter-xpp24 from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.281: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.281: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:34:09.281: INFO: multus-7qs8w from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:34:09.282: INFO: multus-additional-cni-plugins-v48tc from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:34:09.282: INFO: network-metrics-daemon-4w594 from openshift-multus started at 2023-05-01 16:54:18 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.282: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:34:09.282: INFO: network-check-target-z87pw from openshift-network-diagnostics started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:34:09.282: INFO: collect-profiles-28049535-cmpbk from openshift-operator-lifecycle-manager started at 2023-05-01 20:15:00 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 20:34:09.282: INFO: collect-profiles-28049550-vjnr9 from openshift-operator-lifecycle-manager started at 2023-05-01 20:30:00 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container collect-profiles ready: false, restart count 0
    May  1 20:34:09.282: INFO: sonobuoy from sonobuoy started at 2023-05-01 18:48:05 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May  1 20:34:09.282: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.282: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:34:09.282: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:34:09.282: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.126 before test
    May  1 20:34:09.382: INFO: calico-node-4s5vb from calico-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.382: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:34:09.382: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-8nbfm from ibm-system started at 2023-05-01 17:00:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.382: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 20:34:09.382: INFO: ibm-keepalived-watcher-qbrsx from kube-system started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.382: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:34:09.382: INFO: ibm-master-proxy-static-10.45.145.126 from kube-system started at 2023-05-01 16:54:19 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.382: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:34:09.382: INFO: 	Container pause ready: true, restart count 0
    May  1 20:34:09.383: INFO: ibmcloud-block-storage-driver-ffmmx from kube-system started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:34:09.383: INFO: vpn-597d5865cf-9xpdf from kube-system started at 2023-05-01 17:01:30 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container vpn ready: true, restart count 0
    May  1 20:34:09.383: INFO: tuned-q8c2h from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:34:09.383: INFO: console-568f9d5f69-6h2kn from openshift-console started at 2023-05-01 17:02:01 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container console ready: true, restart count 0
    May  1 20:34:09.383: INFO: downloads-85fbdb68d8-tzwqx from openshift-console started at 2023-05-01 16:58:46 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container download-server ready: true, restart count 0
    May  1 20:34:09.383: INFO: dns-default-28psr from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container dns ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: node-resolver-b2nzd from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:34:09.383: INFO: image-registry-848dd668cf-w4pll from openshift-image-registry started at 2023-05-01 16:59:08 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container registry ready: true, restart count 0
    May  1 20:34:09.383: INFO: node-ca-4pw98 from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:34:09.383: INFO: registry-pvc-permissions-wr9jc from openshift-image-registry started at 2023-05-01 16:59:16 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container pvc-permissions ready: false, restart count 0
    May  1 20:34:09.383: INFO: ingress-canary-k9br4 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:34:09.383: INFO: router-default-dc48bc679-bqjs8 from openshift-ingress started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container router ready: true, restart count 0
    May  1 20:34:09.383: INFO: openshift-kube-proxy-77zfm from openshift-kube-proxy started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-01 17:01:09 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 20:34:09.383: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: kube-state-metrics-7fddd57764-fp7dw from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May  1 20:34:09.383: INFO: node-exporter-c7g5p from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:34:09.383: INFO: openshift-state-metrics-75944bd6bd-qpcd4 from openshift-monitoring started at 2023-05-01 16:58:40 +0000 UTC (3 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May  1 20:34:09.383: INFO: prometheus-adapter-d8df9dbf9-6l9wt from openshift-monitoring started at 2023-05-01 17:00:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 20:34:09.383: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-01 17:00:50 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container prometheus ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 20:34:09.383: INFO: prometheus-operator-6c947f76fc-lzm9g from openshift-monitoring started at 2023-05-01 16:58:11 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.383: INFO: 	Container prometheus-operator ready: true, restart count 0
    May  1 20:34:09.383: INFO: prometheus-operator-admission-webhook-98cbdbf8f-nnznl from openshift-monitoring started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.383: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 20:34:09.384: INFO: telemeter-client-5c79955f45-4vgg9 from openshift-monitoring started at 2023-05-01 17:00:23 +0000 UTC (3 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container reload ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container telemeter-client ready: true, restart count 0
    May  1 20:34:09.384: INFO: thanos-querier-6dcb8b776-sl7w4 from openshift-monitoring started at 2023-05-01 16:58:50 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 20:34:09.384: INFO: multus-7gn4h from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:34:09.384: INFO: multus-additional-cni-plugins-7wpc7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:34:09.384: INFO: multus-admission-controller-6b76fd464f-pxt8v from openshift-multus started at 2023-05-01 16:57:22 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 20:34:09.384: INFO: network-metrics-daemon-mxrp7 from openshift-multus started at 2023-05-01 16:54:22 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:34:09.384: INFO: network-check-target-7gwwh from openshift-network-diagnostics started at 2023-05-01 16:54:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:34:09.384: INFO: packageserver-596c56d895-5gw4x from openshift-operator-lifecycle-manager started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container packageserver ready: true, restart count 0
    May  1 20:34:09.384: INFO: sonobuoy-e2e-job-9a58787263364bb6 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container e2e ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:34:09.384: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-mq98k from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:34:09.384: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:34:09.384: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-05-01 16:57:56 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.384: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    May  1 20:34:09.384: INFO: 
    Logging pods the apiserver thinks is on node 10.45.145.71 before test
    May  1 20:34:09.450: INFO: calico-kube-controllers-79f474fb8-jn672 from calico-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    May  1 20:34:09.450: INFO: calico-node-m6plq from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container calico-node ready: true, restart count 0
    May  1 20:34:09.450: INFO: calico-typha-75ff8c8c66-4zgxv from calico-system started at 2023-05-01 16:52:26 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container calico-typha ready: true, restart count 0
    May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-72pdm from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-x9sv5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:34:09.450: INFO: managed-storage-validation-webhooks-77db8bf5b-zbpz5 from ibm-odf-validation-webhook started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    May  1 20:34:09.450: INFO: ibm-cloud-provider-ip-158-175-159-10-57f5c9d45f-f9zmq from ibm-system started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.450: INFO: 	Container ibm-cloud-provider-ip-158-175-159-10 ready: true, restart count 0
    May  1 20:34:09.450: INFO: ibm-file-plugin-5fdffc884f-4bkrn from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.455: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    May  1 20:34:09.455: INFO: ibm-keepalived-watcher-cpgdp from kube-system started at 2023-05-01 16:51:17 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.455: INFO: 	Container keepalived-watcher ready: true, restart count 0
    May  1 20:34:09.455: INFO: ibm-master-proxy-static-10.45.145.71 from kube-system started at 2023-05-01 16:51:15 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.455: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    May  1 20:34:09.455: INFO: 	Container pause ready: true, restart count 0
    May  1 20:34:09.456: INFO: ibm-storage-metrics-agent-7f994d5df5-6j4zc from kube-system started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.456: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    May  1 20:34:09.456: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    May  1 20:34:09.456: INFO: ibm-storage-watcher-545798cc6f-lrgms from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.456: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    May  1 20:34:09.456: INFO: ibmcloud-block-storage-driver-f82m7 from kube-system started at 2023-05-01 16:51:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.456: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    May  1 20:34:09.456: INFO: ibmcloud-block-storage-plugin-5f677b8577-9mpkl from kube-system started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.456: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    May  1 20:34:09.456: INFO: cluster-node-tuning-operator-6567656d4-hddjg from openshift-cluster-node-tuning-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.456: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    May  1 20:34:09.456: INFO: tuned-hfhlr from openshift-cluster-node-tuning-operator started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container tuned ready: true, restart count 0
    May  1 20:34:09.457: INFO: cluster-samples-operator-7d69df847f-nlkl4 from openshift-cluster-samples-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    May  1 20:34:09.457: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    May  1 20:34:09.457: INFO: cluster-storage-operator-6b6bc9bd94-7mshg from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    May  1 20:34:09.457: INFO: csi-snapshot-controller-567b6b4d78-4z8tw from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 20:34:09.457: INFO: csi-snapshot-controller-567b6b4d78-86bnm from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container snapshot-controller ready: true, restart count 0
    May  1 20:34:09.457: INFO: csi-snapshot-controller-operator-7985c7d9c-sr9rq from openshift-cluster-storage-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.457: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    May  1 20:34:09.458: INFO: csi-snapshot-webhook-7f5c5bc774-cnqv7 from openshift-cluster-storage-operator started at 2023-05-01 16:54:18 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.458: INFO: 	Container webhook ready: true, restart count 0
    May  1 20:34:09.458: INFO: csi-snapshot-webhook-7f5c5bc774-csznr from openshift-cluster-storage-operator started at 2023-05-01 16:53:33 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.458: INFO: 	Container webhook ready: true, restart count 0
    May  1 20:34:09.458: INFO: console-operator-5bcc5564c6-g78ch from openshift-console-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.458: INFO: 	Container console-operator ready: true, restart count 1
    May  1 20:34:09.458: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    May  1 20:34:09.458: INFO: console-568f9d5f69-v7kx2 from openshift-console started at 2023-05-01 17:02:28 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.458: INFO: 	Container console ready: true, restart count 0
    May  1 20:34:09.459: INFO: downloads-85fbdb68d8-v9dbs from openshift-console started at 2023-05-01 16:53:45 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.459: INFO: 	Container download-server ready: true, restart count 0
    May  1 20:34:09.459: INFO: dns-operator-5496b6bfdb-8qxdd from openshift-dns-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.459: INFO: 	Container dns-operator ready: true, restart count 0
    May  1 20:34:09.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.459: INFO: dns-default-h4p5f from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.459: INFO: 	Container dns ready: true, restart count 0
    May  1 20:34:09.459: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.459: INFO: node-resolver-k54ph from openshift-dns started at 2023-05-01 16:57:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.459: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May  1 20:34:09.459: INFO: cluster-image-registry-operator-6dc45cdc69-wxffb from openshift-image-registry started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.459: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    May  1 20:34:09.460: INFO: node-ca-7wbwj from openshift-image-registry started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.460: INFO: 	Container node-ca ready: true, restart count 0
    May  1 20:34:09.460: INFO: ingress-canary-wxrz9 from openshift-ingress-canary started at 2023-05-01 16:57:22 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.460: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May  1 20:34:09.460: INFO: ingress-operator-5bdd9bbb4d-rg9xh from openshift-ingress-operator started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.460: INFO: 	Container ingress-operator ready: true, restart count 0
    May  1 20:34:09.460: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.460: INFO: router-default-dc48bc679-rtqn9 from openshift-ingress started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.460: INFO: 	Container router ready: true, restart count 0
    May  1 20:34:09.460: INFO: insights-operator-5dd7768ccd-kgnwj from openshift-insights started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.460: INFO: 	Container insights-operator ready: true, restart count 1
    May  1 20:34:09.461: INFO: openshift-kube-proxy-xcd7n from openshift-kube-proxy started at 2023-05-01 16:51:56 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.461: INFO: 	Container kube-proxy ready: true, restart count 0
    May  1 20:34:09.461: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.461: INFO: kube-storage-version-migrator-operator-8485684586-ln9xn from openshift-kube-storage-version-migrator-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.461: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    May  1 20:34:09.461: INFO: migrator-59fb996c9c-rgzx9 from openshift-kube-storage-version-migrator started at 2023-05-01 16:53:52 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.461: INFO: 	Container migrator ready: true, restart count 0
    May  1 20:34:09.461: INFO: certified-operators-kw674 from openshift-marketplace started at 2023-05-01 18:22:04 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.461: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:34:09.461: INFO: community-operators-qz7gr from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:34:09.462: INFO: marketplace-operator-65dd67f5bd-7k26x from openshift-marketplace started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.462: INFO: 	Container marketplace-operator ready: true, restart count 0
    May  1 20:34:09.462: INFO: redhat-marketplace-gsdhv from openshift-marketplace started at 2023-05-01 16:55:02 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:34:09.462: INFO: redhat-operators-zb9j9 from openshift-marketplace started at 2023-05-01 19:34:11 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.462: INFO: 	Container registry-server ready: true, restart count 0
    May  1 20:34:09.462: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-01 20:03:39 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.462: INFO: 	Container alertmanager ready: true, restart count 1
    May  1 20:34:09.462: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May  1 20:34:09.462: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May  1 20:34:09.463: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:34:09.463: INFO: cluster-monitoring-operator-68fccd9857-q6vhq from openshift-monitoring started at 2023-05-01 16:53:09 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.463: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.463: INFO: node-exporter-rv48f from openshift-monitoring started at 2023-05-01 16:58:41 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.463: INFO: 	Container node-exporter ready: true, restart count 0
    May  1 20:34:09.463: INFO: prometheus-adapter-d8df9dbf9-wc62l from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.463: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May  1 20:34:09.463: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-01 20:03:38 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.464: INFO: 	Container config-reloader ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container prometheus ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May  1 20:34:09.464: INFO: prometheus-operator-admission-webhook-98cbdbf8f-xhqsl from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.464: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May  1 20:34:09.464: INFO: thanos-querier-6dcb8b776-mh4qn from openshift-monitoring started at 2023-05-01 19:08:12 +0000 UTC (6 container statuses recorded)
    May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.464: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May  1 20:34:09.465: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May  1 20:34:09.465: INFO: 	Container oauth-proxy ready: true, restart count 0
    May  1 20:34:09.465: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May  1 20:34:09.465: INFO: 	Container thanos-query ready: true, restart count 0
    May  1 20:34:09.465: INFO: multus-additional-cni-plugins-5rf4h from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.465: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May  1 20:34:09.465: INFO: multus-admission-controller-6b76fd464f-8hb9z from openshift-multus started at 2023-05-01 20:03:34 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.465: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.465: INFO: 	Container multus-admission-controller ready: true, restart count 0
    May  1 20:34:09.465: INFO: multus-hrxxk from openshift-multus started at 2023-05-01 16:51:51 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.465: INFO: 	Container kube-multus ready: true, restart count 0
    May  1 20:34:09.466: INFO: network-metrics-daemon-7dbgd from openshift-multus started at 2023-05-01 16:51:52 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.466: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May  1 20:34:09.466: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May  1 20:34:09.466: INFO: network-check-source-5f544d4b86-mwpjv from openshift-network-diagnostics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.466: INFO: 	Container check-endpoints ready: true, restart count 0
    May  1 20:34:09.466: INFO: network-check-target-8nwlk from openshift-network-diagnostics started at 2023-05-01 16:51:59 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.466: INFO: 	Container network-check-target-container ready: true, restart count 0
    May  1 20:34:09.466: INFO: network-operator-7ccbcfd5c6-pl4m5 from openshift-network-operator started at 2023-05-01 16:51:27 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.466: INFO: 	Container network-operator ready: true, restart count 1
    May  1 20:34:09.466: INFO: catalog-operator-779888458b-hmq8s from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.466: INFO: 	Container catalog-operator ready: true, restart count 0
    May  1 20:34:09.466: INFO: olm-operator-6bd5767fdb-2wvmg from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container olm-operator ready: true, restart count 0
    May  1 20:34:09.467: INFO: package-server-manager-65bf985c9c-x4lwk from openshift-operator-lifecycle-manager started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container package-server-manager ready: true, restart count 0
    May  1 20:34:09.467: INFO: packageserver-596c56d895-jj7sp from openshift-operator-lifecycle-manager started at 2023-05-01 16:54:23 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container packageserver ready: true, restart count 0
    May  1 20:34:09.467: INFO: metrics-6d46d44d8f-kjrzq from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container metrics ready: true, restart count 2
    May  1 20:34:09.467: INFO: push-gateway-5465b544cc-wcgd9 from openshift-roks-metrics started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container push-gateway ready: true, restart count 0
    May  1 20:34:09.467: INFO: service-ca-operator-658f9bdfcb-csr6d from openshift-service-ca-operator started at 2023-05-01 16:53:09 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.467: INFO: 	Container service-ca-operator ready: true, restart count 1
    May  1 20:34:09.468: INFO: service-ca-77bcb8d48b-rcv9l from openshift-service-ca started at 2023-05-01 16:53:55 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.468: INFO: 	Container service-ca-controller ready: true, restart count 0
    May  1 20:34:09.468: INFO: sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-tjk65 from sonobuoy started at 2023-05-01 18:48:13 +0000 UTC (2 container statuses recorded)
    May  1 20:34:09.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May  1 20:34:09.468: INFO: 	Container systemd-logs ready: true, restart count 0
    May  1 20:34:09.468: INFO: tigera-operator-765c48479c-g55zf from tigera-operator started at 2023-05-01 16:51:28 +0000 UTC (1 container statuses recorded)
    May  1 20:34:09.468: INFO: 	Container tigera-operator ready: true, restart count 4
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/01/23 20:34:09.468
    May  1 20:34:09.558: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4026" to be "running"
    May  1 20:34:09.569: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.289285ms
    May  1 20:34:11.598: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040241707s
    May  1 20:34:13.589: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.030845286s
    May  1 20:34:13.589: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/01/23 20:34:13.601
    STEP: Trying to apply a random label on the found node. 05/01/23 20:34:13.742
    STEP: verifying the node has the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 95 05/01/23 20:34:13.78
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/01/23 20:34:13.795
    May  1 20:34:13.872: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4026" to be "not pending"
    May  1 20:34:13.887: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.660473ms
    May  1 20:34:15.931: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059044028s
    May  1 20:34:17.898: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.025929926s
    May  1 20:34:17.898: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.45.145.124 on the node which pod4 resides and expect not scheduled 05/01/23 20:34:17.898
    May  1 20:34:17.971: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4026" to be "not pending"
    May  1 20:34:17.983: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.849315ms
    May  1 20:34:20.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036033526s
    May  1 20:34:21.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027692549s
    May  1 20:34:24.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02836064s
    May  1 20:34:26.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039874036s
    May  1 20:34:28.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030208249s
    May  1 20:34:29.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025276486s
    May  1 20:34:31.994: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023306129s
    May  1 20:34:34.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032541914s
    May  1 20:34:35.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02730035s
    May  1 20:34:37.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.025419018s
    May  1 20:34:40.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.029036591s
    May  1 20:34:41.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026423379s
    May  1 20:34:44.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.028814442s
    May  1 20:34:45.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02731005s
    May  1 20:34:47.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02646208s
    May  1 20:34:49.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027114658s
    May  1 20:34:52.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.029642189s
    May  1 20:34:54.017: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.046280418s
    May  1 20:34:56.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02996438s
    May  1 20:34:58.019: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.047753515s
    May  1 20:35:00.009: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.03742652s
    May  1 20:35:02.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033799457s
    May  1 20:35:04.014: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.042767219s
    May  1 20:35:06.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.056624662s
    May  1 20:35:07.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.027627654s
    May  1 20:35:09.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.025957096s
    May  1 20:35:12.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.03066949s
    May  1 20:35:14.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.035249398s
    May  1 20:35:16.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.029320013s
    May  1 20:35:18.016: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.044786659s
    May  1 20:35:20.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.030002967s
    May  1 20:35:22.028: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.056999866s
    May  1 20:35:24.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.033137543s
    May  1 20:35:25.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024980202s
    May  1 20:35:27.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027173287s
    May  1 20:35:30.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.029947405s
    May  1 20:35:32.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.030111343s
    May  1 20:35:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025959074s
    May  1 20:35:36.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.032435329s
    May  1 20:35:38.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.031890998s
    May  1 20:35:40.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.029199548s
    May  1 20:35:41.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026563825s
    May  1 20:35:43.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.026805654s
    May  1 20:35:45.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.024694811s
    May  1 20:35:47.995: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.023919425s
    May  1 20:35:49.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027825701s
    May  1 20:35:52.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.029867595s
    May  1 20:35:54.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031248799s
    May  1 20:35:56.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.040125859s
    May  1 20:35:58.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.035026452s
    May  1 20:36:00.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.028900477s
    May  1 20:36:02.024: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.053266964s
    May  1 20:36:04.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02934905s
    May  1 20:36:05.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.027114419s
    May  1 20:36:08.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029646334s
    May  1 20:36:10.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.035685639s
    May  1 20:36:11.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.025473997s
    May  1 20:36:14.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.034686404s
    May  1 20:36:16.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.032486289s
    May  1 20:36:18.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.031113793s
    May  1 20:36:19.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.028216561s
    May  1 20:36:21.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.027277423s
    May  1 20:36:23.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.026871511s
    May  1 20:36:26.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.030617725s
    May  1 20:36:28.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.031549094s
    May  1 20:36:30.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.037102325s
    May  1 20:36:32.020: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.049316392s
    May  1 20:36:33.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.02535708s
    May  1 20:36:36.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.033045333s
    May  1 20:36:37.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.025727038s
    May  1 20:36:40.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.032393652s
    May  1 20:36:41.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.027360584s
    May  1 20:36:44.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.030365126s
    May  1 20:36:46.007: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.03545358s
    May  1 20:36:47.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.025231345s
    May  1 20:36:50.012: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.040861391s
    May  1 20:36:52.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.034274723s
    May  1 20:36:54.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.03432066s
    May  1 20:36:56.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.032675402s
    May  1 20:36:58.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.087155568s
    May  1 20:37:00.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.032009513s
    May  1 20:37:02.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.033762022s
    May  1 20:37:04.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.051839273s
    May  1 20:37:06.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.031471076s
    May  1 20:37:07.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.025784731s
    May  1 20:37:10.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.031072358s
    May  1 20:37:12.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.030276075s
    May  1 20:37:14.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.038560824s
    May  1 20:37:16.010: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.03910757s
    May  1 20:37:18.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.029027653s
    May  1 20:37:19.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.02690645s
    May  1 20:37:22.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.034386556s
    May  1 20:37:23.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.025823671s
    May  1 20:37:25.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.027922378s
    May  1 20:37:28.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.036888606s
    May  1 20:37:30.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.035111441s
    May  1 20:37:31.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.027347418s
    May  1 20:37:34.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.034086373s
    May  1 20:37:36.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.030133685s
    May  1 20:37:37.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.0280548s
    May  1 20:37:39.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.026792421s
    May  1 20:37:42.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.031755785s
    May  1 20:37:44.031: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.059565328s
    May  1 20:37:45.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.025698644s
    May  1 20:37:48.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.029873187s
    May  1 20:37:49.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.02668748s
    May  1 20:37:51.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.026305245s
    May  1 20:37:53.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.027081395s
    May  1 20:37:55.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.026810783s
    May  1 20:37:57.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.025139368s
    May  1 20:38:00.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.03344842s
    May  1 20:38:02.008: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.036732499s
    May  1 20:38:03.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.02792655s
    May  1 20:38:06.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.030420874s
    May  1 20:38:08.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.032484093s
    May  1 20:38:10.011: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.03994842s
    May  1 20:38:12.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.031791911s
    May  1 20:38:14.023: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.05227988s
    May  1 20:38:16.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.029815641s
    May  1 20:38:17.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.026873408s
    May  1 20:38:19.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.027400141s
    May  1 20:38:21.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.027974497s
    May  1 20:38:23.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.027917019s
    May  1 20:38:26.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.028954233s
    May  1 20:38:27.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.025276802s
    May  1 20:38:30.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.033701633s
    May  1 20:38:31.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.026009134s
    May  1 20:38:34.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.030780352s
    May  1 20:38:36.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.02898209s
    May  1 20:38:38.006: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.035288601s
    May  1 20:38:39.996: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.024777306s
    May  1 20:38:42.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.029907411s
    May  1 20:38:43.997: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.026069073s
    May  1 20:38:46.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.028702557s
    May  1 20:38:48.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.030927254s
    May  1 20:38:50.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.029199694s
    May  1 20:38:51.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02749161s
    May  1 20:38:54.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.029387139s
    May  1 20:38:55.999: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.028061987s
    May  1 20:38:58.015: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.043544421s
    May  1 20:39:00.004: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.032736138s
    May  1 20:39:02.000: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.028640872s
    May  1 20:39:04.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.031870475s
    May  1 20:39:06.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.034261499s
    May  1 20:39:08.001: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.03022639s
    May  1 20:39:10.005: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.034307401s
    May  1 20:39:11.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.027235586s
    May  1 20:39:14.003: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.031415328s
    May  1 20:39:15.998: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.027299223s
    May  1 20:39:18.002: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.031287621s
    May  1 20:39:18.035: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.063620528s
    STEP: removing the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 off the node 10.45.145.124 05/01/23 20:39:18.035
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-3b86850d-f7c3-45b5-8b3d-9d84da1f3d66 05/01/23 20:39:18.092
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:39:18.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4026" for this suite. 05/01/23 20:39:18.15
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:39:18.222
May  1 20:39:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename configmap 05/01/23 20:39:18.225
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:18.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:18.362
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-8fcb4796-1a02-4a21-a754-254f270eb515 05/01/23 20:39:18.38
STEP: Creating a pod to test consume configMaps 05/01/23 20:39:18.431
May  1 20:39:18.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0" in namespace "configmap-6661" to be "Succeeded or Failed"
May  1 20:39:18.593: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.063116ms
May  1 20:39:20.609: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046573309s
May  1 20:39:22.618: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055061311s
May  1 20:39:24.611: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048126272s
STEP: Saw pod success 05/01/23 20:39:24.611
May  1 20:39:24.611: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0" satisfied condition "Succeeded or Failed"
May  1 20:39:24.623: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:39:24.728
May  1 20:39:24.774: INFO: Waiting for pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 to disappear
May  1 20:39:24.789: INFO: Pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
May  1 20:39:24.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6661" for this suite. 05/01/23 20:39:24.813
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5677,"failed":0}
------------------------------
• [SLOW TEST] [6.626 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:39:18.222
    May  1 20:39:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename configmap 05/01/23 20:39:18.225
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:18.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:18.362
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-8fcb4796-1a02-4a21-a754-254f270eb515 05/01/23 20:39:18.38
    STEP: Creating a pod to test consume configMaps 05/01/23 20:39:18.431
    May  1 20:39:18.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0" in namespace "configmap-6661" to be "Succeeded or Failed"
    May  1 20:39:18.593: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.063116ms
    May  1 20:39:20.609: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046573309s
    May  1 20:39:22.618: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055061311s
    May  1 20:39:24.611: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048126272s
    STEP: Saw pod success 05/01/23 20:39:24.611
    May  1 20:39:24.611: INFO: Pod "pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0" satisfied condition "Succeeded or Failed"
    May  1 20:39:24.623: INFO: Trying to get logs from node 10.45.145.124 pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:39:24.728
    May  1 20:39:24.774: INFO: Waiting for pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 to disappear
    May  1 20:39:24.789: INFO: Pod pod-configmaps-95bc5544-c0c3-4c2d-b107-114684f09db0 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    May  1 20:39:24.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6661" for this suite. 05/01/23 20:39:24.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:39:24.853
May  1 20:39:24.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:39:24.855
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:24.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:25.019
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 05/01/23 20:39:25.038
May  1 20:39:25.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 create -f -'
May  1 20:39:26.426: INFO: stderr: ""
May  1 20:39:26.426: INFO: stdout: "pod/pause created\n"
May  1 20:39:26.426: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  1 20:39:26.426: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-538" to be "running and ready"
May  1 20:39:26.458: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 31.454519ms
May  1 20:39:26.458: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.45.145.124' to be 'Running' but was 'Pending'
May  1 20:39:28.471: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044882363s
May  1 20:39:28.471: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.45.145.124' to be 'Running' but was 'Pending'
May  1 20:39:30.471: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.044603774s
May  1 20:39:30.471: INFO: Pod "pause" satisfied condition "running and ready"
May  1 20:39:30.471: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 05/01/23 20:39:30.471
May  1 20:39:30.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 label pods pause testing-label=testing-label-value'
May  1 20:39:30.869: INFO: stderr: ""
May  1 20:39:30.869: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/01/23 20:39:30.869
May  1 20:39:30.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pod pause -L testing-label'
May  1 20:39:31.063: INFO: stderr: ""
May  1 20:39:31.063: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/01/23 20:39:31.063
May  1 20:39:31.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 label pods pause testing-label-'
May  1 20:39:31.340: INFO: stderr: ""
May  1 20:39:31.341: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/01/23 20:39:31.341
May  1 20:39:31.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pod pause -L testing-label'
May  1 20:39:31.544: INFO: stderr: ""
May  1 20:39:31.544: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 05/01/23 20:39:31.544
May  1 20:39:31.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 delete --grace-period=0 --force -f -'
May  1 20:39:31.800: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  1 20:39:31.800: INFO: stdout: "pod \"pause\" force deleted\n"
May  1 20:39:31.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get rc,svc -l name=pause --no-headers'
May  1 20:39:32.201: INFO: stderr: "No resources found in kubectl-538 namespace.\n"
May  1 20:39:32.202: INFO: stdout: ""
May  1 20:39:32.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  1 20:39:32.394: INFO: stderr: ""
May  1 20:39:32.394: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:39:32.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-538" for this suite. 05/01/23 20:39:32.42
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":312,"skipped":5746,"failed":0}
------------------------------
• [SLOW TEST] [7.789 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:39:24.853
    May  1 20:39:24.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:39:24.855
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:24.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:25.019
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 05/01/23 20:39:25.038
    May  1 20:39:25.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 create -f -'
    May  1 20:39:26.426: INFO: stderr: ""
    May  1 20:39:26.426: INFO: stdout: "pod/pause created\n"
    May  1 20:39:26.426: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May  1 20:39:26.426: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-538" to be "running and ready"
    May  1 20:39:26.458: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 31.454519ms
    May  1 20:39:26.458: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.45.145.124' to be 'Running' but was 'Pending'
    May  1 20:39:28.471: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044882363s
    May  1 20:39:28.471: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.45.145.124' to be 'Running' but was 'Pending'
    May  1 20:39:30.471: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.044603774s
    May  1 20:39:30.471: INFO: Pod "pause" satisfied condition "running and ready"
    May  1 20:39:30.471: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 05/01/23 20:39:30.471
    May  1 20:39:30.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 label pods pause testing-label=testing-label-value'
    May  1 20:39:30.869: INFO: stderr: ""
    May  1 20:39:30.869: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/01/23 20:39:30.869
    May  1 20:39:30.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pod pause -L testing-label'
    May  1 20:39:31.063: INFO: stderr: ""
    May  1 20:39:31.063: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/01/23 20:39:31.063
    May  1 20:39:31.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 label pods pause testing-label-'
    May  1 20:39:31.340: INFO: stderr: ""
    May  1 20:39:31.341: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/01/23 20:39:31.341
    May  1 20:39:31.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pod pause -L testing-label'
    May  1 20:39:31.544: INFO: stderr: ""
    May  1 20:39:31.544: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 05/01/23 20:39:31.544
    May  1 20:39:31.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 delete --grace-period=0 --force -f -'
    May  1 20:39:31.800: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May  1 20:39:31.800: INFO: stdout: "pod \"pause\" force deleted\n"
    May  1 20:39:31.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get rc,svc -l name=pause --no-headers'
    May  1 20:39:32.201: INFO: stderr: "No resources found in kubectl-538 namespace.\n"
    May  1 20:39:32.202: INFO: stdout: ""
    May  1 20:39:32.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-538 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May  1 20:39:32.394: INFO: stderr: ""
    May  1 20:39:32.394: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:39:32.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-538" for this suite. 05/01/23 20:39:32.42
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:39:32.643
May  1 20:39:32.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pod-network-test 05/01/23 20:39:32.645
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:32.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:32.833
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-733 05/01/23 20:39:32.861
STEP: creating a selector 05/01/23 20:39:32.861
STEP: Creating the service pods in kubernetes 05/01/23 20:39:32.862
May  1 20:39:32.862: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  1 20:39:33.445: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-733" to be "running and ready"
May  1 20:39:33.477: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.648784ms
May  1 20:39:33.477: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:39:35.504: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058071354s
May  1 20:39:35.504: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:39:37.502: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.056858157s
May  1 20:39:37.502: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:39.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046683149s
May  1 20:39:39.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:41.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.051803323s
May  1 20:39:41.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:43.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.050230713s
May  1 20:39:43.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:45.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.053242748s
May  1 20:39:45.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:47.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.04555809s
May  1 20:39:47.491: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:49.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049644966s
May  1 20:39:49.495: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:51.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.04673353s
May  1 20:39:51.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:53.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.06666383s
May  1 20:39:53.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:39:55.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.049621197s
May  1 20:39:55.495: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  1 20:39:55.495: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  1 20:39:55.508: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-733" to be "running and ready"
May  1 20:39:55.521: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.735836ms
May  1 20:39:55.521: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  1 20:39:55.521: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  1 20:39:55.536: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-733" to be "running and ready"
May  1 20:39:55.547: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.550301ms
May  1 20:39:55.547: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  1 20:39:55.547: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/01/23 20:39:55.567
May  1 20:39:55.691: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-733" to be "running"
May  1 20:39:55.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354198ms
May  1 20:39:57.718: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027269656s
May  1 20:39:59.723: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032880837s
May  1 20:39:59.724: INFO: Pod "test-container-pod" satisfied condition "running"
May  1 20:39:59.753: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-733" to be "running"
May  1 20:39:59.767: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.130346ms
May  1 20:39:59.767: INFO: Pod "host-test-container-pod" satisfied condition "running"
May  1 20:39:59.780: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  1 20:39:59.780: INFO: Going to poll 172.30.42.100 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  1 20:39:59.801: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.42.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:39:59.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:39:59.801: INFO: ExecWithOptions: Clientset creation
May  1 20:39:59.801: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.42.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 20:40:01.229: INFO: Found all 1 expected endpoints: [netserver-0]
May  1 20:40:01.229: INFO: Going to poll 172.30.38.219 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  1 20:40:01.249: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.38.219 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:40:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:40:01.250: INFO: ExecWithOptions: Clientset creation
May  1 20:40:01.251: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.38.219+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 20:40:02.773: INFO: Found all 1 expected endpoints: [netserver-1]
May  1 20:40:02.773: INFO: Going to poll 172.30.244.125 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May  1 20:40:02.795: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.244.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:40:02.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:40:02.797: INFO: ExecWithOptions: Clientset creation
May  1 20:40:02.797: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.244.125+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May  1 20:40:04.157: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  1 20:40:04.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-733" for this suite. 05/01/23 20:40:04.181
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":313,"skipped":5750,"failed":0}
------------------------------
• [SLOW TEST] [31.580 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:39:32.643
    May  1 20:39:32.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pod-network-test 05/01/23 20:39:32.645
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:39:32.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:39:32.833
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-733 05/01/23 20:39:32.861
    STEP: creating a selector 05/01/23 20:39:32.861
    STEP: Creating the service pods in kubernetes 05/01/23 20:39:32.862
    May  1 20:39:32.862: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  1 20:39:33.445: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-733" to be "running and ready"
    May  1 20:39:33.477: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.648784ms
    May  1 20:39:33.477: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:39:35.504: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058071354s
    May  1 20:39:35.504: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:39:37.502: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.056858157s
    May  1 20:39:37.502: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:39.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.046683149s
    May  1 20:39:39.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:41.497: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.051803323s
    May  1 20:39:41.497: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:43.496: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.050230713s
    May  1 20:39:43.496: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:45.499: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.053242748s
    May  1 20:39:45.499: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:47.491: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.04555809s
    May  1 20:39:47.491: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:49.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.049644966s
    May  1 20:39:49.495: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:51.492: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.04673353s
    May  1 20:39:51.492: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:53.512: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.06666383s
    May  1 20:39:53.512: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:39:55.495: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.049621197s
    May  1 20:39:55.495: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  1 20:39:55.495: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  1 20:39:55.508: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-733" to be "running and ready"
    May  1 20:39:55.521: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 12.735836ms
    May  1 20:39:55.521: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  1 20:39:55.521: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  1 20:39:55.536: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-733" to be "running and ready"
    May  1 20:39:55.547: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 11.550301ms
    May  1 20:39:55.547: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  1 20:39:55.547: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/01/23 20:39:55.567
    May  1 20:39:55.691: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-733" to be "running"
    May  1 20:39:55.704: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.354198ms
    May  1 20:39:57.718: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027269656s
    May  1 20:39:59.723: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032880837s
    May  1 20:39:59.724: INFO: Pod "test-container-pod" satisfied condition "running"
    May  1 20:39:59.753: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-733" to be "running"
    May  1 20:39:59.767: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 13.130346ms
    May  1 20:39:59.767: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May  1 20:39:59.780: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  1 20:39:59.780: INFO: Going to poll 172.30.42.100 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  1 20:39:59.801: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.42.100 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:39:59.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:39:59.801: INFO: ExecWithOptions: Clientset creation
    May  1 20:39:59.801: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.42.100+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 20:40:01.229: INFO: Found all 1 expected endpoints: [netserver-0]
    May  1 20:40:01.229: INFO: Going to poll 172.30.38.219 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  1 20:40:01.249: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.38.219 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:40:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:40:01.250: INFO: ExecWithOptions: Clientset creation
    May  1 20:40:01.251: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.38.219+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 20:40:02.773: INFO: Found all 1 expected endpoints: [netserver-1]
    May  1 20:40:02.773: INFO: Going to poll 172.30.244.125 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May  1 20:40:02.795: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.244.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-733 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:40:02.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:40:02.797: INFO: ExecWithOptions: Clientset creation
    May  1 20:40:02.797: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-733/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.244.125+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May  1 20:40:04.157: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  1 20:40:04.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-733" for this suite. 05/01/23 20:40:04.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:40:04.226
May  1 20:40:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename events 05/01/23 20:40:04.228
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:40:04.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:40:04.297
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/01/23 20:40:04.313
May  1 20:40:04.331: INFO: created test-event-1
May  1 20:40:04.360: INFO: created test-event-2
May  1 20:40:04.409: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/01/23 20:40:04.409
STEP: delete collection of events 05/01/23 20:40:04.448
May  1 20:40:04.448: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/01/23 20:40:04.56
May  1 20:40:04.560: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  1 20:40:04.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8392" for this suite. 05/01/23 20:40:04.589
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":314,"skipped":5768,"failed":0}
------------------------------
• [0.402 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:40:04.226
    May  1 20:40:04.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename events 05/01/23 20:40:04.228
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:40:04.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:40:04.297
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/01/23 20:40:04.313
    May  1 20:40:04.331: INFO: created test-event-1
    May  1 20:40:04.360: INFO: created test-event-2
    May  1 20:40:04.409: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/01/23 20:40:04.409
    STEP: delete collection of events 05/01/23 20:40:04.448
    May  1 20:40:04.448: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/01/23 20:40:04.56
    May  1 20:40:04.560: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  1 20:40:04.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8392" for this suite. 05/01/23 20:40:04.589
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:40:04.636
May  1 20:40:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 20:40:04.639
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:40:04.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:40:04.747
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b in namespace container-probe-6911 05/01/23 20:40:04.774
May  1 20:40:04.881: INFO: Waiting up to 5m0s for pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b" in namespace "container-probe-6911" to be "not pending"
May  1 20:40:04.892: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.361929ms
May  1 20:40:06.907: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025193617s
May  1 20:40:08.908: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Running", Reason="", readiness=true. Elapsed: 4.026139562s
May  1 20:40:08.908: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b" satisfied condition "not pending"
May  1 20:40:08.908: INFO: Started pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b in namespace container-probe-6911
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 20:40:08.908
May  1 20:40:08.927: INFO: Initial restart count of pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is 0
May  1 20:40:27.185: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 1 (18.257127582s elapsed)
May  1 20:40:47.373: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 2 (38.444987174s elapsed)
May  1 20:41:07.613: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 3 (58.685903677s elapsed)
May  1 20:41:27.845: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 4 (1m18.917504408s elapsed)
May  1 20:42:38.705: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 5 (2m29.777017574s elapsed)
STEP: deleting the pod 05/01/23 20:42:38.705
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 20:42:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6911" for this suite. 05/01/23 20:42:38.863
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":315,"skipped":5771,"failed":0}
------------------------------
• [SLOW TEST] [154.308 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:40:04.636
    May  1 20:40:04.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 20:40:04.639
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:40:04.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:40:04.747
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b in namespace container-probe-6911 05/01/23 20:40:04.774
    May  1 20:40:04.881: INFO: Waiting up to 5m0s for pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b" in namespace "container-probe-6911" to be "not pending"
    May  1 20:40:04.892: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.361929ms
    May  1 20:40:06.907: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025193617s
    May  1 20:40:08.908: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b": Phase="Running", Reason="", readiness=true. Elapsed: 4.026139562s
    May  1 20:40:08.908: INFO: Pod "liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b" satisfied condition "not pending"
    May  1 20:40:08.908: INFO: Started pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b in namespace container-probe-6911
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 20:40:08.908
    May  1 20:40:08.927: INFO: Initial restart count of pod liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is 0
    May  1 20:40:27.185: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 1 (18.257127582s elapsed)
    May  1 20:40:47.373: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 2 (38.444987174s elapsed)
    May  1 20:41:07.613: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 3 (58.685903677s elapsed)
    May  1 20:41:27.845: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 4 (1m18.917504408s elapsed)
    May  1 20:42:38.705: INFO: Restart count of pod container-probe-6911/liveness-39a8e5ff-c34c-4761-a3aa-7112a62b5a8b is now 5 (2m29.777017574s elapsed)
    STEP: deleting the pod 05/01/23 20:42:38.705
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 20:42:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6911" for this suite. 05/01/23 20:42:38.863
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:42:38.945
May  1 20:42:38.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 20:42:38.947
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:39.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:39.085
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:42:39.109
May  1 20:42:39.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4" in namespace "downward-api-7596" to be "Succeeded or Failed"
May  1 20:42:39.223: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.25072ms
May  1 20:42:41.260: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052578622s
May  1 20:42:43.241: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032848713s
May  1 20:42:45.237: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029272247s
STEP: Saw pod success 05/01/23 20:42:45.237
May  1 20:42:45.237: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4" satisfied condition "Succeeded or Failed"
May  1 20:42:45.250: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 container client-container: <nil>
STEP: delete the pod 05/01/23 20:42:45.328
May  1 20:42:45.367: INFO: Waiting for pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 to disappear
May  1 20:42:45.381: INFO: Pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 20:42:45.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7596" for this suite. 05/01/23 20:42:45.42
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":316,"skipped":5774,"failed":0}
------------------------------
• [SLOW TEST] [6.507 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:42:38.945
    May  1 20:42:38.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 20:42:38.947
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:39.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:39.085
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:42:39.109
    May  1 20:42:39.208: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4" in namespace "downward-api-7596" to be "Succeeded or Failed"
    May  1 20:42:39.223: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 15.25072ms
    May  1 20:42:41.260: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052578622s
    May  1 20:42:43.241: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032848713s
    May  1 20:42:45.237: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029272247s
    STEP: Saw pod success 05/01/23 20:42:45.237
    May  1 20:42:45.237: INFO: Pod "downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4" satisfied condition "Succeeded or Failed"
    May  1 20:42:45.250: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 container client-container: <nil>
    STEP: delete the pod 05/01/23 20:42:45.328
    May  1 20:42:45.367: INFO: Waiting for pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 to disappear
    May  1 20:42:45.381: INFO: Pod downwardapi-volume-94a55fe7-93f2-42da-8b18-43b8813adae4 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 20:42:45.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7596" for this suite. 05/01/23 20:42:45.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:42:45.454
May  1 20:42:45.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:42:45.458
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:45.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:45.603
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 05/01/23 20:42:45.639
May  1 20:42:45.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-7927 cluster-info'
May  1 20:42:45.877: INFO: stderr: ""
May  1 20:42:45.877: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:42:45.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7927" for this suite. 05/01/23 20:42:45.928
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":317,"skipped":5788,"failed":0}
------------------------------
• [0.549 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:42:45.454
    May  1 20:42:45.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:42:45.458
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:45.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:45.603
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 05/01/23 20:42:45.639
    May  1 20:42:45.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-7927 cluster-info'
    May  1 20:42:45.877: INFO: stderr: ""
    May  1 20:42:45.877: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:42:45.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7927" for this suite. 05/01/23 20:42:45.928
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:42:46.004
May  1 20:42:46.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:42:46.005
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:46.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:46.16
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/01/23 20:42:46.181
May  1 20:42:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:43:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:44:33.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-194" for this suite. 05/01/23 20:44:33.148
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":318,"skipped":5790,"failed":0}
------------------------------
• [SLOW TEST] [107.179 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:42:46.004
    May  1 20:42:46.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:42:46.005
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:42:46.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:42:46.16
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/01/23 20:42:46.181
    May  1 20:42:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:43:09.033: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:44:33.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-194" for this suite. 05/01/23 20:44:33.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:44:33.188
May  1 20:44:33.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:44:33.19
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:44:33.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:44:33.293
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/01/23 20:44:33.327
May  1 20:44:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/01/23 20:45:48.794
May  1 20:45:48.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:46:09.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:47:33.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7811" for this suite. 05/01/23 20:47:33.494
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":319,"skipped":5798,"failed":0}
------------------------------
• [SLOW TEST] [180.369 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:44:33.188
    May  1 20:44:33.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename crd-publish-openapi 05/01/23 20:44:33.19
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:44:33.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:44:33.293
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/01/23 20:44:33.327
    May  1 20:44:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/01/23 20:45:48.794
    May  1 20:45:48.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:46:09.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:47:33.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7811" for this suite. 05/01/23 20:47:33.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:47:33.562
May  1 20:47:33.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:47:33.564
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:33.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:33.668
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-7ef63720-e1a1-4983-8be8-486e3d29cf87 05/01/23 20:47:33.727
STEP: Creating a pod to test consume configMaps 05/01/23 20:47:33.758
May  1 20:47:33.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4" in namespace "projected-7493" to be "Succeeded or Failed"
May  1 20:47:33.873: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.141509ms
May  1 20:47:35.946: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098135007s
May  1 20:47:37.894: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046789697s
May  1 20:47:39.913: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065649108s
STEP: Saw pod success 05/01/23 20:47:39.913
May  1 20:47:39.913: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4" satisfied condition "Succeeded or Failed"
May  1 20:47:39.943: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/01/23 20:47:40.143
May  1 20:47:40.211: INFO: Waiting for pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 to disappear
May  1 20:47:40.237: INFO: Pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:47:40.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7493" for this suite. 05/01/23 20:47:40.264
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":320,"skipped":5825,"failed":0}
------------------------------
• [SLOW TEST] [6.735 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:47:33.562
    May  1 20:47:33.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:47:33.564
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:33.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:33.668
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-7ef63720-e1a1-4983-8be8-486e3d29cf87 05/01/23 20:47:33.727
    STEP: Creating a pod to test consume configMaps 05/01/23 20:47:33.758
    May  1 20:47:33.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4" in namespace "projected-7493" to be "Succeeded or Failed"
    May  1 20:47:33.873: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 25.141509ms
    May  1 20:47:35.946: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098135007s
    May  1 20:47:37.894: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046789697s
    May  1 20:47:39.913: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065649108s
    STEP: Saw pod success 05/01/23 20:47:39.913
    May  1 20:47:39.913: INFO: Pod "pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4" satisfied condition "Succeeded or Failed"
    May  1 20:47:39.943: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:47:40.143
    May  1 20:47:40.211: INFO: Waiting for pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 to disappear
    May  1 20:47:40.237: INFO: Pod pod-projected-configmaps-2baf185c-6b2f-429b-a1c3-2fb01457d8b4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:47:40.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7493" for this suite. 05/01/23 20:47:40.264
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:47:40.303
May  1 20:47:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:47:40.305
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:40.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:40.429
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:47:40.546
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:47:41.615
STEP: Deploying the webhook pod 05/01/23 20:47:41.706
STEP: Wait for the deployment to be ready 05/01/23 20:47:41.802
May  1 20:47:41.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:47:43.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:47:45.937
STEP: Verifying the service has paired with the endpoint 05/01/23 20:47:45.985
May  1 20:47:46.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 05/01/23 20:47:47.008
STEP: create a pod that should be denied by the webhook 05/01/23 20:47:47.1
STEP: create a pod that causes the webhook to hang 05/01/23 20:47:47.205
STEP: create a configmap that should be denied by the webhook 05/01/23 20:47:57.267
STEP: create a configmap that should be admitted by the webhook 05/01/23 20:47:57.337
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/01/23 20:47:57.392
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/01/23 20:47:57.443
STEP: create a namespace that bypass the webhook 05/01/23 20:47:57.472
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/01/23 20:47:57.507
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:47:57.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9309" for this suite. 05/01/23 20:47:57.716
STEP: Destroying namespace "webhook-9309-markers" for this suite. 05/01/23 20:47:57.747
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":321,"skipped":5827,"failed":0}
------------------------------
• [SLOW TEST] [17.713 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:47:40.303
    May  1 20:47:40.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:47:40.305
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:40.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:40.429
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:47:40.546
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:47:41.615
    STEP: Deploying the webhook pod 05/01/23 20:47:41.706
    STEP: Wait for the deployment to be ready 05/01/23 20:47:41.802
    May  1 20:47:41.865: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:47:43.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 47, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 47, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:47:45.937
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:47:45.985
    May  1 20:47:46.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 05/01/23 20:47:47.008
    STEP: create a pod that should be denied by the webhook 05/01/23 20:47:47.1
    STEP: create a pod that causes the webhook to hang 05/01/23 20:47:47.205
    STEP: create a configmap that should be denied by the webhook 05/01/23 20:47:57.267
    STEP: create a configmap that should be admitted by the webhook 05/01/23 20:47:57.337
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/01/23 20:47:57.392
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/01/23 20:47:57.443
    STEP: create a namespace that bypass the webhook 05/01/23 20:47:57.472
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/01/23 20:47:57.507
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:47:57.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9309" for this suite. 05/01/23 20:47:57.716
    STEP: Destroying namespace "webhook-9309-markers" for this suite. 05/01/23 20:47:57.747
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:47:58.048
May  1 20:47:58.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename events 05/01/23 20:47:58.053
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:58.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:58.21
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/01/23 20:47:58.23
STEP: listing all events in all namespaces 05/01/23 20:47:58.282
STEP: patching the test event 05/01/23 20:47:58.393
STEP: fetching the test event 05/01/23 20:47:58.436
STEP: updating the test event 05/01/23 20:47:58.478
STEP: getting the test event 05/01/23 20:47:58.543
STEP: deleting the test event 05/01/23 20:47:58.563
STEP: listing all events in all namespaces 05/01/23 20:47:58.613
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
May  1 20:47:58.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3695" for this suite. 05/01/23 20:47:58.732
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":322,"skipped":5827,"failed":0}
------------------------------
• [0.715 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:47:58.048
    May  1 20:47:58.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename events 05/01/23 20:47:58.053
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:58.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:58.21
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/01/23 20:47:58.23
    STEP: listing all events in all namespaces 05/01/23 20:47:58.282
    STEP: patching the test event 05/01/23 20:47:58.393
    STEP: fetching the test event 05/01/23 20:47:58.436
    STEP: updating the test event 05/01/23 20:47:58.478
    STEP: getting the test event 05/01/23 20:47:58.543
    STEP: deleting the test event 05/01/23 20:47:58.563
    STEP: listing all events in all namespaces 05/01/23 20:47:58.613
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    May  1 20:47:58.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3695" for this suite. 05/01/23 20:47:58.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:47:58.769
May  1 20:47:58.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename tables 05/01/23 20:47:58.772
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:58.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:58.872
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
May  1 20:47:58.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-9938" for this suite. 05/01/23 20:47:58.937
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":323,"skipped":5843,"failed":0}
------------------------------
• [0.212 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:47:58.769
    May  1 20:47:58.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename tables 05/01/23 20:47:58.772
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:58.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:58.872
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    May  1 20:47:58.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-9938" for this suite. 05/01/23 20:47:58.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:47:58.991
May  1 20:47:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 20:47:58.993
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:59.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:59.098
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 05/01/23 20:47:59.117
STEP: Creating a ResourceQuota 05/01/23 20:48:04.147
STEP: Ensuring resource quota status is calculated 05/01/23 20:48:04.181
STEP: Creating a ReplicationController 05/01/23 20:48:06.207
STEP: Ensuring resource quota status captures replication controller creation 05/01/23 20:48:06.321
STEP: Deleting a ReplicationController 05/01/23 20:48:08.341
STEP: Ensuring resource quota status released usage 05/01/23 20:48:08.392
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 20:48:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6818" for this suite. 05/01/23 20:48:10.473
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":324,"skipped":5877,"failed":0}
------------------------------
• [SLOW TEST] [11.529 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:47:58.991
    May  1 20:47:58.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 20:47:58.993
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:47:59.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:47:59.098
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 05/01/23 20:47:59.117
    STEP: Creating a ResourceQuota 05/01/23 20:48:04.147
    STEP: Ensuring resource quota status is calculated 05/01/23 20:48:04.181
    STEP: Creating a ReplicationController 05/01/23 20:48:06.207
    STEP: Ensuring resource quota status captures replication controller creation 05/01/23 20:48:06.321
    STEP: Deleting a ReplicationController 05/01/23 20:48:08.341
    STEP: Ensuring resource quota status released usage 05/01/23 20:48:08.392
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 20:48:10.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6818" for this suite. 05/01/23 20:48:10.473
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:10.522
May  1 20:48:10.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename security-context-test 05/01/23 20:48:10.524
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:10.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:10.651
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
May  1 20:48:10.779: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39" in namespace "security-context-test-2113" to be "Succeeded or Failed"
May  1 20:48:10.814: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 34.07467ms
May  1 20:48:12.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056016158s
May  1 20:48:14.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05602638s
May  1 20:48:16.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056443142s
May  1 20:48:16.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39" satisfied condition "Succeeded or Failed"
May  1 20:48:16.913: INFO: Got logs for pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
May  1 20:48:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2113" for this suite. 05/01/23 20:48:16.955
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":325,"skipped":5878,"failed":0}
------------------------------
• [SLOW TEST] [6.481 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:10.522
    May  1 20:48:10.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename security-context-test 05/01/23 20:48:10.524
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:10.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:10.651
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    May  1 20:48:10.779: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39" in namespace "security-context-test-2113" to be "Succeeded or Failed"
    May  1 20:48:10.814: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 34.07467ms
    May  1 20:48:12.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056016158s
    May  1 20:48:14.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05602638s
    May  1 20:48:16.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056443142s
    May  1 20:48:16.836: INFO: Pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39" satisfied condition "Succeeded or Failed"
    May  1 20:48:16.913: INFO: Got logs for pod "busybox-privileged-false-aefeb7ec-4d29-486f-bee1-5ce0259e5d39": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    May  1 20:48:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2113" for this suite. 05/01/23 20:48:16.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:17.01
May  1 20:48:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:48:17.013
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:17.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:17.16
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 05/01/23 20:48:17.179
May  1 20:48:17.276: INFO: Waiting up to 5m0s for pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288" in namespace "emptydir-9362" to be "Succeeded or Failed"
May  1 20:48:17.307: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 30.697513ms
May  1 20:48:19.334: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057185494s
May  1 20:48:21.376: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098882359s
May  1 20:48:23.342: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065703686s
STEP: Saw pod success 05/01/23 20:48:23.342
May  1 20:48:23.343: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288" satisfied condition "Succeeded or Failed"
May  1 20:48:23.365: INFO: Trying to get logs from node 10.45.145.124 pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 container test-container: <nil>
STEP: delete the pod 05/01/23 20:48:23.412
May  1 20:48:23.482: INFO: Waiting for pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 to disappear
May  1 20:48:23.508: INFO: Pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:48:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9362" for this suite. 05/01/23 20:48:23.542
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":326,"skipped":5895,"failed":0}
------------------------------
• [SLOW TEST] [6.559 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:17.01
    May  1 20:48:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:48:17.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:17.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:17.16
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/01/23 20:48:17.179
    May  1 20:48:17.276: INFO: Waiting up to 5m0s for pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288" in namespace "emptydir-9362" to be "Succeeded or Failed"
    May  1 20:48:17.307: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 30.697513ms
    May  1 20:48:19.334: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057185494s
    May  1 20:48:21.376: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098882359s
    May  1 20:48:23.342: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.065703686s
    STEP: Saw pod success 05/01/23 20:48:23.342
    May  1 20:48:23.343: INFO: Pod "pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288" satisfied condition "Succeeded or Failed"
    May  1 20:48:23.365: INFO: Trying to get logs from node 10.45.145.124 pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 container test-container: <nil>
    STEP: delete the pod 05/01/23 20:48:23.412
    May  1 20:48:23.482: INFO: Waiting for pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 to disappear
    May  1 20:48:23.508: INFO: Pod pod-fa1fe7e8-083e-4ce2-8b37-5d337a1db288 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:48:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9362" for this suite. 05/01/23 20:48:23.542
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:23.571
May  1 20:48:23.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:48:23.577
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:23.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:23.669
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 05/01/23 20:48:23.695
May  1 20:48:23.795: INFO: Waiting up to 5m0s for pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87" in namespace "projected-4294" to be "running and ready"
May  1 20:48:23.832: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Pending", Reason="", readiness=false. Elapsed: 36.813776ms
May  1 20:48:23.832: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:48:25.853: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057495874s
May  1 20:48:25.853: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:48:27.852: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Running", Reason="", readiness=true. Elapsed: 4.057229814s
May  1 20:48:27.853: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Running (Ready = true)
May  1 20:48:27.853: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87" satisfied condition "running and ready"
May  1 20:48:28.498: INFO: Successfully updated pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
May  1 20:48:30.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4294" for this suite. 05/01/23 20:48:30.714
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":327,"skipped":5899,"failed":0}
------------------------------
• [SLOW TEST] [7.170 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:23.571
    May  1 20:48:23.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:48:23.577
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:23.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:23.669
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 05/01/23 20:48:23.695
    May  1 20:48:23.795: INFO: Waiting up to 5m0s for pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87" in namespace "projected-4294" to be "running and ready"
    May  1 20:48:23.832: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Pending", Reason="", readiness=false. Elapsed: 36.813776ms
    May  1 20:48:23.832: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:48:25.853: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057495874s
    May  1 20:48:25.853: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:48:27.852: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87": Phase="Running", Reason="", readiness=true. Elapsed: 4.057229814s
    May  1 20:48:27.853: INFO: The phase of Pod annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87 is Running (Ready = true)
    May  1 20:48:27.853: INFO: Pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87" satisfied condition "running and ready"
    May  1 20:48:28.498: INFO: Successfully updated pod "annotationupdate04d9005e-2e9a-4a87-af8c-a046a0bd0e87"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    May  1 20:48:30.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4294" for this suite. 05/01/23 20:48:30.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:30.742
May  1 20:48:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename init-container 05/01/23 20:48:30.746
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:30.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:30.876
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 05/01/23 20:48:30.921
May  1 20:48:30.921: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 20:48:35.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4579" for this suite. 05/01/23 20:48:35.549
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":328,"skipped":5904,"failed":0}
------------------------------
• [4.843 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:30.742
    May  1 20:48:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename init-container 05/01/23 20:48:30.746
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:30.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:30.876
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 05/01/23 20:48:30.921
    May  1 20:48:30.921: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 20:48:35.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4579" for this suite. 05/01/23 20:48:35.549
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:35.588
May  1 20:48:35.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 20:48:35.609
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:35.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:35.76
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/01/23 20:48:35.776
STEP: delete the rc 05/01/23 20:48:40.875
STEP: wait for all pods to be garbage collected 05/01/23 20:48:40.965
STEP: Gathering metrics 05/01/23 20:48:46.005
W0501 20:48:46.044182      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 20:48:46.044: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 20:48:46.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8773" for this suite. 05/01/23 20:48:46.068
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":329,"skipped":5905,"failed":0}
------------------------------
• [SLOW TEST] [10.536 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:35.588
    May  1 20:48:35.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 20:48:35.609
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:35.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:35.76
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/01/23 20:48:35.776
    STEP: delete the rc 05/01/23 20:48:40.875
    STEP: wait for all pods to be garbage collected 05/01/23 20:48:40.965
    STEP: Gathering metrics 05/01/23 20:48:46.005
    W0501 20:48:46.044182      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 20:48:46.044: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 20:48:46.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8773" for this suite. 05/01/23 20:48:46.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:46.128
May  1 20:48:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 20:48:46.131
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:46.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:46.274
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:48:46.291
May  1 20:48:46.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0" in namespace "downward-api-6688" to be "Succeeded or Failed"
May  1 20:48:46.406: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.294027ms
May  1 20:48:48.433: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052741767s
May  1 20:48:50.435: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054690828s
May  1 20:48:52.433: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052291939s
May  1 20:48:54.428: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.047549821s
STEP: Saw pod success 05/01/23 20:48:54.428
May  1 20:48:54.429: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0" satisfied condition "Succeeded or Failed"
May  1 20:48:54.449: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 container client-container: <nil>
STEP: delete the pod 05/01/23 20:48:54.498
May  1 20:48:54.572: INFO: Waiting for pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 to disappear
May  1 20:48:54.590: INFO: Pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 20:48:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6688" for this suite. 05/01/23 20:48:54.612
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":330,"skipped":5919,"failed":0}
------------------------------
• [SLOW TEST] [8.525 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:46.128
    May  1 20:48:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 20:48:46.131
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:46.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:46.274
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:48:46.291
    May  1 20:48:46.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0" in namespace "downward-api-6688" to be "Succeeded or Failed"
    May  1 20:48:46.406: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.294027ms
    May  1 20:48:48.433: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052741767s
    May  1 20:48:50.435: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054690828s
    May  1 20:48:52.433: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052291939s
    May  1 20:48:54.428: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.047549821s
    STEP: Saw pod success 05/01/23 20:48:54.428
    May  1 20:48:54.429: INFO: Pod "downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0" satisfied condition "Succeeded or Failed"
    May  1 20:48:54.449: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 container client-container: <nil>
    STEP: delete the pod 05/01/23 20:48:54.498
    May  1 20:48:54.572: INFO: Waiting for pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 to disappear
    May  1 20:48:54.590: INFO: Pod downwardapi-volume-7cc70dd3-be3e-4d80-9edc-ae5ba7cc4db0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 20:48:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6688" for this suite. 05/01/23 20:48:54.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:48:54.656
May  1 20:48:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename secrets 05/01/23 20:48:54.657
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:54.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:54.746
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-e8637ae4-5fe6-4cc9-a9f3-9ec3cfe70c05 05/01/23 20:48:54.763
STEP: Creating a pod to test consume secrets 05/01/23 20:48:54.804
May  1 20:48:54.893: INFO: Waiting up to 5m0s for pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5" in namespace "secrets-6829" to be "Succeeded or Failed"
May  1 20:48:54.916: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.27913ms
May  1 20:48:56.938: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045580977s
May  1 20:48:58.937: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044184402s
May  1 20:49:00.936: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043700286s
STEP: Saw pod success 05/01/23 20:49:00.937
May  1 20:49:00.937: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5" satisfied condition "Succeeded or Failed"
May  1 20:49:00.959: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 container secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:49:01.021
May  1 20:49:01.071: INFO: Waiting for pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 to disappear
May  1 20:49:01.091: INFO: Pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
May  1 20:49:01.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6829" for this suite. 05/01/23 20:49:01.117
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":331,"skipped":5929,"failed":0}
------------------------------
• [SLOW TEST] [6.487 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:48:54.656
    May  1 20:48:54.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename secrets 05/01/23 20:48:54.657
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:48:54.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:48:54.746
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-e8637ae4-5fe6-4cc9-a9f3-9ec3cfe70c05 05/01/23 20:48:54.763
    STEP: Creating a pod to test consume secrets 05/01/23 20:48:54.804
    May  1 20:48:54.893: INFO: Waiting up to 5m0s for pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5" in namespace "secrets-6829" to be "Succeeded or Failed"
    May  1 20:48:54.916: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.27913ms
    May  1 20:48:56.938: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045580977s
    May  1 20:48:58.937: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044184402s
    May  1 20:49:00.936: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043700286s
    STEP: Saw pod success 05/01/23 20:49:00.937
    May  1 20:49:00.937: INFO: Pod "pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5" satisfied condition "Succeeded or Failed"
    May  1 20:49:00.959: INFO: Trying to get logs from node 10.45.145.124 pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 container secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:49:01.021
    May  1 20:49:01.071: INFO: Waiting for pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 to disappear
    May  1 20:49:01.091: INFO: Pod pod-secrets-cae01bd5-97c6-437a-b302-5c4b00164bd5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    May  1 20:49:01.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6829" for this suite. 05/01/23 20:49:01.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:49:01.153
May  1 20:49:01.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 20:49:01.155
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:49:01.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:49:01.288
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 05/01/23 20:49:18.325
STEP: Creating a ResourceQuota 05/01/23 20:49:23.354
STEP: Ensuring resource quota status is calculated 05/01/23 20:49:23.379
STEP: Creating a ConfigMap 05/01/23 20:49:25.407
STEP: Ensuring resource quota status captures configMap creation 05/01/23 20:49:25.459
STEP: Deleting a ConfigMap 05/01/23 20:49:27.479
STEP: Ensuring resource quota status released usage 05/01/23 20:49:27.501
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 20:49:29.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1999" for this suite. 05/01/23 20:49:29.549
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":332,"skipped":5979,"failed":0}
------------------------------
• [SLOW TEST] [28.420 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:49:01.153
    May  1 20:49:01.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 20:49:01.155
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:49:01.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:49:01.288
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 05/01/23 20:49:18.325
    STEP: Creating a ResourceQuota 05/01/23 20:49:23.354
    STEP: Ensuring resource quota status is calculated 05/01/23 20:49:23.379
    STEP: Creating a ConfigMap 05/01/23 20:49:25.407
    STEP: Ensuring resource quota status captures configMap creation 05/01/23 20:49:25.459
    STEP: Deleting a ConfigMap 05/01/23 20:49:27.479
    STEP: Ensuring resource quota status released usage 05/01/23 20:49:27.501
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 20:49:29.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1999" for this suite. 05/01/23 20:49:29.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:49:29.578
May  1 20:49:29.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename cronjob 05/01/23 20:49:29.58
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:49:29.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:49:29.68
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/01/23 20:49:29.7
STEP: Ensuring a job is scheduled 05/01/23 20:49:29.735
STEP: Ensuring exactly one is scheduled 05/01/23 20:50:01.769
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/01/23 20:50:01.814
STEP: Ensuring no more jobs are scheduled 05/01/23 20:50:01.854
STEP: Removing cronjob 05/01/23 20:55:01.888
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
May  1 20:55:01.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-727" for this suite. 05/01/23 20:55:01.95
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":333,"skipped":6010,"failed":0}
------------------------------
• [SLOW TEST] [332.400 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:49:29.578
    May  1 20:49:29.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename cronjob 05/01/23 20:49:29.58
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:49:29.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:49:29.68
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/01/23 20:49:29.7
    STEP: Ensuring a job is scheduled 05/01/23 20:49:29.735
    STEP: Ensuring exactly one is scheduled 05/01/23 20:50:01.769
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/01/23 20:50:01.814
    STEP: Ensuring no more jobs are scheduled 05/01/23 20:50:01.854
    STEP: Removing cronjob 05/01/23 20:55:01.888
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    May  1 20:55:01.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-727" for this suite. 05/01/23 20:55:01.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:55:01.998
May  1 20:55:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:55:02
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:02.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:02.146
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-397e7934-7bc2-4b03-ac69-294747a08720 05/01/23 20:55:02.167
STEP: Creating a pod to test consume configMaps 05/01/23 20:55:02.193
May  1 20:55:02.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304" in namespace "projected-2722" to be "Succeeded or Failed"
May  1 20:55:02.332: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 35.404598ms
May  1 20:55:04.362: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06555069s
May  1 20:55:06.358: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061612771s
May  1 20:55:08.353: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056726666s
May  1 20:55:10.370: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07379099s
STEP: Saw pod success 05/01/23 20:55:10.37
May  1 20:55:10.371: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304" satisfied condition "Succeeded or Failed"
May  1 20:55:10.394: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 container agnhost-container: <nil>
STEP: delete the pod 05/01/23 20:55:10.527
May  1 20:55:10.617: INFO: Waiting for pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 to disappear
May  1 20:55:10.662: INFO: Pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
May  1 20:55:10.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2722" for this suite. 05/01/23 20:55:10.71
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6052,"failed":0}
------------------------------
• [SLOW TEST] [8.744 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:55:01.998
    May  1 20:55:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:55:02
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:02.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:02.146
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-397e7934-7bc2-4b03-ac69-294747a08720 05/01/23 20:55:02.167
    STEP: Creating a pod to test consume configMaps 05/01/23 20:55:02.193
    May  1 20:55:02.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304" in namespace "projected-2722" to be "Succeeded or Failed"
    May  1 20:55:02.332: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 35.404598ms
    May  1 20:55:04.362: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06555069s
    May  1 20:55:06.358: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061612771s
    May  1 20:55:08.353: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056726666s
    May  1 20:55:10.370: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07379099s
    STEP: Saw pod success 05/01/23 20:55:10.37
    May  1 20:55:10.371: INFO: Pod "pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304" satisfied condition "Succeeded or Failed"
    May  1 20:55:10.394: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 container agnhost-container: <nil>
    STEP: delete the pod 05/01/23 20:55:10.527
    May  1 20:55:10.617: INFO: Waiting for pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 to disappear
    May  1 20:55:10.662: INFO: Pod pod-projected-configmaps-b996f27e-06d5-4bf5-bfa7-ad8ed5c45304 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    May  1 20:55:10.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2722" for this suite. 05/01/23 20:55:10.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:55:10.745
May  1 20:55:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 20:55:10.747
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:10.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:10.841
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/01/23 20:55:10.901
STEP: create the rc2 05/01/23 20:55:10.927
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/01/23 20:55:16.018
STEP: delete the rc simpletest-rc-to-be-deleted 05/01/23 20:55:19.307
STEP: wait for the rc to be deleted 05/01/23 20:55:19.352
STEP: Gathering metrics 05/01/23 20:55:24.475
W0501 20:55:24.559799      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 20:55:24.559: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May  1 20:55:24.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-29fqd" in namespace "gc-1055"
May  1 20:55:24.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-29zsd" in namespace "gc-1055"
May  1 20:55:24.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b9j8" in namespace "gc-1055"
May  1 20:55:24.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vnt6" in namespace "gc-1055"
May  1 20:55:24.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-46zbd" in namespace "gc-1055"
May  1 20:55:25.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dtd2" in namespace "gc-1055"
May  1 20:55:25.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rj6j" in namespace "gc-1055"
May  1 20:55:25.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w929" in namespace "gc-1055"
May  1 20:55:25.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-58z4n" in namespace "gc-1055"
May  1 20:55:25.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j5mk" in namespace "gc-1055"
May  1 20:55:25.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n7n4" in namespace "gc-1055"
May  1 20:55:25.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-62bqc" in namespace "gc-1055"
May  1 20:55:25.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k7tb" in namespace "gc-1055"
May  1 20:55:25.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s9b7" in namespace "gc-1055"
May  1 20:55:25.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n2d5" in namespace "gc-1055"
May  1 20:55:25.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-84qv2" in namespace "gc-1055"
May  1 20:55:26.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-8k4fw" in namespace "gc-1055"
May  1 20:55:26.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-8m56b" in namespace "gc-1055"
May  1 20:55:26.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-8s8xq" in namespace "gc-1055"
May  1 20:55:26.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h68g" in namespace "gc-1055"
May  1 20:55:26.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j295" in namespace "gc-1055"
May  1 20:55:26.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5tsh" in namespace "gc-1055"
May  1 20:55:26.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6vmx" in namespace "gc-1055"
May  1 20:55:26.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8djg" in namespace "gc-1055"
May  1 20:55:26.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhztb" in namespace "gc-1055"
May  1 20:55:26.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw5xv" in namespace "gc-1055"
May  1 20:55:26.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9wmz" in namespace "gc-1055"
May  1 20:55:27.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmzfz" in namespace "gc-1055"
May  1 20:55:27.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfps2" in namespace "gc-1055"
May  1 20:55:27.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxh4f" in namespace "gc-1055"
May  1 20:55:27.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz6rl" in namespace "gc-1055"
May  1 20:55:27.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-f57zv" in namespace "gc-1055"
May  1 20:55:27.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5bcw" in namespace "gc-1055"
May  1 20:55:27.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5zqt" in namespace "gc-1055"
May  1 20:55:27.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs76g" in namespace "gc-1055"
May  1 20:55:27.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftxvb" in namespace "gc-1055"
May  1 20:55:27.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7srm" in namespace "gc-1055"
May  1 20:55:27.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd645" in namespace "gc-1055"
May  1 20:55:27.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-gg5j8" in namespace "gc-1055"
May  1 20:55:28.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmtdj" in namespace "gc-1055"
May  1 20:55:28.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw4hx" in namespace "gc-1055"
May  1 20:55:28.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcdcn" in namespace "gc-1055"
May  1 20:55:28.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdv22" in namespace "gc-1055"
May  1 20:55:28.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfkhw" in namespace "gc-1055"
May  1 20:55:28.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4b99" in namespace "gc-1055"
May  1 20:55:28.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5zfb" in namespace "gc-1055"
May  1 20:55:28.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6xjk" in namespace "gc-1055"
May  1 20:55:28.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfwtm" in namespace "gc-1055"
May  1 20:55:28.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-kb6gw" in namespace "gc-1055"
May  1 20:55:29.054: INFO: Deleting pod "simpletest-rc-to-be-deleted-kdhkf" in namespace "gc-1055"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 20:55:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1055" for this suite. 05/01/23 20:55:29.221
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":335,"skipped":6092,"failed":0}
------------------------------
• [SLOW TEST] [18.541 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:55:10.745
    May  1 20:55:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 20:55:10.747
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:10.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:10.841
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/01/23 20:55:10.901
    STEP: create the rc2 05/01/23 20:55:10.927
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/01/23 20:55:16.018
    STEP: delete the rc simpletest-rc-to-be-deleted 05/01/23 20:55:19.307
    STEP: wait for the rc to be deleted 05/01/23 20:55:19.352
    STEP: Gathering metrics 05/01/23 20:55:24.475
    W0501 20:55:24.559799      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 20:55:24.559: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May  1 20:55:24.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-29fqd" in namespace "gc-1055"
    May  1 20:55:24.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-29zsd" in namespace "gc-1055"
    May  1 20:55:24.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-2b9j8" in namespace "gc-1055"
    May  1 20:55:24.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vnt6" in namespace "gc-1055"
    May  1 20:55:24.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-46zbd" in namespace "gc-1055"
    May  1 20:55:25.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-4dtd2" in namespace "gc-1055"
    May  1 20:55:25.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-4rj6j" in namespace "gc-1055"
    May  1 20:55:25.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w929" in namespace "gc-1055"
    May  1 20:55:25.412: INFO: Deleting pod "simpletest-rc-to-be-deleted-58z4n" in namespace "gc-1055"
    May  1 20:55:25.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j5mk" in namespace "gc-1055"
    May  1 20:55:25.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n7n4" in namespace "gc-1055"
    May  1 20:55:25.669: INFO: Deleting pod "simpletest-rc-to-be-deleted-62bqc" in namespace "gc-1055"
    May  1 20:55:25.746: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k7tb" in namespace "gc-1055"
    May  1 20:55:25.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s9b7" in namespace "gc-1055"
    May  1 20:55:25.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n2d5" in namespace "gc-1055"
    May  1 20:55:25.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-84qv2" in namespace "gc-1055"
    May  1 20:55:26.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-8k4fw" in namespace "gc-1055"
    May  1 20:55:26.076: INFO: Deleting pod "simpletest-rc-to-be-deleted-8m56b" in namespace "gc-1055"
    May  1 20:55:26.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-8s8xq" in namespace "gc-1055"
    May  1 20:55:26.269: INFO: Deleting pod "simpletest-rc-to-be-deleted-9h68g" in namespace "gc-1055"
    May  1 20:55:26.357: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j295" in namespace "gc-1055"
    May  1 20:55:26.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5tsh" in namespace "gc-1055"
    May  1 20:55:26.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6vmx" in namespace "gc-1055"
    May  1 20:55:26.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8djg" in namespace "gc-1055"
    May  1 20:55:26.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-bhztb" in namespace "gc-1055"
    May  1 20:55:26.772: INFO: Deleting pod "simpletest-rc-to-be-deleted-bw5xv" in namespace "gc-1055"
    May  1 20:55:26.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9wmz" in namespace "gc-1055"
    May  1 20:55:27.008: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmzfz" in namespace "gc-1055"
    May  1 20:55:27.142: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfps2" in namespace "gc-1055"
    May  1 20:55:27.215: INFO: Deleting pod "simpletest-rc-to-be-deleted-dxh4f" in namespace "gc-1055"
    May  1 20:55:27.313: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz6rl" in namespace "gc-1055"
    May  1 20:55:27.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-f57zv" in namespace "gc-1055"
    May  1 20:55:27.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5bcw" in namespace "gc-1055"
    May  1 20:55:27.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5zqt" in namespace "gc-1055"
    May  1 20:55:27.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs76g" in namespace "gc-1055"
    May  1 20:55:27.742: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftxvb" in namespace "gc-1055"
    May  1 20:55:27.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-g7srm" in namespace "gc-1055"
    May  1 20:55:27.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-gd645" in namespace "gc-1055"
    May  1 20:55:27.996: INFO: Deleting pod "simpletest-rc-to-be-deleted-gg5j8" in namespace "gc-1055"
    May  1 20:55:28.060: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmtdj" in namespace "gc-1055"
    May  1 20:55:28.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-gw4hx" in namespace "gc-1055"
    May  1 20:55:28.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcdcn" in namespace "gc-1055"
    May  1 20:55:28.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdv22" in namespace "gc-1055"
    May  1 20:55:28.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-hfkhw" in namespace "gc-1055"
    May  1 20:55:28.584: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4b99" in namespace "gc-1055"
    May  1 20:55:28.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-j5zfb" in namespace "gc-1055"
    May  1 20:55:28.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6xjk" in namespace "gc-1055"
    May  1 20:55:28.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfwtm" in namespace "gc-1055"
    May  1 20:55:28.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-kb6gw" in namespace "gc-1055"
    May  1 20:55:29.054: INFO: Deleting pod "simpletest-rc-to-be-deleted-kdhkf" in namespace "gc-1055"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 20:55:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1055" for this suite. 05/01/23 20:55:29.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:55:29.37
May  1 20:55:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename emptydir 05/01/23 20:55:29.492
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:29.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:29.628
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 05/01/23 20:55:29.647
May  1 20:55:29.734: INFO: Waiting up to 5m0s for pod "pod-b32179d2-2aac-4300-8c27-ae572a496708" in namespace "emptydir-5864" to be "Succeeded or Failed"
May  1 20:55:29.762: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 27.906297ms
May  1 20:55:31.818: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083653179s
May  1 20:55:33.799: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065280384s
May  1 20:55:35.835: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101446007s
May  1 20:55:37.788: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053889739s
May  1 20:55:39.786: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052211399s
May  1 20:55:41.793: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 12.059095612s
May  1 20:55:43.796: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 14.061706818s
May  1 20:55:45.809: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.075520359s
STEP: Saw pod success 05/01/23 20:55:45.81
May  1 20:55:45.810: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708" satisfied condition "Succeeded or Failed"
May  1 20:55:45.841: INFO: Trying to get logs from node 10.45.145.124 pod pod-b32179d2-2aac-4300-8c27-ae572a496708 container test-container: <nil>
STEP: delete the pod 05/01/23 20:55:46.022
May  1 20:55:46.175: INFO: Waiting for pod pod-b32179d2-2aac-4300-8c27-ae572a496708 to disappear
May  1 20:55:46.234: INFO: Pod pod-b32179d2-2aac-4300-8c27-ae572a496708 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
May  1 20:55:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5864" for this suite. 05/01/23 20:55:46.268
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6099,"failed":0}
------------------------------
• [SLOW TEST] [16.961 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:55:29.37
    May  1 20:55:29.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename emptydir 05/01/23 20:55:29.492
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:29.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:29.628
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/01/23 20:55:29.647
    May  1 20:55:29.734: INFO: Waiting up to 5m0s for pod "pod-b32179d2-2aac-4300-8c27-ae572a496708" in namespace "emptydir-5864" to be "Succeeded or Failed"
    May  1 20:55:29.762: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 27.906297ms
    May  1 20:55:31.818: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083653179s
    May  1 20:55:33.799: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065280384s
    May  1 20:55:35.835: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101446007s
    May  1 20:55:37.788: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 8.053889739s
    May  1 20:55:39.786: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052211399s
    May  1 20:55:41.793: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 12.059095612s
    May  1 20:55:43.796: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Pending", Reason="", readiness=false. Elapsed: 14.061706818s
    May  1 20:55:45.809: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.075520359s
    STEP: Saw pod success 05/01/23 20:55:45.81
    May  1 20:55:45.810: INFO: Pod "pod-b32179d2-2aac-4300-8c27-ae572a496708" satisfied condition "Succeeded or Failed"
    May  1 20:55:45.841: INFO: Trying to get logs from node 10.45.145.124 pod pod-b32179d2-2aac-4300-8c27-ae572a496708 container test-container: <nil>
    STEP: delete the pod 05/01/23 20:55:46.022
    May  1 20:55:46.175: INFO: Waiting for pod pod-b32179d2-2aac-4300-8c27-ae572a496708 to disappear
    May  1 20:55:46.234: INFO: Pod pod-b32179d2-2aac-4300-8c27-ae572a496708 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    May  1 20:55:46.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5864" for this suite. 05/01/23 20:55:46.268
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:55:46.315
May  1 20:55:46.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename replicaset 05/01/23 20:55:46.32
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:46.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:46.451
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/01/23 20:55:46.478
May  1 20:55:46.578: INFO: Pod name sample-pod: Found 0 pods out of 1
May  1 20:55:51.621: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/01/23 20:55:51.621
STEP: getting scale subresource 05/01/23 20:55:51.622
STEP: updating a scale subresource 05/01/23 20:55:51.638
STEP: verifying the replicaset Spec.Replicas was modified 05/01/23 20:55:51.68
STEP: Patch a scale subresource 05/01/23 20:55:51.696
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
May  1 20:55:51.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3141" for this suite. 05/01/23 20:55:51.876
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":337,"skipped":6099,"failed":0}
------------------------------
• [SLOW TEST] [5.599 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:55:46.315
    May  1 20:55:46.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename replicaset 05/01/23 20:55:46.32
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:46.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:46.451
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/01/23 20:55:46.478
    May  1 20:55:46.578: INFO: Pod name sample-pod: Found 0 pods out of 1
    May  1 20:55:51.621: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/01/23 20:55:51.621
    STEP: getting scale subresource 05/01/23 20:55:51.622
    STEP: updating a scale subresource 05/01/23 20:55:51.638
    STEP: verifying the replicaset Spec.Replicas was modified 05/01/23 20:55:51.68
    STEP: Patch a scale subresource 05/01/23 20:55:51.696
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    May  1 20:55:51.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3141" for this suite. 05/01/23 20:55:51.876
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:55:51.919
May  1 20:55:51.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:55:51.922
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:52.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:52.027
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:55:52.102
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:55:53.072
STEP: Deploying the webhook pod 05/01/23 20:55:53.136
STEP: Wait for the deployment to be ready 05/01/23 20:55:53.183
May  1 20:55:53.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:55:55.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:55:57.304
STEP: Verifying the service has paired with the endpoint 05/01/23 20:55:57.343
May  1 20:55:58.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
May  1 20:55:58.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7334-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:55:58.949
STEP: Creating a custom resource that should be mutated by the webhook 05/01/23 20:55:59.076
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:56:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7233" for this suite. 05/01/23 20:56:01.915
STEP: Destroying namespace "webhook-7233-markers" for this suite. 05/01/23 20:56:01.952
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":338,"skipped":6099,"failed":0}
------------------------------
• [SLOW TEST] [10.374 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:55:51.919
    May  1 20:55:51.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:55:51.922
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:55:52.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:55:52.027
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:55:52.102
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:55:53.072
    STEP: Deploying the webhook pod 05/01/23 20:55:53.136
    STEP: Wait for the deployment to be ready 05/01/23 20:55:53.183
    May  1 20:55:53.241: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:55:55.289: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 55, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:55:57.304
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:55:57.343
    May  1 20:55:58.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    May  1 20:55:58.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7334-crds.webhook.example.com via the AdmissionRegistration API 05/01/23 20:55:58.949
    STEP: Creating a custom resource that should be mutated by the webhook 05/01/23 20:55:59.076
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:56:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7233" for this suite. 05/01/23 20:56:01.915
    STEP: Destroying namespace "webhook-7233-markers" for this suite. 05/01/23 20:56:01.952
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:02.295
May  1 20:56:02.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-runtime 05/01/23 20:56:02.299
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:02.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:02.507
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 05/01/23 20:56:02.57
STEP: wait for the container to reach Succeeded 05/01/23 20:56:02.666
STEP: get the container status 05/01/23 20:56:09.957
STEP: the container should be terminated 05/01/23 20:56:09.983
STEP: the termination message should be set 05/01/23 20:56:09.983
May  1 20:56:09.983: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/01/23 20:56:09.983
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
May  1 20:56:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6139" for this suite. 05/01/23 20:56:10.143
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":339,"skipped":6118,"failed":0}
------------------------------
• [SLOW TEST] [7.883 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:02.295
    May  1 20:56:02.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-runtime 05/01/23 20:56:02.299
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:02.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:02.507
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 05/01/23 20:56:02.57
    STEP: wait for the container to reach Succeeded 05/01/23 20:56:02.666
    STEP: get the container status 05/01/23 20:56:09.957
    STEP: the container should be terminated 05/01/23 20:56:09.983
    STEP: the termination message should be set 05/01/23 20:56:09.983
    May  1 20:56:09.983: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/01/23 20:56:09.983
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    May  1 20:56:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6139" for this suite. 05/01/23 20:56:10.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:10.181
May  1 20:56:10.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename gc 05/01/23 20:56:10.184
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:10.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:10.337
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/01/23 20:56:10.411
STEP: delete the rc 05/01/23 20:56:15.512
STEP: wait for the rc to be deleted 05/01/23 20:56:15.575
May  1 20:56:16.688: INFO: 0 pods remaining
May  1 20:56:16.688: INFO: 0 pods has nil DeletionTimestamp
May  1 20:56:16.688: INFO: 
STEP: Gathering metrics 05/01/23 20:56:17.664
W0501 20:56:17.728895      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May  1 20:56:17.729: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
May  1 20:56:17.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3575" for this suite. 05/01/23 20:56:17.78
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":340,"skipped":6131,"failed":0}
------------------------------
• [SLOW TEST] [7.631 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:10.181
    May  1 20:56:10.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename gc 05/01/23 20:56:10.184
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:10.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:10.337
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/01/23 20:56:10.411
    STEP: delete the rc 05/01/23 20:56:15.512
    STEP: wait for the rc to be deleted 05/01/23 20:56:15.575
    May  1 20:56:16.688: INFO: 0 pods remaining
    May  1 20:56:16.688: INFO: 0 pods has nil DeletionTimestamp
    May  1 20:56:16.688: INFO: 
    STEP: Gathering metrics 05/01/23 20:56:17.664
    W0501 20:56:17.728895      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May  1 20:56:17.729: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    May  1 20:56:17.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3575" for this suite. 05/01/23 20:56:17.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:17.885
May  1 20:56:17.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:56:17.891
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:18.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:18.127
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
May  1 20:56:18.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 create -f -'
May  1 20:56:24.407: INFO: stderr: ""
May  1 20:56:24.407: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May  1 20:56:24.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 create -f -'
May  1 20:56:28.321: INFO: stderr: ""
May  1 20:56:28.321: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/01/23 20:56:28.321
May  1 20:56:29.371: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 20:56:29.371: INFO: Found 0 / 1
May  1 20:56:30.373: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 20:56:30.373: INFO: Found 0 / 1
May  1 20:56:31.374: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 20:56:31.374: INFO: Found 1 / 1
May  1 20:56:31.374: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  1 20:56:31.414: INFO: Selector matched 1 pods for map[app:agnhost]
May  1 20:56:31.414: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  1 20:56:31.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe pod agnhost-primary-2x7br'
May  1 20:56:32.245: INFO: stderr: ""
May  1 20:56:32.245: INFO: stdout: "Name:             agnhost-primary-2x7br\nNamespace:        kubectl-6947\nPriority:         0\nService Account:  default\nNode:             10.45.145.124/10.45.145.124\nStart Time:       Mon, 01 May 2023 20:56:24 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 5755a9adb2a78805df54d6275c37fefbb4e1f55c54ab2a4c9c298d200c32b772\n                  cni.projectcalico.org/podIP: 172.30.42.92/32\n                  cni.projectcalico.org/podIPs: 172.30.42.92/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.42.92\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.42.92\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.42.92\nIPs:\n  IP:           172.30.42.92\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://d017090b92f38c41cd659c55b9ab441d1d90c9f814130d872bcf1758dc27c473\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 01 May 2023 20:56:30 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xc69p (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xc69p:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       8s    default-scheduler  Successfully assigned kubectl-6947/agnhost-primary-2x7br to 10.45.145.124\n  Normal  AddedInterface  4s    multus             Add eth0 [172.30.42.92/32] from k8s-pod-network\n  Normal  Pulled          3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
May  1 20:56:32.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe rc agnhost-primary'
May  1 20:56:32.818: INFO: stderr: ""
May  1 20:56:32.818: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6947\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: agnhost-primary-2x7br\n"
May  1 20:56:32.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe service agnhost-primary'
May  1 20:56:33.320: INFO: stderr: ""
May  1 20:56:33.320: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6947\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.234.110\nIPs:               172.21.234.110\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.42.92:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  1 20:56:33.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe node 10.45.145.124'
May  1 20:56:34.116: INFO: stderr: ""
May  1 20:56:34.116: INFO: stdout: "Name:               10.45.145.124\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.175.149.198\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.45.145.124\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-ch7uig7l0mgsqaf1r1s0-kubee2epvgi-default-000003d9\n                    ibm-cloud.kubernetes.io/worker-pool-id=ch7uig7l0mgsqaf1r1s0-b4e292a\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.13_1539_openshift\n                    ibm-cloud.kubernetes.io/zone=lon04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.45.145.124\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722966\n                    publicVLAN=2722964\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon04\nAnnotations:        projectcalico.org/IPv4Address: 10.45.145.124/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.42.64\nCreationTimestamp:  Mon, 01 May 2023 16:54:18 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.45.145.124\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 01 May 2023 20:56:31 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 01 May 2023 16:56:01 +0000   Mon, 01 May 2023 16:56:01 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:55:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.145.124\n  ExternalIP:  158.175.149.198\n  Hostname:    10.45.145.124\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102609848Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16386540Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93913280025\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13597164Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             3cc8cc5b6f454f44b7147fb036672a13\n  System UUID:                            73e24c95-48c8-a6e5-5d64-48c6820d4ba6\n  Boot ID:                                8b48257e-cc0c-462b-a04d-b52ba0311c5f\n  Kernel Version:                         4.18.0-425.19.2.el8_7.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.3-2.rhaos4.12.git592efcd.el8\n  Kubelet Version:                        v1.25.8+27e744f\n  Kube-Proxy Version:                     v1.25.8+27e744f\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///ch7uig7l0mgsqaf1r1s0/kube-ch7uig7l0mgsqaf1r1s0-kubee2epvgi-default-000003d9\nNon-terminated Pods:                      (19 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-slcg9                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  calico-system                           calico-typha-75ff8c8c66-6z5c6                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  kube-system                             ibm-keepalived-watcher-ncf96                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h2m\n  kube-system                             ibm-master-proxy-static-10.45.145.124                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h2m\n  kube-system                             ibmcloud-block-storage-driver-cmlmj                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h2m\n  kubectl-6947                            agnhost-primary-2x7br                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10s\n  openshift-cluster-node-tuning-operator  tuned-lpz6l                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h59m\n  openshift-dns                           dns-default-mmw2q                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         52m\n  openshift-dns                           node-resolver-krvqc                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h59m\n  openshift-image-registry                node-ca-xjld6                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h59m\n  openshift-ingress-canary                ingress-canary-t55qt                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         52m\n  openshift-kube-proxy                    openshift-kube-proxy-xbgzr                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h2m\n  openshift-monitoring                    node-exporter-xpp24                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h57m\n  openshift-multus                        multus-7qs8w                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        multus-additional-cni-plugins-v48tc                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        network-metrics-daemon-4w594                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h2m\n  openshift-network-diagnostics           network-check-target-z87pw                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h2m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng    0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  845m (21%)      800m (20%)\n  memory               1012243Ki (7%)  826572800 (5%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
May  1 20:56:34.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe namespace kubectl-6947'
May  1 20:56:34.420: INFO: stderr: ""
May  1 20:56:34.420: INFO: stdout: "Name:         kubectl-6947\nLabels:       e2e-framework=kubectl\n              e2e-run=c4933137-f4e4-456c-9f68-8eb0e25c26fe\n              kubernetes.io/metadata.name=kubectl-6947\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c67,c39\n              openshift.io/sa.scc.supplemental-groups: 1004500000/10000\n              openshift.io/sa.scc.uid-range: 1004500000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:56:34.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6947" for this suite. 05/01/23 20:56:34.449
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":341,"skipped":6156,"failed":0}
------------------------------
• [SLOW TEST] [16.589 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:17.885
    May  1 20:56:17.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:56:17.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:18.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:18.127
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    May  1 20:56:18.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 create -f -'
    May  1 20:56:24.407: INFO: stderr: ""
    May  1 20:56:24.407: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May  1 20:56:24.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 create -f -'
    May  1 20:56:28.321: INFO: stderr: ""
    May  1 20:56:28.321: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/01/23 20:56:28.321
    May  1 20:56:29.371: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 20:56:29.371: INFO: Found 0 / 1
    May  1 20:56:30.373: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 20:56:30.373: INFO: Found 0 / 1
    May  1 20:56:31.374: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 20:56:31.374: INFO: Found 1 / 1
    May  1 20:56:31.374: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May  1 20:56:31.414: INFO: Selector matched 1 pods for map[app:agnhost]
    May  1 20:56:31.414: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May  1 20:56:31.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe pod agnhost-primary-2x7br'
    May  1 20:56:32.245: INFO: stderr: ""
    May  1 20:56:32.245: INFO: stdout: "Name:             agnhost-primary-2x7br\nNamespace:        kubectl-6947\nPriority:         0\nService Account:  default\nNode:             10.45.145.124/10.45.145.124\nStart Time:       Mon, 01 May 2023 20:56:24 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 5755a9adb2a78805df54d6275c37fefbb4e1f55c54ab2a4c9c298d200c32b772\n                  cni.projectcalico.org/podIP: 172.30.42.92/32\n                  cni.projectcalico.org/podIPs: 172.30.42.92/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.42.92\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.42.92\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.42.92\nIPs:\n  IP:           172.30.42.92\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://d017090b92f38c41cd659c55b9ab441d1d90c9f814130d872bcf1758dc27c473\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 01 May 2023 20:56:30 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-xc69p (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-xc69p:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       8s    default-scheduler  Successfully assigned kubectl-6947/agnhost-primary-2x7br to 10.45.145.124\n  Normal  AddedInterface  4s    multus             Add eth0 [172.30.42.92/32] from k8s-pod-network\n  Normal  Pulled          3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         3s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
    May  1 20:56:32.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe rc agnhost-primary'
    May  1 20:56:32.818: INFO: stderr: ""
    May  1 20:56:32.818: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6947\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: agnhost-primary-2x7br\n"
    May  1 20:56:32.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe service agnhost-primary'
    May  1 20:56:33.320: INFO: stderr: ""
    May  1 20:56:33.320: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6947\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.234.110\nIPs:               172.21.234.110\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.42.92:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May  1 20:56:33.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe node 10.45.145.124'
    May  1 20:56:34.116: INFO: stderr: ""
    May  1 20:56:34.116: INFO: stdout: "Name:               10.45.145.124\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.175.149.198\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.45.145.124\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-ch7uig7l0mgsqaf1r1s0-kubee2epvgi-default-000003d9\n                    ibm-cloud.kubernetes.io/worker-pool-id=ch7uig7l0mgsqaf1r1s0-b4e292a\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.13_1539_openshift\n                    ibm-cloud.kubernetes.io/zone=lon04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.45.145.124\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722966\n                    publicVLAN=2722964\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon04\nAnnotations:        projectcalico.org/IPv4Address: 10.45.145.124/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.42.64\nCreationTimestamp:  Mon, 01 May 2023 16:54:18 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.45.145.124\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 01 May 2023 20:56:31 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 01 May 2023 16:56:01 +0000   Mon, 01 May 2023 16:56:01 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:54:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 01 May 2023 20:55:30 +0000   Mon, 01 May 2023 16:55:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.145.124\n  ExternalIP:  158.175.149.198\n  Hostname:    10.45.145.124\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102609848Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16386540Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93913280025\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13597164Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             3cc8cc5b6f454f44b7147fb036672a13\n  System UUID:                            73e24c95-48c8-a6e5-5d64-48c6820d4ba6\n  Boot ID:                                8b48257e-cc0c-462b-a04d-b52ba0311c5f\n  Kernel Version:                         4.18.0-425.19.2.el8_7.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.7 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.3-2.rhaos4.12.git592efcd.el8\n  Kubelet Version:                        v1.25.8+27e744f\n  Kube-Proxy Version:                     v1.25.8+27e744f\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///ch7uig7l0mgsqaf1r1s0/kube-ch7uig7l0mgsqaf1r1s0-kubee2epvgi-default-000003d9\nNon-terminated Pods:                      (19 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-slcg9                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  calico-system                           calico-typha-75ff8c8c66-6z5c6                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  kube-system                             ibm-keepalived-watcher-ncf96                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h2m\n  kube-system                             ibm-master-proxy-static-10.45.145.124                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h2m\n  kube-system                             ibmcloud-block-storage-driver-cmlmj                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h2m\n  kubectl-6947                            agnhost-primary-2x7br                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         10s\n  openshift-cluster-node-tuning-operator  tuned-lpz6l                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h59m\n  openshift-dns                           dns-default-mmw2q                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         52m\n  openshift-dns                           node-resolver-krvqc                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h59m\n  openshift-image-registry                node-ca-xjld6                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h59m\n  openshift-ingress-canary                ingress-canary-t55qt                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         52m\n  openshift-kube-proxy                    openshift-kube-proxy-xbgzr                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h2m\n  openshift-monitoring                    node-exporter-xpp24                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h57m\n  openshift-multus                        multus-7qs8w                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        multus-additional-cni-plugins-v48tc                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        network-metrics-daemon-4w594                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h2m\n  openshift-network-diagnostics           network-check-target-z87pw                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h2m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-5cff3d587f2c4606-fd4ng    0 (0%)        0 (0%)      0 (0%)           0 (0%)         128m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  845m (21%)      800m (20%)\n  memory               1012243Ki (7%)  826572800 (5%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
    May  1 20:56:34.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-6947 describe namespace kubectl-6947'
    May  1 20:56:34.420: INFO: stderr: ""
    May  1 20:56:34.420: INFO: stdout: "Name:         kubectl-6947\nLabels:       e2e-framework=kubectl\n              e2e-run=c4933137-f4e4-456c-9f68-8eb0e25c26fe\n              kubernetes.io/metadata.name=kubectl-6947\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c67,c39\n              openshift.io/sa.scc.supplemental-groups: 1004500000/10000\n              openshift.io/sa.scc.uid-range: 1004500000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:56:34.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6947" for this suite. 05/01/23 20:56:34.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:34.48
May  1 20:56:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename sysctl 05/01/23 20:56:34.482
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:34.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:34.604
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/01/23 20:56:34.627
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
May  1 20:56:34.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-642" for this suite. 05/01/23 20:56:34.775
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":342,"skipped":6179,"failed":0}
------------------------------
• [0.328 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:34.48
    May  1 20:56:34.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename sysctl 05/01/23 20:56:34.482
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:34.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:34.604
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/01/23 20:56:34.627
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    May  1 20:56:34.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-642" for this suite. 05/01/23 20:56:34.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:34.816
May  1 20:56:34.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename daemonsets 05/01/23 20:56:34.819
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:34.997
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:35.028
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 05/01/23 20:56:35.186
STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:56:35.27
May  1 20:56:35.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:56:35.346: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:56:36.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:56:36.408: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:56:37.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May  1 20:56:37.420: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
May  1 20:56:38.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May  1 20:56:38.395: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
May  1 20:56:39.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May  1 20:56:39.416: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 05/01/23 20:56:39.432
STEP: DeleteCollection of the DaemonSets 05/01/23 20:56:39.499
STEP: Verify that ReplicaSets have been deleted 05/01/23 20:56:39.539
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
May  1 20:56:39.623: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"143745"},"items":null}

May  1 20:56:39.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"143745"},"items":[{"metadata":{"name":"daemon-set-4kmdn","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"b861cfe4-a1f4-4149-b173-bd7384de99fc","resourceVersion":"143732","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ac5d3d2b21b514a61b847468fde1b3ebbe78dee071a004db106765b949e12c59","cni.projectcalico.org/podIP":"172.30.244.167/32","cni.projectcalico.org/podIPs":"172.30.244.167/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.244.167\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.244.167\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kc8qs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kc8qs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.71","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.71"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.71","podIP":"172.30.244.167","podIPs":[{"ip":"172.30.244.167"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://abc879d59c338479a4b4d3a1f1b566c10e86b5c01a4a4cc8f979f1d4cad4a13c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cvnl8","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"0c3bcc05-f9ee-46ee-a239-dbdc8b4853d3","resourceVersion":"143737","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"efca72dffb52bd85897f17dea8772d9deaedd071fa1c582954df17cf626c0e26","cni.projectcalico.org/podIP":"172.30.38.221/32","cni.projectcalico.org/podIPs":"172.30.38.221/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.221\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.221\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qzbdp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qzbdp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.126","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.126"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.126","podIP":"172.30.38.221","podIPs":[{"ip":"172.30.38.221"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b5eda37ed727ad58f85ec77bfb249857de8c3e47ea618b01114a1a634f130e42","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m5r6b","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"d46f0c4f-93ce-4775-a5f0-f34793e87feb","resourceVersion":"143729","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"947d3638343a9aaac1aa214d5e112db35f04fcb1a9a2ab40227d8372a3af5116","cni.projectcalico.org/podIP":"172.30.42.116/32","cni.projectcalico.org/podIPs":"172.30.42.116/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pjdmn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pjdmn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.124","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.124"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.124","podIP":"172.30.42.116","podIPs":[{"ip":"172.30.42.116"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://7f7dc7810c643ded4375ce68678c211de1cef69ccce799ca19f6d0707b7114aa","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
May  1 20:56:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7769" for this suite. 05/01/23 20:56:39.842
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":343,"skipped":6191,"failed":0}
------------------------------
• [SLOW TEST] [5.053 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:34.816
    May  1 20:56:34.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename daemonsets 05/01/23 20:56:34.819
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:34.997
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:35.028
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 05/01/23 20:56:35.186
    STEP: Check that daemon pods launch on every node of the cluster. 05/01/23 20:56:35.27
    May  1 20:56:35.345: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:56:35.346: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:56:36.408: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:56:36.408: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:56:37.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May  1 20:56:37.420: INFO: Node 10.45.145.124 is running 0 daemon pod, expected 1
    May  1 20:56:38.395: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May  1 20:56:38.395: INFO: Node 10.45.145.126 is running 0 daemon pod, expected 1
    May  1 20:56:39.416: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May  1 20:56:39.416: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 05/01/23 20:56:39.432
    STEP: DeleteCollection of the DaemonSets 05/01/23 20:56:39.499
    STEP: Verify that ReplicaSets have been deleted 05/01/23 20:56:39.539
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    May  1 20:56:39.623: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"143745"},"items":null}

    May  1 20:56:39.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"143745"},"items":[{"metadata":{"name":"daemon-set-4kmdn","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"b861cfe4-a1f4-4149-b173-bd7384de99fc","resourceVersion":"143732","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ac5d3d2b21b514a61b847468fde1b3ebbe78dee071a004db106765b949e12c59","cni.projectcalico.org/podIP":"172.30.244.167/32","cni.projectcalico.org/podIPs":"172.30.244.167/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.244.167\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.244.167\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.244.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kc8qs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kc8qs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.71","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.71"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.71","podIP":"172.30.244.167","podIPs":[{"ip":"172.30.244.167"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://abc879d59c338479a4b4d3a1f1b566c10e86b5c01a4a4cc8f979f1d4cad4a13c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-cvnl8","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"0c3bcc05-f9ee-46ee-a239-dbdc8b4853d3","resourceVersion":"143737","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"efca72dffb52bd85897f17dea8772d9deaedd071fa1c582954df17cf626c0e26","cni.projectcalico.org/podIP":"172.30.38.221/32","cni.projectcalico.org/podIPs":"172.30.38.221/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.221\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.38.221\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.38.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qzbdp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qzbdp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.126","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.126"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.126","podIP":"172.30.38.221","podIPs":[{"ip":"172.30.38.221"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://b5eda37ed727ad58f85ec77bfb249857de8c3e47ea618b01114a1a634f130e42","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-m5r6b","generateName":"daemon-set-","namespace":"daemonsets-7769","uid":"d46f0c4f-93ce-4775-a5f0-f34793e87feb","resourceVersion":"143729","creationTimestamp":"2023-05-01T20:56:35Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"947d3638343a9aaac1aa214d5e112db35f04fcb1a9a2ab40227d8372a3af5116","cni.projectcalico.org/podIP":"172.30.42.116/32","cni.projectcalico.org/podIPs":"172.30.42.116/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.42.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6cb8d0ff-7b10-4a29-845d-6ad061712c51","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cb8d0ff-7b10-4a29-845d-6ad061712c51\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-01T20:56:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.42.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pjdmn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pjdmn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.45.145.124","securityContext":{"seLinuxOptions":{"level":"s0:c67,c49"}},"imagePullSecrets":[{"name":"default-dockercfg-f668n"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.45.145.124"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:37Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-01T20:56:35Z"}],"hostIP":"10.45.145.124","podIP":"172.30.42.116","podIPs":[{"ip":"172.30.42.116"}],"startTime":"2023-05-01T20:56:35Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-01T20:56:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://7f7dc7810c643ded4375ce68678c211de1cef69ccce799ca19f6d0707b7114aa","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    May  1 20:56:39.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7769" for this suite. 05/01/23 20:56:39.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:39.873
May  1 20:56:39.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename endpointslicemirroring 05/01/23 20:56:39.876
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:39.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:39.995
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/01/23 20:56:40.162
May  1 20:56:40.203: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/01/23 20:56:42.295
STEP: mirroring deletion of a custom Endpoint 05/01/23 20:56:42.356
May  1 20:56:42.441: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
May  1 20:56:44.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5601" for this suite. 05/01/23 20:56:44.518
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":344,"skipped":6237,"failed":0}
------------------------------
• [4.669 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:39.873
    May  1 20:56:39.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename endpointslicemirroring 05/01/23 20:56:39.876
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:39.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:39.995
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/01/23 20:56:40.162
    May  1 20:56:40.203: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/01/23 20:56:42.295
    STEP: mirroring deletion of a custom Endpoint 05/01/23 20:56:42.356
    May  1 20:56:42.441: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    May  1 20:56:44.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5601" for this suite. 05/01/23 20:56:44.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:44.551
May  1 20:56:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:56:44.555
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:44.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:44.684
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 05/01/23 20:56:44.714
May  1 20:56:44.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-352 proxy --unix-socket=/tmp/kubectl-proxy-unix2010382573/test'
STEP: retrieving proxy /api/ output 05/01/23 20:56:44.927
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:56:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-352" for this suite. 05/01/23 20:56:44.973
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":345,"skipped":6254,"failed":0}
------------------------------
• [0.456 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:44.551
    May  1 20:56:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:56:44.555
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:44.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:44.684
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 05/01/23 20:56:44.714
    May  1 20:56:44.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-352 proxy --unix-socket=/tmp/kubectl-proxy-unix2010382573/test'
    STEP: retrieving proxy /api/ output 05/01/23 20:56:44.927
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:56:44.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-352" for this suite. 05/01/23 20:56:44.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:45.035
May  1 20:56:45.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename webhook 05/01/23 20:56:45.04
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:45.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:45.218
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 05/01/23 20:56:45.325
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:56:46.349
STEP: Deploying the webhook pod 05/01/23 20:56:46.409
STEP: Wait for the deployment to be ready 05/01/23 20:56:46.48
May  1 20:56:46.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May  1 20:56:48.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/01/23 20:56:50.669
STEP: Verifying the service has paired with the endpoint 05/01/23 20:56:50.873
May  1 20:56:51.874: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 05/01/23 20:56:51.892
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/01/23 20:56:51.904
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/01/23 20:56:51.904
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/01/23 20:56:51.904
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/01/23 20:56:51.912
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/01/23 20:56:51.912
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/01/23 20:56:51.927
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:56:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6334" for this suite. 05/01/23 20:56:51.979
STEP: Destroying namespace "webhook-6334-markers" for this suite. 05/01/23 20:56:52.013
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":346,"skipped":6324,"failed":0}
------------------------------
• [SLOW TEST] [7.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:45.035
    May  1 20:56:45.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename webhook 05/01/23 20:56:45.04
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:45.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:45.218
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 05/01/23 20:56:45.325
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/01/23 20:56:46.349
    STEP: Deploying the webhook pod 05/01/23 20:56:46.409
    STEP: Wait for the deployment to be ready 05/01/23 20:56:46.48
    May  1 20:56:46.539: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May  1 20:56:48.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 1, 20, 56, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/01/23 20:56:50.669
    STEP: Verifying the service has paired with the endpoint 05/01/23 20:56:50.873
    May  1 20:56:51.874: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 05/01/23 20:56:51.892
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/01/23 20:56:51.904
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/01/23 20:56:51.904
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/01/23 20:56:51.904
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/01/23 20:56:51.912
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/01/23 20:56:51.912
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/01/23 20:56:51.927
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:56:51.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6334" for this suite. 05/01/23 20:56:51.979
    STEP: Destroying namespace "webhook-6334-markers" for this suite. 05/01/23 20:56:52.013
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:56:52.283
May  1 20:56:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename downward-api 05/01/23 20:56:52.286
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:52.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:52.426
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 05/01/23 20:56:52.478
May  1 20:56:52.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb" in namespace "downward-api-8427" to be "Succeeded or Failed"
May  1 20:56:52.623: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 46.340813ms
May  1 20:56:54.652: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074868273s
May  1 20:56:56.644: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067307657s
May  1 20:56:58.650: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073549554s
May  1 20:57:00.672: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.095420592s
STEP: Saw pod success 05/01/23 20:57:00.672
May  1 20:57:00.673: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb" satisfied condition "Succeeded or Failed"
May  1 20:57:00.726: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb container client-container: <nil>
STEP: delete the pod 05/01/23 20:57:00.827
May  1 20:57:00.930: INFO: Waiting for pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb to disappear
May  1 20:57:00.983: INFO: Pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
May  1 20:57:00.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8427" for this suite. 05/01/23 20:57:01.044
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":347,"skipped":6330,"failed":0}
------------------------------
• [SLOW TEST] [8.804 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:56:52.283
    May  1 20:56:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename downward-api 05/01/23 20:56:52.286
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:56:52.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:56:52.426
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 05/01/23 20:56:52.478
    May  1 20:56:52.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb" in namespace "downward-api-8427" to be "Succeeded or Failed"
    May  1 20:56:52.623: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 46.340813ms
    May  1 20:56:54.652: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074868273s
    May  1 20:56:56.644: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067307657s
    May  1 20:56:58.650: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073549554s
    May  1 20:57:00.672: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.095420592s
    STEP: Saw pod success 05/01/23 20:57:00.672
    May  1 20:57:00.673: INFO: Pod "downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb" satisfied condition "Succeeded or Failed"
    May  1 20:57:00.726: INFO: Trying to get logs from node 10.45.145.124 pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb container client-container: <nil>
    STEP: delete the pod 05/01/23 20:57:00.827
    May  1 20:57:00.930: INFO: Waiting for pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb to disappear
    May  1 20:57:00.983: INFO: Pod downwardapi-volume-904fbb03-4c7a-4cda-af32-6c93dd5430eb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    May  1 20:57:00.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8427" for this suite. 05/01/23 20:57:01.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:01.089
May  1 20:57:01.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 20:57:01.091
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:01.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:01.307
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 in namespace container-probe-5218 05/01/23 20:57:01.347
May  1 20:57:01.465: INFO: Waiting up to 5m0s for pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3" in namespace "container-probe-5218" to be "not pending"
May  1 20:57:01.497: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.028615ms
May  1 20:57:03.519: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053825843s
May  1 20:57:05.521: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Running", Reason="", readiness=true. Elapsed: 4.05530509s
May  1 20:57:05.521: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3" satisfied condition "not pending"
May  1 20:57:05.521: INFO: Started pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 in namespace container-probe-5218
STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 20:57:05.521
May  1 20:57:05.542: INFO: Initial restart count of pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 is 0
May  1 20:57:23.880: INFO: Restart count of pod container-probe-5218/liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 is now 1 (18.338360318s elapsed)
STEP: deleting the pod 05/01/23 20:57:23.88
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 20:57:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5218" for this suite. 05/01/23 20:57:23.978
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":348,"skipped":6353,"failed":0}
------------------------------
• [SLOW TEST] [22.914 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:01.089
    May  1 20:57:01.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 20:57:01.091
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:01.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:01.307
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 in namespace container-probe-5218 05/01/23 20:57:01.347
    May  1 20:57:01.465: INFO: Waiting up to 5m0s for pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3" in namespace "container-probe-5218" to be "not pending"
    May  1 20:57:01.497: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Pending", Reason="", readiness=false. Elapsed: 32.028615ms
    May  1 20:57:03.519: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053825843s
    May  1 20:57:05.521: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3": Phase="Running", Reason="", readiness=true. Elapsed: 4.05530509s
    May  1 20:57:05.521: INFO: Pod "liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3" satisfied condition "not pending"
    May  1 20:57:05.521: INFO: Started pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 in namespace container-probe-5218
    STEP: checking the pod's current state and verifying that restartCount is present 05/01/23 20:57:05.521
    May  1 20:57:05.542: INFO: Initial restart count of pod liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 is 0
    May  1 20:57:23.880: INFO: Restart count of pod container-probe-5218/liveness-bf2725be-a3e1-4ff6-9f2d-2047a0c69de3 is now 1 (18.338360318s elapsed)
    STEP: deleting the pod 05/01/23 20:57:23.88
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 20:57:23.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5218" for this suite. 05/01/23 20:57:23.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:24.018
May  1 20:57:24.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename services 05/01/23 20:57:24.02
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:24.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:24.142
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-5679 05/01/23 20:57:24.161
STEP: creating service affinity-nodeport-transition in namespace services-5679 05/01/23 20:57:24.162
STEP: creating replication controller affinity-nodeport-transition in namespace services-5679 05/01/23 20:57:24.396
I0501 20:57:24.471154      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5679, replica count: 3
I0501 20:57:27.523051      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:57:30.531548      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:57:30.602: INFO: Creating new exec pod
May  1 20:57:30.671: INFO: Waiting up to 5m0s for pod "execpod-affinitykzxj2" in namespace "services-5679" to be "running"
May  1 20:57:30.723: INFO: Pod "execpod-affinitykzxj2": Phase="Pending", Reason="", readiness=false. Elapsed: 51.604832ms
May  1 20:57:32.746: INFO: Pod "execpod-affinitykzxj2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074559912s
May  1 20:57:34.744: INFO: Pod "execpod-affinitykzxj2": Phase="Running", Reason="", readiness=true. Elapsed: 4.072418356s
May  1 20:57:34.744: INFO: Pod "execpod-affinitykzxj2" satisfied condition "running"
May  1 20:57:35.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May  1 20:57:36.492: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May  1 20:57:36.492: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:57:36.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.248.54 80'
May  1 20:57:37.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.248.54 80\nConnection to 172.21.248.54 80 port [tcp/http] succeeded!\n"
May  1 20:57:37.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:57:37.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30308'
May  1 20:57:37.528: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30308\nConnection to 10.45.145.126 30308 port [tcp/*] succeeded!\n"
May  1 20:57:37.528: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:57:37.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30308'
May  1 20:57:38.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30308\nConnection to 10.45.145.71 30308 port [tcp/*] succeeded!\n"
May  1 20:57:38.086: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May  1 20:57:38.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30308/ ; done'
May  1 20:57:39.040: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n"
May  1 20:57:39.040: INFO: stdout: "\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr"
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30308/ ; done'
May  1 20:57:39.968: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n"
May  1 20:57:39.968: INFO: stdout: "\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr"
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
May  1 20:57:39.969: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5679, will wait for the garbage collector to delete the pods 05/01/23 20:57:40.06
May  1 20:57:40.164: INFO: Deleting ReplicationController affinity-nodeport-transition took: 34.839147ms
May  1 20:57:40.264: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.265613ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
May  1 20:57:43.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5679" for this suite. 05/01/23 20:57:43.815
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":349,"skipped":6401,"failed":0}
------------------------------
• [SLOW TEST] [19.831 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:24.018
    May  1 20:57:24.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename services 05/01/23 20:57:24.02
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:24.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:24.142
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-5679 05/01/23 20:57:24.161
    STEP: creating service affinity-nodeport-transition in namespace services-5679 05/01/23 20:57:24.162
    STEP: creating replication controller affinity-nodeport-transition in namespace services-5679 05/01/23 20:57:24.396
    I0501 20:57:24.471154      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-5679, replica count: 3
    I0501 20:57:27.523051      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:57:30.531548      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:57:30.602: INFO: Creating new exec pod
    May  1 20:57:30.671: INFO: Waiting up to 5m0s for pod "execpod-affinitykzxj2" in namespace "services-5679" to be "running"
    May  1 20:57:30.723: INFO: Pod "execpod-affinitykzxj2": Phase="Pending", Reason="", readiness=false. Elapsed: 51.604832ms
    May  1 20:57:32.746: INFO: Pod "execpod-affinitykzxj2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074559912s
    May  1 20:57:34.744: INFO: Pod "execpod-affinitykzxj2": Phase="Running", Reason="", readiness=true. Elapsed: 4.072418356s
    May  1 20:57:34.744: INFO: Pod "execpod-affinitykzxj2" satisfied condition "running"
    May  1 20:57:35.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    May  1 20:57:36.492: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May  1 20:57:36.492: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:57:36.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.248.54 80'
    May  1 20:57:37.014: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.248.54 80\nConnection to 172.21.248.54 80 port [tcp/http] succeeded!\n"
    May  1 20:57:37.014: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:57:37.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.126 30308'
    May  1 20:57:37.528: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.126 30308\nConnection to 10.45.145.126 30308 port [tcp/*] succeeded!\n"
    May  1 20:57:37.528: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:57:37.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.45.145.71 30308'
    May  1 20:57:38.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.45.145.71 30308\nConnection to 10.45.145.71 30308 port [tcp/*] succeeded!\n"
    May  1 20:57:38.086: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    May  1 20:57:38.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30308/ ; done'
    May  1 20:57:39.040: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n"
    May  1 20:57:39.040: INFO: stdout: "\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-tq45b\naffinity-nodeport-transition-2zdz5\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr"
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-tq45b
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-2zdz5
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.040: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=services-5679 exec execpod-affinitykzxj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.145.124:30308/ ; done'
    May  1 20:57:39.968: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.145.124:30308/\n"
    May  1 20:57:39.968: INFO: stdout: "\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr\naffinity-nodeport-transition-pdmsr"
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.968: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Received response from host: affinity-nodeport-transition-pdmsr
    May  1 20:57:39.969: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5679, will wait for the garbage collector to delete the pods 05/01/23 20:57:40.06
    May  1 20:57:40.164: INFO: Deleting ReplicationController affinity-nodeport-transition took: 34.839147ms
    May  1 20:57:40.264: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.265613ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    May  1 20:57:43.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5679" for this suite. 05/01/23 20:57:43.815
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:43.854
May  1 20:57:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:57:43.857
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:43.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:43.957
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May  1 20:57:43.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
May  1 20:57:47.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3364" for this suite. 05/01/23 20:57:47.744
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":350,"skipped":6421,"failed":0}
------------------------------
• [3.956 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:43.854
    May  1 20:57:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename custom-resource-definition 05/01/23 20:57:43.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:43.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:43.957
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May  1 20:57:43.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    May  1 20:57:47.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-3364" for this suite. 05/01/23 20:57:47.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:47.821
May  1 20:57:47.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pods 05/01/23 20:57:47.823
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:48.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:48.021
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
May  1 20:57:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: creating the pod 05/01/23 20:57:48.044
STEP: submitting the pod to kubernetes 05/01/23 20:57:48.044
May  1 20:57:48.137: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf" in namespace "pods-3216" to be "running and ready"
May  1 20:57:48.195: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Pending", Reason="", readiness=false. Elapsed: 57.648205ms
May  1 20:57:48.195: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Pending, waiting for it to be Running (with Ready = true)
May  1 20:57:50.217: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080027786s
May  1 20:57:50.218: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Pending, waiting for it to be Running (with Ready = true)
May  1 20:57:52.230: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.093064646s
May  1 20:57:52.231: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Running (Ready = true)
May  1 20:57:52.231: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
May  1 20:57:52.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3216" for this suite. 05/01/23 20:57:52.395
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":351,"skipped":6490,"failed":0}
------------------------------
• [4.603 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:47.821
    May  1 20:57:47.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pods 05/01/23 20:57:47.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:48.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:48.021
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    May  1 20:57:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: creating the pod 05/01/23 20:57:48.044
    STEP: submitting the pod to kubernetes 05/01/23 20:57:48.044
    May  1 20:57:48.137: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf" in namespace "pods-3216" to be "running and ready"
    May  1 20:57:48.195: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Pending", Reason="", readiness=false. Elapsed: 57.648205ms
    May  1 20:57:48.195: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:57:50.217: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080027786s
    May  1 20:57:50.218: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:57:52.230: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.093064646s
    May  1 20:57:52.231: INFO: The phase of Pod pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf is Running (Ready = true)
    May  1 20:57:52.231: INFO: Pod "pod-logs-websocket-644e75c7-964a-4add-b95c-640d2c7378cf" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    May  1 20:57:52.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3216" for this suite. 05/01/23 20:57:52.395
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:52.424
May  1 20:57:52.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename projected 05/01/23 20:57:52.427
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:52.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:52.526
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-8c3bad80-7e81-45e8-9c68-c79401001542 05/01/23 20:57:52.549
STEP: Creating a pod to test consume secrets 05/01/23 20:57:52.571
May  1 20:57:52.670: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a" in namespace "projected-6580" to be "Succeeded or Failed"
May  1 20:57:52.699: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.281416ms
May  1 20:57:54.797: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127103083s
May  1 20:57:56.724: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053915693s
May  1 20:57:58.727: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056713488s
STEP: Saw pod success 05/01/23 20:57:58.727
May  1 20:57:58.727: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a" satisfied condition "Succeeded or Failed"
May  1 20:57:58.749: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a container projected-secret-volume-test: <nil>
STEP: delete the pod 05/01/23 20:57:58.83
May  1 20:57:58.924: INFO: Waiting for pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a to disappear
May  1 20:57:58.947: INFO: Pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
May  1 20:57:58.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6580" for this suite. 05/01/23 20:57:58.992
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":352,"skipped":6493,"failed":0}
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:52.424
    May  1 20:57:52.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename projected 05/01/23 20:57:52.427
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:52.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:52.526
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-8c3bad80-7e81-45e8-9c68-c79401001542 05/01/23 20:57:52.549
    STEP: Creating a pod to test consume secrets 05/01/23 20:57:52.571
    May  1 20:57:52.670: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a" in namespace "projected-6580" to be "Succeeded or Failed"
    May  1 20:57:52.699: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.281416ms
    May  1 20:57:54.797: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127103083s
    May  1 20:57:56.724: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053915693s
    May  1 20:57:58.727: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.056713488s
    STEP: Saw pod success 05/01/23 20:57:58.727
    May  1 20:57:58.727: INFO: Pod "pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a" satisfied condition "Succeeded or Failed"
    May  1 20:57:58.749: INFO: Trying to get logs from node 10.45.145.124 pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/01/23 20:57:58.83
    May  1 20:57:58.924: INFO: Waiting for pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a to disappear
    May  1 20:57:58.947: INFO: Pod pod-projected-secrets-2277a527-70ea-4e8f-a44f-5678ae0a604a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    May  1 20:57:58.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6580" for this suite. 05/01/23 20:57:58.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:59.025
May  1 20:57:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename watch 05/01/23 20:57:59.027
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:59.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:59.246
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/01/23 20:57:59.293
STEP: creating a new configmap 05/01/23 20:57:59.336
STEP: modifying the configmap once 05/01/23 20:57:59.358
STEP: closing the watch once it receives two notifications 05/01/23 20:57:59.4
May  1 20:57:59.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145075 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:57:59.401: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145079 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/01/23 20:57:59.401
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/01/23 20:57:59.455
STEP: deleting the configmap 05/01/23 20:57:59.49
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/01/23 20:57:59.524
May  1 20:57:59.524: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145083 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May  1 20:57:59.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145088 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
May  1 20:57:59.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8388" for this suite. 05/01/23 20:57:59.556
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":353,"skipped":6502,"failed":0}
------------------------------
• [0.580 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:59.025
    May  1 20:57:59.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename watch 05/01/23 20:57:59.027
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:59.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:59.246
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/01/23 20:57:59.293
    STEP: creating a new configmap 05/01/23 20:57:59.336
    STEP: modifying the configmap once 05/01/23 20:57:59.358
    STEP: closing the watch once it receives two notifications 05/01/23 20:57:59.4
    May  1 20:57:59.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145075 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:57:59.401: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145079 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/01/23 20:57:59.401
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/01/23 20:57:59.455
    STEP: deleting the configmap 05/01/23 20:57:59.49
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/01/23 20:57:59.524
    May  1 20:57:59.524: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145083 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May  1 20:57:59.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8388  a8ded180-65a4-4d38-a192-738f384f2650 145088 0 2023-05-01 20:57:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-01 20:57:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    May  1 20:57:59.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8388" for this suite. 05/01/23 20:57:59.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:57:59.609
May  1 20:57:59.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename kubectl 05/01/23 20:57:59.612
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:59.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:59.819
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 05/01/23 20:57:59.875
May  1 20:57:59.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1671 api-versions'
May  1 20:58:00.040: INFO: stderr: ""
May  1 20:58:00.040: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
May  1 20:58:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1671" for this suite. 05/01/23 20:58:00.076
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":354,"skipped":6524,"failed":0}
------------------------------
• [0.499 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:57:59.609
    May  1 20:57:59.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename kubectl 05/01/23 20:57:59.612
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:57:59.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:57:59.819
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 05/01/23 20:57:59.875
    May  1 20:57:59.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2919092296 --namespace=kubectl-1671 api-versions'
    May  1 20:58:00.040: INFO: stderr: ""
    May  1 20:58:00.040: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    May  1 20:58:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1671" for this suite. 05/01/23 20:58:00.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:58:00.111
May  1 20:58:00.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename var-expansion 05/01/23 20:58:00.114
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:00.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:00.253
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 05/01/23 20:58:00.272
May  1 20:58:00.388: INFO: Waiting up to 5m0s for pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a" in namespace "var-expansion-9650" to be "Succeeded or Failed"
May  1 20:58:00.428: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.132313ms
May  1 20:58:02.455: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06700506s
May  1 20:58:04.449: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060206083s
May  1 20:58:06.456: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067945615s
STEP: Saw pod success 05/01/23 20:58:06.457
May  1 20:58:06.457: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a" satisfied condition "Succeeded or Failed"
May  1 20:58:06.480: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a container dapi-container: <nil>
STEP: delete the pod 05/01/23 20:58:06.532
May  1 20:58:06.598: INFO: Waiting for pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a to disappear
May  1 20:58:06.626: INFO: Pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
May  1 20:58:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9650" for this suite. 05/01/23 20:58:06.651
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":355,"skipped":6545,"failed":0}
------------------------------
• [SLOW TEST] [6.566 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:58:00.111
    May  1 20:58:00.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename var-expansion 05/01/23 20:58:00.114
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:00.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:00.253
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 05/01/23 20:58:00.272
    May  1 20:58:00.388: INFO: Waiting up to 5m0s for pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a" in namespace "var-expansion-9650" to be "Succeeded or Failed"
    May  1 20:58:00.428: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 39.132313ms
    May  1 20:58:02.455: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06700506s
    May  1 20:58:04.449: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060206083s
    May  1 20:58:06.456: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067945615s
    STEP: Saw pod success 05/01/23 20:58:06.457
    May  1 20:58:06.457: INFO: Pod "var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a" satisfied condition "Succeeded or Failed"
    May  1 20:58:06.480: INFO: Trying to get logs from node 10.45.145.124 pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a container dapi-container: <nil>
    STEP: delete the pod 05/01/23 20:58:06.532
    May  1 20:58:06.598: INFO: Waiting for pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a to disappear
    May  1 20:58:06.626: INFO: Pod var-expansion-1362adb4-84e5-4cb5-8842-2a4c650c8d7a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    May  1 20:58:06.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9650" for this suite. 05/01/23 20:58:06.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:58:06.683
May  1 20:58:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename proxy 05/01/23 20:58:06.685
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:06.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:06.809
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/01/23 20:58:07.005
STEP: creating replication controller proxy-service-q5jwz in namespace proxy-4524 05/01/23 20:58:07.006
I0501 20:58:07.048409      21 runners.go:193] Created replication controller with name: proxy-service-q5jwz, namespace: proxy-4524, replica count: 1
I0501 20:58:08.100837      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:58:09.101198      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0501 20:58:10.102387      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0501 20:58:11.103532      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  1 20:58:11.119: INFO: setup took 4.292873782s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/01/23 20:58:11.12
May  1 20:58:11.169: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 48.000388ms)
May  1 20:58:11.171: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 51.303268ms)
May  1 20:58:11.211: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 89.209017ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 89.112332ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 89.931568ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 91.066472ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 92.261224ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 91.808901ms)
May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 90.278901ms)
May  1 20:58:11.216: INFO: (0) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 95.100326ms)
May  1 20:58:11.216: INFO: (0) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 92.983922ms)
May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 134.673746ms)
May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 134.401447ms)
May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 135.992817ms)
May  1 20:58:11.258: INFO: (0) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 133.835526ms)
May  1 20:58:11.258: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 134.34557ms)
May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 38.83246ms)
May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 37.994699ms)
May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 38.581801ms)
May  1 20:58:11.299: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 40.494983ms)
May  1 20:58:11.299: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 39.096983ms)
May  1 20:58:11.300: INFO: (1) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 40.96562ms)
May  1 20:58:11.300: INFO: (1) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 39.733964ms)
May  1 20:58:11.303: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 42.38864ms)
May  1 20:58:11.303: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 40.52306ms)
May  1 20:58:11.306: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 45.014288ms)
May  1 20:58:11.306: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 43.792975ms)
May  1 20:58:11.315: INFO: (1) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 56.510253ms)
May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 55.452272ms)
May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 53.227996ms)
May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 55.097874ms)
May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.22627ms)
May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 32.774662ms)
May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.300813ms)
May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 33.99085ms)
May  1 20:58:11.355: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 34.470737ms)
May  1 20:58:11.356: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 35.239443ms)
May  1 20:58:11.356: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.265402ms)
May  1 20:58:11.358: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.726763ms)
May  1 20:58:11.359: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 35.746301ms)
May  1 20:58:11.359: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.48828ms)
May  1 20:58:11.360: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 37.441262ms)
May  1 20:58:11.361: INFO: (2) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 44.584407ms)
May  1 20:58:11.381: INFO: (2) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 59.109123ms)
May  1 20:58:11.382: INFO: (2) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 58.56345ms)
May  1 20:58:11.382: INFO: (2) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 58.79882ms)
May  1 20:58:11.384: INFO: (2) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 62.466816ms)
May  1 20:58:11.384: INFO: (2) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 62.506796ms)
May  1 20:58:11.414: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 29.988585ms)
May  1 20:58:11.436: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 50.44234ms)
May  1 20:58:11.436: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 50.520607ms)
May  1 20:58:11.438: INFO: (3) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 51.935243ms)
May  1 20:58:11.438: INFO: (3) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 51.649805ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 53.040269ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.744545ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 54.318547ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.161607ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 53.219816ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 53.447376ms)
May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 54.476395ms)
May  1 20:58:11.448: INFO: (3) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 62.417453ms)
May  1 20:58:11.450: INFO: (3) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 64.36448ms)
May  1 20:58:11.450: INFO: (3) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 64.721928ms)
May  1 20:58:11.466: INFO: (3) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 80.977603ms)
May  1 20:58:11.517: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 47.8116ms)
May  1 20:58:11.518: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 47.552961ms)
May  1 20:58:11.518: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.041347ms)
May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 49.062041ms)
May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 51.022644ms)
May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 51.394525ms)
May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 51.842979ms)
May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 52.485561ms)
May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.027066ms)
May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 52.717078ms)
May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 50.24765ms)
May  1 20:58:11.523: INFO: (4) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 54.207958ms)
May  1 20:58:11.525: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.770042ms)
May  1 20:58:11.525: INFO: (4) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 56.208897ms)
May  1 20:58:11.528: INFO: (4) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 58.866668ms)
May  1 20:58:11.529: INFO: (4) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 59.800761ms)
May  1 20:58:11.585: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.078608ms)
May  1 20:58:11.593: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 61.427067ms)
May  1 20:58:11.594: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 63.948958ms)
May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 64.939564ms)
May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 62.596716ms)
May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 63.863609ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 69.37854ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 71.190131ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 70.178305ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 70.401103ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 70.924959ms)
May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 72.120806ms)
May  1 20:58:11.602: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 71.516084ms)
May  1 20:58:11.610: INFO: (5) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 78.181342ms)
May  1 20:58:11.611: INFO: (5) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 80.394564ms)
May  1 20:58:11.612: INFO: (5) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 82.563917ms)
May  1 20:58:11.646: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 33.769707ms)
May  1 20:58:11.654: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 39.579069ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 45.633554ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 44.597858ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 45.447686ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 44.179009ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 45.93904ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 44.111693ms)
May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 44.940826ms)
May  1 20:58:11.662: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 47.504213ms)
May  1 20:58:11.664: INFO: (6) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 49.694279ms)
May  1 20:58:11.665: INFO: (6) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 50.501347ms)
May  1 20:58:11.668: INFO: (6) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.417749ms)
May  1 20:58:11.672: INFO: (6) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 58.808268ms)
May  1 20:58:11.673: INFO: (6) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 58.986076ms)
May  1 20:58:11.673: INFO: (6) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 58.742319ms)
May  1 20:58:11.705: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 31.646207ms)
May  1 20:58:11.710: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 36.2571ms)
May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 37.031581ms)
May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 37.610804ms)
May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 37.574475ms)
May  1 20:58:11.726: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 52.876944ms)
May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 53.496295ms)
May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.867983ms)
May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 52.825422ms)
May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 52.808773ms)
May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.508793ms)
May  1 20:58:11.759: INFO: (7) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 85.554101ms)
May  1 20:58:11.759: INFO: (7) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 85.686374ms)
May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 85.675126ms)
May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 85.94612ms)
May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 86.099472ms)
May  1 20:58:11.798: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 36.896143ms)
May  1 20:58:11.803: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 42.248759ms)
May  1 20:58:11.804: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 42.336366ms)
May  1 20:58:11.804: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 43.045519ms)
May  1 20:58:11.807: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 45.00002ms)
May  1 20:58:11.807: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 45.119029ms)
May  1 20:58:11.808: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 47.420716ms)
May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 47.520281ms)
May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 47.368771ms)
May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 47.648369ms)
May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 47.849387ms)
May  1 20:58:11.811: INFO: (8) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 49.447575ms)
May  1 20:58:11.811: INFO: (8) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 49.729746ms)
May  1 20:58:11.812: INFO: (8) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 50.783923ms)
May  1 20:58:11.812: INFO: (8) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 51.019705ms)
May  1 20:58:11.816: INFO: (8) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 54.282869ms)
May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.626935ms)
May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 35.622492ms)
May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 33.90708ms)
May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 35.923592ms)
May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 34.458923ms)
May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 34.838464ms)
May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 35.046364ms)
May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 35.396474ms)
May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 35.294244ms)
May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 38.002521ms)
May  1 20:58:11.861: INFO: (9) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 43.072521ms)
May  1 20:58:11.862: INFO: (9) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 44.675448ms)
May  1 20:58:11.863: INFO: (9) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 46.042644ms)
May  1 20:58:11.863: INFO: (9) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 45.794358ms)
May  1 20:58:11.864: INFO: (9) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 46.259934ms)
May  1 20:58:11.864: INFO: (9) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 47.393024ms)
May  1 20:58:11.893: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 29.495134ms)
May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 38.322527ms)
May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 38.371329ms)
May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 38.463597ms)
May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 39.048281ms)
May  1 20:58:11.904: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 39.834398ms)
May  1 20:58:11.915: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 50.983712ms)
May  1 20:58:11.917: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 53.077922ms)
May  1 20:58:11.918: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 53.187378ms)
May  1 20:58:11.922: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 57.955265ms)
May  1 20:58:11.926: INFO: (10) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 62.161443ms)
May  1 20:58:11.926: INFO: (10) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 62.259002ms)
May  1 20:58:11.944: INFO: (10) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 79.433213ms)
May  1 20:58:11.944: INFO: (10) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 79.67725ms)
May  1 20:58:11.945: INFO: (10) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 80.602487ms)
May  1 20:58:11.949: INFO: (10) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 84.574295ms)
May  1 20:58:11.986: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.068066ms)
May  1 20:58:11.988: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 39.281736ms)
May  1 20:58:11.988: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 38.158581ms)
May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 39.369383ms)
May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 38.371766ms)
May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 38.348732ms)
May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 38.650395ms)
May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 39.308828ms)
May  1 20:58:11.992: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 42.57836ms)
May  1 20:58:11.992: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 42.464699ms)
May  1 20:58:11.996: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 46.83807ms)
May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 57.082931ms)
May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 57.548877ms)
May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 57.419708ms)
May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 57.00509ms)
May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 57.945227ms)
May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 63.70563ms)
May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 65.444898ms)
May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 65.681637ms)
May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 64.490347ms)
May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 64.870977ms)
May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 76.169151ms)
May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 76.382342ms)
May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 76.011713ms)
May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 75.632727ms)
May  1 20:58:12.085: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 75.544698ms)
May  1 20:58:12.085: INFO: (12) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 76.65091ms)
May  1 20:58:12.087: INFO: (12) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 79.010508ms)
May  1 20:58:12.088: INFO: (12) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 80.25342ms)
May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 86.715587ms)
May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 87.883526ms)
May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 87.207408ms)
May  1 20:58:12.149: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 52.698242ms)
May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 52.940494ms)
May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 53.086163ms)
May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.53768ms)
May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 53.459253ms)
May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 53.882854ms)
May  1 20:58:12.151: INFO: (13) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 54.523824ms)
May  1 20:58:12.151: INFO: (13) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 55.122252ms)
May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 55.477682ms)
May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 54.996813ms)
May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.823302ms)
May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 56.294316ms)
May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 69.488956ms)
May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 69.735666ms)
May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 69.728301ms)
May  1 20:58:12.167: INFO: (13) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 70.244388ms)
May  1 20:58:12.208: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 41.201202ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 50.519998ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 55.673528ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 50.369497ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.522853ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 56.247915ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 55.534468ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 57.280707ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 56.989151ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 50.997665ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.957626ms)
May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 56.724126ms)
May  1 20:58:12.249: INFO: (14) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 80.370709ms)
May  1 20:58:12.249: INFO: (14) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 81.352708ms)
May  1 20:58:12.250: INFO: (14) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 82.333536ms)
May  1 20:58:12.254: INFO: (14) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 85.547394ms)
May  1 20:58:12.291: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.295697ms)
May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 44.586155ms)
May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 44.814431ms)
May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 43.783316ms)
May  1 20:58:12.307: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 51.264318ms)
May  1 20:58:12.307: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 51.261078ms)
May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 53.452859ms)
May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.604144ms)
May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 53.743705ms)
May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 52.624601ms)
May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 51.728972ms)
May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 52.208171ms)
May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 52.372434ms)
May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.788739ms)
May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 55.38934ms)
May  1 20:58:12.323: INFO: (15) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 66.526071ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 71.379413ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 72.965123ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 71.585394ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 71.764016ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 72.025334ms)
May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 72.132237ms)
May  1 20:58:12.397: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 73.36311ms)
May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 72.978122ms)
May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 73.799583ms)
May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 72.815363ms)
May  1 20:58:12.407: INFO: (16) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 82.560247ms)
May  1 20:58:12.408: INFO: (16) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 83.88843ms)
May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 88.065762ms)
May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 88.526464ms)
May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 87.875876ms)
May  1 20:58:12.413: INFO: (16) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 88.450042ms)
May  1 20:58:12.478: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 63.746593ms)
May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 78.901082ms)
May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 80.754591ms)
May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 78.423924ms)
May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 81.784938ms)
May  1 20:58:12.495: INFO: (17) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 81.203459ms)
May  1 20:58:12.496: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 81.684657ms)
May  1 20:58:12.498: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 82.19004ms)
May  1 20:58:12.498: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 82.346268ms)
May  1 20:58:12.499: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 82.940345ms)
May  1 20:58:12.499: INFO: (17) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 84.075007ms)
May  1 20:58:12.501: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 86.191205ms)
May  1 20:58:12.506: INFO: (17) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 90.742882ms)
May  1 20:58:12.508: INFO: (17) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 94.771747ms)
May  1 20:58:12.509: INFO: (17) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 94.394413ms)
May  1 20:58:12.509: INFO: (17) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 93.774533ms)
May  1 20:58:12.561: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 50.574675ms)
May  1 20:58:12.563: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 51.356963ms)
May  1 20:58:12.563: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 52.667313ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 54.610726ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 55.961178ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.538263ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 55.694084ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.882922ms)
May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 54.701404ms)
May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 55.63243ms)
May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 55.570422ms)
May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 55.123458ms)
May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 61.059433ms)
May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 60.582134ms)
May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 59.378382ms)
May  1 20:58:12.572: INFO: (18) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 60.059234ms)
May  1 20:58:12.635: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 62.7364ms)
May  1 20:58:12.643: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 69.805185ms)
May  1 20:58:12.643: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 69.66361ms)
May  1 20:58:12.645: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 71.625692ms)
May  1 20:58:12.652: INFO: (19) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 79.669623ms)
May  1 20:58:12.653: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 80.064268ms)
May  1 20:58:12.653: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 79.718158ms)
May  1 20:58:12.654: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 80.769069ms)
May  1 20:58:12.654: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 81.079716ms)
May  1 20:58:12.655: INFO: (19) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 81.346956ms)
May  1 20:58:12.655: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 82.127646ms)
May  1 20:58:12.656: INFO: (19) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 82.289951ms)
May  1 20:58:12.656: INFO: (19) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 83.049247ms)
May  1 20:58:12.657: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 83.686156ms)
May  1 20:58:12.667: INFO: (19) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 93.788213ms)
May  1 20:58:12.667: INFO: (19) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 94.18075ms)
STEP: deleting ReplicationController proxy-service-q5jwz in namespace proxy-4524, will wait for the garbage collector to delete the pods 05/01/23 20:58:12.667
May  1 20:58:12.778: INFO: Deleting ReplicationController proxy-service-q5jwz took: 38.314909ms
May  1 20:58:12.979: INFO: Terminating ReplicationController proxy-service-q5jwz pods took: 201.507017ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
May  1 20:58:15.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4524" for this suite. 05/01/23 20:58:15.706
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":356,"skipped":6589,"failed":0}
------------------------------
• [SLOW TEST] [9.054 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:58:06.683
    May  1 20:58:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename proxy 05/01/23 20:58:06.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:06.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:06.809
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/01/23 20:58:07.005
    STEP: creating replication controller proxy-service-q5jwz in namespace proxy-4524 05/01/23 20:58:07.006
    I0501 20:58:07.048409      21 runners.go:193] Created replication controller with name: proxy-service-q5jwz, namespace: proxy-4524, replica count: 1
    I0501 20:58:08.100837      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:58:09.101198      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0501 20:58:10.102387      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0501 20:58:11.103532      21 runners.go:193] proxy-service-q5jwz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May  1 20:58:11.119: INFO: setup took 4.292873782s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/01/23 20:58:11.12
    May  1 20:58:11.169: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 48.000388ms)
    May  1 20:58:11.171: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 51.303268ms)
    May  1 20:58:11.211: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 89.209017ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 89.112332ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 89.931568ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 91.066472ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 92.261224ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 91.808901ms)
    May  1 20:58:11.212: INFO: (0) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 90.278901ms)
    May  1 20:58:11.216: INFO: (0) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 95.100326ms)
    May  1 20:58:11.216: INFO: (0) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 92.983922ms)
    May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 134.673746ms)
    May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 134.401447ms)
    May  1 20:58:11.257: INFO: (0) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 135.992817ms)
    May  1 20:58:11.258: INFO: (0) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 133.835526ms)
    May  1 20:58:11.258: INFO: (0) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 134.34557ms)
    May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 38.83246ms)
    May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 37.994699ms)
    May  1 20:58:11.298: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 38.581801ms)
    May  1 20:58:11.299: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 40.494983ms)
    May  1 20:58:11.299: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 39.096983ms)
    May  1 20:58:11.300: INFO: (1) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 40.96562ms)
    May  1 20:58:11.300: INFO: (1) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 39.733964ms)
    May  1 20:58:11.303: INFO: (1) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 42.38864ms)
    May  1 20:58:11.303: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 40.52306ms)
    May  1 20:58:11.306: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 45.014288ms)
    May  1 20:58:11.306: INFO: (1) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 43.792975ms)
    May  1 20:58:11.315: INFO: (1) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 56.510253ms)
    May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 55.452272ms)
    May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 53.227996ms)
    May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 55.097874ms)
    May  1 20:58:11.316: INFO: (1) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.22627ms)
    May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 32.774662ms)
    May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.300813ms)
    May  1 20:58:11.354: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 33.99085ms)
    May  1 20:58:11.355: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 34.470737ms)
    May  1 20:58:11.356: INFO: (2) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 35.239443ms)
    May  1 20:58:11.356: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.265402ms)
    May  1 20:58:11.358: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.726763ms)
    May  1 20:58:11.359: INFO: (2) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 35.746301ms)
    May  1 20:58:11.359: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.48828ms)
    May  1 20:58:11.360: INFO: (2) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 37.441262ms)
    May  1 20:58:11.361: INFO: (2) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 44.584407ms)
    May  1 20:58:11.381: INFO: (2) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 59.109123ms)
    May  1 20:58:11.382: INFO: (2) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 58.56345ms)
    May  1 20:58:11.382: INFO: (2) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 58.79882ms)
    May  1 20:58:11.384: INFO: (2) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 62.466816ms)
    May  1 20:58:11.384: INFO: (2) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 62.506796ms)
    May  1 20:58:11.414: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 29.988585ms)
    May  1 20:58:11.436: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 50.44234ms)
    May  1 20:58:11.436: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 50.520607ms)
    May  1 20:58:11.438: INFO: (3) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 51.935243ms)
    May  1 20:58:11.438: INFO: (3) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 51.649805ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 53.040269ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.744545ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 54.318547ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.161607ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 53.219816ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 53.447376ms)
    May  1 20:58:11.439: INFO: (3) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 54.476395ms)
    May  1 20:58:11.448: INFO: (3) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 62.417453ms)
    May  1 20:58:11.450: INFO: (3) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 64.36448ms)
    May  1 20:58:11.450: INFO: (3) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 64.721928ms)
    May  1 20:58:11.466: INFO: (3) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 80.977603ms)
    May  1 20:58:11.517: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 47.8116ms)
    May  1 20:58:11.518: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 47.552961ms)
    May  1 20:58:11.518: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.041347ms)
    May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 49.062041ms)
    May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 51.022644ms)
    May  1 20:58:11.519: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 51.394525ms)
    May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 51.842979ms)
    May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 52.485561ms)
    May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.027066ms)
    May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 52.717078ms)
    May  1 20:58:11.520: INFO: (4) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 50.24765ms)
    May  1 20:58:11.523: INFO: (4) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 54.207958ms)
    May  1 20:58:11.525: INFO: (4) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.770042ms)
    May  1 20:58:11.525: INFO: (4) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 56.208897ms)
    May  1 20:58:11.528: INFO: (4) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 58.866668ms)
    May  1 20:58:11.529: INFO: (4) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 59.800761ms)
    May  1 20:58:11.585: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.078608ms)
    May  1 20:58:11.593: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 61.427067ms)
    May  1 20:58:11.594: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 63.948958ms)
    May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 64.939564ms)
    May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 62.596716ms)
    May  1 20:58:11.595: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 63.863609ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 69.37854ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 71.190131ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 70.178305ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 70.401103ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 70.924959ms)
    May  1 20:58:11.601: INFO: (5) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 72.120806ms)
    May  1 20:58:11.602: INFO: (5) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 71.516084ms)
    May  1 20:58:11.610: INFO: (5) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 78.181342ms)
    May  1 20:58:11.611: INFO: (5) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 80.394564ms)
    May  1 20:58:11.612: INFO: (5) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 82.563917ms)
    May  1 20:58:11.646: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 33.769707ms)
    May  1 20:58:11.654: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 39.579069ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 45.633554ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 44.597858ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 45.447686ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 44.179009ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 45.93904ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 44.111693ms)
    May  1 20:58:11.658: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 44.940826ms)
    May  1 20:58:11.662: INFO: (6) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 47.504213ms)
    May  1 20:58:11.664: INFO: (6) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 49.694279ms)
    May  1 20:58:11.665: INFO: (6) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 50.501347ms)
    May  1 20:58:11.668: INFO: (6) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.417749ms)
    May  1 20:58:11.672: INFO: (6) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 58.808268ms)
    May  1 20:58:11.673: INFO: (6) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 58.986076ms)
    May  1 20:58:11.673: INFO: (6) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 58.742319ms)
    May  1 20:58:11.705: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 31.646207ms)
    May  1 20:58:11.710: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 36.2571ms)
    May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 37.031581ms)
    May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 37.610804ms)
    May  1 20:58:11.711: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 37.574475ms)
    May  1 20:58:11.726: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 52.876944ms)
    May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 53.496295ms)
    May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.867983ms)
    May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 52.825422ms)
    May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 52.808773ms)
    May  1 20:58:11.727: INFO: (7) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 53.508793ms)
    May  1 20:58:11.759: INFO: (7) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 85.554101ms)
    May  1 20:58:11.759: INFO: (7) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 85.686374ms)
    May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 85.675126ms)
    May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 85.94612ms)
    May  1 20:58:11.760: INFO: (7) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 86.099472ms)
    May  1 20:58:11.798: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 36.896143ms)
    May  1 20:58:11.803: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 42.248759ms)
    May  1 20:58:11.804: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 42.336366ms)
    May  1 20:58:11.804: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 43.045519ms)
    May  1 20:58:11.807: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 45.00002ms)
    May  1 20:58:11.807: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 45.119029ms)
    May  1 20:58:11.808: INFO: (8) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 47.420716ms)
    May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 47.520281ms)
    May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 47.368771ms)
    May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 47.648369ms)
    May  1 20:58:11.809: INFO: (8) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 47.849387ms)
    May  1 20:58:11.811: INFO: (8) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 49.447575ms)
    May  1 20:58:11.811: INFO: (8) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 49.729746ms)
    May  1 20:58:11.812: INFO: (8) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 50.783923ms)
    May  1 20:58:11.812: INFO: (8) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 51.019705ms)
    May  1 20:58:11.816: INFO: (8) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 54.282869ms)
    May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 34.626935ms)
    May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 35.622492ms)
    May  1 20:58:11.852: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 33.90708ms)
    May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 35.923592ms)
    May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 34.458923ms)
    May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 34.838464ms)
    May  1 20:58:11.853: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 35.046364ms)
    May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 35.396474ms)
    May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 35.294244ms)
    May  1 20:58:11.854: INFO: (9) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 38.002521ms)
    May  1 20:58:11.861: INFO: (9) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 43.072521ms)
    May  1 20:58:11.862: INFO: (9) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 44.675448ms)
    May  1 20:58:11.863: INFO: (9) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 46.042644ms)
    May  1 20:58:11.863: INFO: (9) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 45.794358ms)
    May  1 20:58:11.864: INFO: (9) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 46.259934ms)
    May  1 20:58:11.864: INFO: (9) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 47.393024ms)
    May  1 20:58:11.893: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 29.495134ms)
    May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 38.322527ms)
    May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 38.371329ms)
    May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 38.463597ms)
    May  1 20:58:11.903: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 39.048281ms)
    May  1 20:58:11.904: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 39.834398ms)
    May  1 20:58:11.915: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 50.983712ms)
    May  1 20:58:11.917: INFO: (10) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 53.077922ms)
    May  1 20:58:11.918: INFO: (10) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 53.187378ms)
    May  1 20:58:11.922: INFO: (10) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 57.955265ms)
    May  1 20:58:11.926: INFO: (10) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 62.161443ms)
    May  1 20:58:11.926: INFO: (10) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 62.259002ms)
    May  1 20:58:11.944: INFO: (10) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 79.433213ms)
    May  1 20:58:11.944: INFO: (10) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 79.67725ms)
    May  1 20:58:11.945: INFO: (10) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 80.602487ms)
    May  1 20:58:11.949: INFO: (10) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 84.574295ms)
    May  1 20:58:11.986: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.068066ms)
    May  1 20:58:11.988: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 39.281736ms)
    May  1 20:58:11.988: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 38.158581ms)
    May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 39.369383ms)
    May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 38.371766ms)
    May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 38.348732ms)
    May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 38.650395ms)
    May  1 20:58:11.989: INFO: (11) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 39.308828ms)
    May  1 20:58:11.992: INFO: (11) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 42.57836ms)
    May  1 20:58:11.992: INFO: (11) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 42.464699ms)
    May  1 20:58:11.996: INFO: (11) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 46.83807ms)
    May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 57.082931ms)
    May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 57.548877ms)
    May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 57.419708ms)
    May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 57.00509ms)
    May  1 20:58:12.007: INFO: (11) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 57.945227ms)
    May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 63.70563ms)
    May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 65.444898ms)
    May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 65.681637ms)
    May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 64.490347ms)
    May  1 20:58:12.073: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 64.870977ms)
    May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 76.169151ms)
    May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 76.382342ms)
    May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 76.011713ms)
    May  1 20:58:12.084: INFO: (12) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 75.632727ms)
    May  1 20:58:12.085: INFO: (12) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 75.544698ms)
    May  1 20:58:12.085: INFO: (12) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 76.65091ms)
    May  1 20:58:12.087: INFO: (12) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 79.010508ms)
    May  1 20:58:12.088: INFO: (12) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 80.25342ms)
    May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 86.715587ms)
    May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 87.883526ms)
    May  1 20:58:12.095: INFO: (12) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 87.207408ms)
    May  1 20:58:12.149: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 52.698242ms)
    May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 52.940494ms)
    May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 53.086163ms)
    May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 53.53768ms)
    May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 53.459253ms)
    May  1 20:58:12.150: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 53.882854ms)
    May  1 20:58:12.151: INFO: (13) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 54.523824ms)
    May  1 20:58:12.151: INFO: (13) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 55.122252ms)
    May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 55.477682ms)
    May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 54.996813ms)
    May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.823302ms)
    May  1 20:58:12.152: INFO: (13) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 56.294316ms)
    May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 69.488956ms)
    May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 69.735666ms)
    May  1 20:58:12.166: INFO: (13) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 69.728301ms)
    May  1 20:58:12.167: INFO: (13) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 70.244388ms)
    May  1 20:58:12.208: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 41.201202ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 50.519998ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 55.673528ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 50.369497ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.522853ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 56.247915ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 55.534468ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 57.280707ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 56.989151ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 50.997665ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.957626ms)
    May  1 20:58:12.224: INFO: (14) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 56.724126ms)
    May  1 20:58:12.249: INFO: (14) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 80.370709ms)
    May  1 20:58:12.249: INFO: (14) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 81.352708ms)
    May  1 20:58:12.250: INFO: (14) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 82.333536ms)
    May  1 20:58:12.254: INFO: (14) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 85.547394ms)
    May  1 20:58:12.291: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 36.295697ms)
    May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 44.586155ms)
    May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 44.814431ms)
    May  1 20:58:12.302: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 43.783316ms)
    May  1 20:58:12.307: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 51.264318ms)
    May  1 20:58:12.307: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 51.261078ms)
    May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 53.452859ms)
    May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 52.604144ms)
    May  1 20:58:12.309: INFO: (15) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 53.743705ms)
    May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 52.624601ms)
    May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 51.728972ms)
    May  1 20:58:12.310: INFO: (15) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 52.208171ms)
    May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 52.372434ms)
    May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.788739ms)
    May  1 20:58:12.311: INFO: (15) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 55.38934ms)
    May  1 20:58:12.323: INFO: (15) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 66.526071ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 71.379413ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 72.965123ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 71.585394ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 71.764016ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 72.025334ms)
    May  1 20:58:12.396: INFO: (16) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 72.132237ms)
    May  1 20:58:12.397: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 73.36311ms)
    May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 72.978122ms)
    May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 73.799583ms)
    May  1 20:58:12.398: INFO: (16) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 72.815363ms)
    May  1 20:58:12.407: INFO: (16) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 82.560247ms)
    May  1 20:58:12.408: INFO: (16) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 83.88843ms)
    May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 88.065762ms)
    May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 88.526464ms)
    May  1 20:58:12.412: INFO: (16) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 87.875876ms)
    May  1 20:58:12.413: INFO: (16) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 88.450042ms)
    May  1 20:58:12.478: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 63.746593ms)
    May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 78.901082ms)
    May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 80.754591ms)
    May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 78.423924ms)
    May  1 20:58:12.494: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 81.784938ms)
    May  1 20:58:12.495: INFO: (17) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 81.203459ms)
    May  1 20:58:12.496: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 81.684657ms)
    May  1 20:58:12.498: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 82.19004ms)
    May  1 20:58:12.498: INFO: (17) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 82.346268ms)
    May  1 20:58:12.499: INFO: (17) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 82.940345ms)
    May  1 20:58:12.499: INFO: (17) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 84.075007ms)
    May  1 20:58:12.501: INFO: (17) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 86.191205ms)
    May  1 20:58:12.506: INFO: (17) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 90.742882ms)
    May  1 20:58:12.508: INFO: (17) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 94.771747ms)
    May  1 20:58:12.509: INFO: (17) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 94.394413ms)
    May  1 20:58:12.509: INFO: (17) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 93.774533ms)
    May  1 20:58:12.561: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 50.574675ms)
    May  1 20:58:12.563: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 51.356963ms)
    May  1 20:58:12.563: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 52.667313ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 54.610726ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 55.961178ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 56.538263ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 55.694084ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 55.882922ms)
    May  1 20:58:12.566: INFO: (18) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 54.701404ms)
    May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 55.63243ms)
    May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 55.570422ms)
    May  1 20:58:12.567: INFO: (18) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 55.123458ms)
    May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 61.059433ms)
    May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 60.582134ms)
    May  1 20:58:12.571: INFO: (18) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 59.378382ms)
    May  1 20:58:12.572: INFO: (18) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 60.059234ms)
    May  1 20:58:12.635: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:1080/proxy/rewriteme">test<... (200; 62.7364ms)
    May  1 20:58:12.643: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:160/proxy/: foo (200; 69.805185ms)
    May  1 20:58:12.643: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:160/proxy/: foo (200; 69.66361ms)
    May  1 20:58:12.645: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt:162/proxy/: bar (200; 71.625692ms)
    May  1 20:58:12.652: INFO: (19) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname1/proxy/: foo (200; 79.669623ms)
    May  1 20:58:12.653: INFO: (19) /api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/proxy-service-q5jwz-277tt/proxy/rewriteme">test</a> (200; 80.064268ms)
    May  1 20:58:12.653: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:162/proxy/: bar (200; 79.718158ms)
    May  1 20:58:12.654: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:462/proxy/: tls qux (200; 80.769069ms)
    May  1 20:58:12.654: INFO: (19) /api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/http:proxy-service-q5jwz-277tt:1080/proxy/rewriteme">... (200; 81.079716ms)
    May  1 20:58:12.655: INFO: (19) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname2/proxy/: tls qux (200; 81.346956ms)
    May  1 20:58:12.655: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/: <a href="/api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:443/proxy/tlsrewritem... (200; 82.127646ms)
    May  1 20:58:12.656: INFO: (19) /api/v1/namespaces/proxy-4524/services/https:proxy-service-q5jwz:tlsportname1/proxy/: tls baz (200; 82.289951ms)
    May  1 20:58:12.656: INFO: (19) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname1/proxy/: foo (200; 83.049247ms)
    May  1 20:58:12.657: INFO: (19) /api/v1/namespaces/proxy-4524/pods/https:proxy-service-q5jwz-277tt:460/proxy/: tls baz (200; 83.686156ms)
    May  1 20:58:12.667: INFO: (19) /api/v1/namespaces/proxy-4524/services/http:proxy-service-q5jwz:portname2/proxy/: bar (200; 93.788213ms)
    May  1 20:58:12.667: INFO: (19) /api/v1/namespaces/proxy-4524/services/proxy-service-q5jwz:portname2/proxy/: bar (200; 94.18075ms)
    STEP: deleting ReplicationController proxy-service-q5jwz in namespace proxy-4524, will wait for the garbage collector to delete the pods 05/01/23 20:58:12.667
    May  1 20:58:12.778: INFO: Deleting ReplicationController proxy-service-q5jwz took: 38.314909ms
    May  1 20:58:12.979: INFO: Terminating ReplicationController proxy-service-q5jwz pods took: 201.507017ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    May  1 20:58:15.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4524" for this suite. 05/01/23 20:58:15.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:58:15.74
May  1 20:58:15.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename subpath 05/01/23 20:58:15.741
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:15.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:15.841
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/01/23 20:58:15.859
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-4cmh 05/01/23 20:58:15.909
STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:58:15.909
May  1 20:58:15.999: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4cmh" in namespace "subpath-6070" to be "Succeeded or Failed"
May  1 20:58:16.030: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Pending", Reason="", readiness=false. Elapsed: 30.759154ms
May  1 20:58:18.078: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078511747s
May  1 20:58:20.057: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.05745425s
May  1 20:58:22.054: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.054777127s
May  1 20:58:24.061: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.061550345s
May  1 20:58:26.057: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.057916303s
May  1 20:58:28.082: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.082270641s
May  1 20:58:30.062: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.062439475s
May  1 20:58:32.056: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.056288628s
May  1 20:58:34.063: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.063722068s
May  1 20:58:36.059: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.060059117s
May  1 20:58:38.056: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 22.056621176s
May  1 20:58:40.071: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=false. Elapsed: 24.071764305s
May  1 20:58:42.074: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.074552076s
STEP: Saw pod success 05/01/23 20:58:42.095
May  1 20:58:42.095: INFO: Pod "pod-subpath-test-configmap-4cmh" satisfied condition "Succeeded or Failed"
May  1 20:58:42.124: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-configmap-4cmh container test-container-subpath-configmap-4cmh: <nil>
STEP: delete the pod 05/01/23 20:58:42.302
May  1 20:58:42.375: INFO: Waiting for pod pod-subpath-test-configmap-4cmh to disappear
May  1 20:58:42.466: INFO: Pod pod-subpath-test-configmap-4cmh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4cmh 05/01/23 20:58:42.466
May  1 20:58:42.466: INFO: Deleting pod "pod-subpath-test-configmap-4cmh" in namespace "subpath-6070"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
May  1 20:58:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6070" for this suite. 05/01/23 20:58:42.533
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":357,"skipped":6597,"failed":0}
------------------------------
• [SLOW TEST] [26.848 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:58:15.74
    May  1 20:58:15.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename subpath 05/01/23 20:58:15.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:15.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:15.841
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/01/23 20:58:15.859
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-4cmh 05/01/23 20:58:15.909
    STEP: Creating a pod to test atomic-volume-subpath 05/01/23 20:58:15.909
    May  1 20:58:15.999: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4cmh" in namespace "subpath-6070" to be "Succeeded or Failed"
    May  1 20:58:16.030: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Pending", Reason="", readiness=false. Elapsed: 30.759154ms
    May  1 20:58:18.078: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078511747s
    May  1 20:58:20.057: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.05745425s
    May  1 20:58:22.054: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.054777127s
    May  1 20:58:24.061: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.061550345s
    May  1 20:58:26.057: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.057916303s
    May  1 20:58:28.082: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.082270641s
    May  1 20:58:30.062: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.062439475s
    May  1 20:58:32.056: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.056288628s
    May  1 20:58:34.063: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.063722068s
    May  1 20:58:36.059: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.060059117s
    May  1 20:58:38.056: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=true. Elapsed: 22.056621176s
    May  1 20:58:40.071: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Running", Reason="", readiness=false. Elapsed: 24.071764305s
    May  1 20:58:42.074: INFO: Pod "pod-subpath-test-configmap-4cmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.074552076s
    STEP: Saw pod success 05/01/23 20:58:42.095
    May  1 20:58:42.095: INFO: Pod "pod-subpath-test-configmap-4cmh" satisfied condition "Succeeded or Failed"
    May  1 20:58:42.124: INFO: Trying to get logs from node 10.45.145.124 pod pod-subpath-test-configmap-4cmh container test-container-subpath-configmap-4cmh: <nil>
    STEP: delete the pod 05/01/23 20:58:42.302
    May  1 20:58:42.375: INFO: Waiting for pod pod-subpath-test-configmap-4cmh to disappear
    May  1 20:58:42.466: INFO: Pod pod-subpath-test-configmap-4cmh no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-4cmh 05/01/23 20:58:42.466
    May  1 20:58:42.466: INFO: Deleting pod "pod-subpath-test-configmap-4cmh" in namespace "subpath-6070"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    May  1 20:58:42.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6070" for this suite. 05/01/23 20:58:42.533
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:58:42.588
May  1 20:58:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename ingressclass 05/01/23 20:58:42.594
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:42.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:42.703
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/01/23 20:58:42.723
STEP: getting /apis/networking.k8s.io 05/01/23 20:58:42.751
STEP: getting /apis/networking.k8s.iov1 05/01/23 20:58:42.787
STEP: creating 05/01/23 20:58:42.811
STEP: getting 05/01/23 20:58:43.191
STEP: listing 05/01/23 20:58:43.224
STEP: watching 05/01/23 20:58:43.245
May  1 20:58:43.246: INFO: starting watch
STEP: patching 05/01/23 20:58:43.257
STEP: updating 05/01/23 20:58:43.284
May  1 20:58:43.308: INFO: waiting for watch events with expected annotations
May  1 20:58:43.309: INFO: saw patched and updated annotations
STEP: deleting 05/01/23 20:58:43.309
STEP: deleting a collection 05/01/23 20:58:43.368
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
May  1 20:58:43.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-679" for this suite. 05/01/23 20:58:43.486
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":358,"skipped":6601,"failed":0}
------------------------------
• [0.924 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:58:42.588
    May  1 20:58:42.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename ingressclass 05/01/23 20:58:42.594
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:42.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:42.703
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/01/23 20:58:42.723
    STEP: getting /apis/networking.k8s.io 05/01/23 20:58:42.751
    STEP: getting /apis/networking.k8s.iov1 05/01/23 20:58:42.787
    STEP: creating 05/01/23 20:58:42.811
    STEP: getting 05/01/23 20:58:43.191
    STEP: listing 05/01/23 20:58:43.224
    STEP: watching 05/01/23 20:58:43.245
    May  1 20:58:43.246: INFO: starting watch
    STEP: patching 05/01/23 20:58:43.257
    STEP: updating 05/01/23 20:58:43.284
    May  1 20:58:43.308: INFO: waiting for watch events with expected annotations
    May  1 20:58:43.309: INFO: saw patched and updated annotations
    STEP: deleting 05/01/23 20:58:43.309
    STEP: deleting a collection 05/01/23 20:58:43.368
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    May  1 20:58:43.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-679" for this suite. 05/01/23 20:58:43.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:58:43.519
May  1 20:58:43.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename pod-network-test 05/01/23 20:58:43.521
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:43.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:43.628
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-2305 05/01/23 20:58:43.65
STEP: creating a selector 05/01/23 20:58:43.65
STEP: Creating the service pods in kubernetes 05/01/23 20:58:43.651
May  1 20:58:43.651: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May  1 20:58:43.946: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2305" to be "running and ready"
May  1 20:58:43.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.170772ms
May  1 20:58:43.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May  1 20:58:45.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.045198792s
May  1 20:58:45.992: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:48.009: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.063175056s
May  1 20:58:48.009: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:50.022: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.075541975s
May  1 20:58:50.022: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:52.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.071815829s
May  1 20:58:52.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:53.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044935378s
May  1 20:58:53.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:55.990: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044034559s
May  1 20:58:55.990: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:57.997: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050550394s
May  1 20:58:57.997: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:58:59.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042221574s
May  1 20:58:59.989: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:59:02.014: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.068049332s
May  1 20:59:02.014: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:59:04.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.08036637s
May  1 20:59:04.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May  1 20:59:06.013: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.06655057s
May  1 20:59:06.013: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May  1 20:59:06.013: INFO: Pod "netserver-0" satisfied condition "running and ready"
May  1 20:59:06.033: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2305" to be "running and ready"
May  1 20:59:06.053: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 19.037209ms
May  1 20:59:06.053: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May  1 20:59:06.053: INFO: Pod "netserver-1" satisfied condition "running and ready"
May  1 20:59:06.076: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2305" to be "running and ready"
May  1 20:59:06.096: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.954033ms
May  1 20:59:06.096: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May  1 20:59:06.096: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/01/23 20:59:06.116
May  1 20:59:06.178: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2305" to be "running"
May  1 20:59:06.196: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766915ms
May  1 20:59:08.230: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052097913s
May  1 20:59:10.216: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038101636s
May  1 20:59:10.216: INFO: Pod "test-container-pod" satisfied condition "running"
May  1 20:59:10.236: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May  1 20:59:10.237: INFO: Breadth first check of 172.30.42.115 on host 10.45.145.124...
May  1 20:59:10.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.42.115&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:10.259: INFO: ExecWithOptions: Clientset creation
May  1 20:59:10.259: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.42.115%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 20:59:10.551: INFO: Waiting for responses: map[]
May  1 20:59:10.552: INFO: reached 172.30.42.115 after 0/1 tries
May  1 20:59:10.552: INFO: Breadth first check of 172.30.38.224 on host 10.45.145.126...
May  1 20:59:10.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.38.224&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:10.649: INFO: ExecWithOptions: Clientset creation
May  1 20:59:10.649: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.38.224%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 20:59:11.148: INFO: Waiting for responses: map[]
May  1 20:59:11.148: INFO: reached 172.30.38.224 after 0/1 tries
May  1 20:59:11.149: INFO: Breadth first check of 172.30.244.171 on host 10.45.145.71...
May  1 20:59:11.171: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.244.171&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:11.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:11.172: INFO: ExecWithOptions: Clientset creation
May  1 20:59:11.173: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.244.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May  1 20:59:11.573: INFO: Waiting for responses: map[]
May  1 20:59:11.573: INFO: reached 172.30.244.171 after 0/1 tries
May  1 20:59:11.573: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
May  1 20:59:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2305" for this suite. 05/01/23 20:59:11.607
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":359,"skipped":6638,"failed":0}
------------------------------
• [SLOW TEST] [28.118 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:58:43.519
    May  1 20:58:43.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename pod-network-test 05/01/23 20:58:43.521
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:58:43.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:58:43.628
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-2305 05/01/23 20:58:43.65
    STEP: creating a selector 05/01/23 20:58:43.65
    STEP: Creating the service pods in kubernetes 05/01/23 20:58:43.651
    May  1 20:58:43.651: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May  1 20:58:43.946: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2305" to be "running and ready"
    May  1 20:58:43.968: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.170772ms
    May  1 20:58:43.968: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:58:45.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.045198792s
    May  1 20:58:45.992: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:48.009: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.063175056s
    May  1 20:58:48.009: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:50.022: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.075541975s
    May  1 20:58:50.022: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:52.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.071815829s
    May  1 20:58:52.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:53.991: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.044935378s
    May  1 20:58:53.991: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:55.990: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044034559s
    May  1 20:58:55.990: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:57.997: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.050550394s
    May  1 20:58:57.997: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:58:59.988: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042221574s
    May  1 20:58:59.989: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:59:02.014: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.068049332s
    May  1 20:59:02.014: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:59:04.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.08036637s
    May  1 20:59:04.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May  1 20:59:06.013: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.06655057s
    May  1 20:59:06.013: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May  1 20:59:06.013: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May  1 20:59:06.033: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2305" to be "running and ready"
    May  1 20:59:06.053: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 19.037209ms
    May  1 20:59:06.053: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May  1 20:59:06.053: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May  1 20:59:06.076: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2305" to be "running and ready"
    May  1 20:59:06.096: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.954033ms
    May  1 20:59:06.096: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May  1 20:59:06.096: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/01/23 20:59:06.116
    May  1 20:59:06.178: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2305" to be "running"
    May  1 20:59:06.196: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.766915ms
    May  1 20:59:08.230: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052097913s
    May  1 20:59:10.216: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038101636s
    May  1 20:59:10.216: INFO: Pod "test-container-pod" satisfied condition "running"
    May  1 20:59:10.236: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May  1 20:59:10.237: INFO: Breadth first check of 172.30.42.115 on host 10.45.145.124...
    May  1 20:59:10.257: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.42.115&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:10.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:10.259: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:10.259: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.42.115%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 20:59:10.551: INFO: Waiting for responses: map[]
    May  1 20:59:10.552: INFO: reached 172.30.42.115 after 0/1 tries
    May  1 20:59:10.552: INFO: Breadth first check of 172.30.38.224 on host 10.45.145.126...
    May  1 20:59:10.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.38.224&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:10.649: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:10.649: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.38.224%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 20:59:11.148: INFO: Waiting for responses: map[]
    May  1 20:59:11.148: INFO: reached 172.30.38.224 after 0/1 tries
    May  1 20:59:11.149: INFO: Breadth first check of 172.30.244.171 on host 10.45.145.71...
    May  1 20:59:11.171: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.42.73:9080/dial?request=hostname&protocol=udp&host=172.30.244.171&port=8081&tries=1'] Namespace:pod-network-test-2305 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:11.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:11.172: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:11.173: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.42.73%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.244.171%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May  1 20:59:11.573: INFO: Waiting for responses: map[]
    May  1 20:59:11.573: INFO: reached 172.30.244.171 after 0/1 tries
    May  1 20:59:11.573: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    May  1 20:59:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-2305" for this suite. 05/01/23 20:59:11.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:59:11.64
May  1 20:59:11.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/01/23 20:59:11.643
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:11.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:11.761
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/01/23 20:59:11.781
STEP: Creating hostNetwork=false pod 05/01/23 20:59:11.781
May  1 20:59:11.876: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1397" to be "running and ready"
May  1 20:59:11.910: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.719777ms
May  1 20:59:11.910: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  1 20:59:13.935: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058989115s
May  1 20:59:13.938: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May  1 20:59:15.939: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.063232592s
May  1 20:59:15.939: INFO: The phase of Pod test-pod is Running (Ready = true)
May  1 20:59:15.939: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/01/23 20:59:15.958
May  1 20:59:16.020: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1397" to be "running and ready"
May  1 20:59:16.042: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.197134ms
May  1 20:59:16.043: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May  1 20:59:18.064: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044088447s
May  1 20:59:18.064: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May  1 20:59:18.064: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/01/23 20:59:18.083
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/01/23 20:59:18.084
May  1 20:59:18.084: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:18.085: INFO: ExecWithOptions: Clientset creation
May  1 20:59:18.086: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  1 20:59:18.626: INFO: Exec stderr: ""
May  1 20:59:18.626: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:18.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:18.627: INFO: ExecWithOptions: Clientset creation
May  1 20:59:18.627: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  1 20:59:18.973: INFO: Exec stderr: ""
May  1 20:59:18.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:18.975: INFO: ExecWithOptions: Clientset creation
May  1 20:59:18.975: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  1 20:59:19.270: INFO: Exec stderr: ""
May  1 20:59:19.270: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:19.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:19.272: INFO: ExecWithOptions: Clientset creation
May  1 20:59:19.273: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  1 20:59:19.651: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/01/23 20:59:19.651
May  1 20:59:19.652: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:19.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:19.655: INFO: ExecWithOptions: Clientset creation
May  1 20:59:19.655: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  1 20:59:20.023: INFO: Exec stderr: ""
May  1 20:59:20.023: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:20.025: INFO: ExecWithOptions: Clientset creation
May  1 20:59:20.025: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May  1 20:59:20.524: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/01/23 20:59:20.524
May  1 20:59:20.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:20.526: INFO: ExecWithOptions: Clientset creation
May  1 20:59:20.526: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  1 20:59:20.888: INFO: Exec stderr: ""
May  1 20:59:20.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:20.897: INFO: ExecWithOptions: Clientset creation
May  1 20:59:20.898: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May  1 20:59:21.343: INFO: Exec stderr: ""
May  1 20:59:21.343: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:21.344: INFO: ExecWithOptions: Clientset creation
May  1 20:59:21.344: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  1 20:59:21.841: INFO: Exec stderr: ""
May  1 20:59:21.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May  1 20:59:21.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
May  1 20:59:21.843: INFO: ExecWithOptions: Clientset creation
May  1 20:59:21.843: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May  1 20:59:22.224: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
May  1 20:59:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1397" for this suite. 05/01/23 20:59:22.299
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":360,"skipped":6659,"failed":0}
------------------------------
• [SLOW TEST] [10.701 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:59:11.64
    May  1 20:59:11.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/01/23 20:59:11.643
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:11.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:11.761
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/01/23 20:59:11.781
    STEP: Creating hostNetwork=false pod 05/01/23 20:59:11.781
    May  1 20:59:11.876: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1397" to be "running and ready"
    May  1 20:59:11.910: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 33.719777ms
    May  1 20:59:11.910: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:59:13.935: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058989115s
    May  1 20:59:13.938: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:59:15.939: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.063232592s
    May  1 20:59:15.939: INFO: The phase of Pod test-pod is Running (Ready = true)
    May  1 20:59:15.939: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/01/23 20:59:15.958
    May  1 20:59:16.020: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1397" to be "running and ready"
    May  1 20:59:16.042: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 22.197134ms
    May  1 20:59:16.043: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:59:18.064: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.044088447s
    May  1 20:59:18.064: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May  1 20:59:18.064: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/01/23 20:59:18.083
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/01/23 20:59:18.084
    May  1 20:59:18.084: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:18.085: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:18.086: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  1 20:59:18.626: INFO: Exec stderr: ""
    May  1 20:59:18.626: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:18.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:18.627: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:18.627: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  1 20:59:18.973: INFO: Exec stderr: ""
    May  1 20:59:18.973: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:18.975: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:18.975: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  1 20:59:19.270: INFO: Exec stderr: ""
    May  1 20:59:19.270: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:19.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:19.272: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:19.273: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  1 20:59:19.651: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/01/23 20:59:19.651
    May  1 20:59:19.652: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:19.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:19.655: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:19.655: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  1 20:59:20.023: INFO: Exec stderr: ""
    May  1 20:59:20.023: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:20.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:20.025: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:20.025: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May  1 20:59:20.524: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/01/23 20:59:20.524
    May  1 20:59:20.524: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:20.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:20.526: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:20.526: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  1 20:59:20.888: INFO: Exec stderr: ""
    May  1 20:59:20.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:20.897: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:20.898: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May  1 20:59:21.343: INFO: Exec stderr: ""
    May  1 20:59:21.343: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:21.344: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:21.344: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  1 20:59:21.841: INFO: Exec stderr: ""
    May  1 20:59:21.841: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1397 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May  1 20:59:21.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    May  1 20:59:21.843: INFO: ExecWithOptions: Clientset creation
    May  1 20:59:21.843: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1397/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May  1 20:59:22.224: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    May  1 20:59:22.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1397" for this suite. 05/01/23 20:59:22.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:59:22.346
May  1 20:59:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename container-probe 05/01/23 20:59:22.349
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:22.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:22.515
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
May  1 20:59:22.672: INFO: Waiting up to 5m0s for pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d" in namespace "container-probe-2527" to be "running and ready"
May  1 20:59:22.695: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.046395ms
May  1 20:59:22.695: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Pending, waiting for it to be Running (with Ready = true)
May  1 20:59:24.717: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045720322s
May  1 20:59:24.717: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Pending, waiting for it to be Running (with Ready = true)
May  1 20:59:26.721: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 4.049867296s
May  1 20:59:26.722: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:28.718: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 6.04597739s
May  1 20:59:28.718: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:30.735: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 8.062924713s
May  1 20:59:30.735: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:32.721: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 10.04943049s
May  1 20:59:32.721: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:34.718: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 12.046705703s
May  1 20:59:34.718: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:36.736: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 14.064176558s
May  1 20:59:36.736: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:38.717: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 16.045843967s
May  1 20:59:38.717: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:40.758: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 18.08625948s
May  1 20:59:40.758: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:42.715: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 20.043713371s
May  1 20:59:42.715: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
May  1 20:59:44.714: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=true. Elapsed: 22.042759329s
May  1 20:59:44.714: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = true)
May  1 20:59:44.714: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d" satisfied condition "running and ready"
May  1 20:59:44.756: INFO: Container started at 2023-05-01 20:59:24 +0000 UTC, pod became ready at 2023-05-01 20:59:43 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
May  1 20:59:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2527" for this suite. 05/01/23 20:59:44.782
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":361,"skipped":6680,"failed":0}
------------------------------
• [SLOW TEST] [22.470 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:59:22.346
    May  1 20:59:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename container-probe 05/01/23 20:59:22.349
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:22.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:22.515
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    May  1 20:59:22.672: INFO: Waiting up to 5m0s for pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d" in namespace "container-probe-2527" to be "running and ready"
    May  1 20:59:22.695: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.046395ms
    May  1 20:59:22.695: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:59:24.717: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045720322s
    May  1 20:59:24.717: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Pending, waiting for it to be Running (with Ready = true)
    May  1 20:59:26.721: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 4.049867296s
    May  1 20:59:26.722: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:28.718: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 6.04597739s
    May  1 20:59:28.718: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:30.735: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 8.062924713s
    May  1 20:59:30.735: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:32.721: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 10.04943049s
    May  1 20:59:32.721: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:34.718: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 12.046705703s
    May  1 20:59:34.718: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:36.736: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 14.064176558s
    May  1 20:59:36.736: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:38.717: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 16.045843967s
    May  1 20:59:38.717: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:40.758: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 18.08625948s
    May  1 20:59:40.758: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:42.715: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=false. Elapsed: 20.043713371s
    May  1 20:59:42.715: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = false)
    May  1 20:59:44.714: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d": Phase="Running", Reason="", readiness=true. Elapsed: 22.042759329s
    May  1 20:59:44.714: INFO: The phase of Pod test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d is Running (Ready = true)
    May  1 20:59:44.714: INFO: Pod "test-webserver-8b43d31e-2d66-48a5-b089-b38a7930d34d" satisfied condition "running and ready"
    May  1 20:59:44.756: INFO: Container started at 2023-05-01 20:59:24 +0000 UTC, pod became ready at 2023-05-01 20:59:43 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    May  1 20:59:44.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2527" for this suite. 05/01/23 20:59:44.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 05/01/23 20:59:44.82
May  1 20:59:44.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
STEP: Building a namespace api object, basename resourcequota 05/01/23 20:59:44.823
STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:44.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:44.998
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 05/01/23 20:59:45.021
STEP: Ensuring ResourceQuota status is calculated 05/01/23 20:59:45.056
STEP: Creating a ResourceQuota with not terminating scope 05/01/23 20:59:47.091
STEP: Ensuring ResourceQuota status is calculated 05/01/23 20:59:47.122
STEP: Creating a long running pod 05/01/23 20:59:49.143
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/01/23 20:59:49.237
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/01/23 20:59:51.264
STEP: Deleting the pod 05/01/23 20:59:53.286
STEP: Ensuring resource quota status released the pod usage 05/01/23 20:59:53.363
STEP: Creating a terminating pod 05/01/23 20:59:55.39
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/01/23 20:59:55.478
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/01/23 20:59:57.499
STEP: Deleting the pod 05/01/23 20:59:59.534
STEP: Ensuring resource quota status released the pod usage 05/01/23 20:59:59.608
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
May  1 21:00:01.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8110" for this suite. 05/01/23 21:00:01.696
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":362,"skipped":6699,"failed":0}
------------------------------
• [SLOW TEST] [16.905 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 05/01/23 20:59:44.82
    May  1 20:59:44.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2919092296
    STEP: Building a namespace api object, basename resourcequota 05/01/23 20:59:44.823
    STEP: Waiting for a default service account to be provisioned in namespace 05/01/23 20:59:44.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/01/23 20:59:44.998
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 05/01/23 20:59:45.021
    STEP: Ensuring ResourceQuota status is calculated 05/01/23 20:59:45.056
    STEP: Creating a ResourceQuota with not terminating scope 05/01/23 20:59:47.091
    STEP: Ensuring ResourceQuota status is calculated 05/01/23 20:59:47.122
    STEP: Creating a long running pod 05/01/23 20:59:49.143
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/01/23 20:59:49.237
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/01/23 20:59:51.264
    STEP: Deleting the pod 05/01/23 20:59:53.286
    STEP: Ensuring resource quota status released the pod usage 05/01/23 20:59:53.363
    STEP: Creating a terminating pod 05/01/23 20:59:55.39
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/01/23 20:59:55.478
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/01/23 20:59:57.499
    STEP: Deleting the pod 05/01/23 20:59:59.534
    STEP: Ensuring resource quota status released the pod usage 05/01/23 20:59:59.608
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    May  1 21:00:01.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8110" for this suite. 05/01/23 21:00:01.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
May  1 21:00:01.727: INFO: Running AfterSuite actions on all nodes
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May  1 21:00:01.728: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
May  1 21:00:01.728: INFO: Running AfterSuite actions on node 1
May  1 21:00:01.728: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  1 21:00:01.727: INFO: Running AfterSuite actions on all nodes
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    May  1 21:00:01.727: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    May  1 21:00:01.728: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    May  1 21:00:01.728: INFO: Running AfterSuite actions on node 1
    May  1 21:00:01.728: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.258 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 7870.227 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 2h11m11.278481571s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

