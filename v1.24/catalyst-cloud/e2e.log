I0117 04:10:17.409333      22 e2e.go:129] Starting e2e run "e9644791-5268-4049-8eb1-b98ca49fd852" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1673928617 - Will randomize all specs
Will run 356 of 6973 specs

Jan 17 04:10:19.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:10:19.241: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0117 04:10:19.240887      22 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 17 04:10:19.397: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 17 04:10:19.460: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 17 04:10:19.460: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Jan 17 04:10:19.460: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 17 04:10:19.471: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'csi-cinder-nodeplugin' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'magnum-prometheus-node-exporter' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
Jan 17 04:10:19.472: INFO: e2e test version: v1.24.9
Jan 17 04:10:19.476: INFO: kube-apiserver version: v1.24.9
Jan 17 04:10:19.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:10:19.485: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:10:19.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
W0117 04:10:19.539444      22 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 17 04:10:19.539: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jan 17 04:10:19.554: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3923
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 17 04:10:19.623: INFO: Found 0 stateful pods, waiting for 3
Jan 17 04:10:29.638: INFO: Found 1 stateful pods, waiting for 3
Jan 17 04:10:39.630: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:10:39.630: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:10:39.630: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jan 17 04:10:49.643: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:10:49.643: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:10:49.643: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:10:49.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-3923 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:10:50.368: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:10:50.368: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:10:50.368: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 17 04:11:00.434: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 17 04:11:10.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-3923 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:11:10.692: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:11:10.692: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:11:10.692: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:11:20.735: INFO: Waiting for StatefulSet statefulset-3923/ss2 to complete update
Jan 17 04:11:20.735: INFO: Waiting for Pod statefulset-3923/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 17 04:11:20.735: INFO: Waiting for Pod statefulset-3923/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 17 04:11:30.757: INFO: Waiting for StatefulSet statefulset-3923/ss2 to complete update
Jan 17 04:11:30.757: INFO: Waiting for Pod statefulset-3923/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 17 04:11:40.754: INFO: Waiting for StatefulSet statefulset-3923/ss2 to complete update
STEP: Rolling back to a previous revision
Jan 17 04:11:50.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-3923 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:11:51.029: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:11:51.029: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:11:51.029: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:12:01.105: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 17 04:12:11.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-3923 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:12:11.420: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:12:11.420: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:12:11.420: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 04:12:21.465: INFO: Deleting all statefulset in ns statefulset-3923
Jan 17 04:12:21.470: INFO: Scaling statefulset ss2 to 0
Jan 17 04:12:31.511: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:12:31.515: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 04:12:31.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3923" for this suite.

• [SLOW TEST:132.114 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":1,"skipped":31,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:12:31.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 17 04:12:32.248: INFO: Pod name wrapped-volume-race-e84ff068-0caf-4738-a100-506ac1c0112a: Found 0 pods out of 5
Jan 17 04:12:37.300: INFO: Pod name wrapped-volume-race-e84ff068-0caf-4738-a100-506ac1c0112a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e84ff068-0caf-4738-a100-506ac1c0112a in namespace emptydir-wrapper-4477, will wait for the garbage collector to delete the pods
Jan 17 04:12:53.469: INFO: Deleting ReplicationController wrapped-volume-race-e84ff068-0caf-4738-a100-506ac1c0112a took: 18.383995ms
Jan 17 04:12:53.670: INFO: Terminating ReplicationController wrapped-volume-race-e84ff068-0caf-4738-a100-506ac1c0112a pods took: 201.046086ms
STEP: Creating RC which spawns configmap-volume pods
Jan 17 04:12:56.814: INFO: Pod name wrapped-volume-race-d80a786e-ae6e-4fb7-9d87-b0826d945826: Found 0 pods out of 5
Jan 17 04:13:01.829: INFO: Pod name wrapped-volume-race-d80a786e-ae6e-4fb7-9d87-b0826d945826: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d80a786e-ae6e-4fb7-9d87-b0826d945826 in namespace emptydir-wrapper-4477, will wait for the garbage collector to delete the pods
Jan 17 04:13:13.961: INFO: Deleting ReplicationController wrapped-volume-race-d80a786e-ae6e-4fb7-9d87-b0826d945826 took: 20.049372ms
Jan 17 04:13:14.162: INFO: Terminating ReplicationController wrapped-volume-race-d80a786e-ae6e-4fb7-9d87-b0826d945826 pods took: 200.885184ms
STEP: Creating RC which spawns configmap-volume pods
Jan 17 04:13:17.019: INFO: Pod name wrapped-volume-race-7694d37d-bcc8-4f9f-8fc9-193f067f23a0: Found 0 pods out of 5
Jan 17 04:13:22.070: INFO: Pod name wrapped-volume-race-7694d37d-bcc8-4f9f-8fc9-193f067f23a0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7694d37d-bcc8-4f9f-8fc9-193f067f23a0 in namespace emptydir-wrapper-4477, will wait for the garbage collector to delete the pods
Jan 17 04:13:34.200: INFO: Deleting ReplicationController wrapped-volume-race-7694d37d-bcc8-4f9f-8fc9-193f067f23a0 took: 23.043044ms
Jan 17 04:13:34.400: INFO: Terminating ReplicationController wrapped-volume-race-7694d37d-bcc8-4f9f-8fc9-193f067f23a0 pods took: 200.300981ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 17 04:13:38.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4477" for this suite.

• [SLOW TEST:66.566 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":2,"skipped":32,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:13:38.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-510
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 17 04:13:38.335: INFO: Found 0 stateful pods, waiting for 3
Jan 17 04:13:48.354: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:13:48.354: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:13:48.354: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 17 04:13:48.409: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 17 04:13:58.493: INFO: Updating stateful set ss2
Jan 17 04:13:58.515: INFO: Waiting for Pod statefulset-510/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 17 04:14:08.732: INFO: Found 2 stateful pods, waiting for 3
Jan 17 04:14:18.750: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:14:18.750: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:14:18.750: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 17 04:14:18.796: INFO: Updating stateful set ss2
Jan 17 04:14:18.816: INFO: Waiting for Pod statefulset-510/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 17 04:14:28.868: INFO: Updating stateful set ss2
Jan 17 04:14:28.890: INFO: Waiting for StatefulSet statefulset-510/ss2 to complete update
Jan 17 04:14:28.890: INFO: Waiting for Pod statefulset-510/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 04:14:38.912: INFO: Deleting all statefulset in ns statefulset-510
Jan 17 04:14:38.918: INFO: Scaling statefulset ss2 to 0
Jan 17 04:14:49.000: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:14:49.005: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 04:14:49.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-510" for this suite.

• [SLOW TEST:70.913 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":3,"skipped":52,"failed":0}
SSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:14:49.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-1879
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1879
STEP: Deleting pre-stop pod
Jan 17 04:15:10.247: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jan 17 04:15:10.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1879" for this suite.

• [SLOW TEST:21.244 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":4,"skipped":57,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:10.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:15:10.421: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d" in namespace "projected-1542" to be "Succeeded or Failed"
Jan 17 04:15:10.430: INFO: Pod "downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.777812ms
Jan 17 04:15:12.442: INFO: Pod "downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020839537s
Jan 17 04:15:14.451: INFO: Pod "downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029667499s
STEP: Saw pod success
Jan 17 04:15:14.451: INFO: Pod "downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d" satisfied condition "Succeeded or Failed"
Jan 17 04:15:14.457: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d container client-container: <nil>
STEP: delete the pod
Jan 17 04:15:14.578: INFO: Waiting for pod downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d to disappear
Jan 17 04:15:14.585: INFO: Pod downwardapi-volume-1b97c108-dadd-4647-b4a5-faf18137523d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 04:15:14.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1542" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":5,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:14.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jan 17 04:15:14.656: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-3038 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:15:14.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3038" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":6,"skipped":136,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:14.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 17 04:15:14.846: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:14.846: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:14.901: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:14.901: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:14.998: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:14.998: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:15.068: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:15.068: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 17 04:15:16.699: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 04:15:16.699: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 17 04:15:22.203: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 17 04:15:22.254: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 0
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.259: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.299: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.299: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.346: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.346: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.404: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.404: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:22.434: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:22.434: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:27.809: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:27.809: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:27.855: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
STEP: listing Deployments
Jan 17 04:15:27.877: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 17 04:15:27.911: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 17 04:15:27.940: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:27.942: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:28.026: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:28.077: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:29.257: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:29.332: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:29.381: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 17 04:15:30.805: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 17 04:15:30.902: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:30.902: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:30.902: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:30.903: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 1
Jan 17 04:15:30.903: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:30.903: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:30.903: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 2
Jan 17 04:15:30.904: INFO: observed Deployment test-deployment in namespace deployment-4245 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 17 04:15:30.938: INFO: observed event type MODIFIED
Jan 17 04:15:30.938: INFO: observed event type MODIFIED
Jan 17 04:15:30.938: INFO: observed event type MODIFIED
Jan 17 04:15:30.938: INFO: observed event type MODIFIED
Jan 17 04:15:30.939: INFO: observed event type MODIFIED
Jan 17 04:15:30.939: INFO: observed event type MODIFIED
Jan 17 04:15:30.939: INFO: observed event type MODIFIED
Jan 17 04:15:30.939: INFO: observed event type MODIFIED
Jan 17 04:15:30.939: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 04:15:31.003: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 17 04:15:31.018: INFO: ReplicaSet "test-deployment-6b48c869b6":
&ReplicaSet{ObjectMeta:{test-deployment-6b48c869b6  deployment-4245  147b1012-4306-49fd-bca6-79e89704b4df 18466 3 2023-01-17 04:15:14 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 1d540cc3-5295-424b-a613-70be59687b12 0xc0027077f7 0xc0027077f8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:15:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d540cc3-5295-424b-a613-70be59687b12\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:15:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6b48c869b6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6b48c869b6 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002707880 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 17 04:15:31.048: INFO: ReplicaSet "test-deployment-74c6dd549b":
&ReplicaSet{ObjectMeta:{test-deployment-74c6dd549b  deployment-4245  d5dc4c28-f3fd-42d3-ab48-d1a02d6f6b78 18538 2 2023-01-17 04:15:27 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 1d540cc3-5295-424b-a613-70be59687b12 0xc0027078e7 0xc0027078e8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d540cc3-5295-424b-a613-70be59687b12\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:15:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 74c6dd549b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002707970 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 17 04:15:31.068: INFO: pod: "test-deployment-74c6dd549b-6dg5t":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-6dg5t test-deployment-74c6dd549b- deployment-4245  ea65bcba-102c-46a2-8d2a-2d08887718ff 18509 0 2023-01-17 04:15:27 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:e19b299854d8bc37558b4ca23c194ae80bd3643136cc0186c456a652ba69f990 cni.projectcalico.org/podIP:10.100.44.218/32 cni.projectcalico.org/podIPs:10.100.44.218/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b d5dc4c28-f3fd-42d3-ab48-d1a02d6f6b78 0xc002707fa7 0xc002707fa8}] []  [{kube-controller-manager Update v1 2023-01-17 04:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5dc4c28-f3fd-42d3-ab48-d1a02d6f6b78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 04:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 04:15:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsglt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsglt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.218,StartTime:2023-01-17 04:15:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 04:15:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b1b6c2ba3f0abcda9af0414a3c5cfba72c5212eb570677d9e4bdfcf8d091a590,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 04:15:31.069: INFO: pod: "test-deployment-74c6dd549b-mrjxm":
&Pod{ObjectMeta:{test-deployment-74c6dd549b-mrjxm test-deployment-74c6dd549b- deployment-4245  ac9f364b-32cc-45d7-a121-1e333ce8bf41 18537 0 2023-01-17 04:15:29 +0000 UTC <nil> <nil> map[pod-template-hash:74c6dd549b test-deployment-static:true] map[cni.projectcalico.org/containerID:ce01378f8399f1806b2e43e5cbb0724a6344630a10e88ba91c691831009c17c0 cni.projectcalico.org/podIP:10.100.11.204/32 cni.projectcalico.org/podIPs:10.100.11.204/32] [{apps/v1 ReplicaSet test-deployment-74c6dd549b d5dc4c28-f3fd-42d3-ab48-d1a02d6f6b78 0xc0037401d7 0xc0037401d8}] []  [{Go-http-client Update v1 2023-01-17 04:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 04:15:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5dc4c28-f3fd-42d3-ab48-d1a02d6f6b78\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 04:15:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4c225,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4c225,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.204,StartTime:2023-01-17 04:15:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 04:15:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cf51e9e47214f1f87c9bf845f0f68fb0fcb0a9dcd51109e037d8ecdf79f9e82e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 04:15:31.069: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-4245  a67166d9-6c8b-4316-b61d-ab003bb51b92 18546 4 2023-01-17 04:15:22 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 1d540cc3-5295-424b-a613-70be59687b12 0xc0027079d7 0xc0027079d8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d540cc3-5295-424b-a613-70be59687b12\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:15:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002707a60 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 17 04:15:31.074: INFO: pod: "test-deployment-84b949bdfc-5p8m7":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-5p8m7 test-deployment-84b949bdfc- deployment-4245  7584be59-970b-4b51-a95a-e58c1b6f7698 18512 0 2023-01-17 04:15:27 +0000 UTC 2023-01-17 04:15:30 +0000 UTC 0xc003741578 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:3c8c53508575b96dc30d0f4fd71805f6e846da699442ea01a2ccda28a705bb03 cni.projectcalico.org/podIP:10.100.106.77/32 cni.projectcalico.org/podIPs:10.100.106.77/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc a67166d9-6c8b-4316-b61d-ab003bb51b92 0xc0037415c7 0xc0037415c8}] []  [{kube-controller-manager Update v1 2023-01-17 04:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a67166d9-6c8b-4316-b61d-ab003bb51b92\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 04:15:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 04:15:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cb6wg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cb6wg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 04:15:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 17 04:15:31.075: INFO: pod: "test-deployment-84b949bdfc-lmxtj":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-lmxtj test-deployment-84b949bdfc- deployment-4245  e6dece90-9cc5-4ac3-b4ef-ecc632873e23 18542 0 2023-01-17 04:15:22 +0000 UTC 2023-01-17 04:15:31 +0000 UTC 0xc003741790 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:6f269796ac33ab8936c4d6c0dcdd69accf1c52d775e5172928f81a0a0912ab03 cni.projectcalico.org/podIP:10.100.11.203/32 cni.projectcalico.org/podIPs:10.100.11.203/32] [{apps/v1 ReplicaSet test-deployment-84b949bdfc a67166d9-6c8b-4316-b61d-ab003bb51b92 0xc0037417e7 0xc0037417e8}] []  [{kube-controller-manager Update v1 2023-01-17 04:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a67166d9-6c8b-4316-b61d-ab003bb51b92\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 04:15:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 04:15:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-skd44,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-skd44,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:15:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.203,StartTime:2023-01-17 04:15:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 04:15:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://b2ae3ed3b9a47b9a50edc81135eb9a2c93e5a4dd48dcda0d8d174e9f8310b952,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 04:15:31.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4245" for this suite.

• [SLOW TEST:16.382 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":7,"skipped":159,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:15:31.699: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:15:34.761: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:15:34.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6086" for this suite.
STEP: Destroying namespace "webhook-6086-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":8,"skipped":173,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:35.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jan 17 04:15:35.324: INFO: created test-event-1
Jan 17 04:15:35.332: INFO: created test-event-2
Jan 17 04:15:35.345: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 17 04:15:35.354: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 17 04:15:35.397: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 17 04:15:35.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4494" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":9,"skipped":179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:35.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:15:36.109: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:15:39.174: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:15:39.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3407" for this suite.
STEP: Destroying namespace "webhook-3407-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":10,"skipped":211,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:39.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5588
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5588
STEP: creating replication controller externalsvc in namespace services-5588
I0117 04:15:39.687393      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5588, replica count: 2
I0117 04:15:42.738977      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 17 04:15:42.794: INFO: Creating new exec pod
Jan 17 04:15:44.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-5588 exec execpodt8ldt -- /bin/sh -x -c nslookup nodeport-service.services-5588.svc.cluster.local'
Jan 17 04:15:45.193: INFO: stderr: "+ nslookup nodeport-service.services-5588.svc.cluster.local\n"
Jan 17 04:15:45.193: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-5588.svc.cluster.local\tcanonical name = externalsvc.services-5588.svc.cluster.local.\nName:\texternalsvc.services-5588.svc.cluster.local\nAddress: 10.254.229.41\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5588, will wait for the garbage collector to delete the pods
Jan 17 04:15:45.266: INFO: Deleting ReplicationController externalsvc took: 13.917288ms
Jan 17 04:15:45.366: INFO: Terminating ReplicationController externalsvc pods took: 100.900188ms
Jan 17 04:15:47.433: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:15:47.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5588" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":11,"skipped":224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:15:47.506: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 17 04:15:49.614: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3113 PodName:var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:15:49.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:15:49.615: INFO: ExecWithOptions: Clientset creation
Jan 17 04:15:49.615: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3113/pods/var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jan 17 04:15:49.746: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3113 PodName:var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:15:49.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:15:49.747: INFO: ExecWithOptions: Clientset creation
Jan 17 04:15:49.747: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/var-expansion-3113/pods/var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jan 17 04:15:50.397: INFO: Successfully updated pod "var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 17 04:15:50.403: INFO: Deleting pod "var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d" in namespace "var-expansion-3113"
Jan 17 04:15:50.419: INFO: Wait up to 5m0s for pod "var-expansion-fbcde7a1-61b2-4583-b163-6934569c385d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:16:24.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3113" for this suite.

• [SLOW TEST:36.960 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":12,"skipped":252,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:16:24.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:16:24.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 17 04:16:31.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1588 --namespace=crd-publish-openapi-1588 create -f -'
Jan 17 04:16:32.944: INFO: stderr: ""
Jan 17 04:16:32.944: INFO: stdout: "e2e-test-crd-publish-openapi-2912-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 04:16:32.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1588 --namespace=crd-publish-openapi-1588 delete e2e-test-crd-publish-openapi-2912-crds test-cr'
Jan 17 04:16:33.072: INFO: stderr: ""
Jan 17 04:16:33.072: INFO: stdout: "e2e-test-crd-publish-openapi-2912-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 17 04:16:33.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1588 --namespace=crd-publish-openapi-1588 apply -f -'
Jan 17 04:16:34.045: INFO: stderr: ""
Jan 17 04:16:34.045: INFO: stdout: "e2e-test-crd-publish-openapi-2912-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 17 04:16:34.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1588 --namespace=crd-publish-openapi-1588 delete e2e-test-crd-publish-openapi-2912-crds test-cr'
Jan 17 04:16:34.179: INFO: stderr: ""
Jan 17 04:16:34.179: INFO: stdout: "e2e-test-crd-publish-openapi-2912-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 17 04:16:34.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1588 explain e2e-test-crd-publish-openapi-2912-crds'
Jan 17 04:16:34.459: INFO: stderr: ""
Jan 17 04:16:34.459: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2912-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:16:38.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1588" for this suite.

• [SLOW TEST:14.275 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":13,"skipped":272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:16:38.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 17 04:16:38.810: INFO: Waiting up to 5m0s for pod "pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994" in namespace "emptydir-3471" to be "Succeeded or Failed"
Jan 17 04:16:38.826: INFO: Pod "pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994": Phase="Pending", Reason="", readiness=false. Elapsed: 15.971495ms
Jan 17 04:16:40.839: INFO: Pod "pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029080812s
Jan 17 04:16:42.855: INFO: Pod "pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044703261s
STEP: Saw pod success
Jan 17 04:16:42.855: INFO: Pod "pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994" satisfied condition "Succeeded or Failed"
Jan 17 04:16:42.865: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994 container test-container: <nil>
STEP: delete the pod
Jan 17 04:16:42.905: INFO: Waiting for pod pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994 to disappear
Jan 17 04:16:42.914: INFO: Pod pod-ccd2156b-86b5-4cc1-9f46-f633a4f7c994 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:16:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3471" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":14,"skipped":322,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:16:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-lhs5h in namespace proxy-8052
I0117 04:16:43.090937      22 runners.go:193] Created replication controller with name: proxy-service-lhs5h, namespace: proxy-8052, replica count: 1
I0117 04:16:44.141820      22 runners.go:193] proxy-service-lhs5h Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 04:16:45.141997      22 runners.go:193] proxy-service-lhs5h Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 04:16:45.151: INFO: setup took 2.148421647s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 17 04:16:45.186: INFO: (0) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 34.377115ms)
Jan 17 04:16:45.186: INFO: (0) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 34.490346ms)
Jan 17 04:16:45.186: INFO: (0) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 34.371848ms)
Jan 17 04:16:45.186: INFO: (0) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 34.440308ms)
Jan 17 04:16:45.187: INFO: (0) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 35.685169ms)
Jan 17 04:16:45.187: INFO: (0) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 36.006338ms)
Jan 17 04:16:45.187: INFO: (0) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 36.206614ms)
Jan 17 04:16:45.188: INFO: (0) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 36.771648ms)
Jan 17 04:16:45.188: INFO: (0) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 37.025908ms)
Jan 17 04:16:45.189: INFO: (0) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 37.586679ms)
Jan 17 04:16:45.189: INFO: (0) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 38.073037ms)
Jan 17 04:16:45.191: INFO: (0) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 39.967933ms)
Jan 17 04:16:45.191: INFO: (0) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 40.222187ms)
Jan 17 04:16:45.192: INFO: (0) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 40.336826ms)
Jan 17 04:16:45.193: INFO: (0) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 42.121085ms)
Jan 17 04:16:45.193: INFO: (0) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 42.334517ms)
Jan 17 04:16:45.202: INFO: (1) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 8.448793ms)
Jan 17 04:16:45.207: INFO: (1) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 13.169133ms)
Jan 17 04:16:45.207: INFO: (1) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 13.401367ms)
Jan 17 04:16:45.207: INFO: (1) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 13.363698ms)
Jan 17 04:16:45.207: INFO: (1) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 13.514319ms)
Jan 17 04:16:45.207: INFO: (1) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 13.576235ms)
Jan 17 04:16:45.210: INFO: (1) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 16.721168ms)
Jan 17 04:16:45.210: INFO: (1) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 16.79319ms)
Jan 17 04:16:45.211: INFO: (1) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 17.19732ms)
Jan 17 04:16:45.211: INFO: (1) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 17.130895ms)
Jan 17 04:16:45.211: INFO: (1) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 17.165495ms)
Jan 17 04:16:45.211: INFO: (1) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 17.275724ms)
Jan 17 04:16:45.211: INFO: (1) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 17.240623ms)
Jan 17 04:16:45.212: INFO: (1) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 18.131903ms)
Jan 17 04:16:45.212: INFO: (1) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 18.023373ms)
Jan 17 04:16:45.212: INFO: (1) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 18.457951ms)
Jan 17 04:16:45.223: INFO: (2) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 10.937465ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 14.186695ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 14.482379ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 14.773865ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 14.845667ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 14.822164ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 14.919331ms)
Jan 17 04:16:45.227: INFO: (2) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 14.969101ms)
Jan 17 04:16:45.228: INFO: (2) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 15.228579ms)
Jan 17 04:16:45.228: INFO: (2) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 15.298766ms)
Jan 17 04:16:45.228: INFO: (2) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 15.345997ms)
Jan 17 04:16:45.229: INFO: (2) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 16.665267ms)
Jan 17 04:16:45.230: INFO: (2) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 17.698356ms)
Jan 17 04:16:45.230: INFO: (2) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 17.954807ms)
Jan 17 04:16:45.231: INFO: (2) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 18.886504ms)
Jan 17 04:16:45.232: INFO: (2) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 19.37732ms)
Jan 17 04:16:45.241: INFO: (3) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 8.386864ms)
Jan 17 04:16:45.244: INFO: (3) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 10.769642ms)
Jan 17 04:16:45.244: INFO: (3) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 10.905511ms)
Jan 17 04:16:45.247: INFO: (3) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 14.244411ms)
Jan 17 04:16:45.248: INFO: (3) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.151387ms)
Jan 17 04:16:45.249: INFO: (3) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 16.474399ms)
Jan 17 04:16:45.249: INFO: (3) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 16.54028ms)
Jan 17 04:16:45.250: INFO: (3) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.72349ms)
Jan 17 04:16:45.250: INFO: (3) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 17.050645ms)
Jan 17 04:16:45.250: INFO: (3) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 16.954303ms)
Jan 17 04:16:45.251: INFO: (3) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 18.076744ms)
Jan 17 04:16:45.252: INFO: (3) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 18.599163ms)
Jan 17 04:16:45.252: INFO: (3) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 18.624612ms)
Jan 17 04:16:45.252: INFO: (3) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 18.875709ms)
Jan 17 04:16:45.252: INFO: (3) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 18.920096ms)
Jan 17 04:16:45.252: INFO: (3) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 19.253809ms)
Jan 17 04:16:45.266: INFO: (4) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 13.436357ms)
Jan 17 04:16:45.267: INFO: (4) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 14.351109ms)
Jan 17 04:16:45.267: INFO: (4) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 14.81082ms)
Jan 17 04:16:45.269: INFO: (4) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.501297ms)
Jan 17 04:16:45.270: INFO: (4) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 17.464314ms)
Jan 17 04:16:45.270: INFO: (4) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 17.790356ms)
Jan 17 04:16:45.271: INFO: (4) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 18.49327ms)
Jan 17 04:16:45.273: INFO: (4) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 19.935619ms)
Jan 17 04:16:45.273: INFO: (4) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 20.300774ms)
Jan 17 04:16:45.273: INFO: (4) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 20.461089ms)
Jan 17 04:16:45.274: INFO: (4) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.122941ms)
Jan 17 04:16:45.274: INFO: (4) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 21.191288ms)
Jan 17 04:16:45.274: INFO: (4) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 21.095664ms)
Jan 17 04:16:45.274: INFO: (4) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 21.96541ms)
Jan 17 04:16:45.274: INFO: (4) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 22.109855ms)
Jan 17 04:16:45.277: INFO: (4) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 24.415337ms)
Jan 17 04:16:45.288: INFO: (5) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 10.733511ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 16.36532ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 16.750525ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 16.746ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 16.523924ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 16.60594ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 16.630317ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 16.82424ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 16.774432ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.759408ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 16.855655ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.842325ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 16.976472ms)
Jan 17 04:16:45.294: INFO: (5) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 17.369899ms)
Jan 17 04:16:45.295: INFO: (5) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 17.665132ms)
Jan 17 04:16:45.295: INFO: (5) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 17.973988ms)
Jan 17 04:16:45.301: INFO: (6) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 6.252884ms)
Jan 17 04:16:45.302: INFO: (6) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 7.249922ms)
Jan 17 04:16:45.304: INFO: (6) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 8.978181ms)
Jan 17 04:16:45.307: INFO: (6) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 11.966069ms)
Jan 17 04:16:45.307: INFO: (6) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 12.026893ms)
Jan 17 04:16:45.309: INFO: (6) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 13.986013ms)
Jan 17 04:16:45.310: INFO: (6) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 15.149808ms)
Jan 17 04:16:45.312: INFO: (6) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 16.895355ms)
Jan 17 04:16:45.312: INFO: (6) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 17.162215ms)
Jan 17 04:16:45.313: INFO: (6) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 17.935319ms)
Jan 17 04:16:45.313: INFO: (6) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 18.171114ms)
Jan 17 04:16:45.314: INFO: (6) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 18.267346ms)
Jan 17 04:16:45.314: INFO: (6) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 18.555206ms)
Jan 17 04:16:45.314: INFO: (6) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 18.675016ms)
Jan 17 04:16:45.316: INFO: (6) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 20.831729ms)
Jan 17 04:16:45.316: INFO: (6) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 20.863903ms)
Jan 17 04:16:45.332: INFO: (7) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 15.500083ms)
Jan 17 04:16:45.334: INFO: (7) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 18.044376ms)
Jan 17 04:16:45.335: INFO: (7) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 18.155829ms)
Jan 17 04:16:45.336: INFO: (7) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 19.068002ms)
Jan 17 04:16:45.336: INFO: (7) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 19.690425ms)
Jan 17 04:16:45.337: INFO: (7) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 20.739644ms)
Jan 17 04:16:45.339: INFO: (7) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 22.363674ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 23.149977ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 23.434819ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 23.580857ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 23.535096ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 23.575366ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 23.712801ms)
Jan 17 04:16:45.340: INFO: (7) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 23.66672ms)
Jan 17 04:16:45.341: INFO: (7) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 24.036134ms)
Jan 17 04:16:45.341: INFO: (7) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 24.069224ms)
Jan 17 04:16:45.349: INFO: (8) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 8.025809ms)
Jan 17 04:16:45.351: INFO: (8) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 10.499005ms)
Jan 17 04:16:45.352: INFO: (8) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 11.236653ms)
Jan 17 04:16:45.356: INFO: (8) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 14.461005ms)
Jan 17 04:16:45.356: INFO: (8) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 14.56856ms)
Jan 17 04:16:45.356: INFO: (8) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 14.965194ms)
Jan 17 04:16:45.357: INFO: (8) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 16.047366ms)
Jan 17 04:16:45.362: INFO: (8) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 20.787612ms)
Jan 17 04:16:45.362: INFO: (8) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 20.904612ms)
Jan 17 04:16:45.362: INFO: (8) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.223476ms)
Jan 17 04:16:45.362: INFO: (8) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 21.194341ms)
Jan 17 04:16:45.363: INFO: (8) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 21.467511ms)
Jan 17 04:16:45.363: INFO: (8) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 21.516426ms)
Jan 17 04:16:45.363: INFO: (8) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 21.872944ms)
Jan 17 04:16:45.367: INFO: (8) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 26.379708ms)
Jan 17 04:16:45.367: INFO: (8) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 26.487292ms)
Jan 17 04:16:45.378: INFO: (9) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 10.092201ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 18.619822ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 19.133233ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 19.103242ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 18.97184ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 19.060859ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 19.28281ms)
Jan 17 04:16:45.387: INFO: (9) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 19.460126ms)
Jan 17 04:16:45.389: INFO: (9) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 20.516211ms)
Jan 17 04:16:45.389: INFO: (9) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.12425ms)
Jan 17 04:16:45.389: INFO: (9) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 20.872298ms)
Jan 17 04:16:45.389: INFO: (9) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 20.884264ms)
Jan 17 04:16:45.390: INFO: (9) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 21.513922ms)
Jan 17 04:16:45.390: INFO: (9) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 22.52028ms)
Jan 17 04:16:45.393: INFO: (9) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 25.293469ms)
Jan 17 04:16:45.394: INFO: (9) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 25.454948ms)
Jan 17 04:16:45.402: INFO: (10) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 8.760602ms)
Jan 17 04:16:45.410: INFO: (10) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.144623ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 23.711557ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 23.924875ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 23.823554ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 23.86319ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 24.170781ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 24.177748ms)
Jan 17 04:16:45.418: INFO: (10) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 24.642049ms)
Jan 17 04:16:45.424: INFO: (10) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 30.172145ms)
Jan 17 04:16:45.430: INFO: (10) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 36.265126ms)
Jan 17 04:16:45.430: INFO: (10) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 36.557896ms)
Jan 17 04:16:45.431: INFO: (10) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 37.166277ms)
Jan 17 04:16:45.431: INFO: (10) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 37.088205ms)
Jan 17 04:16:45.431: INFO: (10) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 37.123448ms)
Jan 17 04:16:45.431: INFO: (10) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 37.518418ms)
Jan 17 04:16:45.445: INFO: (11) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 12.915904ms)
Jan 17 04:16:45.445: INFO: (11) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 13.102686ms)
Jan 17 04:16:45.449: INFO: (11) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 17.156591ms)
Jan 17 04:16:45.449: INFO: (11) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 17.530576ms)
Jan 17 04:16:45.450: INFO: (11) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 17.75761ms)
Jan 17 04:16:45.450: INFO: (11) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 18.039212ms)
Jan 17 04:16:45.450: INFO: (11) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 18.004828ms)
Jan 17 04:16:45.451: INFO: (11) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 19.311851ms)
Jan 17 04:16:45.451: INFO: (11) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 19.556839ms)
Jan 17 04:16:45.453: INFO: (11) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 20.547858ms)
Jan 17 04:16:45.453: INFO: (11) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 21.340135ms)
Jan 17 04:16:45.453: INFO: (11) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 20.931122ms)
Jan 17 04:16:45.453: INFO: (11) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 20.973612ms)
Jan 17 04:16:45.453: INFO: (11) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 21.052334ms)
Jan 17 04:16:45.454: INFO: (11) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 22.673543ms)
Jan 17 04:16:45.454: INFO: (11) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 22.450749ms)
Jan 17 04:16:45.462: INFO: (12) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 7.005164ms)
Jan 17 04:16:45.463: INFO: (12) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 8.344192ms)
Jan 17 04:16:45.466: INFO: (12) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 10.767126ms)
Jan 17 04:16:45.468: INFO: (12) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 12.743268ms)
Jan 17 04:16:45.469: INFO: (12) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 13.366406ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 15.667547ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 16.270711ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.941117ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 16.082792ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 16.631501ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 16.097872ms)
Jan 17 04:16:45.471: INFO: (12) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 16.371442ms)
Jan 17 04:16:45.473: INFO: (12) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 18.524332ms)
Jan 17 04:16:45.475: INFO: (12) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 20.114804ms)
Jan 17 04:16:45.475: INFO: (12) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 20.207162ms)
Jan 17 04:16:45.475: INFO: (12) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 20.101874ms)
Jan 17 04:16:45.483: INFO: (13) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 7.469274ms)
Jan 17 04:16:45.485: INFO: (13) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 9.856156ms)
Jan 17 04:16:45.485: INFO: (13) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 10.020465ms)
Jan 17 04:16:45.485: INFO: (13) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 10.069468ms)
Jan 17 04:16:45.486: INFO: (13) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 10.190643ms)
Jan 17 04:16:45.487: INFO: (13) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 11.381688ms)
Jan 17 04:16:45.487: INFO: (13) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 11.572591ms)
Jan 17 04:16:45.487: INFO: (13) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 11.996916ms)
Jan 17 04:16:45.488: INFO: (13) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 12.154051ms)
Jan 17 04:16:45.490: INFO: (13) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 14.336199ms)
Jan 17 04:16:45.490: INFO: (13) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 14.413228ms)
Jan 17 04:16:45.490: INFO: (13) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 14.378311ms)
Jan 17 04:16:45.491: INFO: (13) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 15.156554ms)
Jan 17 04:16:45.492: INFO: (13) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 16.692654ms)
Jan 17 04:16:45.493: INFO: (13) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 17.459165ms)
Jan 17 04:16:45.493: INFO: (13) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 17.560472ms)
Jan 17 04:16:45.502: INFO: (14) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 8.748645ms)
Jan 17 04:16:45.506: INFO: (14) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 12.753907ms)
Jan 17 04:16:45.506: INFO: (14) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 13.219048ms)
Jan 17 04:16:45.506: INFO: (14) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 12.921392ms)
Jan 17 04:16:45.506: INFO: (14) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 13.070185ms)
Jan 17 04:16:45.509: INFO: (14) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 15.11492ms)
Jan 17 04:16:45.509: INFO: (14) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 15.115072ms)
Jan 17 04:16:45.509: INFO: (14) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.232951ms)
Jan 17 04:16:45.509: INFO: (14) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 15.26998ms)
Jan 17 04:16:45.509: INFO: (14) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 15.405632ms)
Jan 17 04:16:45.510: INFO: (14) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 17.105156ms)
Jan 17 04:16:45.512: INFO: (14) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 18.490283ms)
Jan 17 04:16:45.514: INFO: (14) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 20.528792ms)
Jan 17 04:16:45.515: INFO: (14) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 21.234962ms)
Jan 17 04:16:45.515: INFO: (14) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 21.338668ms)
Jan 17 04:16:45.515: INFO: (14) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 21.746177ms)
Jan 17 04:16:45.530: INFO: (15) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 14.39099ms)
Jan 17 04:16:45.530: INFO: (15) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.175203ms)
Jan 17 04:16:45.530: INFO: (15) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 14.954603ms)
Jan 17 04:16:45.530: INFO: (15) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 15.009283ms)
Jan 17 04:16:45.531: INFO: (15) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.565194ms)
Jan 17 04:16:45.531: INFO: (15) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 15.57663ms)
Jan 17 04:16:45.536: INFO: (15) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 20.643095ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 21.553705ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.614952ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 21.649189ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.772004ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 21.822004ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 21.722539ms)
Jan 17 04:16:45.537: INFO: (15) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 22.11248ms)
Jan 17 04:16:45.538: INFO: (15) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 22.583319ms)
Jan 17 04:16:45.538: INFO: (15) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 22.461848ms)
Jan 17 04:16:45.547: INFO: (16) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 8.221017ms)
Jan 17 04:16:45.550: INFO: (16) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 11.937917ms)
Jan 17 04:16:45.550: INFO: (16) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 11.702276ms)
Jan 17 04:16:45.550: INFO: (16) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 12.252332ms)
Jan 17 04:16:45.551: INFO: (16) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 12.351058ms)
Jan 17 04:16:45.554: INFO: (16) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.120601ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 17.747416ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 17.350699ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 17.622009ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 17.752928ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 17.675348ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 17.717413ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 17.656808ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 17.510987ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 17.788451ms)
Jan 17 04:16:45.556: INFO: (16) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 17.765885ms)
Jan 17 04:16:45.564: INFO: (17) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 7.431381ms)
Jan 17 04:16:45.564: INFO: (17) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 8.020421ms)
Jan 17 04:16:45.567: INFO: (17) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 11.159272ms)
Jan 17 04:16:45.568: INFO: (17) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 11.282147ms)
Jan 17 04:16:45.568: INFO: (17) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 11.424744ms)
Jan 17 04:16:45.568: INFO: (17) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 11.56947ms)
Jan 17 04:16:45.568: INFO: (17) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 11.82025ms)
Jan 17 04:16:45.569: INFO: (17) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 12.33766ms)
Jan 17 04:16:45.569: INFO: (17) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 13.161385ms)
Jan 17 04:16:45.570: INFO: (17) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 13.542569ms)
Jan 17 04:16:45.570: INFO: (17) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 14.029277ms)
Jan 17 04:16:45.570: INFO: (17) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 14.054525ms)
Jan 17 04:16:45.571: INFO: (17) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 14.530089ms)
Jan 17 04:16:45.572: INFO: (17) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 15.777579ms)
Jan 17 04:16:45.573: INFO: (17) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 16.806751ms)
Jan 17 04:16:45.573: INFO: (17) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 16.900795ms)
Jan 17 04:16:45.584: INFO: (18) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 10.440765ms)
Jan 17 04:16:45.584: INFO: (18) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 10.843943ms)
Jan 17 04:16:45.584: INFO: (18) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 11.047168ms)
Jan 17 04:16:45.584: INFO: (18) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 11.146138ms)
Jan 17 04:16:45.593: INFO: (18) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 19.93044ms)
Jan 17 04:16:45.593: INFO: (18) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 20.066567ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 20.495122ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 20.301316ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 20.37952ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 20.662444ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 20.708068ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 20.774417ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 20.88392ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 21.043915ms)
Jan 17 04:16:45.594: INFO: (18) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 21.200213ms)
Jan 17 04:16:45.595: INFO: (18) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 21.54673ms)
Jan 17 04:16:45.605: INFO: (19) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">test<... (200; 10.194841ms)
Jan 17 04:16:45.606: INFO: (19) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:460/proxy/: tls baz (200; 10.954724ms)
Jan 17 04:16:45.606: INFO: (19) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:462/proxy/: tls qux (200; 11.072347ms)
Jan 17 04:16:45.607: INFO: (19) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 11.610059ms)
Jan 17 04:16:45.611: INFO: (19) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:162/proxy/: bar (200; 15.824214ms)
Jan 17 04:16:45.611: INFO: (19) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s/proxy/rewriteme">test</a> (200; 15.846446ms)
Jan 17 04:16:45.611: INFO: (19) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 15.823755ms)
Jan 17 04:16:45.611: INFO: (19) /api/v1/namespaces/proxy-8052/pods/proxy-service-lhs5h-54q4s:160/proxy/: foo (200; 16.040732ms)
Jan 17 04:16:45.612: INFO: (19) /api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/https:proxy-service-lhs5h-54q4s:443/proxy/tlsrewritem... (200; 16.992047ms)
Jan 17 04:16:45.613: INFO: (19) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname1/proxy/: foo (200; 18.307644ms)
Jan 17 04:16:45.613: INFO: (19) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname2/proxy/: bar (200; 18.212042ms)
Jan 17 04:16:45.613: INFO: (19) /api/v1/namespaces/proxy-8052/services/proxy-service-lhs5h:portname2/proxy/: bar (200; 18.17383ms)
Jan 17 04:16:45.618: INFO: (19) /api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/: <a href="/api/v1/namespaces/proxy-8052/pods/http:proxy-service-lhs5h-54q4s:1080/proxy/rewriteme">... (200; 22.673524ms)
Jan 17 04:16:45.619: INFO: (19) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname1/proxy/: tls baz (200; 23.60891ms)
Jan 17 04:16:45.622: INFO: (19) /api/v1/namespaces/proxy-8052/services/http:proxy-service-lhs5h:portname1/proxy/: foo (200; 26.625531ms)
Jan 17 04:16:45.622: INFO: (19) /api/v1/namespaces/proxy-8052/services/https:proxy-service-lhs5h:tlsportname2/proxy/: tls qux (200; 26.878844ms)
STEP: deleting ReplicationController proxy-service-lhs5h in namespace proxy-8052, will wait for the garbage collector to delete the pods
Jan 17 04:16:45.695: INFO: Deleting ReplicationController proxy-service-lhs5h took: 12.106561ms
Jan 17 04:16:45.796: INFO: Terminating ReplicationController proxy-service-lhs5h pods took: 100.52372ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 17 04:16:48.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8052" for this suite.

• [SLOW TEST:5.223 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":15,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:16:48.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 17 04:16:56.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8744" for this suite.

• [SLOW TEST:8.145 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:81
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":16,"skipped":360,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:16:56.308: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 17 04:22:00.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4391" for this suite.

• [SLOW TEST:304.182 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":17,"skipped":371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:00.495: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-1301f65a-8862-4785-a3ec-4a46a675aef6
STEP: Creating a pod to test consume secrets
Jan 17 04:22:00.587: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112" in namespace "projected-1453" to be "Succeeded or Failed"
Jan 17 04:22:00.603: INFO: Pod "pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112": Phase="Pending", Reason="", readiness=false. Elapsed: 16.350098ms
Jan 17 04:22:02.613: INFO: Pod "pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026223097s
Jan 17 04:22:04.619: INFO: Pod "pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032718462s
STEP: Saw pod success
Jan 17 04:22:04.619: INFO: Pod "pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112" satisfied condition "Succeeded or Failed"
Jan 17 04:22:04.624: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 17 04:22:04.718: INFO: Waiting for pod pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112 to disappear
Jan 17 04:22:04.733: INFO: Pod pod-projected-secrets-616ec19d-c32a-478a-b7e3-988cf1425112 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 04:22:04.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1453" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":18,"skipped":412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 17 04:22:04.826: INFO: The status of Pod pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:22:06.838: INFO: The status of Pod pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:22:08.837: INFO: The status of Pod pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:22:10.838: INFO: The status of Pod pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:22:12.838: INFO: The status of Pod pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 17 04:22:13.385: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77"
Jan 17 04:22:13.385: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77" in namespace "pods-6" to be "terminated due to deadline exceeded"
Jan 17 04:22:13.389: INFO: Pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77": Phase="Running", Reason="", readiness=true. Elapsed: 4.086999ms
Jan 17 04:22:15.397: INFO: Pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77": Phase="Running", Reason="", readiness=false. Elapsed: 2.012942717s
Jan 17 04:22:17.410: INFO: Pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.025284166s
Jan 17 04:22:17.410: INFO: Pod "pod-update-activedeadlineseconds-e0fbf3f5-4cc5-4a27-9f98-d722e7308b77" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 04:22:17.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6" for this suite.

• [SLOW TEST:12.682 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":19,"skipped":448,"failed":0}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:17.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:22:28.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4327" for this suite.

• [SLOW TEST:11.403 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":20,"skipped":449,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:28.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 17 04:22:28.907: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 17 04:22:33.929: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 17 04:22:34.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8326" for this suite.

• [SLOW TEST:6.161 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":21,"skipped":459,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:34.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jan 17 04:22:35.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7280 create -f -'
Jan 17 04:22:36.010: INFO: stderr: ""
Jan 17 04:22:36.010: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 17 04:22:36.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7280 diff -f -'
Jan 17 04:22:36.920: INFO: rc: 1
Jan 17 04:22:36.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7280 delete -f -'
Jan 17 04:22:37.033: INFO: stderr: ""
Jan 17 04:22:37.033: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:22:37.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7280" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":22,"skipped":469,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:37.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jan 17 04:22:37.159: INFO: Waiting up to 5m0s for pod "var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955" in namespace "var-expansion-7779" to be "Succeeded or Failed"
Jan 17 04:22:37.171: INFO: Pod "var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955": Phase="Pending", Reason="", readiness=false. Elapsed: 12.414178ms
Jan 17 04:22:39.180: INFO: Pod "var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021672672s
Jan 17 04:22:41.193: INFO: Pod "var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033838649s
STEP: Saw pod success
Jan 17 04:22:41.193: INFO: Pod "var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955" satisfied condition "Succeeded or Failed"
Jan 17 04:22:41.197: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955 container dapi-container: <nil>
STEP: delete the pod
Jan 17 04:22:41.260: INFO: Waiting for pod var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955 to disappear
Jan 17 04:22:41.271: INFO: Pod var-expansion-006114e0-f41c-4196-9fd8-960a60a7f955 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:22:41.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7779" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":23,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:41.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jan 17 04:22:47.977: INFO: 80 pods remaining
Jan 17 04:22:47.977: INFO: 80 pods has nil DeletionTimestamp
Jan 17 04:22:47.977: INFO: 
Jan 17 04:22:48.942: INFO: 71 pods remaining
Jan 17 04:22:48.942: INFO: 70 pods has nil DeletionTimestamp
Jan 17 04:22:48.942: INFO: 
Jan 17 04:22:49.851: INFO: 59 pods remaining
Jan 17 04:22:49.851: INFO: 59 pods has nil DeletionTimestamp
Jan 17 04:22:49.851: INFO: 
Jan 17 04:22:50.843: INFO: 40 pods remaining
Jan 17 04:22:50.843: INFO: 40 pods has nil DeletionTimestamp
Jan 17 04:22:50.843: INFO: 
Jan 17 04:22:51.851: INFO: 31 pods remaining
Jan 17 04:22:51.851: INFO: 31 pods has nil DeletionTimestamp
Jan 17 04:22:51.851: INFO: 
Jan 17 04:22:52.874: INFO: 18 pods remaining
Jan 17 04:22:52.874: INFO: 18 pods has nil DeletionTimestamp
Jan 17 04:22:52.874: INFO: 
Jan 17 04:22:53.846: INFO: 0 pods remaining
Jan 17 04:22:53.847: INFO: 0 pods has nil DeletionTimestamp
Jan 17 04:22:53.847: INFO: 
STEP: Gathering metrics
W0117 04:22:54.864613      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 04:22:54.864: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 04:22:54.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-24" for this suite.

• [SLOW TEST:13.581 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":24,"skipped":503,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:22:55.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5482" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":25,"skipped":512,"failed":0}
SS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:22:55.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:22:55.209: INFO: Creating pod...
Jan 17 04:23:05.279: INFO: Creating service...
Jan 17 04:23:05.307: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/DELETE
Jan 17 04:23:05.320: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 04:23:05.320: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/GET
Jan 17 04:23:05.336: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 04:23:05.336: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/HEAD
Jan 17 04:23:05.341: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 04:23:05.341: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 17 04:23:05.354: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 04:23:05.354: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/PATCH
Jan 17 04:23:05.361: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 04:23:05.361: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/POST
Jan 17 04:23:05.369: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 04:23:05.369: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/pods/agnhost/proxy/some/path/with/PUT
Jan 17 04:23:05.380: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 04:23:05.380: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/DELETE
Jan 17 04:23:05.394: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 04:23:05.394: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/GET
Jan 17 04:23:05.404: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 17 04:23:05.404: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/HEAD
Jan 17 04:23:05.413: INFO: http.Client request:HEAD | StatusCode:200
Jan 17 04:23:05.413: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/OPTIONS
Jan 17 04:23:05.423: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 04:23:05.423: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/PATCH
Jan 17 04:23:05.442: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 04:23:05.442: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/POST
Jan 17 04:23:05.451: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 04:23:05.451: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-2735/services/test-service/proxy/some/path/with/PUT
Jan 17 04:23:05.461: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 17 04:23:05.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2735" for this suite.

• [SLOW TEST:10.385 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":26,"skipped":514,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:23:05.489: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jan 17 04:23:05.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-3840 cluster-info'
Jan 17 04:23:05.645: INFO: stderr: ""
Jan 17 04:23:05.645: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:23:05.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3840" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":27,"skipped":527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:23:05.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 04:23:05.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1289" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":28,"skipped":554,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:23:05.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 17 04:23:09.943: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 17 04:23:09.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3972" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":29,"skipped":562,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:23:10.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jan 17 04:23:10.039: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 17 04:23:10.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:10.356: INFO: stderr: ""
Jan 17 04:23:10.356: INFO: stdout: "service/agnhost-replica created\n"
Jan 17 04:23:10.356: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 17 04:23:10.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:10.648: INFO: stderr: ""
Jan 17 04:23:10.648: INFO: stdout: "service/agnhost-primary created\n"
Jan 17 04:23:10.649: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 17 04:23:10.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:10.932: INFO: stderr: ""
Jan 17 04:23:10.932: INFO: stdout: "service/frontend created\n"
Jan 17 04:23:10.932: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 17 04:23:10.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:11.207: INFO: stderr: ""
Jan 17 04:23:11.208: INFO: stdout: "deployment.apps/frontend created\n"
Jan 17 04:23:11.208: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 04:23:11.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:13.143: INFO: stderr: ""
Jan 17 04:23:13.143: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 17 04:23:13.143: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 17 04:23:13.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 create -f -'
Jan 17 04:23:13.420: INFO: stderr: ""
Jan 17 04:23:13.420: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 17 04:23:13.420: INFO: Waiting for all frontend pods to be Running.
Jan 17 04:23:23.472: INFO: Waiting for frontend to serve content.
Jan 17 04:23:23.491: INFO: Trying to add a new entry to the guestbook.
Jan 17 04:23:23.510: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 17 04:23:23.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:23.721: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:23.721: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 17 04:23:23.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:23.853: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:23.853: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 17 04:23:23.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:24.021: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:24.021: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 17 04:23:24.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:24.152: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:24.152: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 17 04:23:24.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:24.264: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:24.264: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 17 04:23:24.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7244 delete --grace-period=0 --force -f -'
Jan 17 04:23:24.373: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:23:24.373: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:23:24.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7244" for this suite.

• [SLOW TEST:14.428 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":30,"skipped":584,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:23:24.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ed135897-ec03-43a9-8c93-b871b4b014bc
STEP: Creating the pod
Jan 17 04:23:24.561: INFO: The status of Pod pod-projected-configmaps-c2f90020-c054-4d1d-bb27-774f2bfb50fd is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:23:26.580: INFO: The status of Pod pod-projected-configmaps-c2f90020-c054-4d1d-bb27-774f2bfb50fd is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-ed135897-ec03-43a9-8c93-b871b4b014bc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 04:24:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9028" for this suite.

• [SLOW TEST:72.684 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":31,"skipped":589,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:24:37.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 17 04:24:37.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 17 04:24:57.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:25:03.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:25:23.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6745" for this suite.

• [SLOW TEST:46.021 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":32,"skipped":603,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:25:23.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-7232ba85-8b28-4605-a82c-c052c8d19639
STEP: Creating a pod to test consume secrets
Jan 17 04:25:23.204: INFO: Waiting up to 5m0s for pod "pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074" in namespace "secrets-4164" to be "Succeeded or Failed"
Jan 17 04:25:23.210: INFO: Pod "pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074": Phase="Pending", Reason="", readiness=false. Elapsed: 5.904115ms
Jan 17 04:25:25.219: INFO: Pod "pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01425668s
Jan 17 04:25:27.230: INFO: Pod "pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02509152s
STEP: Saw pod success
Jan 17 04:25:27.230: INFO: Pod "pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074" satisfied condition "Succeeded or Failed"
Jan 17 04:25:27.233: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074 container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 04:25:27.314: INFO: Waiting for pod pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074 to disappear
Jan 17 04:25:27.319: INFO: Pod pod-secrets-3f904e55-1637-46b1-ac2b-94d065831074 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:25:27.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4164" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":33,"skipped":605,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:25:27.335: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 17 04:25:27.421: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:27.421: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:27.421: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:27.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:25:27.426: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:28.438: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:28.438: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:28.438: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:28.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:25:28.447: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:29.436: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.436: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.436: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.439: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:25:29.439: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 17 04:25:29.467: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.467: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.467: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:29.471: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 04:25:29.471: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:30.479: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:30.480: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:30.480: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:30.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 04:25:30.483: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:31.481: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:31.481: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:31.481: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:31.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 04:25:31.485: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:32.482: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:32.483: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:32.483: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:32.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 04:25:32.486: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:25:33.481: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:33.481: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:33.482: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:25:33.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:25:33.486: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6863, will wait for the garbage collector to delete the pods
Jan 17 04:25:33.559: INFO: Deleting DaemonSet.extensions daemon-set took: 12.110622ms
Jan 17 04:25:33.660: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.156222ms
Jan 17 04:25:35.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:25:35.970: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 04:25:35.975: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"23095"},"items":null}

Jan 17 04:25:35.979: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"23095"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:25:35.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6863" for this suite.

• [SLOW TEST:8.675 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":34,"skipped":630,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:25:36.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 17 04:25:36.077: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3393  2bf653ba-d38c-4157-bd9e-816bde6a9c99 23100 0 2023-01-17 04:25:36 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2023-01-17 04:25:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgxxz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgxxz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 04:25:36.092: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:25:38.101: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 17 04:25:38.101: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3393 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:25:38.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:25:38.101: INFO: ExecWithOptions: Clientset creation
Jan 17 04:25:38.101: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-3393/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jan 17 04:25:38.250: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3393 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:25:38.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:25:38.251: INFO: ExecWithOptions: Clientset creation
Jan 17 04:25:38.251: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/dns-3393/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 04:25:38.409: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 04:25:38.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3393" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":35,"skipped":640,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:25:38.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4730, will wait for the garbage collector to delete the pods
Jan 17 04:25:42.620: INFO: Deleting Job.batch foo took: 17.478426ms
Jan 17 04:25:42.721: INFO: Terminating Job.batch foo pods took: 100.623159ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 17 04:26:14.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4730" for this suite.

• [SLOW TEST:35.734 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":36,"skipped":658,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:14.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-b27f2da9-9b93-44cb-84f2-bebdc64a1f8d
STEP: Creating a pod to test consume configMaps
Jan 17 04:26:14.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6" in namespace "projected-7894" to be "Succeeded or Failed"
Jan 17 04:26:14.338: INFO: Pod "pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814329ms
Jan 17 04:26:16.348: INFO: Pod "pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014774742s
Jan 17 04:26:18.358: INFO: Pod "pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024430563s
STEP: Saw pod success
Jan 17 04:26:18.358: INFO: Pod "pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6" satisfied condition "Succeeded or Failed"
Jan 17 04:26:18.361: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:26:18.394: INFO: Waiting for pod pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6 to disappear
Jan 17 04:26:18.399: INFO: Pod pod-projected-configmaps-b4e48b1f-15f8-4f91-89ae-8e7d59f36fb6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 04:26:18.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7894" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":37,"skipped":660,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:18.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 04:26:22.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4801" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":38,"skipped":680,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:22.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:26:22.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1663" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":39,"skipped":697,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:22.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:26:22.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8093" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":40,"skipped":711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:22.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6505
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-6505
Jan 17 04:26:22.831: INFO: Found 0 stateful pods, waiting for 1
Jan 17 04:26:32.854: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 17 04:26:32.885: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 17 04:26:32.895: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 17 04:26:32.899: INFO: Observed &StatefulSet event: ADDED
Jan 17 04:26:32.899: INFO: Found Statefulset ss in namespace statefulset-6505 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 04:26:32.899: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 17 04:26:32.899: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 04:26:32.914: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 17 04:26:32.917: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 04:26:32.917: INFO: Deleting all statefulset in ns statefulset-6505
Jan 17 04:26:32.922: INFO: Scaling statefulset ss to 0
Jan 17 04:26:42.962: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:26:42.965: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 04:26:42.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6505" for this suite.

• [SLOW TEST:20.258 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":41,"skipped":746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:43.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:26:43.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb" in namespace "downward-api-9509" to be "Succeeded or Failed"
Jan 17 04:26:43.067: INFO: Pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144396ms
Jan 17 04:26:45.074: INFO: Pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.011839613s
Jan 17 04:26:47.084: INFO: Pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb": Phase="Running", Reason="", readiness=false. Elapsed: 4.022191645s
Jan 17 04:26:49.093: INFO: Pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031236014s
STEP: Saw pod success
Jan 17 04:26:49.093: INFO: Pod "downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb" satisfied condition "Succeeded or Failed"
Jan 17 04:26:49.096: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb container client-container: <nil>
STEP: delete the pod
Jan 17 04:26:49.126: INFO: Waiting for pod downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb to disappear
Jan 17 04:26:49.130: INFO: Pod downwardapi-volume-03f501ef-bbc2-4ecf-bcd6-b22ae15fcfbb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:26:49.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9509" for this suite.

• [SLOW TEST:6.142 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":784,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:49.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:26:49.216: INFO: The status of Pod busybox-readonly-fs27eac80a-efa4-411b-8ad4-315b93b9a61f is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:26:51.224: INFO: The status of Pod busybox-readonly-fs27eac80a-efa4-411b-8ad4-315b93b9a61f is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 17 04:26:51.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-395" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":43,"skipped":817,"failed":0}
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:51.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:26:51.332: INFO: The status of Pod busybox-scheduling-070b4ce4-5d7f-4490-9fb9-b88b3ac2ae28 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:26:53.341: INFO: The status of Pod busybox-scheduling-070b4ce4-5d7f-4490-9fb9-b88b3ac2ae28 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 17 04:26:53.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-617" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":44,"skipped":822,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:53.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jan 17 04:26:53.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1850" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":45,"skipped":833,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:53.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 17 04:26:53.565: INFO: Waiting up to 5m0s for pod "pod-30703ee0-0ffa-4861-b94c-2869590f0ec4" in namespace "emptydir-4026" to be "Succeeded or Failed"
Jan 17 04:26:53.576: INFO: Pod "pod-30703ee0-0ffa-4861-b94c-2869590f0ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 11.071241ms
Jan 17 04:26:55.586: INFO: Pod "pod-30703ee0-0ffa-4861-b94c-2869590f0ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021113138s
Jan 17 04:26:57.595: INFO: Pod "pod-30703ee0-0ffa-4861-b94c-2869590f0ec4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029920782s
STEP: Saw pod success
Jan 17 04:26:57.595: INFO: Pod "pod-30703ee0-0ffa-4861-b94c-2869590f0ec4" satisfied condition "Succeeded or Failed"
Jan 17 04:26:57.599: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-30703ee0-0ffa-4861-b94c-2869590f0ec4 container test-container: <nil>
STEP: delete the pod
Jan 17 04:26:57.625: INFO: Waiting for pod pod-30703ee0-0ffa-4861-b94c-2869590f0ec4 to disappear
Jan 17 04:26:57.630: INFO: Pod pod-30703ee0-0ffa-4861-b94c-2869590f0ec4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:26:57.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4026" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":46,"skipped":834,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:26:57.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:26:57.703: INFO: Waiting up to 5m0s for pod "busybox-user-65534-b1ca53cd-9504-4aac-9944-1653146452c5" in namespace "security-context-test-6334" to be "Succeeded or Failed"
Jan 17 04:26:57.708: INFO: Pod "busybox-user-65534-b1ca53cd-9504-4aac-9944-1653146452c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281954ms
Jan 17 04:26:59.718: INFO: Pod "busybox-user-65534-b1ca53cd-9504-4aac-9944-1653146452c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014308797s
Jan 17 04:27:01.728: INFO: Pod "busybox-user-65534-b1ca53cd-9504-4aac-9944-1653146452c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024560343s
Jan 17 04:27:01.728: INFO: Pod "busybox-user-65534-b1ca53cd-9504-4aac-9944-1653146452c5" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 04:27:01.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6334" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":844,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:27:01.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-3e26c95b-0988-4d0c-9725-303fe6aa574a
STEP: Creating a pod to test consume secrets
Jan 17 04:27:01.828: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c" in namespace "projected-8032" to be "Succeeded or Failed"
Jan 17 04:27:01.835: INFO: Pod "pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.550425ms
Jan 17 04:27:03.846: INFO: Pod "pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01733722s
Jan 17 04:27:05.854: INFO: Pod "pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02607821s
STEP: Saw pod success
Jan 17 04:27:05.855: INFO: Pod "pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c" satisfied condition "Succeeded or Failed"
Jan 17 04:27:05.858: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 17 04:27:05.884: INFO: Waiting for pod pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c to disappear
Jan 17 04:27:05.888: INFO: Pod pod-projected-secrets-d10875cc-7b21-4ad3-8402-611187f3eb4c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 04:27:05.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8032" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":48,"skipped":847,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:27:05.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-39a2b87a-e54d-4151-8fa8-c00c64747e1f
STEP: Creating a pod to test consume configMaps
Jan 17 04:27:06.001: INFO: Waiting up to 5m0s for pod "pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a" in namespace "configmap-8554" to be "Succeeded or Failed"
Jan 17 04:27:06.007: INFO: Pod "pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.193203ms
Jan 17 04:27:08.016: INFO: Pod "pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013223688s
Jan 17 04:27:10.026: INFO: Pod "pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023633113s
STEP: Saw pod success
Jan 17 04:27:10.027: INFO: Pod "pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a" satisfied condition "Succeeded or Failed"
Jan 17 04:27:10.030: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:27:10.059: INFO: Waiting for pod pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a to disappear
Jan 17 04:27:10.065: INFO: Pod pod-configmaps-906adf7b-82bf-45ef-8652-2f52b417f10a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:27:10.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8554" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":864,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:27:10.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:27:10.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:27:13.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:27:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-223-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:27:16.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2308" for this suite.
STEP: Destroying namespace "webhook-2308-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.836 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":50,"skipped":927,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:27:16.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:27:17.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 17 04:27:21.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-5326 --namespace=crd-publish-openapi-5326 create -f -'
Jan 17 04:27:23.510: INFO: stderr: ""
Jan 17 04:27:23.510: INFO: stdout: "e2e-test-crd-publish-openapi-2118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 04:27:23.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-5326 --namespace=crd-publish-openapi-5326 delete e2e-test-crd-publish-openapi-2118-crds test-cr'
Jan 17 04:27:23.629: INFO: stderr: ""
Jan 17 04:27:23.629: INFO: stdout: "e2e-test-crd-publish-openapi-2118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 17 04:27:23.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-5326 --namespace=crd-publish-openapi-5326 apply -f -'
Jan 17 04:27:23.938: INFO: stderr: ""
Jan 17 04:27:23.938: INFO: stdout: "e2e-test-crd-publish-openapi-2118-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 17 04:27:23.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-5326 --namespace=crd-publish-openapi-5326 delete e2e-test-crd-publish-openapi-2118-crds test-cr'
Jan 17 04:27:24.052: INFO: stderr: ""
Jan 17 04:27:24.052: INFO: stdout: "e2e-test-crd-publish-openapi-2118-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 17 04:27:24.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-5326 explain e2e-test-crd-publish-openapi-2118-crds'
Jan 17 04:27:25.068: INFO: stderr: ""
Jan 17 04:27:25.068: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2118-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:27:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5326" for this suite.

• [SLOW TEST:13.820 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":51,"skipped":929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:27:30.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0117 04:28:11.148433      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 04:28:11.148: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 04:28:11.148: INFO: Deleting pod "simpletest.rc-28466" in namespace "gc-5927"
Jan 17 04:28:11.182: INFO: Deleting pod "simpletest.rc-2l6t2" in namespace "gc-5927"
Jan 17 04:28:11.245: INFO: Deleting pod "simpletest.rc-46d7j" in namespace "gc-5927"
Jan 17 04:28:11.319: INFO: Deleting pod "simpletest.rc-49glj" in namespace "gc-5927"
Jan 17 04:28:11.348: INFO: Deleting pod "simpletest.rc-4cnfx" in namespace "gc-5927"
Jan 17 04:28:11.400: INFO: Deleting pod "simpletest.rc-4h4kc" in namespace "gc-5927"
Jan 17 04:28:11.474: INFO: Deleting pod "simpletest.rc-4h9hk" in namespace "gc-5927"
Jan 17 04:28:11.559: INFO: Deleting pod "simpletest.rc-4jv26" in namespace "gc-5927"
Jan 17 04:28:11.589: INFO: Deleting pod "simpletest.rc-4n86z" in namespace "gc-5927"
Jan 17 04:28:11.706: INFO: Deleting pod "simpletest.rc-52lqk" in namespace "gc-5927"
Jan 17 04:28:11.740: INFO: Deleting pod "simpletest.rc-54n4x" in namespace "gc-5927"
Jan 17 04:28:11.787: INFO: Deleting pod "simpletest.rc-5cwhx" in namespace "gc-5927"
Jan 17 04:28:11.828: INFO: Deleting pod "simpletest.rc-5zxw4" in namespace "gc-5927"
Jan 17 04:28:11.892: INFO: Deleting pod "simpletest.rc-6dcqf" in namespace "gc-5927"
Jan 17 04:28:11.940: INFO: Deleting pod "simpletest.rc-6tb5z" in namespace "gc-5927"
Jan 17 04:28:11.981: INFO: Deleting pod "simpletest.rc-6wbpx" in namespace "gc-5927"
Jan 17 04:28:12.030: INFO: Deleting pod "simpletest.rc-6znc5" in namespace "gc-5927"
Jan 17 04:28:12.110: INFO: Deleting pod "simpletest.rc-7l9gt" in namespace "gc-5927"
Jan 17 04:28:12.172: INFO: Deleting pod "simpletest.rc-7m8lh" in namespace "gc-5927"
Jan 17 04:28:12.237: INFO: Deleting pod "simpletest.rc-8fjrb" in namespace "gc-5927"
Jan 17 04:28:12.316: INFO: Deleting pod "simpletest.rc-8mwss" in namespace "gc-5927"
Jan 17 04:28:12.408: INFO: Deleting pod "simpletest.rc-9hdg4" in namespace "gc-5927"
Jan 17 04:28:12.486: INFO: Deleting pod "simpletest.rc-9nwx4" in namespace "gc-5927"
Jan 17 04:28:12.582: INFO: Deleting pod "simpletest.rc-9ztrf" in namespace "gc-5927"
Jan 17 04:28:12.671: INFO: Deleting pod "simpletest.rc-b7qm2" in namespace "gc-5927"
Jan 17 04:28:12.709: INFO: Deleting pod "simpletest.rc-bbng4" in namespace "gc-5927"
Jan 17 04:28:12.756: INFO: Deleting pod "simpletest.rc-bwjx2" in namespace "gc-5927"
Jan 17 04:28:12.800: INFO: Deleting pod "simpletest.rc-bwnsl" in namespace "gc-5927"
Jan 17 04:28:12.841: INFO: Deleting pod "simpletest.rc-c95nc" in namespace "gc-5927"
Jan 17 04:28:12.902: INFO: Deleting pod "simpletest.rc-cfrpk" in namespace "gc-5927"
Jan 17 04:28:12.938: INFO: Deleting pod "simpletest.rc-ch2pd" in namespace "gc-5927"
Jan 17 04:28:12.996: INFO: Deleting pod "simpletest.rc-chxgs" in namespace "gc-5927"
Jan 17 04:28:13.062: INFO: Deleting pod "simpletest.rc-ck84j" in namespace "gc-5927"
Jan 17 04:28:13.144: INFO: Deleting pod "simpletest.rc-cmqp6" in namespace "gc-5927"
Jan 17 04:28:13.185: INFO: Deleting pod "simpletest.rc-cp5bn" in namespace "gc-5927"
Jan 17 04:28:13.248: INFO: Deleting pod "simpletest.rc-csr46" in namespace "gc-5927"
Jan 17 04:28:13.315: INFO: Deleting pod "simpletest.rc-cx2nd" in namespace "gc-5927"
Jan 17 04:28:13.361: INFO: Deleting pod "simpletest.rc-d4xwx" in namespace "gc-5927"
Jan 17 04:28:13.405: INFO: Deleting pod "simpletest.rc-d5wj8" in namespace "gc-5927"
Jan 17 04:28:13.460: INFO: Deleting pod "simpletest.rc-d8lsg" in namespace "gc-5927"
Jan 17 04:28:13.504: INFO: Deleting pod "simpletest.rc-dhxmr" in namespace "gc-5927"
Jan 17 04:28:13.606: INFO: Deleting pod "simpletest.rc-dx7fz" in namespace "gc-5927"
Jan 17 04:28:13.647: INFO: Deleting pod "simpletest.rc-f8jkt" in namespace "gc-5927"
Jan 17 04:28:13.773: INFO: Deleting pod "simpletest.rc-fx4jr" in namespace "gc-5927"
Jan 17 04:28:13.838: INFO: Deleting pod "simpletest.rc-g9bn7" in namespace "gc-5927"
Jan 17 04:28:13.987: INFO: Deleting pod "simpletest.rc-gtjww" in namespace "gc-5927"
Jan 17 04:28:14.084: INFO: Deleting pod "simpletest.rc-hbfnp" in namespace "gc-5927"
Jan 17 04:28:14.200: INFO: Deleting pod "simpletest.rc-hdmtr" in namespace "gc-5927"
Jan 17 04:28:14.367: INFO: Deleting pod "simpletest.rc-hlzsg" in namespace "gc-5927"
Jan 17 04:28:14.445: INFO: Deleting pod "simpletest.rc-j5mpv" in namespace "gc-5927"
Jan 17 04:28:14.577: INFO: Deleting pod "simpletest.rc-jbs9k" in namespace "gc-5927"
Jan 17 04:28:14.665: INFO: Deleting pod "simpletest.rc-jqkh9" in namespace "gc-5927"
Jan 17 04:28:14.712: INFO: Deleting pod "simpletest.rc-jwhc9" in namespace "gc-5927"
Jan 17 04:28:14.767: INFO: Deleting pod "simpletest.rc-k46sl" in namespace "gc-5927"
Jan 17 04:28:14.808: INFO: Deleting pod "simpletest.rc-l29p4" in namespace "gc-5927"
Jan 17 04:28:14.866: INFO: Deleting pod "simpletest.rc-l78vs" in namespace "gc-5927"
Jan 17 04:28:14.904: INFO: Deleting pod "simpletest.rc-lf9pl" in namespace "gc-5927"
Jan 17 04:28:15.038: INFO: Deleting pod "simpletest.rc-llbkd" in namespace "gc-5927"
Jan 17 04:28:15.116: INFO: Deleting pod "simpletest.rc-lpcbf" in namespace "gc-5927"
Jan 17 04:28:15.199: INFO: Deleting pod "simpletest.rc-m2wg7" in namespace "gc-5927"
Jan 17 04:28:15.272: INFO: Deleting pod "simpletest.rc-mnwmq" in namespace "gc-5927"
Jan 17 04:28:15.340: INFO: Deleting pod "simpletest.rc-mw96d" in namespace "gc-5927"
Jan 17 04:28:15.460: INFO: Deleting pod "simpletest.rc-mwxt7" in namespace "gc-5927"
Jan 17 04:28:15.538: INFO: Deleting pod "simpletest.rc-mx7tr" in namespace "gc-5927"
Jan 17 04:28:15.674: INFO: Deleting pod "simpletest.rc-mzzmf" in namespace "gc-5927"
Jan 17 04:28:15.757: INFO: Deleting pod "simpletest.rc-n2xgn" in namespace "gc-5927"
Jan 17 04:28:15.809: INFO: Deleting pod "simpletest.rc-nrdkw" in namespace "gc-5927"
Jan 17 04:28:15.876: INFO: Deleting pod "simpletest.rc-nvxmm" in namespace "gc-5927"
Jan 17 04:28:15.918: INFO: Deleting pod "simpletest.rc-p462q" in namespace "gc-5927"
Jan 17 04:28:15.962: INFO: Deleting pod "simpletest.rc-ppk64" in namespace "gc-5927"
Jan 17 04:28:16.001: INFO: Deleting pod "simpletest.rc-pq86k" in namespace "gc-5927"
Jan 17 04:28:16.078: INFO: Deleting pod "simpletest.rc-qfqlc" in namespace "gc-5927"
Jan 17 04:28:16.192: INFO: Deleting pod "simpletest.rc-r9j42" in namespace "gc-5927"
Jan 17 04:28:16.254: INFO: Deleting pod "simpletest.rc-rcjcp" in namespace "gc-5927"
Jan 17 04:28:16.318: INFO: Deleting pod "simpletest.rc-rlcv6" in namespace "gc-5927"
Jan 17 04:28:16.402: INFO: Deleting pod "simpletest.rc-rqkfj" in namespace "gc-5927"
Jan 17 04:28:16.496: INFO: Deleting pod "simpletest.rc-smrdm" in namespace "gc-5927"
Jan 17 04:28:16.562: INFO: Deleting pod "simpletest.rc-sndnk" in namespace "gc-5927"
Jan 17 04:28:16.637: INFO: Deleting pod "simpletest.rc-t2jwq" in namespace "gc-5927"
Jan 17 04:28:16.681: INFO: Deleting pod "simpletest.rc-t4rlv" in namespace "gc-5927"
Jan 17 04:28:16.763: INFO: Deleting pod "simpletest.rc-t89mx" in namespace "gc-5927"
Jan 17 04:28:16.890: INFO: Deleting pod "simpletest.rc-t954q" in namespace "gc-5927"
Jan 17 04:28:16.968: INFO: Deleting pod "simpletest.rc-tgshn" in namespace "gc-5927"
Jan 17 04:28:17.070: INFO: Deleting pod "simpletest.rc-tp89p" in namespace "gc-5927"
Jan 17 04:28:17.145: INFO: Deleting pod "simpletest.rc-trbpl" in namespace "gc-5927"
Jan 17 04:28:17.199: INFO: Deleting pod "simpletest.rc-trlss" in namespace "gc-5927"
Jan 17 04:28:17.258: INFO: Deleting pod "simpletest.rc-v6hvc" in namespace "gc-5927"
Jan 17 04:28:17.304: INFO: Deleting pod "simpletest.rc-vlg2g" in namespace "gc-5927"
Jan 17 04:28:17.372: INFO: Deleting pod "simpletest.rc-vw5pd" in namespace "gc-5927"
Jan 17 04:28:17.424: INFO: Deleting pod "simpletest.rc-vwmlh" in namespace "gc-5927"
Jan 17 04:28:17.492: INFO: Deleting pod "simpletest.rc-wfl2b" in namespace "gc-5927"
Jan 17 04:28:17.545: INFO: Deleting pod "simpletest.rc-x2lsq" in namespace "gc-5927"
Jan 17 04:28:17.628: INFO: Deleting pod "simpletest.rc-x4tkd" in namespace "gc-5927"
Jan 17 04:28:17.748: INFO: Deleting pod "simpletest.rc-xjl67" in namespace "gc-5927"
Jan 17 04:28:17.838: INFO: Deleting pod "simpletest.rc-xw55l" in namespace "gc-5927"
Jan 17 04:28:17.878: INFO: Deleting pod "simpletest.rc-z2dxq" in namespace "gc-5927"
Jan 17 04:28:17.929: INFO: Deleting pod "simpletest.rc-z4js6" in namespace "gc-5927"
Jan 17 04:28:17.968: INFO: Deleting pod "simpletest.rc-zbtn4" in namespace "gc-5927"
Jan 17 04:28:18.018: INFO: Deleting pod "simpletest.rc-zlrhp" in namespace "gc-5927"
Jan 17 04:28:18.067: INFO: Deleting pod "simpletest.rc-zm864" in namespace "gc-5927"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 04:28:18.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5927" for this suite.

• [SLOW TEST:47.474 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":52,"skipped":960,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:28:18.218: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-993993d3-10ef-4190-81c8-e8ad98918e12
STEP: Creating secret with name secret-projected-all-test-volume-eeaa82ae-0de7-495a-98b7-3e66ba8552c2
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 17 04:28:18.506: INFO: Waiting up to 5m0s for pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282" in namespace "projected-5163" to be "Succeeded or Failed"
Jan 17 04:28:18.528: INFO: Pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282": Phase="Pending", Reason="", readiness=false. Elapsed: 22.057972ms
Jan 17 04:28:20.537: INFO: Pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031246373s
Jan 17 04:28:22.547: INFO: Pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041516115s
Jan 17 04:28:24.554: INFO: Pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048091492s
STEP: Saw pod success
Jan 17 04:28:24.554: INFO: Pod "projected-volume-9d26022b-6d75-4672-8a6b-16661918b282" satisfied condition "Succeeded or Failed"
Jan 17 04:28:24.558: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod projected-volume-9d26022b-6d75-4672-8a6b-16661918b282 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 17 04:28:24.674: INFO: Waiting for pod projected-volume-9d26022b-6d75-4672-8a6b-16661918b282 to disappear
Jan 17 04:28:24.683: INFO: Pod projected-volume-9d26022b-6d75-4672-8a6b-16661918b282 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jan 17 04:28:24.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5163" for this suite.

• [SLOW TEST:6.490 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":53,"skipped":980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:28:24.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:28:25.672: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:28:28.748: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:28:28.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:28:32.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3374" for this suite.
STEP: Destroying namespace "webhook-3374-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.586 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":54,"skipped":1005,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:28:32.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 04:28:32.361: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 04:28:32.375: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 04:28:32.382: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-0 before test
Jan 17 04:28:32.399: INFO: calico-node-xdhq9 from kube-system started at 2023-01-17 02:35:55 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:32.399: INFO: csi-cinder-nodeplugin-tbzrq from kube-system started at 2023-01-17 02:36:15 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:32.399: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:32.399: INFO: magnum-kube-prometheus-sta-operator-675d58f6dc-bc5jf from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 04:28:32.399: INFO: magnum-kube-state-metrics-7645bf695b-2vs67 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 04:28:32.399: INFO: magnum-metrics-server-d6ddcd656-4phlx from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 04:28:32.399: INFO: magnum-prometheus-node-exporter-6ln7n from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:32.399: INFO: npd-8pbfg from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:32.399: INFO: sonobuoy from sonobuoy started at 2023-01-17 04:09:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 04:28:32.399: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-fcb54 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:32.399: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:28:32.399: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-1 before test
Jan 17 04:28:32.417: INFO: calico-node-9shzb from kube-system started at 2023-01-17 02:35:49 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:32.417: INFO: csi-cinder-nodeplugin-qhmg4 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:32.417: INFO: kube-dns-autoscaler-5b9649896b-427d6 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 04:28:32.417: INFO: magnum-grafana-d59cf7df4-shfs9 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (3 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container grafana ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 04:28:32.417: INFO: magnum-prometheus-node-exporter-sr8z8 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:32.417: INFO: npd-b7kb8 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:32.417: INFO: sonobuoy-e2e-job-6c6d2af065634bb1 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container e2e ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:32.417: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-lnwl6 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.417: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:28:32.417: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-2 before test
Jan 17 04:28:32.436: INFO: calico-node-wwxlv from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:32.436: INFO: csi-cinder-nodeplugin-lxsqw from kube-system started at 2023-01-17 02:36:35 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:32.436: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:32.436: INFO: magnum-prometheus-node-exporter-xdw8z from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:32.436: INFO: npd-c4sq2 from kube-system started at 2023-01-17 02:36:35 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:32.436: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 02:36:39 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 04:28:32.436: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 04:28:32.436: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-9j7nc from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:32.436: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:32.436: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fa744a5f-0b0e-4841-b157-41275b3cdc43 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-fa744a5f-0b0e-4841-b157-41275b3cdc43 off the node cluster124-apihrjet4zqi-node-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fa744a5f-0b0e-4841-b157-41275b3cdc43
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:28:36.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6070" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":55,"skipped":1007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:28:36.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 04:28:36.744: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 04:28:36.765: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 04:28:36.770: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-0 before test
Jan 17 04:28:36.781: INFO: calico-node-xdhq9 from kube-system started at 2023-01-17 02:35:55 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:36.781: INFO: csi-cinder-nodeplugin-tbzrq from kube-system started at 2023-01-17 02:36:15 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:36.781: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:36.781: INFO: magnum-kube-prometheus-sta-operator-675d58f6dc-bc5jf from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 04:28:36.781: INFO: magnum-kube-state-metrics-7645bf695b-2vs67 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 04:28:36.781: INFO: magnum-metrics-server-d6ddcd656-4phlx from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 04:28:36.781: INFO: magnum-prometheus-node-exporter-6ln7n from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:36.781: INFO: npd-8pbfg from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:36.781: INFO: sonobuoy from sonobuoy started at 2023-01-17 04:09:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 04:28:36.781: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-fcb54 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:36.781: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:28:36.781: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-1 before test
Jan 17 04:28:36.793: INFO: calico-node-9shzb from kube-system started at 2023-01-17 02:35:49 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.793: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:36.793: INFO: csi-cinder-nodeplugin-qhmg4 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:36.794: INFO: kube-dns-autoscaler-5b9649896b-427d6 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 04:28:36.794: INFO: magnum-grafana-d59cf7df4-shfs9 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (3 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container grafana ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 04:28:36.794: INFO: magnum-prometheus-node-exporter-sr8z8 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:36.794: INFO: npd-b7kb8 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:36.794: INFO: sonobuoy-e2e-job-6c6d2af065634bb1 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container e2e ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:36.794: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-lnwl6 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.794: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:28:36.794: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-2 before test
Jan 17 04:28:36.806: INFO: calico-node-wwxlv from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:28:36.806: INFO: csi-cinder-nodeplugin-lxsqw from kube-system started at 2023-01-17 02:36:35 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:28:36.806: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:28:36.806: INFO: magnum-prometheus-node-exporter-xdw8z from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:28:36.806: INFO: npd-c4sq2 from kube-system started at 2023-01-17 02:36:35 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:28:36.806: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 02:36:39 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 04:28:36.806: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 04:28:36.806: INFO: with-labels from sched-pred-6070 started at 2023-01-17 04:28:34 +0000 UTC (1 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container with-labels ready: true, restart count 0
Jan 17 04:28:36.806: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-9j7nc from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:28:36.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:28:36.806: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node cluster124-apihrjet4zqi-node-0
STEP: verifying the node has the label node cluster124-apihrjet4zqi-node-1
STEP: verifying the node has the label node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.942: INFO: Pod calico-node-9shzb requesting resource cpu=250m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.942: INFO: Pod calico-node-wwxlv requesting resource cpu=250m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.942: INFO: Pod calico-node-xdhq9 requesting resource cpu=250m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.942: INFO: Pod csi-cinder-nodeplugin-lxsqw requesting resource cpu=50m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.943: INFO: Pod csi-cinder-nodeplugin-qhmg4 requesting resource cpu=50m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.943: INFO: Pod csi-cinder-nodeplugin-tbzrq requesting resource cpu=50m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod kube-dns-autoscaler-5b9649896b-427d6 requesting resource cpu=20m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.943: INFO: Pod magnum-grafana-d59cf7df4-shfs9 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.943: INFO: Pod magnum-kube-prometheus-sta-operator-675d58f6dc-bc5jf requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod magnum-kube-state-metrics-7645bf695b-2vs67 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod magnum-metrics-server-d6ddcd656-4phlx requesting resource cpu=100m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod magnum-prometheus-node-exporter-6ln7n requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod magnum-prometheus-node-exporter-sr8z8 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.943: INFO: Pod magnum-prometheus-node-exporter-xdw8z requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.943: INFO: Pod npd-8pbfg requesting resource cpu=20m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod npd-b7kb8 requesting resource cpu=20m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.943: INFO: Pod npd-c4sq2 requesting resource cpu=20m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.943: INFO: Pod prometheus-magnum-kube-prometheus-sta-prometheus-0 requesting resource cpu=356m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.943: INFO: Pod with-labels requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.943: INFO: Pod sonobuoy requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.943: INFO: Pod sonobuoy-e2e-job-6c6d2af065634bb1 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:36.944: INFO: Pod sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-9j7nc requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-2
Jan 17 04:28:36.944: INFO: Pod sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-fcb54 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.944: INFO: Pod sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-lnwl6 requesting resource cpu=0m on Node cluster124-apihrjet4zqi-node-1
STEP: Starting Pods to consume most of the cluster CPU.
Jan 17 04:28:36.944: INFO: Creating a pod which consumes cpu=2506m on Node cluster124-apihrjet4zqi-node-0
Jan 17 04:28:36.967: INFO: Creating a pod which consumes cpu=2562m on Node cluster124-apihrjet4zqi-node-1
Jan 17 04:28:37.002: INFO: Creating a pod which consumes cpu=2326m on Node cluster124-apihrjet4zqi-node-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee.173afe35751eddd5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1595/filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee to cluster124-apihrjet4zqi-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee.173afe35a37dfff0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee.173afe35a62d7e34], Reason = [Created], Message = [Created container filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee.173afe35abe7bfb1], Reason = [Started], Message = [Started container filler-pod-245f2f93-4378-4676-a0de-503ed9b60bee]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7.173afe35718feb61], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1595/filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7 to cluster124-apihrjet4zqi-node-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7.173afe35b03c8db7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7.173afe35b33d5107], Reason = [Created], Message = [Created container filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7.173afe35b88cf878], Reason = [Started], Message = [Started container filler-pod-9a8db582-debf-4ed0-b076-6ba7b7a8d3b7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64.173afe35727048a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1595/filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64 to cluster124-apihrjet4zqi-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64.173afe35a3822419], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64.173afe35a5ea02bf], Reason = [Created], Message = [Created container filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64.173afe35aad5d751], Reason = [Started], Message = [Started container filler-pod-f0bda3a0-61e5-4f67-b3e4-65bb0bd95e64]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173afe35ef7f05cd], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.]
STEP: removing the label node off the node cluster124-apihrjet4zqi-node-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cluster124-apihrjet4zqi-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node cluster124-apihrjet4zqi-node-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:28:40.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1595" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":56,"skipped":1041,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:28:40.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1504
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-1504
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1504
Jan 17 04:28:40.377: INFO: Found 0 stateful pods, waiting for 1
Jan 17 04:28:50.388: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 17 04:28:50.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:28:50.639: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:28:50.639: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:28:50.639: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:28:50.645: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 04:29:00.657: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:29:00.657: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:29:00.695: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 04:29:00.695: INFO: ss-0  cluster124-apihrjet4zqi-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:40 +0000 UTC  }]
Jan 17 04:29:00.695: INFO: 
Jan 17 04:29:00.696: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 17 04:29:01.705: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99339475s
Jan 17 04:29:02.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983767536s
Jan 17 04:29:03.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.966748986s
Jan 17 04:29:04.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951388707s
Jan 17 04:29:05.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.939143037s
Jan 17 04:29:06.767: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.930667127s
Jan 17 04:29:07.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.92191929s
Jan 17 04:29:08.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.911131882s
Jan 17 04:29:09.801: INFO: Verifying statefulset ss doesn't scale past 3 for another 899.136014ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1504
Jan 17 04:29:10.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:29:11.092: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:29:11.092: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:29:11.092: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:29:11.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:29:11.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 04:29:11.386: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:29:11.386: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:29:11.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:29:11.652: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 17 04:29:11.652: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:29:11.652: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:29:11.660: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 17 04:29:21.673: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:29:21.673: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:29:21.673: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 17 04:29:21.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:29:21.924: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:29:21.924: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:29:21.924: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:29:21.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:29:22.147: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:29:22.147: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:29:22.147: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:29:22.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-1504 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:29:22.407: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:29:22.407: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:29:22.407: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:29:22.407: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:29:22.418: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 17 04:29:32.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:29:32.435: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:29:32.435: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:29:32.462: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 04:29:32.462: INFO: ss-0  cluster124-apihrjet4zqi-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:28:40 +0000 UTC  }]
Jan 17 04:29:32.462: INFO: ss-1  cluster124-apihrjet4zqi-node-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  }]
Jan 17 04:29:32.462: INFO: ss-2  cluster124-apihrjet4zqi-node-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  }]
Jan 17 04:29:32.462: INFO: 
Jan 17 04:29:32.462: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 17 04:29:33.472: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Jan 17 04:29:33.472: INFO: ss-1  cluster124-apihrjet4zqi-node-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  }]
Jan 17 04:29:33.472: INFO: ss-2  cluster124-apihrjet4zqi-node-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:23 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:29:00 +0000 UTC  }]
Jan 17 04:29:33.472: INFO: 
Jan 17 04:29:33.472: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 17 04:29:34.482: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.983732597s
Jan 17 04:29:35.490: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.974532548s
Jan 17 04:29:36.505: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.966057898s
Jan 17 04:29:37.514: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.951101055s
Jan 17 04:29:38.521: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.942513927s
Jan 17 04:29:39.527: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935243314s
Jan 17 04:29:40.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.929187702s
Jan 17 04:29:41.545: INFO: Verifying statefulset ss doesn't scale past 0 for another 920.878899ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1504
Jan 17 04:29:42.554: INFO: Scaling statefulset ss to 0
Jan 17 04:29:42.568: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 04:29:42.572: INFO: Deleting all statefulset in ns statefulset-1504
Jan 17 04:29:42.579: INFO: Scaling statefulset ss to 0
Jan 17 04:29:42.595: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:29:42.599: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 04:29:42.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1504" for this suite.

• [SLOW TEST:62.398 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":57,"skipped":1042,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:42.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 17 04:29:42.750: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:29:44.757: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 17 04:29:45.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3361" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":58,"skipped":1050,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:29:47.919: INFO: Deleting pod "var-expansion-2f579654-88a9-426c-9351-a4d7ccfd70bb" in namespace "var-expansion-6647"
Jan 17 04:29:47.941: INFO: Wait up to 5m0s for pod "var-expansion-2f579654-88a9-426c-9351-a4d7ccfd70bb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:29:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6647" for this suite.

• [SLOW TEST:6.157 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":59,"skipped":1051,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:51.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:29:52.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-361" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":60,"skipped":1060,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:52.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 17 04:29:52.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7935" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":1076,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:52.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 17 04:29:52.297: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 17 04:29:57.321: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 17 04:29:57.327: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 04:29:57.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5503" for this suite.

• [SLOW TEST:5.231 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":62,"skipped":1088,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:29:57.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:30:10.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2980" for this suite.

• [SLOW TEST:13.270 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":63,"skipped":1095,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:30:10.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d" in namespace "projected-2465" to be "Succeeded or Failed"
Jan 17 04:30:10.797: INFO: Pod "downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.313728ms
Jan 17 04:30:12.804: INFO: Pod "downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01503733s
Jan 17 04:30:14.810: INFO: Pod "downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021860861s
STEP: Saw pod success
Jan 17 04:30:14.810: INFO: Pod "downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d" satisfied condition "Succeeded or Failed"
Jan 17 04:30:14.814: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d container client-container: <nil>
STEP: delete the pod
Jan 17 04:30:14.906: INFO: Waiting for pod downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d to disappear
Jan 17 04:30:14.916: INFO: Pod downwardapi-volume-c5423adb-e2fc-4cbd-8ebf-b9e97a63730d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 04:30:14.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2465" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":1108,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:14.962: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 17 04:30:15.025: INFO: Waiting up to 5m0s for pod "security-context-d845fdca-6d1c-4879-945e-f31573ff5589" in namespace "security-context-6690" to be "Succeeded or Failed"
Jan 17 04:30:15.045: INFO: Pod "security-context-d845fdca-6d1c-4879-945e-f31573ff5589": Phase="Pending", Reason="", readiness=false. Elapsed: 20.273695ms
Jan 17 04:30:17.077: INFO: Pod "security-context-d845fdca-6d1c-4879-945e-f31573ff5589": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052239549s
Jan 17 04:30:19.090: INFO: Pod "security-context-d845fdca-6d1c-4879-945e-f31573ff5589": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065163127s
STEP: Saw pod success
Jan 17 04:30:19.090: INFO: Pod "security-context-d845fdca-6d1c-4879-945e-f31573ff5589" satisfied condition "Succeeded or Failed"
Jan 17 04:30:19.094: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod security-context-d845fdca-6d1c-4879-945e-f31573ff5589 container test-container: <nil>
STEP: delete the pod
Jan 17 04:30:19.135: INFO: Waiting for pod security-context-d845fdca-6d1c-4879-945e-f31573ff5589 to disappear
Jan 17 04:30:19.141: INFO: Pod security-context-d845fdca-6d1c-4879-945e-f31573ff5589 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 04:30:19.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6690" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":65,"skipped":1122,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:19.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 17 04:30:19.226: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 17 04:30:19.246: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 04:30:19.246: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 17 04:30:19.285: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 17 04:30:19.285: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 17 04:30:19.305: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 17 04:30:19.305: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 17 04:30:26.392: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jan 17 04:30:26.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-6878" for this suite.

• [SLOW TEST:7.288 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":66,"skipped":1140,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:26.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:30:26.559: INFO: Create a RollingUpdate DaemonSet
Jan 17 04:30:26.573: INFO: Check that daemon pods launch on every node of the cluster
Jan 17 04:30:26.586: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:26.586: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:26.586: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:26.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:30:26.593: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:30:27.610: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:27.610: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:27.610: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:27.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:30:27.616: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:30:28.605: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:28.605: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:28.606: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:28.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:30:28.610: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 17 04:30:28.610: INFO: Update the DaemonSet to trigger a rollout
Jan 17 04:30:28.636: INFO: Updating DaemonSet daemon-set
Jan 17 04:30:31.663: INFO: Roll back the DaemonSet before rollout is complete
Jan 17 04:30:31.684: INFO: Updating DaemonSet daemon-set
Jan 17 04:30:31.684: INFO: Make sure DaemonSet rollback is complete
Jan 17 04:30:31.693: INFO: Wrong image for pod: daemon-set-ztj25. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 17 04:30:31.693: INFO: Pod daemon-set-ztj25 is not available
Jan 17 04:30:31.706: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:31.706: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:31.706: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:32.721: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:32.721: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:32.721: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:33.722: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:33.722: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:33.722: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:34.725: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:34.725: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:34.725: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:35.717: INFO: Pod daemon-set-tdx7j is not available
Jan 17 04:30:35.723: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:35.723: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:30:35.723: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5907, will wait for the garbage collector to delete the pods
Jan 17 04:30:35.839: INFO: Deleting DaemonSet.extensions daemon-set took: 46.123845ms
Jan 17 04:30:36.039: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.354258ms
Jan 17 04:30:38.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:30:38.983: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 04:30:39.025: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27703"},"items":null}

Jan 17 04:30:39.032: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27703"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:30:39.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5907" for this suite.

• [SLOW TEST:12.645 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":67,"skipped":1143,"failed":0}
SSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-3169-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 17 04:30:39.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3169" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":68,"skipped":1148,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:30:39.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 17 04:30:39.310: INFO: PodSpec: initContainers in spec.initContainers
Jan 17 04:31:22.615: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7b1a01a1-5bda-4f9c-8684-fd2cdc23f3b8", GenerateName:"", Namespace:"init-container-1792", SelfLink:"", UID:"c7b9795f-46d1-4f9d-a02d-ac8f1aa010c1", ResourceVersion:"27888", Generation:0, CreationTimestamp:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"310318072"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"d198811d81a459de2b31f8715b9c5226e4026bf0daa2a732690e08fe028a3e5b", "cni.projectcalico.org/podIP":"10.100.11.241/32", "cni.projectcalico.org/podIPs":"10.100.11.241/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002cca1f8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 4, 30, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002cca228), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 17, 4, 30, 40, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002cca258), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-smjpj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0070a8480), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-smjpj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-smjpj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-smjpj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004bfe818), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"cluster124-apihrjet4zqi-node-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0022c3110), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bfe890)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004bfe8b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004bfe8b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004bfe8bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001ed2ae0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.9", PodIP:"10.100.11.241", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.11.241"}}, StartTime:time.Date(2023, time.January, 17, 4, 30, 39, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022c31f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022c3260)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://406d24b875e5af00ab60c04df325943212e4f7b6e86feaf06be338f790a36a1b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0070a8500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0070a84e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc004bfe91c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 04:31:22.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1792" for this suite.

• [SLOW TEST:43.419 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":69,"skipped":1155,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:22.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jan 17 04:31:22.773: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jan 17 04:31:22.894: INFO: waiting for watch events with expected annotations in namespace
Jan 17 04:31:22.894: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jan 17 04:31:22.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-5181" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":70,"skipped":1170,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:23.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:31:23.072: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74" in namespace "projected-4263" to be "Succeeded or Failed"
Jan 17 04:31:23.106: INFO: Pod "downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74": Phase="Pending", Reason="", readiness=false. Elapsed: 33.897369ms
Jan 17 04:31:25.115: INFO: Pod "downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042434987s
Jan 17 04:31:27.129: INFO: Pod "downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057022447s
STEP: Saw pod success
Jan 17 04:31:27.129: INFO: Pod "downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74" satisfied condition "Succeeded or Failed"
Jan 17 04:31:27.134: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74 container client-container: <nil>
STEP: delete the pod
Jan 17 04:31:27.227: INFO: Waiting for pod downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74 to disappear
Jan 17 04:31:27.234: INFO: Pod downwardapi-volume-5e5d1f8b-931f-432b-b739-f43c60b7bc74 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 04:31:27.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4263" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":71,"skipped":1178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:27.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 17 04:31:27.327: INFO: Waiting up to 5m0s for pod "pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0" in namespace "emptydir-6253" to be "Succeeded or Failed"
Jan 17 04:31:27.365: INFO: Pod "pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.118777ms
Jan 17 04:31:29.391: INFO: Pod "pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064453962s
Jan 17 04:31:31.409: INFO: Pod "pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081906558s
STEP: Saw pod success
Jan 17 04:31:31.409: INFO: Pod "pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0" satisfied condition "Succeeded or Failed"
Jan 17 04:31:31.413: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0 container test-container: <nil>
STEP: delete the pod
Jan 17 04:31:31.455: INFO: Waiting for pod pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0 to disappear
Jan 17 04:31:31.465: INFO: Pod pod-b856d73e-14cf-4d4c-aceb-a6e6b17079e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:31:31.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6253" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":72,"skipped":1225,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:31.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
STEP: mirroring an update to a custom Endpoint
Jan 17 04:31:31.640: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
Jan 17 04:31:33.690: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jan 17 04:31:35.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9012" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":73,"skipped":1234,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:35.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 17 04:31:35.803: INFO: Waiting up to 5m0s for pod "downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a" in namespace "downward-api-4014" to be "Succeeded or Failed"
Jan 17 04:31:35.818: INFO: Pod "downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.738425ms
Jan 17 04:31:37.834: INFO: Pod "downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030998611s
Jan 17 04:31:39.848: INFO: Pod "downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044014867s
STEP: Saw pod success
Jan 17 04:31:39.848: INFO: Pod "downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a" satisfied condition "Succeeded or Failed"
Jan 17 04:31:39.861: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a container dapi-container: <nil>
STEP: delete the pod
Jan 17 04:31:39.910: INFO: Waiting for pod downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a to disappear
Jan 17 04:31:39.919: INFO: Pod downward-api-1fb4144e-c462-4ac4-85a9-6f84d362960a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 17 04:31:39.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4014" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":74,"skipped":1236,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:39.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-3c440602-e415-4405-ac12-55f091a5e32f
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:31:40.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1594" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":75,"skipped":1248,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:40.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename conformance-tests
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jan 17 04:31:40.089: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jan 17 04:31:40.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-6437" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":76,"skipped":1324,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:40.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 17 04:31:40.238: INFO: The status of Pod labelsupdate158b4f97-4c29-48dd-93d6-727d88186420 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:31:42.251: INFO: The status of Pod labelsupdate158b4f97-4c29-48dd-93d6-727d88186420 is Running (Ready = true)
Jan 17 04:31:42.802: INFO: Successfully updated pod "labelsupdate158b4f97-4c29-48dd-93d6-727d88186420"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 04:31:46.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8511" for this suite.

• [SLOW TEST:6.722 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":77,"skipped":1330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 17 04:31:47.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3967" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":78,"skipped":1356,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:47.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:31:47.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 create -f -'
Jan 17 04:31:48.583: INFO: stderr: ""
Jan 17 04:31:48.583: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 17 04:31:48.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 create -f -'
Jan 17 04:31:50.315: INFO: stderr: ""
Jan 17 04:31:50.315: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 17 04:31:51.328: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:31:51.328: INFO: Found 1 / 1
Jan 17 04:31:51.328: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 04:31:51.333: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:31:51.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 04:31:51.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 describe pod agnhost-primary-46shw'
Jan 17 04:31:51.436: INFO: stderr: ""
Jan 17 04:31:51.436: INFO: stdout: "Name:         agnhost-primary-46shw\nNamespace:    kubectl-8277\nPriority:     0\nNode:         cluster124-apihrjet4zqi-node-2/10.0.0.9\nStart Time:   Tue, 17 Jan 2023 04:31:48 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 965980601cbf14002f08d5eeb7778bf3731288906f058f596a82b5ee43e9f4bd\n              cni.projectcalico.org/podIP: 10.100.11.250/32\n              cni.projectcalico.org/podIPs: 10.100.11.250/32\nStatus:       Running\nIP:           10.100.11.250\nIPs:\n  IP:           10.100.11.250\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://bb28fcd1de14bb7d1790ff7b1e1525f60c4045310af7722ddb6a42131f11f525\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 17 Jan 2023 04:31:49 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qqwk4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qqwk4:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8277/agnhost-primary-46shw to cluster124-apihrjet4zqi-node-2\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 17 04:31:51.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 describe rc agnhost-primary'
Jan 17 04:31:51.542: INFO: stderr: ""
Jan 17 04:31:51.543: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8277\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-46shw\n"
Jan 17 04:31:51.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 describe service agnhost-primary'
Jan 17 04:31:51.634: INFO: stderr: ""
Jan 17 04:31:51.634: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8277\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.254.54.90\nIPs:               10.254.54.90\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.11.250:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 17 04:31:51.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 describe node cluster124-apihrjet4zqi-master-0'
Jan 17 04:31:51.761: INFO: stderr: ""
Jan 17 04:31:51.761: INFO: stdout: "Name:               cluster124-apihrjet4zqi-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c1.c2r4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=nz-ppd-1\n                    failure-domain.beta.kubernetes.io/zone=nz-ppd-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=cluster124-apihrjet4zqi-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=c1.c2r4\n                    topology.kubernetes.io/region=nz-ppd-1\n                    topology.kubernetes.io/zone=nz-ppd-1a\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.19/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 17 Jan 2023 02:33:33 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  cluster124-apihrjet4zqi-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 17 Jan 2023 04:31:48 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 17 Jan 2023 02:34:17 +0000   Tue, 17 Jan 2023 02:34:17 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 17 Jan 2023 04:30:57 +0000   Tue, 17 Jan 2023 02:33:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 17 Jan 2023 04:30:57 +0000   Tue, 17 Jan 2023 02:33:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 17 Jan 2023 04:30:57 +0000   Tue, 17 Jan 2023 02:33:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 17 Jan 2023 04:30:57 +0000   Tue, 17 Jan 2023 02:34:14 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.19\n  ExternalIP:  10.10.8.58\n  Hostname:    cluster124-apihrjet4zqi-master-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20435948Ki\n  hugepages-2Mi:      0\n  memory:             4011816Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  18833769646\n  hugepages-2Mi:      0\n  memory:             3909416Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 ee4d24e554a94bd289b83ffd116be4ee\n  System UUID:                ee4d24e5-54a9-4bd2-89b8-3ffd116be4ee\n  Boot ID:                    32ffd620-6838-420c-880b-792581ea2142\n  Kernel Version:             6.0.7-301.fc37.x86_64\n  OS Image:                   Fedora CoreOS 37.20221106.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.13\n  Kubelet Version:            v1.24.9\n  Kube-Proxy Version:         v1.24.9\nPodCIDR:                      10.100.2.0/24\nPodCIDRs:                     10.100.2.0/24\nProviderID:                   openstack:///ee4d24e5-54a9-4bd2-89b8-3ffd116be4ee\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-fdjkn                                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 coredns-7c7cdb8f9d-kv57w                                   100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     118m\n  kube-system                 k8s-keystone-auth-crsxt                                    200m (10%)    0 (0%)      0 (0%)           0 (0%)         117m\n  kube-system                 magnum-prometheus-node-exporter-prnjx                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         115m\n  kube-system                 openstack-cloud-controller-manager-xznxm                   200m (10%)    0 (0%)      0 (0%)           0 (0%)         118m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-r7plp    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                750m (37%)  0 (0%)\n  memory             70Mi (1%)   170Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jan 17 04:31:51.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8277 describe namespace kubectl-8277'
Jan 17 04:31:51.850: INFO: stderr: ""
Jan 17 04:31:51.850: INFO: stdout: "Name:         kubectl-8277\nLabels:       e2e-framework=kubectl\n              e2e-run=e9644791-5268-4049-8eb1-b98ca49fd852\n              kubernetes.io/metadata.name=kubectl-8277\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:31:51.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8277" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":79,"skipped":1372,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:51.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:31:51.963: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 17 04:31:53.077: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 17 04:31:54.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3223" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":80,"skipped":1375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:31:54.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-7807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7807 to expose endpoints map[]
Jan 17 04:31:54.337: INFO: successfully validated that service multi-endpoint-test in namespace services-7807 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7807
Jan 17 04:31:54.400: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:31:56.411: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7807 to expose endpoints map[pod1:[100]]
Jan 17 04:31:56.430: INFO: successfully validated that service multi-endpoint-test in namespace services-7807 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7807
Jan 17 04:31:56.453: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:31:58.508: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7807 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 17 04:31:58.558: INFO: successfully validated that service multi-endpoint-test in namespace services-7807 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 17 04:31:58.558: INFO: Creating new exec pod
Jan 17 04:32:01.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7807 exec execpoddpwgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 17 04:32:01.897: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:01.897: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:01.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7807 exec execpoddpwgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.161.209 80'
Jan 17 04:32:02.112: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.161.209 80\nConnection to 10.254.161.209 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:02.112: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:02.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7807 exec execpoddpwgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 17 04:32:02.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 17 04:32:02.435: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:02.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7807 exec execpoddpwgq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.161.209 81'
Jan 17 04:32:02.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.161.209 81\nConnection to 10.254.161.209 81 port [tcp/*] succeeded!\n"
Jan 17 04:32:02.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7807 to expose endpoints map[pod2:[101]]
Jan 17 04:32:03.749: INFO: successfully validated that service multi-endpoint-test in namespace services-7807 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7807
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7807 to expose endpoints map[]
Jan 17 04:32:03.839: INFO: successfully validated that service multi-endpoint-test in namespace services-7807 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:32:03.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7807" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.825 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":81,"skipped":1408,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:03.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500
Jan 17 04:32:04.020: INFO: Pod name my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500: Found 0 pods out of 1
Jan 17 04:32:09.045: INFO: Pod name my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500: Found 1 pods out of 1
Jan 17 04:32:09.045: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500" are running
Jan 17 04:32:09.056: INFO: Pod "my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500-j6hln" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 04:32:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 04:32:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 04:32:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 04:32:04 +0000 UTC Reason: Message:}])
Jan 17 04:32:09.056: INFO: Trying to dial the pod
Jan 17 04:32:14.093: INFO: Controller my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500: Got expected result from replica 1 [my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500-j6hln]: "my-hostname-basic-0e751d39-431e-4719-b340-897c82ec7500-j6hln", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 17 04:32:14.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5529" for this suite.

• [SLOW TEST:10.183 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":82,"skipped":1497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:14.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jan 17 04:32:14.205: INFO: Waiting up to 5m0s for pod "client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe" in namespace "containers-555" to be "Succeeded or Failed"
Jan 17 04:32:14.221: INFO: Pod "client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.490863ms
Jan 17 04:32:16.231: INFO: Pod "client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025855842s
Jan 17 04:32:18.253: INFO: Pod "client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047577278s
STEP: Saw pod success
Jan 17 04:32:18.253: INFO: Pod "client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe" satisfied condition "Succeeded or Failed"
Jan 17 04:32:18.256: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:32:18.320: INFO: Waiting for pod client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe to disappear
Jan 17 04:32:18.330: INFO: Pod client-containers-d5fe638c-4dfb-43b5-92fc-26a3ebd9b1fe no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 17 04:32:18.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-555" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1536,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:18.368: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 17 04:32:18.450: INFO: Waiting up to 5m0s for pod "pod-962f41ee-0050-4e41-b6de-e51c736c06e4" in namespace "emptydir-9788" to be "Succeeded or Failed"
Jan 17 04:32:18.473: INFO: Pod "pod-962f41ee-0050-4e41-b6de-e51c736c06e4": Phase="Pending", Reason="", readiness=false. Elapsed: 23.276666ms
Jan 17 04:32:20.486: INFO: Pod "pod-962f41ee-0050-4e41-b6de-e51c736c06e4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036118295s
Jan 17 04:32:22.498: INFO: Pod "pod-962f41ee-0050-4e41-b6de-e51c736c06e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047810043s
STEP: Saw pod success
Jan 17 04:32:22.498: INFO: Pod "pod-962f41ee-0050-4e41-b6de-e51c736c06e4" satisfied condition "Succeeded or Failed"
Jan 17 04:32:22.501: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-962f41ee-0050-4e41-b6de-e51c736c06e4 container test-container: <nil>
STEP: delete the pod
Jan 17 04:32:22.536: INFO: Waiting for pod pod-962f41ee-0050-4e41-b6de-e51c736c06e4 to disappear
Jan 17 04:32:22.544: INFO: Pod pod-962f41ee-0050-4e41-b6de-e51c736c06e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:32:22.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9788" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":84,"skipped":1537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-2048
STEP: creating service affinity-clusterip-transition in namespace services-2048
STEP: creating replication controller affinity-clusterip-transition in namespace services-2048
I0117 04:32:22.660905      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2048, replica count: 3
I0117 04:32:25.711960      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 04:32:25.725: INFO: Creating new exec pod
Jan 17 04:32:28.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-2048 exec execpod-affinityj7p45 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 17 04:32:29.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:29.063: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:29.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-2048 exec execpod-affinityj7p45 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.73.180 80'
Jan 17 04:32:29.283: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.73.180 80\nConnection to 10.254.73.180 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:29.283: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:29.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-2048 exec execpod-affinityj7p45 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.73.180:80/ ; done'
Jan 17 04:32:29.698: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n"
Jan 17 04:32:29.699: INFO: stdout: "\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-w5frh\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-w5frh\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-bd5s4\naffinity-clusterip-transition-w5frh\naffinity-clusterip-transition-w5frh\naffinity-clusterip-transition-w5frh\naffinity-clusterip-transition-w5frh"
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-bd5s4
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.699: INFO: Received response from host: affinity-clusterip-transition-w5frh
Jan 17 04:32:29.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-2048 exec execpod-affinityj7p45 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.73.180:80/ ; done'
Jan 17 04:32:30.090: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.73.180:80/\n"
Jan 17 04:32:30.090: INFO: stdout: "\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7\naffinity-clusterip-transition-mhdq7"
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Received response from host: affinity-clusterip-transition-mhdq7
Jan 17 04:32:30.090: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2048, will wait for the garbage collector to delete the pods
Jan 17 04:32:30.292: INFO: Deleting ReplicationController affinity-clusterip-transition took: 30.388126ms
Jan 17 04:32:30.493: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 201.175295ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:32:33.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2048" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.519 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":85,"skipped":1547,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:33.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 17 04:32:33.179: INFO: The status of Pod labelsupdate0e4204db-0d8e-4790-8184-6892fe409e76 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:32:35.186: INFO: The status of Pod labelsupdate0e4204db-0d8e-4790-8184-6892fe409e76 is Running (Ready = true)
Jan 17 04:32:35.738: INFO: Successfully updated pod "labelsupdate0e4204db-0d8e-4790-8184-6892fe409e76"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:32:37.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6381" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":86,"skipped":1552,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:37.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0117 04:32:47.942987      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 04:32:47.943: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 04:32:47.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1840" for this suite.

• [SLOW TEST:10.185 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":87,"skipped":1553,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:47.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-cbc8bc03-6cae-47ea-bc41-2de27b05e84e
STEP: Creating a pod to test consume secrets
Jan 17 04:32:48.073: INFO: Waiting up to 5m0s for pod "pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4" in namespace "secrets-4213" to be "Succeeded or Failed"
Jan 17 04:32:48.102: INFO: Pod "pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4": Phase="Pending", Reason="", readiness=false. Elapsed: 29.765552ms
Jan 17 04:32:50.113: INFO: Pod "pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040372221s
Jan 17 04:32:52.135: INFO: Pod "pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062272141s
STEP: Saw pod success
Jan 17 04:32:52.135: INFO: Pod "pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4" satisfied condition "Succeeded or Failed"
Jan 17 04:32:52.161: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4 container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 04:32:52.218: INFO: Waiting for pod pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4 to disappear
Jan 17 04:32:52.225: INFO: Pod pod-secrets-f94e1e50-e7f9-486f-b176-490cc78168f4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:32:52.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4213" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":88,"skipped":1558,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:32:52.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6320
STEP: creating service affinity-nodeport-transition in namespace services-6320
STEP: creating replication controller affinity-nodeport-transition in namespace services-6320
I0117 04:32:52.340578      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6320, replica count: 3
I0117 04:32:55.392277      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 04:32:55.411: INFO: Creating new exec pod
Jan 17 04:32:58.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 17 04:32:58.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:58.721: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:58.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.129.18 80'
Jan 17 04:32:58.937: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.129.18 80\nConnection to 10.254.129.18 80 port [tcp/http] succeeded!\n"
Jan 17 04:32:58.937: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:58.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 30698'
Jan 17 04:32:59.161: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 30698\nConnection to 10.0.0.21 30698 port [tcp/*] succeeded!\n"
Jan 17 04:32:59.161: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:59.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30698'
Jan 17 04:32:59.388: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30698\nConnection to 10.0.0.16 30698 port [tcp/*] succeeded!\n"
Jan 17 04:32:59.388: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:32:59.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30698/ ; done'
Jan 17 04:32:59.699: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n"
Jan 17 04:32:59.699: INFO: stdout: "\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-d46f5\naffinity-nodeport-transition-d46f5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-d46f5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-d46f5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-d46f5\naffinity-nodeport-transition-gcfk5\naffinity-nodeport-transition-d46f5"
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-gcfk5
Jan 17 04:32:59.699: INFO: Received response from host: affinity-nodeport-transition-d46f5
Jan 17 04:32:59.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-6320 exec execpod-affinityd6wcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30698/ ; done'
Jan 17 04:33:00.062: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30698/\n"
Jan 17 04:33:00.062: INFO: stdout: "\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b\naffinity-nodeport-transition-x658b"
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Received response from host: affinity-nodeport-transition-x658b
Jan 17 04:33:00.062: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6320, will wait for the garbage collector to delete the pods
Jan 17 04:33:00.179: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.178276ms
Jan 17 04:33:00.279: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.201213ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:33:02.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6320" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.329 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":89,"skipped":1572,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:02.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 17 04:33:02.659: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 04:33:07.697: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 17 04:33:07.713: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 17 04:33:07.738: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 17 04:33:07.746: INFO: Observed &ReplicaSet event: ADDED
Jan 17 04:33:07.746: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.746: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.747: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.747: INFO: Found replicaset test-rs in namespace replicaset-6400 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 04:33:07.747: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 17 04:33:07.747: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 04:33:07.760: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 17 04:33:07.765: INFO: Observed &ReplicaSet event: ADDED
Jan 17 04:33:07.765: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.765: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.765: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.765: INFO: Observed replicaset test-rs in namespace replicaset-6400 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 04:33:07.766: INFO: Observed &ReplicaSet event: MODIFIED
Jan 17 04:33:07.766: INFO: Found replicaset test-rs in namespace replicaset-6400 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 04:33:07.766: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 04:33:07.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6400" for this suite.

• [SLOW TEST:5.216 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":90,"skipped":1576,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:07.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:33:24.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9928" for this suite.

• [SLOW TEST:16.359 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":91,"skipped":1606,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:24.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 17 04:33:24.273: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3390  a824dc51-e6d7-4233-8eb9-46dfb326c311 29224 0 2023-01-17 04:33:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-17 04:33:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 04:33:24.274: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3390  a824dc51-e6d7-4233-8eb9-46dfb326c311 29225 0 2023-01-17 04:33:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-17 04:33:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 17 04:33:24.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3390" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":92,"skipped":1609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:24.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:33:24.362: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de" in namespace "downward-api-317" to be "Succeeded or Failed"
Jan 17 04:33:24.369: INFO: Pod "downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.35408ms
Jan 17 04:33:26.379: INFO: Pod "downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016544438s
Jan 17 04:33:28.389: INFO: Pod "downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026847229s
STEP: Saw pod success
Jan 17 04:33:28.389: INFO: Pod "downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de" satisfied condition "Succeeded or Failed"
Jan 17 04:33:28.394: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de container client-container: <nil>
STEP: delete the pod
Jan 17 04:33:28.451: INFO: Waiting for pod downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de to disappear
Jan 17 04:33:28.456: INFO: Pod downwardapi-volume-15e2083e-2f8a-4ec3-a87d-ecfce9b716de no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:33:28.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-317" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":93,"skipped":1632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:28.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 17 04:33:30.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3847" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":94,"skipped":1670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:30.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 17 04:33:30.774: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:30.774: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:30.774: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:30.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:33:30.784: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:33:31.803: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:31.803: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:31.803: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:31.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:33:31.808: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:33:32.795: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:32.795: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:32.795: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:32.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 04:33:32.800: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:33:33.794: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:33.794: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:33.794: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:33:33.801: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:33:33.801: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Jan 17 04:33:33.819: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 17 04:33:33.840: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 17 04:33:33.846: INFO: Observed &DaemonSet event: ADDED
Jan 17 04:33:33.847: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.847: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.847: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.847: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.847: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.847: INFO: Found daemon set daemon-set in namespace daemonsets-3372 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 04:33:33.847: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 17 04:33:33.867: INFO: Observed &DaemonSet event: ADDED
Jan 17 04:33:33.867: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Observed daemon set daemon-set in namespace daemonsets-3372 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 04:33:33.868: INFO: Observed &DaemonSet event: MODIFIED
Jan 17 04:33:33.868: INFO: Found daemon set daemon-set in namespace daemonsets-3372 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 17 04:33:33.868: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3372, will wait for the garbage collector to delete the pods
Jan 17 04:33:33.959: INFO: Deleting DaemonSet.extensions daemon-set took: 22.264458ms
Jan 17 04:33:34.159: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.734719ms
Jan 17 04:33:36.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:33:36.577: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 04:33:36.580: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29431"},"items":null}

Jan 17 04:33:36.584: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29431"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:33:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3372" for this suite.

• [SLOW TEST:5.963 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":95,"skipped":1727,"failed":0}
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:36.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:33:36.693: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738" in namespace "security-context-test-7428" to be "Succeeded or Failed"
Jan 17 04:33:36.708: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Pending", Reason="", readiness=false. Elapsed: 14.74531ms
Jan 17 04:33:38.721: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027725332s
Jan 17 04:33:40.733: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039285204s
Jan 17 04:33:42.747: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053915974s
Jan 17 04:33:44.757: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063306139s
Jan 17 04:33:46.770: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.076879236s
Jan 17 04:33:46.770: INFO: Pod "alpine-nnp-false-dec5814f-1c08-4747-9f0e-fe4275552738" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 04:33:46.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7428" for this suite.

• [SLOW TEST:10.193 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:46.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:33:47.637: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:33:50.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:33:50.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4641" for this suite.
STEP: Destroying namespace "webhook-4641-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":97,"skipped":1785,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-3cead097-7850-4e22-875e-dc0beb2de695
STEP: Creating a pod to test consume configMaps
Jan 17 04:33:51.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8" in namespace "projected-419" to be "Succeeded or Failed"
Jan 17 04:33:51.024: INFO: Pod "pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.64787ms
Jan 17 04:33:53.041: INFO: Pod "pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032130842s
Jan 17 04:33:55.048: INFO: Pod "pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039451779s
STEP: Saw pod success
Jan 17 04:33:55.048: INFO: Pod "pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8" satisfied condition "Succeeded or Failed"
Jan 17 04:33:55.054: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:33:55.175: INFO: Waiting for pod pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8 to disappear
Jan 17 04:33:55.182: INFO: Pod pod-projected-configmaps-92bd9676-3e8c-436f-9d7c-9f3b735bd3b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 04:33:55.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-419" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":98,"skipped":1815,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:33:55.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 17 04:38:55.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5496" for this suite.

• [SLOW TEST:300.121 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":99,"skipped":1832,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:38:55.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 17 04:38:55.426: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30379 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 04:38:55.426: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30380 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 04:38:55.426: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30381 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 17 04:39:05.519: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30413 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 04:39:05.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30414 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 04:39:05.519: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1919  d634e8dd-f657-46f7-8323-dcf62fcc3fde 30415 0 2023-01-17 04:38:55 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-17 04:38:55 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 17 04:39:05.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1919" for this suite.

• [SLOW TEST:10.217 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":100,"skipped":1965,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:39:05.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 04:39:05.632: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 04:40:05.695: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 17 04:40:05.775: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 04:40:05.802: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 04:40:05.845: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 04:40:05.860: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 04:40:05.907: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 04:40:05.929: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:40:22.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4820" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.696 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":101,"skipped":1972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:22.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 17 04:40:25.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7594" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":102,"skipped":2028,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:25.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 17 04:40:27.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2860" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":103,"skipped":2043,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:27.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:40:28.595: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:40:31.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:40:31.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6563-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:40:35.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5647" for this suite.
STEP: Destroying namespace "webhook-5647-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.478 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":104,"skipped":2057,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:35.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:40:46.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3812" for this suite.

• [SLOW TEST:11.238 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":105,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:46.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:40:46.709: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ced66492-21f0-452c-8986-834594561090", Controller:(*bool)(0xc003ae806a), BlockOwnerDeletion:(*bool)(0xc003ae806b)}}
Jan 17 04:40:46.736: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e30ba695-87fb-42fb-b1ac-6359aac8404f", Controller:(*bool)(0xc003ae82fe), BlockOwnerDeletion:(*bool)(0xc003ae82ff)}}
Jan 17 04:40:46.764: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"99b9c06b-5e19-4982-9d2a-e3df5cc71046", Controller:(*bool)(0xc0044f7ea6), BlockOwnerDeletion:(*bool)(0xc0044f7ea7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 04:40:51.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7373" for this suite.

• [SLOW TEST:5.325 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":106,"skipped":2083,"failed":0}
SS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:51.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 17 04:40:54.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5425" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":107,"skipped":2085,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jan 17 04:40:56.184: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7362 pod-service-account-fe3c646c-03af-47d9-968f-cb2d3fa76f6b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 17 04:40:56.404: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7362 pod-service-account-fe3c646c-03af-47d9-968f-cb2d3fa76f6b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 17 04:40:56.638: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7362 pod-service-account-fe3c646c-03af-47d9-968f-cb2d3fa76f6b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 17 04:40:56.871: INFO: Got root ca configmap in namespace "svcaccounts-7362"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 04:40:56.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7362" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":108,"skipped":2126,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:40:56.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:40:57.006: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:40:59.014: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:01.017: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:03.018: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:05.014: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:07.018: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:09.016: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:11.016: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:13.018: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:15.021: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:17.018: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = false)
Jan 17 04:41:19.014: INFO: The status of Pod test-webserver-f848b7cb-a6ac-4ec0-9394-c6566809b6b7 is Running (Ready = true)
Jan 17 04:41:19.018: INFO: Container started at 2023-01-17 04:40:57 +0000 UTC, pod became ready at 2023-01-17 04:41:17 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 04:41:19.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2744" for this suite.

• [SLOW TEST:22.140 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":2150,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:41:19.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-4b2ea46e-2bd2-4591-9d29-ffcbabfa3027
STEP: Creating a pod to test consume configMaps
Jan 17 04:41:19.123: INFO: Waiting up to 5m0s for pod "pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad" in namespace "configmap-2337" to be "Succeeded or Failed"
Jan 17 04:41:19.138: INFO: Pod "pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad": Phase="Pending", Reason="", readiness=false. Elapsed: 14.628629ms
Jan 17 04:41:21.148: INFO: Pod "pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024889711s
Jan 17 04:41:23.167: INFO: Pod "pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043308086s
STEP: Saw pod success
Jan 17 04:41:23.167: INFO: Pod "pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad" satisfied condition "Succeeded or Failed"
Jan 17 04:41:23.173: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:41:23.295: INFO: Waiting for pod pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad to disappear
Jan 17 04:41:23.302: INFO: Pod pod-configmaps-90be81cf-e286-4aa4-b2a0-0128dfc43cad no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:41:23.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2337" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":2150,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:41:23.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8915.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8915.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 04:41:37.462: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd: the server could not find the requested resource (get pods dns-test-81289af3-9f86-4708-a937-26fe726eb9bd)
Jan 17 04:41:37.467: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd: the server could not find the requested resource (get pods dns-test-81289af3-9f86-4708-a937-26fe726eb9bd)
Jan 17 04:41:37.481: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd: the server could not find the requested resource (get pods dns-test-81289af3-9f86-4708-a937-26fe726eb9bd)
Jan 17 04:41:37.492: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local from pod dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd: the server could not find the requested resource (get pods dns-test-81289af3-9f86-4708-a937-26fe726eb9bd)
Jan 17 04:41:37.492: INFO: Lookups using dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8915.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8915.svc.cluster.local]

Jan 17 04:41:42.531: INFO: DNS probes using dns-8915/dns-test-81289af3-9f86-4708-a937-26fe726eb9bd succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 04:41:42.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8915" for this suite.

• [SLOW TEST:19.349 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":111,"skipped":2158,"failed":0}
SS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:41:42.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 17 04:43:43.327: INFO: Successfully updated pod "var-expansion-b2c24a91-6ea9-42e2-b0e6-b134e1216cf0"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 17 04:43:45.346: INFO: Deleting pod "var-expansion-b2c24a91-6ea9-42e2-b0e6-b134e1216cf0" in namespace "var-expansion-4879"
Jan 17 04:43:45.364: INFO: Wait up to 5m0s for pod "var-expansion-b2c24a91-6ea9-42e2-b0e6-b134e1216cf0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:44:17.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4879" for this suite.

• [SLOW TEST:154.742 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":112,"skipped":2160,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:17.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:44:17.471: INFO: Creating simple deployment test-new-deployment
Jan 17 04:44:17.509: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 04:44:19.621: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8920  fc2239ce-56d7-40ad-bc2f-f8d876d9a786 31967 3 2023-01-17 04:44:17 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-17 04:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b274e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 04:44:18 +0000 UTC,LastTransitionTime:2023-01-17 04:44:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-01-17 04:44:18 +0000 UTC,LastTransitionTime:2023-01-17 04:44:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 04:44:19.638: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-8920  c9d72ecb-0daa-429e-8a6a-df592ff0b6e7 31966 2 2023-01-17 04:44:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fc2239ce-56d7-40ad-bc2f-f8d876d9a786 0xc005b27927 0xc005b27928}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fc2239ce-56d7-40ad-bc2f-f8d876d9a786\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:44:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b279b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 04:44:19.670: INFO: Pod "test-new-deployment-55df494869-kn9fh" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-kn9fh test-new-deployment-55df494869- deployment-8920  1febbac7-0fb1-43e5-9b9d-f7cf54fc4dd3 31970 0 2023-01-17 04:44:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet test-new-deployment-55df494869 c9d72ecb-0daa-429e-8a6a-df592ff0b6e7 0xc005b27da7 0xc005b27da8}] []  [{kube-controller-manager Update v1 2023-01-17 04:44:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9d72ecb-0daa-429e-8a6a-df592ff0b6e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vzcxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vzcxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:44:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 04:44:19.671: INFO: Pod "test-new-deployment-55df494869-pz254" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-pz254 test-new-deployment-55df494869- deployment-8920  7c7950e7-3ce8-4529-ae5c-3424aa5fe063 31960 0 2023-01-17 04:44:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:8cf063ad023931754b5b6d6ab1378f9f9b066a9e7b87f7bc14393c9f2e7fceb2 cni.projectcalico.org/podIP:10.100.11.220/32 cni.projectcalico.org/podIPs:10.100.11.220/32] [{apps/v1 ReplicaSet test-new-deployment-55df494869 c9d72ecb-0daa-429e-8a6a-df592ff0b6e7 0xc005b27f20 0xc005b27f21}] []  [{kube-controller-manager Update v1 2023-01-17 04:44:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9d72ecb-0daa-429e-8a6a-df592ff0b6e7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 04:44:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 04:44:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jt6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jt6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:44:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:44:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:44:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.220,StartTime:2023-01-17 04:44:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 04:44:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d67a0a68bbe706711192ba42bac695294bc7af343fe08f6380dee518e5177473,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 04:44:19.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8920" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":113,"skipped":2193,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:19.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9426 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9426;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9426 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9426;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9426.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9426.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9426.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9426.svc;check="$$(dig +notcp +noall +answer +search 95.83.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.83.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.83.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.83.95_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9426 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9426;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9426 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9426;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9426.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9426.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9426.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9426.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9426.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9426.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9426.svc;check="$$(dig +notcp +noall +answer +search 95.83.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.83.95_udp@PTR;check="$$(dig +tcp +noall +answer +search 95.83.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.83.95_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 04:44:23.989: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:23.996: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.001: INFO: Unable to read wheezy_udp@dns-test-service.dns-9426 from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.005: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9426 from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.021: INFO: Unable to read wheezy_udp@dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.028: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.032: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.035: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.068: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.072: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.077: INFO: Unable to read jessie_udp@dns-test-service.dns-9426 from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.081: INFO: Unable to read jessie_tcp@dns-test-service.dns-9426 from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.086: INFO: Unable to read jessie_udp@dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.090: INFO: Unable to read jessie_tcp@dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.094: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.100: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9426.svc from pod dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9: the server could not find the requested resource (get pods dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9)
Jan 17 04:44:24.120: INFO: Lookups using dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9426 wheezy_tcp@dns-test-service.dns-9426 wheezy_udp@dns-test-service.dns-9426.svc wheezy_tcp@dns-test-service.dns-9426.svc wheezy_udp@_http._tcp.dns-test-service.dns-9426.svc wheezy_tcp@_http._tcp.dns-test-service.dns-9426.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-9426 jessie_tcp@dns-test-service.dns-9426 jessie_udp@dns-test-service.dns-9426.svc jessie_tcp@dns-test-service.dns-9426.svc jessie_udp@_http._tcp.dns-test-service.dns-9426.svc jessie_tcp@_http._tcp.dns-test-service.dns-9426.svc]

Jan 17 04:44:29.243: INFO: DNS probes using dns-9426/dns-test-0f75fcd2-2354-4401-bcb0-7c4b09ac5fa9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 04:44:29.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9426" for this suite.

• [SLOW TEST:9.729 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":114,"skipped":2197,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:29.453: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 04:44:29.508: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 04:44:29.526: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 04:44:29.532: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-0 before test
Jan 17 04:44:29.550: INFO: calico-node-xdhq9 from kube-system started at 2023-01-17 02:35:55 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:44:29.550: INFO: csi-cinder-nodeplugin-tbzrq from kube-system started at 2023-01-17 02:36:15 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:44:29.550: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:44:29.550: INFO: magnum-kube-prometheus-sta-operator-675d58f6dc-bc5jf from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 04:44:29.550: INFO: magnum-kube-state-metrics-7645bf695b-2vs67 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 04:44:29.550: INFO: magnum-metrics-server-d6ddcd656-4phlx from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 04:44:29.550: INFO: magnum-prometheus-node-exporter-6ln7n from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:44:29.550: INFO: npd-8pbfg from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:44:29.550: INFO: sonobuoy from sonobuoy started at 2023-01-17 04:09:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 04:44:29.550: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-fcb54 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.550: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:44:29.550: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:44:29.550: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-1 before test
Jan 17 04:44:29.563: INFO: calico-node-9shzb from kube-system started at 2023-01-17 02:35:49 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:44:29.563: INFO: csi-cinder-nodeplugin-qhmg4 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:44:29.563: INFO: kube-dns-autoscaler-5b9649896b-427d6 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 04:44:29.563: INFO: magnum-grafana-d59cf7df4-shfs9 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (3 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container grafana ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 04:44:29.563: INFO: magnum-prometheus-node-exporter-sr8z8 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:44:29.563: INFO: npd-b7kb8 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:44:29.563: INFO: sonobuoy-e2e-job-6c6d2af065634bb1 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container e2e ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:44:29.563: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-lnwl6 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.563: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 04:44:29.563: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-2 before test
Jan 17 04:44:29.574: INFO: calico-node-wwxlv from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 04:44:29.574: INFO: csi-cinder-nodeplugin-lxsqw from kube-system started at 2023-01-17 02:36:35 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 04:44:29.574: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 04:44:29.574: INFO: magnum-prometheus-node-exporter-xdw8z from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 04:44:29.574: INFO: npd-c4sq2 from kube-system started at 2023-01-17 02:36:35 +0000 UTC (1 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 04:44:29.574: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 02:36:39 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 04:44:29.574: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 04:44:29.574: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-9j7nc from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 04:44:29.574: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 04:44:29.574: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173aff133d5c3f33], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 node(s) didn't match Pod's node affinity/selector. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:44:30.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2108" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":115,"skipped":2205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:44:30.758: INFO: The status of Pod busybox-host-aliases681eabc4-e375-456e-a978-891edb4ef707 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:44:32.774: INFO: The status of Pod busybox-host-aliases681eabc4-e375-456e-a978-891edb4ef707 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 17 04:44:32.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5519" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":116,"skipped":2230,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:32.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 17 04:44:32.987: INFO: Waiting up to 5m0s for pod "pod-44dc5107-a006-4d4a-9be6-c05effa9b15d" in namespace "emptydir-8060" to be "Succeeded or Failed"
Jan 17 04:44:33.000: INFO: Pod "pod-44dc5107-a006-4d4a-9be6-c05effa9b15d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.804919ms
Jan 17 04:44:35.009: INFO: Pod "pod-44dc5107-a006-4d4a-9be6-c05effa9b15d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021616667s
Jan 17 04:44:37.023: INFO: Pod "pod-44dc5107-a006-4d4a-9be6-c05effa9b15d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03574424s
STEP: Saw pod success
Jan 17 04:44:37.023: INFO: Pod "pod-44dc5107-a006-4d4a-9be6-c05effa9b15d" satisfied condition "Succeeded or Failed"
Jan 17 04:44:37.033: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-44dc5107-a006-4d4a-9be6-c05effa9b15d container test-container: <nil>
STEP: delete the pod
Jan 17 04:44:37.129: INFO: Waiting for pod pod-44dc5107-a006-4d4a-9be6-c05effa9b15d to disappear
Jan 17 04:44:37.136: INFO: Pod pod-44dc5107-a006-4d4a-9be6-c05effa9b15d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:44:37.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8060" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":117,"skipped":2261,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:44:37.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 04:44:37.267: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 04:45:37.332: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:45:37.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 17 04:45:39.466: INFO: found a healthy node: cluster124-apihrjet4zqi-node-2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:45:55.656: INFO: pods created so far: [1 1 1]
Jan 17 04:45:55.656: INFO: length of pods created so far: 3
Jan 17 04:45:57.684: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jan 17 04:46:04.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8358" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:46:04.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6963" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:87.745 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":118,"skipped":2269,"failed":0}
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:04.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-7603/configmap-test-d8dc52f5-4e08-409a-a977-4c98c80207bf
STEP: Creating a pod to test consume configMaps
Jan 17 04:46:04.984: INFO: Waiting up to 5m0s for pod "pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c" in namespace "configmap-7603" to be "Succeeded or Failed"
Jan 17 04:46:04.994: INFO: Pod "pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.735253ms
Jan 17 04:46:07.008: INFO: Pod "pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024230053s
Jan 17 04:46:09.024: INFO: Pod "pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039456683s
STEP: Saw pod success
Jan 17 04:46:09.024: INFO: Pod "pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c" satisfied condition "Succeeded or Failed"
Jan 17 04:46:09.031: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c container env-test: <nil>
STEP: delete the pod
Jan 17 04:46:09.152: INFO: Waiting for pod pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c to disappear
Jan 17 04:46:09.157: INFO: Pod pod-configmaps-0813aec5-5350-40e8-8989-bf8834994e1c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:46:09.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7603" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":119,"skipped":2273,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:09.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-1b38f309-592b-40b0-82ee-cedfb9b667a9
STEP: Creating a pod to test consume configMaps
Jan 17 04:46:09.262: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a" in namespace "projected-9192" to be "Succeeded or Failed"
Jan 17 04:46:09.282: INFO: Pod "pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.472751ms
Jan 17 04:46:11.296: INFO: Pod "pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033797768s
Jan 17 04:46:13.307: INFO: Pod "pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044632726s
STEP: Saw pod success
Jan 17 04:46:13.307: INFO: Pod "pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a" satisfied condition "Succeeded or Failed"
Jan 17 04:46:13.312: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:46:13.352: INFO: Waiting for pod pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a to disappear
Jan 17 04:46:13.359: INFO: Pod pod-projected-configmaps-5c8d4295-557a-44b6-9e43-d23cc1c04f7a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 04:46:13.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9192" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":120,"skipped":2342,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:13.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 17 04:46:13.443: INFO: Waiting up to 5m0s for pod "pod-f52e0401-1850-4c4d-ac9f-336eabfbc992" in namespace "emptydir-969" to be "Succeeded or Failed"
Jan 17 04:46:13.460: INFO: Pod "pod-f52e0401-1850-4c4d-ac9f-336eabfbc992": Phase="Pending", Reason="", readiness=false. Elapsed: 17.246074ms
Jan 17 04:46:15.471: INFO: Pod "pod-f52e0401-1850-4c4d-ac9f-336eabfbc992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027949885s
Jan 17 04:46:17.486: INFO: Pod "pod-f52e0401-1850-4c4d-ac9f-336eabfbc992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043192406s
STEP: Saw pod success
Jan 17 04:46:17.486: INFO: Pod "pod-f52e0401-1850-4c4d-ac9f-336eabfbc992" satisfied condition "Succeeded or Failed"
Jan 17 04:46:17.491: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-f52e0401-1850-4c4d-ac9f-336eabfbc992 container test-container: <nil>
STEP: delete the pod
Jan 17 04:46:17.575: INFO: Waiting for pod pod-f52e0401-1850-4c4d-ac9f-336eabfbc992 to disappear
Jan 17 04:46:17.585: INFO: Pod pod-f52e0401-1850-4c4d-ac9f-336eabfbc992 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:46:17.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-969" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":121,"skipped":2345,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:17.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 17 04:46:29.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5457" for this suite.

• [SLOW TEST:12.116 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":122,"skipped":2365,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 17 04:46:29.809: INFO: Waiting up to 5m0s for pod "pod-6c686430-73d2-4262-ac9a-b84d41a12a5d" in namespace "emptydir-788" to be "Succeeded or Failed"
Jan 17 04:46:29.831: INFO: Pod "pod-6c686430-73d2-4262-ac9a-b84d41a12a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 21.810453ms
Jan 17 04:46:31.841: INFO: Pod "pod-6c686430-73d2-4262-ac9a-b84d41a12a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032322608s
Jan 17 04:46:33.854: INFO: Pod "pod-6c686430-73d2-4262-ac9a-b84d41a12a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044817064s
STEP: Saw pod success
Jan 17 04:46:33.854: INFO: Pod "pod-6c686430-73d2-4262-ac9a-b84d41a12a5d" satisfied condition "Succeeded or Failed"
Jan 17 04:46:33.858: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-6c686430-73d2-4262-ac9a-b84d41a12a5d container test-container: <nil>
STEP: delete the pod
Jan 17 04:46:33.896: INFO: Waiting for pod pod-6c686430-73d2-4262-ac9a-b84d41a12a5d to disappear
Jan 17 04:46:33.901: INFO: Pod pod-6c686430-73d2-4262-ac9a-b84d41a12a5d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:46:33.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-788" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":123,"skipped":2371,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:33.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:46:33.985: INFO: Creating deployment "test-recreate-deployment"
Jan 17 04:46:34.001: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 17 04:46:34.026: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 17 04:46:36.045: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 17 04:46:36.049: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 17 04:46:36.063: INFO: Updating deployment test-recreate-deployment
Jan 17 04:46:36.063: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 04:46:36.238: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1530  98c6df85-f3d0-4295-84a4-63efad69a7cd 33013 2 2023-01-17 04:46:33 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fcea08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 04:46:36 +0000 UTC,LastTransitionTime:2023-01-17 04:46:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-01-17 04:46:36 +0000 UTC,LastTransitionTime:2023-01-17 04:46:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 17 04:46:36.242: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-1530  654837c3-9c2c-4cc5-acc1-97630783effe 33011 1 2023-01-17 04:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 98c6df85-f3d0-4295-84a4-63efad69a7cd 0xc005fceec0 0xc005fceec1}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98c6df85-f3d0-4295-84a4-63efad69a7cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fcef58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 04:46:36.242: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 17 04:46:36.242: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-1530  ecf76dfc-0966-4f14-bdbe-e1732e500185 33001 2 2023-01-17 04:46:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 98c6df85-f3d0-4295-84a4-63efad69a7cd 0xc005fceda7 0xc005fceda8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:46:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98c6df85-f3d0-4295-84a4-63efad69a7cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fcee58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 04:46:36.248: INFO: Pod "test-recreate-deployment-cd8586fc7-gxp96" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-gxp96 test-recreate-deployment-cd8586fc7- deployment-1530  959525a9-dab7-400d-ae64-2f37660d7586 33010 0 2023-01-17 04:46:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 654837c3-9c2c-4cc5-acc1-97630783effe 0xc005fcf3d0 0xc005fcf3d1}] []  [{kube-controller-manager Update v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"654837c3-9c2c-4cc5-acc1-97630783effe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 04:46:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kz5k2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kz5k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:46:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:46:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:46:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 04:46:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 04:46:36.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1530" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":124,"skipped":2375,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:36.274: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 17 04:46:38.357: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6646 PodName:pod-sharedvolume-fbf7e056-aeab-4cab-8a23-d43c69ec00b3 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:46:38.357: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:46:38.358: INFO: ExecWithOptions: Clientset creation
Jan 17 04:46:38.358: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/emptydir-6646/pods/pod-sharedvolume-fbf7e056-aeab-4cab-8a23-d43c69ec00b3/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 17 04:46:38.528: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 04:46:38.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6646" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":125,"skipped":2387,"failed":0}
SSSSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:38.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 17 04:46:38.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1205" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":126,"skipped":2393,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:46:40.850: INFO: Deleting pod "var-expansion-df9553c3-47f8-4ea9-886b-91e756151a50" in namespace "var-expansion-7207"
Jan 17 04:46:40.874: INFO: Wait up to 5m0s for pod "var-expansion-df9553c3-47f8-4ea9-886b-91e756151a50" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:46:44.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7207" for this suite.

• [SLOW TEST:6.224 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":127,"skipped":2394,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:46:44.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-aa4902a7-ff81-4129-a975-bbc930dff868 in namespace container-probe-4381
Jan 17 04:46:47.025: INFO: Started pod busybox-aa4902a7-ff81-4129-a975-bbc930dff868 in namespace container-probe-4381
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 04:46:47.030: INFO: Initial restart count of pod busybox-aa4902a7-ff81-4129-a975-bbc930dff868 is 0
Jan 17 04:47:37.386: INFO: Restart count of pod container-probe-4381/busybox-aa4902a7-ff81-4129-a975-bbc930dff868 is now 1 (50.356604884s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 04:47:37.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4381" for this suite.

• [SLOW TEST:52.528 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":128,"skipped":2406,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:47:37.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jan 17 04:47:37.528: INFO: Waiting up to 5m0s for pod "client-containers-5311c55b-4012-44a1-a863-6d1f96edc023" in namespace "containers-4348" to be "Succeeded or Failed"
Jan 17 04:47:37.544: INFO: Pod "client-containers-5311c55b-4012-44a1-a863-6d1f96edc023": Phase="Pending", Reason="", readiness=false. Elapsed: 15.126213ms
Jan 17 04:47:39.550: INFO: Pod "client-containers-5311c55b-4012-44a1-a863-6d1f96edc023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021747583s
Jan 17 04:47:41.562: INFO: Pod "client-containers-5311c55b-4012-44a1-a863-6d1f96edc023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033282853s
STEP: Saw pod success
Jan 17 04:47:41.562: INFO: Pod "client-containers-5311c55b-4012-44a1-a863-6d1f96edc023" satisfied condition "Succeeded or Failed"
Jan 17 04:47:41.567: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod client-containers-5311c55b-4012-44a1-a863-6d1f96edc023 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:47:41.603: INFO: Waiting for pod client-containers-5311c55b-4012-44a1-a863-6d1f96edc023 to disappear
Jan 17 04:47:41.609: INFO: Pod client-containers-5311c55b-4012-44a1-a863-6d1f96edc023 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 17 04:47:41.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4348" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2423,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:47:41.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 17 04:47:41.710: INFO: observed Pod pod-test in namespace pods-5198 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 17 04:47:41.716: INFO: observed Pod pod-test in namespace pods-5198 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  }]
Jan 17 04:47:41.739: INFO: observed Pod pod-test in namespace pods-5198 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  }]
Jan 17 04:47:42.485: INFO: observed Pod pod-test in namespace pods-5198 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  }]
Jan 17 04:47:43.694: INFO: Found Pod pod-test in namespace pods-5198 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:47:41 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 17 04:47:43.803: INFO: observed event type MODIFIED
Jan 17 04:47:45.025: INFO: observed event type MODIFIED
Jan 17 04:47:45.983: INFO: observed event type MODIFIED
Jan 17 04:47:46.703: INFO: observed event type MODIFIED
Jan 17 04:47:46.721: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 04:47:46.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5198" for this suite.

• [SLOW TEST:5.156 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":130,"skipped":2433,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:47:46.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:47:47.595: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jan 17 04:47:49.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 47, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 47, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 47, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 47, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-656754656d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:47:52.667: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:47:52.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:47:55.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-1416" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:9.287 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":131,"skipped":2444,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:47:56.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 17 04:47:56.296: INFO: starting watch
STEP: patching
STEP: updating
Jan 17 04:47:56.321: INFO: waiting for watch events with expected annotations
Jan 17 04:47:56.321: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jan 17 04:47:56.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8854" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":132,"skipped":2469,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:47:56.386: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 04:48:24.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2181" for this suite.

• [SLOW TEST:28.173 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":133,"skipped":2477,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:24.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-nbm4
STEP: Creating a pod to test atomic-volume-subpath
Jan 17 04:48:24.655: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nbm4" in namespace "subpath-7552" to be "Succeeded or Failed"
Jan 17 04:48:24.661: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50631ms
Jan 17 04:48:26.675: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 2.019916491s
Jan 17 04:48:28.688: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 4.033282852s
Jan 17 04:48:30.700: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 6.045253138s
Jan 17 04:48:32.708: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 8.053284145s
Jan 17 04:48:34.717: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 10.061929354s
Jan 17 04:48:36.730: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 12.075086472s
Jan 17 04:48:38.742: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 14.087254244s
Jan 17 04:48:40.757: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 16.101992001s
Jan 17 04:48:42.765: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 18.109639471s
Jan 17 04:48:44.774: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=true. Elapsed: 20.119249668s
Jan 17 04:48:46.789: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Running", Reason="", readiness=false. Elapsed: 22.134514126s
Jan 17 04:48:48.801: INFO: Pod "pod-subpath-test-secret-nbm4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.14570413s
STEP: Saw pod success
Jan 17 04:48:48.801: INFO: Pod "pod-subpath-test-secret-nbm4" satisfied condition "Succeeded or Failed"
Jan 17 04:48:48.805: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-subpath-test-secret-nbm4 container test-container-subpath-secret-nbm4: <nil>
STEP: delete the pod
Jan 17 04:48:48.856: INFO: Waiting for pod pod-subpath-test-secret-nbm4 to disappear
Jan 17 04:48:48.864: INFO: Pod pod-subpath-test-secret-nbm4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-nbm4
Jan 17 04:48:48.864: INFO: Deleting pod "pod-subpath-test-secret-nbm4" in namespace "subpath-7552"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 17 04:48:48.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7552" for this suite.

• [SLOW TEST:24.333 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":134,"skipped":2494,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:48.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-5df759e6-39b4-4a95-9e9b-7615be4dcc91
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:48:48.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3993" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":135,"skipped":2503,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:48.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:48:49.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc" in namespace "downward-api-1905" to be "Succeeded or Failed"
Jan 17 04:48:49.066: INFO: Pod "downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.483112ms
Jan 17 04:48:51.093: INFO: Pod "downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043110712s
Jan 17 04:48:53.103: INFO: Pod "downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053188693s
STEP: Saw pod success
Jan 17 04:48:53.103: INFO: Pod "downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc" satisfied condition "Succeeded or Failed"
Jan 17 04:48:53.107: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc container client-container: <nil>
STEP: delete the pod
Jan 17 04:48:53.153: INFO: Waiting for pod downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc to disappear
Jan 17 04:48:53.159: INFO: Pod downwardapi-volume-b41fd6f3-16d2-481d-97c4-a596ee7f68dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:48:53.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1905" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2505,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:53.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 17 04:48:53.267: INFO: The status of Pod annotationupdate77484331-b8ce-4f74-bc9f-0f21600ea691 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:48:55.276: INFO: The status of Pod annotationupdate77484331-b8ce-4f74-bc9f-0f21600ea691 is Running (Ready = true)
Jan 17 04:48:55.823: INFO: Successfully updated pod "annotationupdate77484331-b8ce-4f74-bc9f-0f21600ea691"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:48:57.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3233" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2506,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:57.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jan 17 04:48:57.941: INFO: created test-podtemplate-1
Jan 17 04:48:57.950: INFO: created test-podtemplate-2
Jan 17 04:48:57.963: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 17 04:48:57.969: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 17 04:48:57.999: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 17 04:48:58.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2570" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":138,"skipped":2522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:48:58.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:48:58.105: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 04:49:03.111: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 17 04:49:03.149: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 17 04:49:03.186: INFO: observed ReplicaSet test-rs in namespace replicaset-9838 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 04:49:03.227: INFO: observed ReplicaSet test-rs in namespace replicaset-9838 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 04:49:03.276: INFO: observed ReplicaSet test-rs in namespace replicaset-9838 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 04:49:03.290: INFO: observed ReplicaSet test-rs in namespace replicaset-9838 with ReadyReplicas 1, AvailableReplicas 1
Jan 17 04:49:04.972: INFO: observed ReplicaSet test-rs in namespace replicaset-9838 with ReadyReplicas 2, AvailableReplicas 2
Jan 17 04:49:05.234: INFO: observed Replicaset test-rs in namespace replicaset-9838 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 04:49:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9838" for this suite.

• [SLOW TEST:7.248 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":139,"skipped":2549,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:49:05.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-9036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9036 to expose endpoints map[]
Jan 17 04:49:05.390: INFO: successfully validated that service endpoint-test2 in namespace services-9036 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9036
Jan 17 04:49:05.454: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:07.465: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9036 to expose endpoints map[pod1:[80]]
Jan 17 04:49:07.483: INFO: successfully validated that service endpoint-test2 in namespace services-9036 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 17 04:49:07.484: INFO: Creating new exec pod
Jan 17 04:49:10.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 04:49:10.802: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:10.802: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:49:10.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.219 80'
Jan 17 04:49:11.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.219 80\nConnection to 10.254.242.219 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:11.152: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-9036
Jan 17 04:49:11.185: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:13.192: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9036 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 17 04:49:13.214: INFO: successfully validated that service endpoint-test2 in namespace services-9036 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 17 04:49:14.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 04:49:14.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:14.495: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:49:14.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.219 80'
Jan 17 04:49:14.750: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.219 80\nConnection to 10.254.242.219 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:14.750: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9036 to expose endpoints map[pod2:[80]]
Jan 17 04:49:15.845: INFO: successfully validated that service endpoint-test2 in namespace services-9036 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 17 04:49:16.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 17 04:49:17.088: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:17.088: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 04:49:17.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-9036 exec execpoddj2t9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.219 80'
Jan 17 04:49:17.336: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.219 80\nConnection to 10.254.242.219 80 port [tcp/http] succeeded!\n"
Jan 17 04:49:17.336: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-9036
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9036 to expose endpoints map[]
Jan 17 04:49:18.462: INFO: successfully validated that service endpoint-test2 in namespace services-9036 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:49:18.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9036" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:13.311 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":140,"skipped":2568,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:49:18.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:49:18.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 17 04:49:18.663: INFO: The status of Pod pod-exec-websocket-4239fcf8-726a-499a-8792-6ea207dabae3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:20.673: INFO: The status of Pod pod-exec-websocket-4239fcf8-726a-499a-8792-6ea207dabae3 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 04:49:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8662" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":141,"skipped":2578,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:49:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-cn99
STEP: Creating a pod to test atomic-volume-subpath
Jan 17 04:49:20.925: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cn99" in namespace "subpath-8475" to be "Succeeded or Failed"
Jan 17 04:49:20.933: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91833ms
Jan 17 04:49:22.942: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 2.017249252s
Jan 17 04:49:24.949: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 4.024659578s
Jan 17 04:49:26.970: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 6.044997301s
Jan 17 04:49:28.982: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 8.057013677s
Jan 17 04:49:30.992: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 10.067137414s
Jan 17 04:49:32.998: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 12.073285568s
Jan 17 04:49:35.007: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 14.08240712s
Jan 17 04:49:37.024: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 16.099276264s
Jan 17 04:49:39.035: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 18.110166213s
Jan 17 04:49:41.048: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=true. Elapsed: 20.123021633s
Jan 17 04:49:43.057: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Running", Reason="", readiness=false. Elapsed: 22.132219706s
Jan 17 04:49:45.067: INFO: Pod "pod-subpath-test-configmap-cn99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.142855492s
STEP: Saw pod success
Jan 17 04:49:45.067: INFO: Pod "pod-subpath-test-configmap-cn99" satisfied condition "Succeeded or Failed"
Jan 17 04:49:45.073: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-subpath-test-configmap-cn99 container test-container-subpath-configmap-cn99: <nil>
STEP: delete the pod
Jan 17 04:49:45.126: INFO: Waiting for pod pod-subpath-test-configmap-cn99 to disappear
Jan 17 04:49:45.134: INFO: Pod pod-subpath-test-configmap-cn99 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cn99
Jan 17 04:49:45.134: INFO: Deleting pod "pod-subpath-test-configmap-cn99" in namespace "subpath-8475"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 17 04:49:45.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8475" for this suite.

• [SLOW TEST:24.328 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":142,"skipped":2597,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:49:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 17 04:49:45.245: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:47.258: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 17 04:49:47.312: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:49.325: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 17 04:49:49.429: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 04:49:49.436: INFO: Pod pod-with-poststart-http-hook still exists
Jan 17 04:49:51.437: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 17 04:49:51.449: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 17 04:49:51.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9768" for this suite.

• [SLOW TEST:6.307 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":143,"skipped":2606,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:49:51.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-c25fc121-571f-4756-b821-f401a8d68a1e
STEP: Creating secret with name s-test-opt-upd-c018f586-c34d-4b7b-bdb3-7e914a61de00
STEP: Creating the pod
Jan 17 04:49:51.573: INFO: The status of Pod pod-secrets-03e748f3-acce-4a85-8d38-f694fb1e17b1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:53.581: INFO: The status of Pod pod-secrets-03e748f3-acce-4a85-8d38-f694fb1e17b1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:49:55.582: INFO: The status of Pod pod-secrets-03e748f3-acce-4a85-8d38-f694fb1e17b1 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-c25fc121-571f-4756-b821-f401a8d68a1e
STEP: Updating secret s-test-opt-upd-c018f586-c34d-4b7b-bdb3-7e914a61de00
STEP: Creating secret with name s-test-opt-create-06c43fa2-5870-4d59-b696-1bf4ba1f42aa
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:51:22.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9991" for this suite.

• [SLOW TEST:90.820 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":144,"skipped":2613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:22.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jan 17 04:51:22.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-679" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":145,"skipped":2665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:22.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jan 17 04:51:22.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jan 17 04:51:23.028: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 17 04:51:25.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:27.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:29.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:31.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:33.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:35.177: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:37.182: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:39.185: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 4, 51, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 04:51:45.626: INFO: Waited 4.428864124s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 17 04:51:45.808: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jan 17 04:51:46.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3799" for this suite.

• [SLOW TEST:23.874 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":146,"skipped":2727,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:46.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:51:46.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5823" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":147,"skipped":2759,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:46.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 17 04:51:46.767: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 04:51:52.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3081" for this suite.

• [SLOW TEST:5.786 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":148,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:52.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:51:53.337: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:51:56.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 17 04:51:56.432: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:51:56.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4411" for this suite.
STEP: Destroying namespace "webhook-4411-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":149,"skipped":2792,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:51:56.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 17 04:51:56.708: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 04:52:01.721: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 04:52:01.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3856" for this suite.

• [SLOW TEST:5.257 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":150,"skipped":2801,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:52:01.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2845
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2845
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2845
Jan 17 04:52:02.039: INFO: Found 0 stateful pods, waiting for 1
Jan 17 04:52:12.057: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 17 04:52:12.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:52:12.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:52:12.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:52:12.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:52:12.357: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 17 04:52:22.374: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:52:22.374: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:52:22.419: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999791s
Jan 17 04:52:23.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992934899s
Jan 17 04:52:24.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.964449497s
Jan 17 04:52:25.465: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.95588716s
Jan 17 04:52:26.473: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.946852912s
Jan 17 04:52:27.511: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.937850956s
Jan 17 04:52:28.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.900563595s
Jan 17 04:52:29.527: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.891388683s
Jan 17 04:52:30.535: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.885302447s
Jan 17 04:52:31.545: INFO: Verifying statefulset ss doesn't scale past 1 for another 877.141388ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2845
Jan 17 04:52:32.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:52:32.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:52:32.807: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:52:32.807: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:52:32.816: INFO: Found 1 stateful pods, waiting for 3
Jan 17 04:52:42.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:52:42.833: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 04:52:42.833: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 17 04:52:42.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:52:43.060: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:52:43.060: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:52:43.060: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:52:43.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:52:43.313: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:52:43.313: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:52:43.313: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:52:43.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 17 04:52:43.545: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 17 04:52:43.545: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 17 04:52:43.545: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 17 04:52:43.545: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:52:43.551: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 17 04:52:53.579: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:52:53.579: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:52:53.579: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 17 04:52:53.609: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999715s
Jan 17 04:52:54.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993596146s
Jan 17 04:52:55.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987779256s
Jan 17 04:52:56.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979868522s
Jan 17 04:52:57.644: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970255653s
Jan 17 04:52:58.651: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95959901s
Jan 17 04:52:59.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951703098s
Jan 17 04:53:00.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945441833s
Jan 17 04:53:01.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933692537s
Jan 17 04:53:02.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.123299ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2845
Jan 17 04:53:03.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:53:03.974: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:53:03.974: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:53:03.974: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:53:03.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:53:04.228: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:53:04.228: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:53:04.228: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:53:04.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=statefulset-2845 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 17 04:53:04.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 17 04:53:04.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 17 04:53:04.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 17 04:53:04.427: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 04:53:14.483: INFO: Deleting all statefulset in ns statefulset-2845
Jan 17 04:53:14.489: INFO: Scaling statefulset ss to 0
Jan 17 04:53:14.502: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 04:53:14.506: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 04:53:14.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2845" for this suite.

• [SLOW TEST:72.731 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":151,"skipped":2817,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:14.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-b6f9e000-604a-4304-a0eb-227681457f7b
STEP: Creating a pod to test consume configMaps
Jan 17 04:53:14.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0" in namespace "configmap-917" to be "Succeeded or Failed"
Jan 17 04:53:14.717: INFO: Pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.571719ms
Jan 17 04:53:16.727: INFO: Pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.022404808s
Jan 17 04:53:18.740: INFO: Pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.03592501s
Jan 17 04:53:20.750: INFO: Pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046267187s
STEP: Saw pod success
Jan 17 04:53:20.751: INFO: Pod "pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0" satisfied condition "Succeeded or Failed"
Jan 17 04:53:20.758: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 17 04:53:20.861: INFO: Waiting for pod pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0 to disappear
Jan 17 04:53:20.867: INFO: Pod pod-configmaps-3e62bb2b-8d98-473f-9311-75702301f8f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:53:20.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-917" for this suite.

• [SLOW TEST:6.291 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":152,"skipped":2825,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:20.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-678.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-678.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-678.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 04:53:23.106: INFO: DNS probes using dns-678/dns-test-b249e921-42a7-4a7c-b678-1f186dd31405 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 04:53:23.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-678" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":153,"skipped":2834,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:23.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 17 04:53:23.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2586 create -f -'
Jan 17 04:53:25.079: INFO: stderr: ""
Jan 17 04:53:25.079: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 17 04:53:26.101: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:53:26.101: INFO: Found 0 / 1
Jan 17 04:53:27.093: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:53:27.093: INFO: Found 1 / 1
Jan 17 04:53:27.093: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 17 04:53:27.107: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:53:27.107: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 04:53:27.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2586 patch pod agnhost-primary-k2w8c -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 17 04:53:27.234: INFO: stderr: ""
Jan 17 04:53:27.234: INFO: stdout: "pod/agnhost-primary-k2w8c patched\n"
STEP: checking annotations
Jan 17 04:53:27.240: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 04:53:27.240: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:53:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2586" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":154,"skipped":2842,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:27.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 17 04:53:27.339: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:53:29.352: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:53:31.354: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 17 04:53:31.380: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:53:33.391: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 17 04:53:33.395: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:33.396: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:33.397: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 04:53:33.523: INFO: Exec stderr: ""
Jan 17 04:53:33.523: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:33.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:33.524: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:33.524: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 04:53:33.642: INFO: Exec stderr: ""
Jan 17 04:53:33.642: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:33.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:33.643: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:33.643: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 04:53:33.759: INFO: Exec stderr: ""
Jan 17 04:53:33.759: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:33.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:33.759: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:33.759: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 04:53:33.935: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 17 04:53:33.935: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:33.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:33.936: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:33.936: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 04:53:34.454: INFO: Exec stderr: ""
Jan 17 04:53:34.454: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:34.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:34.455: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:34.455: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 17 04:53:34.570: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 17 04:53:34.570: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:34.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:34.571: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:34.571: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 04:53:34.747: INFO: Exec stderr: ""
Jan 17 04:53:34.747: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:34.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:34.748: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:34.748: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 17 04:53:34.921: INFO: Exec stderr: ""
Jan 17 04:53:34.921: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:34.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:34.922: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:34.922: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 04:53:35.112: INFO: Exec stderr: ""
Jan 17 04:53:35.113: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6315 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:53:35.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:53:35.113: INFO: ExecWithOptions: Clientset creation
Jan 17 04:53:35.113: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 17 04:53:35.244: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jan 17 04:53:35.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6315" for this suite.

• [SLOW TEST:8.009 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":155,"skipped":2881,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:35.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jan 17 04:53:35.332: INFO: created test-pod-1
Jan 17 04:53:35.353: INFO: created test-pod-2
Jan 17 04:53:35.379: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jan 17 04:53:35.379: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5242' to be running and ready
Jan 17 04:53:35.429: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 04:53:35.429: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 04:53:35.429: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 17 04:53:35.429: INFO: 0 / 3 pods in namespace 'pods-5242' are running and ready (0 seconds elapsed)
Jan 17 04:53:35.429: INFO: expected 0 pod replicas in namespace 'pods-5242', 0 are Running and Ready.
Jan 17 04:53:35.429: INFO: POD         NODE                            PHASE    GRACE  CONDITIONS
Jan 17 04:53:35.429: INFO: test-pod-1  cluster124-apihrjet4zqi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  }]
Jan 17 04:53:35.429: INFO: test-pod-2  cluster124-apihrjet4zqi-node-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  }]
Jan 17 04:53:35.429: INFO: test-pod-3  cluster124-apihrjet4zqi-node-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-17 04:53:35 +0000 UTC  }]
Jan 17 04:53:35.429: INFO: 
Jan 17 04:53:37.450: INFO: 3 / 3 pods in namespace 'pods-5242' are running and ready (2 seconds elapsed)
Jan 17 04:53:37.450: INFO: expected 0 pod replicas in namespace 'pods-5242', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jan 17 04:53:37.512: INFO: Pod quantity 3 is different from expected quantity 0
Jan 17 04:53:38.522: INFO: Pod quantity 3 is different from expected quantity 0
Jan 17 04:53:39.518: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 04:53:40.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5242" for this suite.

• [SLOW TEST:5.276 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":156,"skipped":2891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:40.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jan 17 04:53:40.619: INFO: Waiting up to 5m0s for pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1" in namespace "var-expansion-1470" to be "Succeeded or Failed"
Jan 17 04:53:40.626: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.607058ms
Jan 17 04:53:42.645: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026224862s
Jan 17 04:53:44.651: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031835814s
Jan 17 04:53:46.663: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043711371s
Jan 17 04:53:48.674: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.055027647s
STEP: Saw pod success
Jan 17 04:53:48.674: INFO: Pod "var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1" satisfied condition "Succeeded or Failed"
Jan 17 04:53:48.681: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-1 pod var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1 container dapi-container: <nil>
STEP: delete the pod
Jan 17 04:53:48.780: INFO: Waiting for pod var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1 to disappear
Jan 17 04:53:48.788: INFO: Pod var-expansion-426231f0-c270-4ce1-b4e5-e04e074a8db1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 04:53:48.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1470" for this suite.

• [SLOW TEST:8.263 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":2918,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 17 04:53:48.878: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 04:53:52.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9254" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":158,"skipped":2973,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:52.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:53:52.985: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77" in namespace "security-context-test-2088" to be "Succeeded or Failed"
Jan 17 04:53:52.992: INFO: Pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77": Phase="Pending", Reason="", readiness=false. Elapsed: 7.670137ms
Jan 17 04:53:55.000: INFO: Pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015033623s
Jan 17 04:53:57.013: INFO: Pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028458134s
Jan 17 04:53:57.013: INFO: Pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77" satisfied condition "Succeeded or Failed"
Jan 17 04:53:57.026: INFO: Got logs for pod "busybox-privileged-false-b78fc321-ba6b-4f3a-9217-133e875e6b77": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 04:53:57.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2088" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":159,"skipped":2980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:53:57.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 17 04:53:57.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:54:26.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5703" for this suite.

• [SLOW TEST:29.451 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":160,"skipped":3018,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:54:26.510: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 17 04:54:28.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3034" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":161,"skipped":3040,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:54:28.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:54:28.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633" in namespace "downward-api-123" to be "Succeeded or Failed"
Jan 17 04:54:28.732: INFO: Pod "downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139267ms
Jan 17 04:54:30.741: INFO: Pod "downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013395411s
Jan 17 04:54:32.749: INFO: Pod "downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021872785s
STEP: Saw pod success
Jan 17 04:54:32.749: INFO: Pod "downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633" satisfied condition "Succeeded or Failed"
Jan 17 04:54:32.753: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633 container client-container: <nil>
STEP: delete the pod
Jan 17 04:54:32.836: INFO: Waiting for pod downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633 to disappear
Jan 17 04:54:32.841: INFO: Pod downwardapi-volume-e508d293-ffef-47c6-87d4-ac1a3bada633 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 04:54:32.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-123" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":162,"skipped":3054,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:54:32.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-790
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 17 04:54:32.907: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 04:54:32.970: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:54:34.979: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:54:36.978: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:38.979: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:40.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:42.983: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:44.979: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:46.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:48.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:50.980: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:52.978: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 04:54:54.981: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 17 04:54:54.990: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 17 04:54:54.998: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 17 04:54:57.038: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 04:54:57.038: INFO: Breadth first check of 10.100.106.99 on host 10.0.0.16...
Jan 17 04:54:57.042: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.244:9080/dial?request=hostname&protocol=http&host=10.100.106.99&port=8083&tries=1'] Namespace:pod-network-test-790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:54:57.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:54:57.043: INFO: ExecWithOptions: Clientset creation
Jan 17 04:54:57.043: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.106.99%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 04:54:57.209: INFO: Waiting for responses: map[]
Jan 17 04:54:57.209: INFO: reached 10.100.106.99 after 0/1 tries
Jan 17 04:54:57.209: INFO: Breadth first check of 10.100.44.230 on host 10.0.0.21...
Jan 17 04:54:57.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.244:9080/dial?request=hostname&protocol=http&host=10.100.44.230&port=8083&tries=1'] Namespace:pod-network-test-790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:54:57.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:54:57.214: INFO: ExecWithOptions: Clientset creation
Jan 17 04:54:57.214: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.44.230%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 04:54:57.401: INFO: Waiting for responses: map[]
Jan 17 04:54:57.401: INFO: reached 10.100.44.230 after 0/1 tries
Jan 17 04:54:57.401: INFO: Breadth first check of 10.100.11.246 on host 10.0.0.9...
Jan 17 04:54:57.406: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.244:9080/dial?request=hostname&protocol=http&host=10.100.11.246&port=8083&tries=1'] Namespace:pod-network-test-790 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 04:54:57.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:54:57.407: INFO: ExecWithOptions: Clientset creation
Jan 17 04:54:57.407: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-790/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.244%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.11.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 04:54:57.608: INFO: Waiting for responses: map[]
Jan 17 04:54:57.608: INFO: reached 10.100.11.246 after 0/1 tries
Jan 17 04:54:57.608: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 17 04:54:57.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-790" for this suite.

• [SLOW TEST:24.774 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3055,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:54:57.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-bhqn
STEP: Creating a pod to test atomic-volume-subpath
Jan 17 04:54:57.706: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bhqn" in namespace "subpath-9022" to be "Succeeded or Failed"
Jan 17 04:54:57.710: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277102ms
Jan 17 04:54:59.721: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 2.015906727s
Jan 17 04:55:01.733: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 4.027349212s
Jan 17 04:55:03.743: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 6.0374276s
Jan 17 04:55:05.752: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 8.046278372s
Jan 17 04:55:07.762: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 10.056538793s
Jan 17 04:55:09.774: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 12.067965073s
Jan 17 04:55:11.784: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 14.078308775s
Jan 17 04:55:13.795: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 16.089072071s
Jan 17 04:55:15.804: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 18.098622241s
Jan 17 04:55:17.815: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=true. Elapsed: 20.109195809s
Jan 17 04:55:19.826: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Running", Reason="", readiness=false. Elapsed: 22.120371557s
Jan 17 04:55:21.838: INFO: Pod "pod-subpath-test-projected-bhqn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.132265329s
STEP: Saw pod success
Jan 17 04:55:21.838: INFO: Pod "pod-subpath-test-projected-bhqn" satisfied condition "Succeeded or Failed"
Jan 17 04:55:21.841: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-subpath-test-projected-bhqn container test-container-subpath-projected-bhqn: <nil>
STEP: delete the pod
Jan 17 04:55:21.875: INFO: Waiting for pod pod-subpath-test-projected-bhqn to disappear
Jan 17 04:55:21.880: INFO: Pod pod-subpath-test-projected-bhqn no longer exists
STEP: Deleting pod pod-subpath-test-projected-bhqn
Jan 17 04:55:21.880: INFO: Deleting pod "pod-subpath-test-projected-bhqn" in namespace "subpath-9022"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 17 04:55:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9022" for this suite.

• [SLOW TEST:24.272 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":164,"skipped":3057,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:55:21.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 17 04:55:21.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 04:55:27.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:55:48.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6864" for this suite.

• [SLOW TEST:27.008 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":165,"skipped":3064,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:55:48.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-74edab60-059d-4a76-860c-ce95cb234ffa in namespace container-probe-4408
Jan 17 04:55:51.005: INFO: Started pod liveness-74edab60-059d-4a76-860c-ce95cb234ffa in namespace container-probe-4408
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 04:55:51.009: INFO: Initial restart count of pod liveness-74edab60-059d-4a76-860c-ce95cb234ffa is 0
Jan 17 04:56:11.120: INFO: Restart count of pod container-probe-4408/liveness-74edab60-059d-4a76-860c-ce95cb234ffa is now 1 (20.111086553s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 04:56:11.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4408" for this suite.

• [SLOW TEST:22.248 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":166,"skipped":3079,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:56:11.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:56:12.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:56:15.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:56:15.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6363" for this suite.
STEP: Destroying namespace "webhook-6363-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":167,"skipped":3079,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:56:15.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:56:15.641: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 17 04:56:15.664: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 17 04:56:20.683: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 17 04:56:20.683: INFO: Creating deployment "test-rolling-update-deployment"
Jan 17 04:56:20.703: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 17 04:56:20.718: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 17 04:56:22.727: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 17 04:56:22.730: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 04:56:22.748: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6195  fd83a9fd-0522-4fcb-a1bd-a6e5044d4a0c 36545 1 2023-01-17 04:56:20 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-01-17 04:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:56:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059e3918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 04:56:20 +0000 UTC,LastTransitionTime:2023-01-17 04:56:20 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-01-17 04:56:22 +0000 UTC,LastTransitionTime:2023-01-17 04:56:20 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 04:56:22.752: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-6195  3b7779fd-1ee2-43f7-96e6-2bbba4993691 36535 1 2023-01-17 04:56:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment fd83a9fd-0522-4fcb-a1bd-a6e5044d4a0c 0xc005db4177 0xc005db4178}] []  [{kube-controller-manager Update apps/v1 2023-01-17 04:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd83a9fd-0522-4fcb-a1bd-a6e5044d4a0c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:56:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005db4258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 04:56:22.752: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 17 04:56:22.752: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6195  2ed3b41c-7806-43db-b882-bf866a7f23d4 36544 2 2023-01-17 04:56:15 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment fd83a9fd-0522-4fcb-a1bd-a6e5044d4a0c 0xc0059e3f97 0xc0059e3f98}] []  [{e2e.test Update apps/v1 2023-01-17 04:56:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:56:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd83a9fd-0522-4fcb-a1bd-a6e5044d4a0c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 04:56:22 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005db40f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 04:56:22.756: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-zmrwt" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-zmrwt test-rolling-update-deployment-67c8f74c6c- deployment-6195  4def20f4-3ea7-4f7b-b437-64129fd84716 36534 0 2023-01-17 04:56:20 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:9bf63cedc2a89e042e6dfb53bc27fa92c58527f45173106308f0aa5d75630c9f cni.projectcalico.org/podIP:10.100.106.82/32 cni.projectcalico.org/podIPs:10.100.106.82/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c 3b7779fd-1ee2-43f7-96e6-2bbba4993691 0xc005b604d7 0xc005b604d8}] []  [{kube-controller-manager Update v1 2023-01-17 04:56:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b7779fd-1ee2-43f7-96e6-2bbba4993691\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 04:56:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 04:56:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.106.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jk29,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jk29,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:56:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:56:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 04:56:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.106.82,StartTime:2023-01-17 04:56:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 04:56:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://f57ea740088c32ba577515362ca8f0f279443432b0f5475e5b470c734ebe3ada,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.106.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 04:56:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6195" for this suite.

• [SLOW TEST:7.218 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":168,"skipped":3086,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:56:22.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jan 17 04:56:22.824: INFO: Creating e2e-svc-a-8jqd8
Jan 17 04:56:22.846: INFO: Creating e2e-svc-b-j7vzz
Jan 17 04:56:22.871: INFO: Creating e2e-svc-c-mlk58
STEP: deleting service collection
Jan 17 04:56:22.973: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:56:22.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3737" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":169,"skipped":3095,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:56:22.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 04:56:23.075: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 04:57:23.129: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 17 04:57:23.180: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 17 04:57:23.197: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 17 04:57:23.239: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 17 04:57:23.258: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 17 04:57:23.297: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 17 04:57:23.315: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 17 04:57:39.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8826" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:76.554 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":170,"skipped":3169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:39.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 17 04:57:39.605: INFO: Waiting up to 5m0s for pod "security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c" in namespace "security-context-3741" to be "Succeeded or Failed"
Jan 17 04:57:39.610: INFO: Pod "security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.69324ms
Jan 17 04:57:41.622: INFO: Pod "security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016181911s
Jan 17 04:57:43.634: INFO: Pod "security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02890014s
STEP: Saw pod success
Jan 17 04:57:43.634: INFO: Pod "security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c" satisfied condition "Succeeded or Failed"
Jan 17 04:57:43.638: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c container test-container: <nil>
STEP: delete the pod
Jan 17 04:57:43.723: INFO: Waiting for pod security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c to disappear
Jan 17 04:57:43.728: INFO: Pod security-context-8f4a869c-c212-4e8e-aba7-f634b9384b9c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 04:57:43.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-3741" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":171,"skipped":3200,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:43.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-9dd7fb9a-fd38-4370-a022-75f069b47c28
STEP: Creating a pod to test consume secrets
Jan 17 04:57:43.828: INFO: Waiting up to 5m0s for pod "pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc" in namespace "secrets-1588" to be "Succeeded or Failed"
Jan 17 04:57:43.838: INFO: Pod "pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744915ms
Jan 17 04:57:45.846: INFO: Pod "pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018581504s
Jan 17 04:57:47.854: INFO: Pod "pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025756344s
STEP: Saw pod success
Jan 17 04:57:47.854: INFO: Pod "pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc" satisfied condition "Succeeded or Failed"
Jan 17 04:57:47.858: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 04:57:47.895: INFO: Waiting for pod pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc to disappear
Jan 17 04:57:47.901: INFO: Pod pod-secrets-3361eaad-8297-40c1-85a1-9f280d879cbc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 04:57:47.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1588" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":172,"skipped":3251,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:47.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 04:57:48.455: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 04:57:51.498: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:57:51.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4294" for this suite.
STEP: Destroying namespace "webhook-4294-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":173,"skipped":3258,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:51.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 17 04:57:51.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5788" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":174,"skipped":3263,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:51.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 17 04:57:51.958: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 04:57:55.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7170" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":175,"skipped":3271,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:57:55.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:57:55.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 17 04:58:01.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-9599 --namespace=crd-publish-openapi-9599 create -f -'
Jan 17 04:58:02.903: INFO: stderr: ""
Jan 17 04:58:02.903: INFO: stdout: "e2e-test-crd-publish-openapi-8990-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 04:58:02.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-9599 --namespace=crd-publish-openapi-9599 delete e2e-test-crd-publish-openapi-8990-crds test-cr'
Jan 17 04:58:03.024: INFO: stderr: ""
Jan 17 04:58:03.024: INFO: stdout: "e2e-test-crd-publish-openapi-8990-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 17 04:58:03.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-9599 --namespace=crd-publish-openapi-9599 apply -f -'
Jan 17 04:58:03.291: INFO: stderr: ""
Jan 17 04:58:03.291: INFO: stdout: "e2e-test-crd-publish-openapi-8990-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 17 04:58:03.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-9599 --namespace=crd-publish-openapi-9599 delete e2e-test-crd-publish-openapi-8990-crds test-cr'
Jan 17 04:58:03.408: INFO: stderr: ""
Jan 17 04:58:03.408: INFO: stdout: "e2e-test-crd-publish-openapi-8990-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 17 04:58:03.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-9599 explain e2e-test-crd-publish-openapi-8990-crds'
Jan 17 04:58:05.032: INFO: stderr: ""
Jan 17 04:58:05.032: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8990-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:58:10.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9599" for this suite.

• [SLOW TEST:14.914 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":176,"skipped":3317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:10.655: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-f6ee0693-7a15-45bf-b28c-78c586b079d4
STEP: Creating a pod to test consume configMaps
Jan 17 04:58:10.733: INFO: Waiting up to 5m0s for pod "pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81" in namespace "configmap-6696" to be "Succeeded or Failed"
Jan 17 04:58:10.739: INFO: Pod "pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81": Phase="Pending", Reason="", readiness=false. Elapsed: 5.386984ms
Jan 17 04:58:12.748: INFO: Pod "pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014269997s
Jan 17 04:58:14.760: INFO: Pod "pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026571522s
STEP: Saw pod success
Jan 17 04:58:14.760: INFO: Pod "pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81" satisfied condition "Succeeded or Failed"
Jan 17 04:58:14.764: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:58:14.849: INFO: Waiting for pod pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81 to disappear
Jan 17 04:58:14.854: INFO: Pod pod-configmaps-c17f8f85-fd56-4b73-b72d-f1302bcbcc81 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:58:14.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6696" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":177,"skipped":3369,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:14.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-13071a42-8e0a-4f7a-bb69-c0a8a283f3df
STEP: Creating secret with name s-test-opt-upd-804a4df1-a628-429b-9e84-4869043b532a
STEP: Creating the pod
Jan 17 04:58:14.960: INFO: The status of Pod pod-projected-secrets-cd50bc59-74cc-4ef6-88f2-62fc10416207 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:58:16.973: INFO: The status of Pod pod-projected-secrets-cd50bc59-74cc-4ef6-88f2-62fc10416207 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-13071a42-8e0a-4f7a-bb69-c0a8a283f3df
STEP: Updating secret s-test-opt-upd-804a4df1-a628-429b-9e84-4869043b532a
STEP: Creating secret with name s-test-opt-create-f625473d-9220-4654-a4df-674459596184
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 04:58:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4374" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":178,"skipped":3370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 04:58:19.143: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6" in namespace "projected-9797" to be "Succeeded or Failed"
Jan 17 04:58:19.149: INFO: Pod "downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.479482ms
Jan 17 04:58:21.161: INFO: Pod "downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017558607s
Jan 17 04:58:23.168: INFO: Pod "downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024562571s
STEP: Saw pod success
Jan 17 04:58:23.168: INFO: Pod "downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6" satisfied condition "Succeeded or Failed"
Jan 17 04:58:23.172: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6 container client-container: <nil>
STEP: delete the pod
Jan 17 04:58:23.209: INFO: Waiting for pod downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6 to disappear
Jan 17 04:58:23.215: INFO: Pod downwardapi-volume-b0cd6d43-af61-46ad-bc87-33b339aeb0e6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 04:58:23.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9797" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":179,"skipped":3443,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-mtzg
STEP: Creating a pod to test atomic-volume-subpath
Jan 17 04:58:23.314: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mtzg" in namespace "subpath-3853" to be "Succeeded or Failed"
Jan 17 04:58:23.326: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Pending", Reason="", readiness=false. Elapsed: 11.360764ms
Jan 17 04:58:25.335: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 2.020982241s
Jan 17 04:58:27.343: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 4.029197841s
Jan 17 04:58:29.351: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 6.037113925s
Jan 17 04:58:31.362: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 8.048108418s
Jan 17 04:58:33.368: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 10.054109662s
Jan 17 04:58:35.379: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 12.064464831s
Jan 17 04:58:37.389: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 14.07523796s
Jan 17 04:58:39.398: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 16.083359957s
Jan 17 04:58:41.403: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 18.089000061s
Jan 17 04:58:43.413: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=true. Elapsed: 20.098569673s
Jan 17 04:58:45.423: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Running", Reason="", readiness=false. Elapsed: 22.109182628s
Jan 17 04:58:47.429: INFO: Pod "pod-subpath-test-configmap-mtzg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.114632306s
STEP: Saw pod success
Jan 17 04:58:47.429: INFO: Pod "pod-subpath-test-configmap-mtzg" satisfied condition "Succeeded or Failed"
Jan 17 04:58:47.435: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-subpath-test-configmap-mtzg container test-container-subpath-configmap-mtzg: <nil>
STEP: delete the pod
Jan 17 04:58:47.469: INFO: Waiting for pod pod-subpath-test-configmap-mtzg to disappear
Jan 17 04:58:47.475: INFO: Pod pod-subpath-test-configmap-mtzg no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mtzg
Jan 17 04:58:47.475: INFO: Deleting pod "pod-subpath-test-configmap-mtzg" in namespace "subpath-3853"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 17 04:58:47.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3853" for this suite.

• [SLOW TEST:24.270 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":180,"skipped":3459,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:47.501: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 17 04:58:47.578: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:58:49.589: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 17 04:58:49.611: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 04:58:51.621: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 17 04:58:51.639: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 04:58:51.645: INFO: Pod pod-with-prestop-http-hook still exists
Jan 17 04:58:53.645: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 17 04:58:53.656: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 17 04:58:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6107" for this suite.

• [SLOW TEST:6.231 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":181,"skipped":3468,"failed":0}
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:53.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:58:53.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9117" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":182,"skipped":3468,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:58:53.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8578
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8578
STEP: creating replication controller externalsvc in namespace services-8578
I0117 04:58:53.960702      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8578, replica count: 2
I0117 04:58:57.012129      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 17 04:58:57.053: INFO: Creating new exec pod
Jan 17 04:58:59.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-8578 exec execpodw9jn5 -- /bin/sh -x -c nslookup clusterip-service.services-8578.svc.cluster.local'
Jan 17 04:58:59.479: INFO: stderr: "+ nslookup clusterip-service.services-8578.svc.cluster.local\n"
Jan 17 04:58:59.479: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-8578.svc.cluster.local\tcanonical name = externalsvc.services-8578.svc.cluster.local.\nName:\texternalsvc.services-8578.svc.cluster.local\nAddress: 10.254.41.104\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8578, will wait for the garbage collector to delete the pods
Jan 17 04:58:59.550: INFO: Deleting ReplicationController externalsvc took: 10.165718ms
Jan 17 04:58:59.650: INFO: Terminating ReplicationController externalsvc pods took: 100.256098ms
Jan 17 04:59:02.088: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:59:02.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8578" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.325 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":183,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:02.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jan 17 04:59:02.213: INFO: Found Service test-service-tql82 in namespace services-7564 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 17 04:59:02.213: INFO: Service test-service-tql82 created
STEP: Getting /status
Jan 17 04:59:02.227: INFO: Service test-service-tql82 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 17 04:59:02.322: INFO: observed Service test-service-tql82 in namespace services-7564 with annotations: map[] & LoadBalancer: {[]}
Jan 17 04:59:02.322: INFO: Found Service test-service-tql82 in namespace services-7564 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 17 04:59:02.322: INFO: Service test-service-tql82 has service status patched
STEP: updating the ServiceStatus
Jan 17 04:59:02.345: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 17 04:59:02.349: INFO: Observed Service test-service-tql82 in namespace services-7564 with annotations: map[] & Conditions: {[]}
Jan 17 04:59:02.349: INFO: Observed event: &Service{ObjectMeta:{test-service-tql82  services-7564  662d1a3e-7b5f-4033-830c-a369244ab9bb 37711 0 2023-01-17 04:59:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-17 04:59:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-17 04:59:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.254.176.88,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.254.176.88],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 17 04:59:02.350: INFO: Found Service test-service-tql82 in namespace services-7564 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 17 04:59:02.350: INFO: Service test-service-tql82 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 17 04:59:02.377: INFO: observed Service test-service-tql82 in namespace services-7564 with labels: map[test-service-static:true]
Jan 17 04:59:02.377: INFO: observed Service test-service-tql82 in namespace services-7564 with labels: map[test-service-static:true]
Jan 17 04:59:02.377: INFO: observed Service test-service-tql82 in namespace services-7564 with labels: map[test-service-static:true]
Jan 17 04:59:02.378: INFO: Found Service test-service-tql82 in namespace services-7564 with labels: map[test-service:patched test-service-static:true]
Jan 17 04:59:02.378: INFO: Service test-service-tql82 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 17 04:59:02.408: INFO: Observed event: ADDED
Jan 17 04:59:02.408: INFO: Observed event: MODIFIED
Jan 17 04:59:02.408: INFO: Observed event: MODIFIED
Jan 17 04:59:02.408: INFO: Observed event: MODIFIED
Jan 17 04:59:02.408: INFO: Found Service test-service-tql82 in namespace services-7564 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 17 04:59:02.408: INFO: Service test-service-tql82 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:59:02.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7564" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":184,"skipped":3518,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:02.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:59:02.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 04:59:03.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6293" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":185,"skipped":3518,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:03.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:59:03.652: INFO: Creating ReplicaSet my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3
Jan 17 04:59:03.712: INFO: Pod name my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3: Found 1 pods out of 1
Jan 17 04:59:03.712: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3" is running
Jan 17 04:59:05.732: INFO: Pod "my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3-fdd88" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-17 04:59:03 +0000 UTC Reason: Message:}])
Jan 17 04:59:05.732: INFO: Trying to dial the pod
Jan 17 04:59:10.754: INFO: Controller my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3: Got expected result from replica 1 [my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3-fdd88]: "my-hostname-basic-0f92979f-5cc0-415a-b8a6-0752ca292ab3-fdd88", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 04:59:10.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1210" for this suite.

• [SLOW TEST:7.216 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":186,"skipped":3538,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:10.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-2399413c-7f3b-4196-9de8-2d7f70079ac5
STEP: Creating a pod to test consume configMaps
Jan 17 04:59:10.864: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c" in namespace "projected-9200" to be "Succeeded or Failed"
Jan 17 04:59:10.869: INFO: Pod "pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741881ms
Jan 17 04:59:12.879: INFO: Pod "pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014776981s
Jan 17 04:59:14.888: INFO: Pod "pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023914776s
STEP: Saw pod success
Jan 17 04:59:14.889: INFO: Pod "pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c" satisfied condition "Succeeded or Failed"
Jan 17 04:59:14.892: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c container agnhost-container: <nil>
STEP: delete the pod
Jan 17 04:59:14.931: INFO: Waiting for pod pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c to disappear
Jan 17 04:59:14.936: INFO: Pod pod-projected-configmaps-1fd0b84a-7c5f-49cb-a683-2f998cc0261c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 04:59:14.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9200" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":187,"skipped":3555,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:14.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 17 04:59:15.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 create -f -'
Jan 17 04:59:16.530: INFO: stderr: ""
Jan 17 04:59:16.530: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 17 04:59:16.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:16.619: INFO: stderr: ""
Jan 17 04:59:16.619: INFO: stdout: "update-demo-nautilus-2sxdm update-demo-nautilus-br2gp "
Jan 17 04:59:16.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-2sxdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:16.735: INFO: stderr: ""
Jan 17 04:59:16.735: INFO: stdout: ""
Jan 17 04:59:16.735: INFO: update-demo-nautilus-2sxdm is created but not running
Jan 17 04:59:21.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:21.823: INFO: stderr: ""
Jan 17 04:59:21.823: INFO: stdout: "update-demo-nautilus-2sxdm update-demo-nautilus-br2gp "
Jan 17 04:59:21.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-2sxdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:21.913: INFO: stderr: ""
Jan 17 04:59:21.913: INFO: stdout: ""
Jan 17 04:59:21.913: INFO: update-demo-nautilus-2sxdm is created but not running
Jan 17 04:59:26.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:26.998: INFO: stderr: ""
Jan 17 04:59:26.998: INFO: stdout: "update-demo-nautilus-2sxdm update-demo-nautilus-br2gp "
Jan 17 04:59:26.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-2sxdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:27.098: INFO: stderr: ""
Jan 17 04:59:27.098: INFO: stdout: "true"
Jan 17 04:59:27.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-2sxdm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:27.185: INFO: stderr: ""
Jan 17 04:59:27.185: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:27.185: INFO: validating pod update-demo-nautilus-2sxdm
Jan 17 04:59:27.192: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:27.192: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:27.192: INFO: update-demo-nautilus-2sxdm is verified up and running
Jan 17 04:59:27.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:27.282: INFO: stderr: ""
Jan 17 04:59:27.282: INFO: stdout: "true"
Jan 17 04:59:27.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:27.363: INFO: stderr: ""
Jan 17 04:59:27.363: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:27.363: INFO: validating pod update-demo-nautilus-br2gp
Jan 17 04:59:27.370: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:27.370: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:27.370: INFO: update-demo-nautilus-br2gp is verified up and running
STEP: scaling down the replication controller
Jan 17 04:59:27.372: INFO: scanned /root for discovery docs: <nil>
Jan 17 04:59:27.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 17 04:59:28.483: INFO: stderr: ""
Jan 17 04:59:28.483: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 17 04:59:28.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:28.594: INFO: stderr: ""
Jan 17 04:59:28.594: INFO: stdout: "update-demo-nautilus-2sxdm update-demo-nautilus-br2gp "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 17 04:59:33.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:33.686: INFO: stderr: ""
Jan 17 04:59:33.686: INFO: stdout: "update-demo-nautilus-br2gp "
Jan 17 04:59:33.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:33.770: INFO: stderr: ""
Jan 17 04:59:33.770: INFO: stdout: "true"
Jan 17 04:59:33.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:33.883: INFO: stderr: ""
Jan 17 04:59:33.883: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:33.883: INFO: validating pod update-demo-nautilus-br2gp
Jan 17 04:59:33.888: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:33.888: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:33.888: INFO: update-demo-nautilus-br2gp is verified up and running
STEP: scaling up the replication controller
Jan 17 04:59:33.890: INFO: scanned /root for discovery docs: <nil>
Jan 17 04:59:33.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 17 04:59:35.015: INFO: stderr: ""
Jan 17 04:59:35.015: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 17 04:59:35.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:35.106: INFO: stderr: ""
Jan 17 04:59:35.107: INFO: stdout: "update-demo-nautilus-br2gp update-demo-nautilus-xz8p2 "
Jan 17 04:59:35.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:35.189: INFO: stderr: ""
Jan 17 04:59:35.189: INFO: stdout: "true"
Jan 17 04:59:35.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:35.268: INFO: stderr: ""
Jan 17 04:59:35.268: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:35.268: INFO: validating pod update-demo-nautilus-br2gp
Jan 17 04:59:35.275: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:35.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:35.275: INFO: update-demo-nautilus-br2gp is verified up and running
Jan 17 04:59:35.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-xz8p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:35.367: INFO: stderr: ""
Jan 17 04:59:35.367: INFO: stdout: ""
Jan 17 04:59:35.367: INFO: update-demo-nautilus-xz8p2 is created but not running
Jan 17 04:59:40.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 04:59:40.453: INFO: stderr: ""
Jan 17 04:59:40.453: INFO: stdout: "update-demo-nautilus-br2gp update-demo-nautilus-xz8p2 "
Jan 17 04:59:40.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:40.536: INFO: stderr: ""
Jan 17 04:59:40.536: INFO: stdout: "true"
Jan 17 04:59:40.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-br2gp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:40.623: INFO: stderr: ""
Jan 17 04:59:40.623: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:40.623: INFO: validating pod update-demo-nautilus-br2gp
Jan 17 04:59:40.628: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:40.628: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:40.628: INFO: update-demo-nautilus-br2gp is verified up and running
Jan 17 04:59:40.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-xz8p2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 04:59:40.714: INFO: stderr: ""
Jan 17 04:59:40.714: INFO: stdout: "true"
Jan 17 04:59:40.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods update-demo-nautilus-xz8p2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 04:59:40.792: INFO: stderr: ""
Jan 17 04:59:40.792: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 04:59:40.792: INFO: validating pod update-demo-nautilus-xz8p2
Jan 17 04:59:40.800: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 04:59:40.800: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 04:59:40.800: INFO: update-demo-nautilus-xz8p2 is verified up and running
STEP: using delete to clean up resources
Jan 17 04:59:40.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 delete --grace-period=0 --force -f -'
Jan 17 04:59:40.897: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 04:59:40.897: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 04:59:40.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get rc,svc -l name=update-demo --no-headers'
Jan 17 04:59:41.102: INFO: stderr: "No resources found in kubectl-5543 namespace.\n"
Jan 17 04:59:41.102: INFO: stdout: ""
Jan 17 04:59:41.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-5543 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 04:59:41.218: INFO: stderr: ""
Jan 17 04:59:41.218: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 04:59:41.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5543" for this suite.

• [SLOW TEST:26.291 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":188,"skipped":3595,"failed":0}
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:41.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 04:59:41.296: INFO: Got root ca configmap in namespace "svcaccounts-92"
Jan 17 04:59:41.313: INFO: Deleted root ca configmap in namespace "svcaccounts-92"
STEP: waiting for a new root ca configmap created
Jan 17 04:59:41.823: INFO: Recreated root ca configmap in namespace "svcaccounts-92"
Jan 17 04:59:41.835: INFO: Updated root ca configmap in namespace "svcaccounts-92"
STEP: waiting for the root ca configmap reconciled
Jan 17 04:59:42.344: INFO: Reconciled root ca configmap in namespace "svcaccounts-92"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 04:59:42.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-92" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":189,"skipped":3595,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:42.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-3940
STEP: creating replication controller nodeport-test in namespace services-3940
I0117 04:59:42.471463      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3940, replica count: 2
Jan 17 04:59:45.522: INFO: Creating new exec pod
I0117 04:59:45.522191      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 04:59:48.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3940 exec execpodv77lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 17 04:59:48.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 17 04:59:48.818: INFO: stdout: "nodeport-test-jskgf"
Jan 17 04:59:48.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3940 exec execpodv77lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.120.91 80'
Jan 17 04:59:49.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.120.91 80\nConnection to 10.254.120.91 80 port [tcp/http] succeeded!\n"
Jan 17 04:59:49.056: INFO: stdout: "nodeport-test-jskgf"
Jan 17 04:59:49.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3940 exec execpodv77lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 30683'
Jan 17 04:59:49.271: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 30683\nConnection to 10.0.0.21 30683 port [tcp/*] succeeded!\n"
Jan 17 04:59:49.271: INFO: stdout: "nodeport-test-ml5c4"
Jan 17 04:59:49.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3940 exec execpodv77lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30683'
Jan 17 04:59:49.519: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30683\nConnection to 10.0.0.16 30683 port [tcp/*] succeeded!\n"
Jan 17 04:59:49.519: INFO: stdout: ""
Jan 17 04:59:50.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3940 exec execpodv77lj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 30683'
Jan 17 04:59:50.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 30683\nConnection to 10.0.0.16 30683 port [tcp/*] succeeded!\n"
Jan 17 04:59:50.786: INFO: stdout: "nodeport-test-ml5c4"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 04:59:50.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3940" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.443 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":190,"skipped":3596,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:50.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-30e13ba5-4ebd-48e4-b1f9-8cdd53034a69
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 04:59:52.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9745" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":191,"skipped":3606,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:52.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 17 04:59:57.074: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 17 04:59:57.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1142" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":192,"skipped":3623,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 04:59:57.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 17 04:59:57.245: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:57.245: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:57.245: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:57.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:59:57.256: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:59:58.307: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:58.307: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:58.307: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:58.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 04:59:58.314: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 04:59:59.265: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.265: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.266: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:59:59.270: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 17 04:59:59.302: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.302: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.302: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 04:59:59.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 04:59:59.311: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3059, will wait for the garbage collector to delete the pods
Jan 17 05:00:00.407: INFO: Deleting DaemonSet.extensions daemon-set took: 10.39125ms
Jan 17 05:00:00.508: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.021327ms
Jan 17 05:00:03.214: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:00:03.215: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 05:00:03.218: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38368"},"items":null}

Jan 17 05:00:03.220: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38368"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:00:03.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3059" for this suite.

• [SLOW TEST:6.139 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":193,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:03.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 17 05:00:03.322: INFO: Waiting up to 5m0s for pod "pod-be6f0d2b-d435-49ef-b657-90681da9868f" in namespace "emptydir-9684" to be "Succeeded or Failed"
Jan 17 05:00:03.327: INFO: Pod "pod-be6f0d2b-d435-49ef-b657-90681da9868f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.602738ms
Jan 17 05:00:05.342: INFO: Pod "pod-be6f0d2b-d435-49ef-b657-90681da9868f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020339446s
Jan 17 05:00:07.352: INFO: Pod "pod-be6f0d2b-d435-49ef-b657-90681da9868f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030566553s
STEP: Saw pod success
Jan 17 05:00:07.352: INFO: Pod "pod-be6f0d2b-d435-49ef-b657-90681da9868f" satisfied condition "Succeeded or Failed"
Jan 17 05:00:07.356: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-be6f0d2b-d435-49ef-b657-90681da9868f container test-container: <nil>
STEP: delete the pod
Jan 17 05:00:07.438: INFO: Waiting for pod pod-be6f0d2b-d435-49ef-b657-90681da9868f to disappear
Jan 17 05:00:07.443: INFO: Pod pod-be6f0d2b-d435-49ef-b657-90681da9868f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:00:07.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9684" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":194,"skipped":3659,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:07.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 17 05:00:07.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-4414 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 17 05:00:07.627: INFO: stderr: ""
Jan 17 05:00:07.627: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jan 17 05:00:07.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-4414 delete pods e2e-test-httpd-pod'
Jan 17 05:00:10.225: INFO: stderr: ""
Jan 17 05:00:10.225: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:00:10.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4414" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":195,"skipped":3664,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:10.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3018
STEP: creating service affinity-clusterip in namespace services-3018
STEP: creating replication controller affinity-clusterip in namespace services-3018
I0117 05:00:10.384274      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3018, replica count: 3
I0117 05:00:13.434898      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:00:13.447: INFO: Creating new exec pod
Jan 17 05:00:16.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3018 exec execpod-affinity8x5b4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 17 05:00:16.728: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 17 05:00:16.728: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:00:16.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3018 exec execpod-affinity8x5b4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.242.74 80'
Jan 17 05:00:16.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.242.74 80\nConnection to 10.254.242.74 80 port [tcp/http] succeeded!\n"
Jan 17 05:00:16.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:00:16.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3018 exec execpod-affinity8x5b4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.242.74:80/ ; done'
Jan 17 05:00:17.302: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.242.74:80/\n"
Jan 17 05:00:17.303: INFO: stdout: "\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b\naffinity-clusterip-z559b"
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Received response from host: affinity-clusterip-z559b
Jan 17 05:00:17.303: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3018, will wait for the garbage collector to delete the pods
Jan 17 05:00:17.405: INFO: Deleting ReplicationController affinity-clusterip took: 13.366522ms
Jan 17 05:00:17.505: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.545131ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:00:19.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3018" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:9.572 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":196,"skipped":3681,"failed":0}
SS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:19.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:00:19.931: INFO: The status of Pod server-envvars-b332173e-d5ef-4294-bc1a-0909786832ba is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:21.943: INFO: The status of Pod server-envvars-b332173e-d5ef-4294-bc1a-0909786832ba is Running (Ready = true)
Jan 17 05:00:21.982: INFO: Waiting up to 5m0s for pod "client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60" in namespace "pods-2967" to be "Succeeded or Failed"
Jan 17 05:00:21.988: INFO: Pod "client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868301ms
Jan 17 05:00:23.994: INFO: Pod "client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012783594s
Jan 17 05:00:26.004: INFO: Pod "client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022224549s
STEP: Saw pod success
Jan 17 05:00:26.004: INFO: Pod "client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60" satisfied condition "Succeeded or Failed"
Jan 17 05:00:26.007: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60 container env3cont: <nil>
STEP: delete the pod
Jan 17 05:00:26.039: INFO: Waiting for pod client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60 to disappear
Jan 17 05:00:26.044: INFO: Pod client-envvars-e20df8f7-b2a8-471b-9eca-df7de9f3ed60 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 05:00:26.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2967" for this suite.

• [SLOW TEST:6.200 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":197,"skipped":3683,"failed":0}
SSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:26.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 17 05:00:26.132: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:28.143: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:30.140: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.9 on the node which pod1 resides and expect scheduled
Jan 17 05:00:30.163: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:32.183: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:34.173: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.9 but use UDP protocol on the node which pod2 resides
Jan 17 05:00:34.191: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:36.199: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 17 05:00:36.213: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:38.224: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 17 05:00:38.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.9 http://127.0.0.1:54323/hostname] Namespace:hostport-169 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:00:38.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:00:38.228: INFO: ExecWithOptions: Clientset creation
Jan 17 05:00:38.228: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-169/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.9+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.9, port: 54323
Jan 17 05:00:38.407: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.9:54323/hostname] Namespace:hostport-169 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:00:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:00:38.408: INFO: ExecWithOptions: Clientset creation
Jan 17 05:00:38.408: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-169/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.9%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.9, port: 54323 UDP
Jan 17 05:00:38.564: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.9 54323] Namespace:hostport-169 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:00:38.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:00:38.564: INFO: ExecWithOptions: Clientset creation
Jan 17 05:00:38.564: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/hostport-169/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.0.0.9+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jan 17 05:00:43.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-169" for this suite.

• [SLOW TEST:17.693 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":198,"skipped":3688,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:43.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 17 05:00:43.820: INFO: Waiting up to 5m0s for pod "pod-60614b0d-224b-47f1-a16f-5de219ddf874" in namespace "emptydir-778" to be "Succeeded or Failed"
Jan 17 05:00:43.826: INFO: Pod "pod-60614b0d-224b-47f1-a16f-5de219ddf874": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163759ms
Jan 17 05:00:45.837: INFO: Pod "pod-60614b0d-224b-47f1-a16f-5de219ddf874": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017199107s
Jan 17 05:00:47.847: INFO: Pod "pod-60614b0d-224b-47f1-a16f-5de219ddf874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026531394s
STEP: Saw pod success
Jan 17 05:00:47.847: INFO: Pod "pod-60614b0d-224b-47f1-a16f-5de219ddf874" satisfied condition "Succeeded or Failed"
Jan 17 05:00:47.851: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-60614b0d-224b-47f1-a16f-5de219ddf874 container test-container: <nil>
STEP: delete the pod
Jan 17 05:00:47.883: INFO: Waiting for pod pod-60614b0d-224b-47f1-a16f-5de219ddf874 to disappear
Jan 17 05:00:47.888: INFO: Pod pod-60614b0d-224b-47f1-a16f-5de219ddf874 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:00:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-778" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":199,"skipped":3697,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:47.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jan 17 05:00:47.972: INFO: The status of Pod pod-hostip-487e286a-d23e-43c3-8ef8-fbfc74941f97 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:00:49.979: INFO: The status of Pod pod-hostip-487e286a-d23e-43c3-8ef8-fbfc74941f97 is Running (Ready = true)
Jan 17 05:00:49.987: INFO: Pod pod-hostip-487e286a-d23e-43c3-8ef8-fbfc74941f97 has hostIP: 10.0.0.16
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 05:00:49.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2097" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":200,"skipped":3711,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:50.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 17 05:00:50.072: INFO: Waiting up to 5m0s for pod "downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3" in namespace "downward-api-1866" to be "Succeeded or Failed"
Jan 17 05:00:50.078: INFO: Pod "downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924065ms
Jan 17 05:00:52.087: INFO: Pod "downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015276912s
Jan 17 05:00:54.098: INFO: Pod "downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026203464s
STEP: Saw pod success
Jan 17 05:00:54.098: INFO: Pod "downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3" satisfied condition "Succeeded or Failed"
Jan 17 05:00:54.102: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3 container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:00:54.138: INFO: Waiting for pod downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3 to disappear
Jan 17 05:00:54.145: INFO: Pod downward-api-03ce2daf-4eb6-43c8-a519-ea11e2d8fcf3 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 17 05:00:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1866" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":3752,"failed":0}
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:54.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:00:54.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-1215
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jan 17 05:01:00.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8354" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 17 05:01:00.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1215" for this suite.

• [SLOW TEST:6.326 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":202,"skipped":3753,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:00.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-522498cb-9a8f-4b3b-a23a-452b6fdbd07e
STEP: Creating a pod to test consume secrets
Jan 17 05:01:00.559: INFO: Waiting up to 5m0s for pod "pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a" in namespace "secrets-5399" to be "Succeeded or Failed"
Jan 17 05:01:00.564: INFO: Pod "pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529361ms
Jan 17 05:01:02.571: INFO: Pod "pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012143047s
Jan 17 05:01:04.583: INFO: Pod "pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024605897s
STEP: Saw pod success
Jan 17 05:01:04.583: INFO: Pod "pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a" satisfied condition "Succeeded or Failed"
Jan 17 05:01:04.587: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:01:04.641: INFO: Waiting for pod pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a to disappear
Jan 17 05:01:04.647: INFO: Pod pod-secrets-3ae73a78-2c72-4d81-9015-23b616804c1a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:01:04.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5399" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":3770,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:01:04.800: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06" in namespace "projected-2542" to be "Succeeded or Failed"
Jan 17 05:01:04.805: INFO: Pod "downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06": Phase="Pending", Reason="", readiness=false. Elapsed: 5.752515ms
Jan 17 05:01:06.818: INFO: Pod "downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018266878s
Jan 17 05:01:08.828: INFO: Pod "downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028787025s
STEP: Saw pod success
Jan 17 05:01:08.828: INFO: Pod "downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06" satisfied condition "Succeeded or Failed"
Jan 17 05:01:08.832: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06 container client-container: <nil>
STEP: delete the pod
Jan 17 05:01:08.861: INFO: Waiting for pod downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06 to disappear
Jan 17 05:01:08.870: INFO: Pod downwardapi-volume-a10325aa-513e-43ce-9b2b-3fc5d07c8c06 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:01:08.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2542" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":204,"skipped":3772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:08.889: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-1fc0c62e-5f17-43ae-a274-ad64541b77f7
STEP: Creating a pod to test consume secrets
Jan 17 05:01:08.954: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608" in namespace "projected-251" to be "Succeeded or Failed"
Jan 17 05:01:08.959: INFO: Pod "pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608": Phase="Pending", Reason="", readiness=false. Elapsed: 5.149985ms
Jan 17 05:01:10.968: INFO: Pod "pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013770782s
Jan 17 05:01:12.976: INFO: Pod "pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021536105s
STEP: Saw pod success
Jan 17 05:01:12.976: INFO: Pod "pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608" satisfied condition "Succeeded or Failed"
Jan 17 05:01:12.980: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:01:13.024: INFO: Waiting for pod pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608 to disappear
Jan 17 05:01:13.029: INFO: Pod pod-projected-secrets-c997fa57-cc24-40be-b0ac-7c43b8bf4608 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 05:01:13.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-251" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3821,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:13.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-6b34e770-977c-472b-985e-4cc2bdd3e372
STEP: Creating a pod to test consume secrets
Jan 17 05:01:13.113: INFO: Waiting up to 5m0s for pod "pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5" in namespace "secrets-2734" to be "Succeeded or Failed"
Jan 17 05:01:13.120: INFO: Pod "pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.114661ms
Jan 17 05:01:15.129: INFO: Pod "pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016154554s
Jan 17 05:01:17.138: INFO: Pod "pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024863178s
STEP: Saw pod success
Jan 17 05:01:17.138: INFO: Pod "pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5" satisfied condition "Succeeded or Failed"
Jan 17 05:01:17.142: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5 container secret-env-test: <nil>
STEP: delete the pod
Jan 17 05:01:17.176: INFO: Waiting for pod pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5 to disappear
Jan 17 05:01:17.182: INFO: Pod pod-secrets-f3551f86-ac3b-4cb2-bb14-2e9482999ad5 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:01:17.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2734" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":206,"skipped":3836,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:17.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-c3ca0feb-7cf2-4777-9913-f8a8bfb54ac4
STEP: Creating a pod to test consume secrets
Jan 17 05:01:17.269: INFO: Waiting up to 5m0s for pod "pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff" in namespace "secrets-1427" to be "Succeeded or Failed"
Jan 17 05:01:17.274: INFO: Pod "pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642364ms
Jan 17 05:01:19.282: INFO: Pod "pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012458783s
Jan 17 05:01:21.294: INFO: Pod "pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024207281s
STEP: Saw pod success
Jan 17 05:01:21.294: INFO: Pod "pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff" satisfied condition "Succeeded or Failed"
Jan 17 05:01:21.297: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:01:21.334: INFO: Waiting for pod pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff to disappear
Jan 17 05:01:21.339: INFO: Pod pod-secrets-31086502-fe85-4bb4-9ac9-27dc5b657bff no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:01:21.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1427" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3837,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:21.358: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 17 05:01:21.426: INFO: Waiting up to 5m0s for pod "pod-961148e4-149c-4d42-8b2a-da15a0011c49" in namespace "emptydir-1573" to be "Succeeded or Failed"
Jan 17 05:01:21.432: INFO: Pod "pod-961148e4-149c-4d42-8b2a-da15a0011c49": Phase="Pending", Reason="", readiness=false. Elapsed: 5.214338ms
Jan 17 05:01:23.441: INFO: Pod "pod-961148e4-149c-4d42-8b2a-da15a0011c49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01484157s
Jan 17 05:01:25.454: INFO: Pod "pod-961148e4-149c-4d42-8b2a-da15a0011c49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028110716s
STEP: Saw pod success
Jan 17 05:01:25.455: INFO: Pod "pod-961148e4-149c-4d42-8b2a-da15a0011c49" satisfied condition "Succeeded or Failed"
Jan 17 05:01:25.458: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-961148e4-149c-4d42-8b2a-da15a0011c49 container test-container: <nil>
STEP: delete the pod
Jan 17 05:01:25.500: INFO: Waiting for pod pod-961148e4-149c-4d42-8b2a-da15a0011c49 to disappear
Jan 17 05:01:25.505: INFO: Pod pod-961148e4-149c-4d42-8b2a-da15a0011c49 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:01:25.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1573" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":208,"skipped":3848,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:25.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:01:25.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa" in namespace "downward-api-7154" to be "Succeeded or Failed"
Jan 17 05:01:25.586: INFO: Pod "downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89077ms
Jan 17 05:01:27.599: INFO: Pod "downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017422793s
Jan 17 05:01:29.604: INFO: Pod "downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022947108s
STEP: Saw pod success
Jan 17 05:01:29.605: INFO: Pod "downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa" satisfied condition "Succeeded or Failed"
Jan 17 05:01:29.608: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa container client-container: <nil>
STEP: delete the pod
Jan 17 05:01:29.643: INFO: Waiting for pod downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa to disappear
Jan 17 05:01:29.649: INFO: Pod downwardapi-volume-fd19af16-d52a-40d5-aa67-dc157344e3fa no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 05:01:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7154" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":209,"skipped":3853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:29.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 17 05:01:29.796: INFO: The status of Pod annotationupdate4b1d9c04-8d70-401a-b955-70f01cc68ae2 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:01:31.804: INFO: The status of Pod annotationupdate4b1d9c04-8d70-401a-b955-70f01cc68ae2 is Running (Ready = true)
Jan 17 05:01:32.350: INFO: Successfully updated pod "annotationupdate4b1d9c04-8d70-401a-b955-70f01cc68ae2"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:01:36.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7201" for this suite.

• [SLOW TEST:6.751 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":210,"skipped":3901,"failed":0}
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:01:36.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-ff8eba62-001d-4b3d-8e1c-09e3b49c35d1 in namespace container-probe-8260
Jan 17 05:01:38.562: INFO: Started pod test-webserver-ff8eba62-001d-4b3d-8e1c-09e3b49c35d1 in namespace container-probe-8260
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 05:01:38.565: INFO: Initial restart count of pod test-webserver-ff8eba62-001d-4b3d-8e1c-09e3b49c35d1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 05:05:39.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8260" for this suite.

• [SLOW TEST:243.375 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":3901,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:05:39.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:05:39.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 17 05:05:39.889: INFO: The status of Pod pod-logs-websocket-7e9d10d8-d0d9-4e6c-8b7f-d82e11f8ab73 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:05:41.897: INFO: The status of Pod pod-logs-websocket-7e9d10d8-d0d9-4e6c-8b7f-d82e11f8ab73 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 05:05:41.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4571" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":212,"skipped":3912,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:05:42.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 17 05:05:46.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3119" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":213,"skipped":3914,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:05:46.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2716
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 17 05:05:46.213: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 05:05:46.330: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:05:48.341: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:05:50.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:05:52.338: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:05:54.340: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:05:56.335: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:05:58.343: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:06:00.342: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:06:02.341: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:06:04.339: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:06:06.340: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:06:08.340: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 17 05:06:08.349: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 17 05:06:08.359: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 17 05:06:10.456: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 05:06:10.456: INFO: Going to poll 10.100.106.90 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:06:10.460: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.106.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2716 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:06:10.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:06:10.461: INFO: ExecWithOptions: Clientset creation
Jan 17 05:06:10.461: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2716/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.106.90+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:06:11.612: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 05:06:11.612: INFO: Going to poll 10.100.44.197 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:06:11.621: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.44.197 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2716 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:06:11.621: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:06:11.623: INFO: ExecWithOptions: Clientset creation
Jan 17 05:06:11.623: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2716/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.44.197+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:06:12.778: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 05:06:12.778: INFO: Going to poll 10.100.11.210 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:06:12.786: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.11.210 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2716 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:06:12.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:06:12.786: INFO: ExecWithOptions: Clientset creation
Jan 17 05:06:12.787: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2716/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.11.210+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:06:13.941: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 17 05:06:13.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2716" for this suite.

• [SLOW TEST:27.792 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":3931,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:13.966: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jan 17 05:06:14.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2206" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":215,"skipped":3938,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:14.079: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:06:14.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197" in namespace "projected-4437" to be "Succeeded or Failed"
Jan 17 05:06:14.177: INFO: Pod "downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197": Phase="Pending", Reason="", readiness=false. Elapsed: 5.813633ms
Jan 17 05:06:16.187: INFO: Pod "downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016443763s
Jan 17 05:06:18.197: INFO: Pod "downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026367889s
STEP: Saw pod success
Jan 17 05:06:18.198: INFO: Pod "downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197" satisfied condition "Succeeded or Failed"
Jan 17 05:06:18.201: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-1 pod downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197 container client-container: <nil>
STEP: delete the pod
Jan 17 05:06:18.310: INFO: Waiting for pod downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197 to disappear
Jan 17 05:06:18.316: INFO: Pod downwardapi-volume-ef457df3-3809-4c51-9479-7087e184a197 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:06:18.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4437" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":216,"skipped":3945,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:18.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:06:18.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b" in namespace "projected-7661" to be "Succeeded or Failed"
Jan 17 05:06:18.424: INFO: Pod "downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.997724ms
Jan 17 05:06:20.436: INFO: Pod "downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01836044s
Jan 17 05:06:22.447: INFO: Pod "downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029342978s
STEP: Saw pod success
Jan 17 05:06:22.447: INFO: Pod "downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b" satisfied condition "Succeeded or Failed"
Jan 17 05:06:22.452: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b container client-container: <nil>
STEP: delete the pod
Jan 17 05:06:22.495: INFO: Waiting for pod downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b to disappear
Jan 17 05:06:22.505: INFO: Pod downwardapi-volume-41902ba4-df37-46a8-aaa4-ee1d71e4649b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:06:22.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7661" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":217,"skipped":3958,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:22.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:06:36.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1800" for this suite.
STEP: Destroying namespace "nsdeletetest-4779" for this suite.
Jan 17 05:06:36.754: INFO: Namespace nsdeletetest-4779 was already deleted
STEP: Destroying namespace "nsdeletetest-4341" for this suite.

• [SLOW TEST:14.241 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":218,"skipped":3966,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:06:36.848: INFO: created pod pod-service-account-defaultsa
Jan 17 05:06:36.848: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 17 05:06:36.861: INFO: created pod pod-service-account-mountsa
Jan 17 05:06:36.861: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 17 05:06:36.872: INFO: created pod pod-service-account-nomountsa
Jan 17 05:06:36.872: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 17 05:06:36.880: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 17 05:06:36.880: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 17 05:06:36.895: INFO: created pod pod-service-account-mountsa-mountspec
Jan 17 05:06:36.895: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 17 05:06:36.922: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 17 05:06:36.922: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 17 05:06:36.936: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 17 05:06:36.936: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 17 05:06:36.952: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 17 05:06:36.952: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 17 05:06:36.977: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 17 05:06:36.977: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 05:06:36.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-345" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":219,"skipped":3972,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:06:37.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:06:37.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Creating first CR 
Jan 17 05:06:39.718: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:39Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:06:39Z]] name:name1 resourceVersion:40472 uid:69a13dc4-aae2-4aba-b860-56ddd51f43d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 17 05:06:49.741: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:49Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:06:49Z]] name:name2 resourceVersion:40558 uid:a4826622-6d79-4f93-8a40-3589950f62ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 17 05:06:59.770: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:06:59Z]] name:name1 resourceVersion:40584 uid:69a13dc4-aae2-4aba-b860-56ddd51f43d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 17 05:07:09.794: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:07:09Z]] name:name2 resourceVersion:40610 uid:a4826622-6d79-4f93-8a40-3589950f62ad] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 17 05:07:19.829: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:39Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:06:59Z]] name:name1 resourceVersion:40637 uid:69a13dc4-aae2-4aba-b860-56ddd51f43d8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 17 05:07:29.862: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-17T05:06:49Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-17T05:07:09Z]] name:name2 resourceVersion:40662 uid:a4826622-6d79-4f93-8a40-3589950f62ad] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:07:40.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2329" for this suite.

• [SLOW TEST:63.407 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":220,"skipped":3975,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:40.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:07:40.969: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:07:44.006: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:07:44.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3425" for this suite.
STEP: Destroying namespace "webhook-3425-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":221,"skipped":3981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:44.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:07:44.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4244" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":222,"skipped":4019,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:44.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:07:44.450: INFO: Creating pod...
Jan 17 05:07:46.478: INFO: Creating service...
Jan 17 05:07:46.504: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=DELETE
Jan 17 05:07:46.514: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 05:07:46.514: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=OPTIONS
Jan 17 05:07:46.528: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 05:07:46.528: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=PATCH
Jan 17 05:07:46.544: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 05:07:46.544: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=POST
Jan 17 05:07:46.558: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 05:07:46.558: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=PUT
Jan 17 05:07:46.564: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 05:07:46.564: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 17 05:07:46.572: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 17 05:07:46.572: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 17 05:07:46.580: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 17 05:07:46.580: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 17 05:07:46.588: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 17 05:07:46.588: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=POST
Jan 17 05:07:46.596: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 17 05:07:46.596: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=PUT
Jan 17 05:07:46.607: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 17 05:07:46.607: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=GET
Jan 17 05:07:46.612: INFO: http.Client request:GET StatusCode:301
Jan 17 05:07:46.612: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=GET
Jan 17 05:07:46.618: INFO: http.Client request:GET StatusCode:301
Jan 17 05:07:46.618: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/pods/agnhost/proxy?method=HEAD
Jan 17 05:07:46.621: INFO: http.Client request:HEAD StatusCode:301
Jan 17 05:07:46.621: INFO: Starting http.Client for https://10.254.0.1:443/api/v1/namespaces/proxy-4427/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 17 05:07:46.626: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 17 05:07:46.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4427" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":223,"skipped":4088,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:07:46.753: INFO: The status of Pod pod-secrets-02f69681-8f78-4b86-9abb-9f6f97a5068d is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:07:48.764: INFO: The status of Pod pod-secrets-02f69681-8f78-4b86-9abb-9f6f97a5068d is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 17 05:07:48.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7078" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":224,"skipped":4105,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:48.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:07:48.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:07:55.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7705" for this suite.

• [SLOW TEST:6.762 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":225,"skipped":4111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:55.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-1920/configmap-test-8f7cf115-96b4-49e6-a590-64d235f2715c
STEP: Creating a pod to test consume configMaps
Jan 17 05:07:55.668: INFO: Waiting up to 5m0s for pod "pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898" in namespace "configmap-1920" to be "Succeeded or Failed"
Jan 17 05:07:55.681: INFO: Pod "pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898": Phase="Pending", Reason="", readiness=false. Elapsed: 12.784374ms
Jan 17 05:07:57.688: INFO: Pod "pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019783228s
Jan 17 05:07:59.705: INFO: Pod "pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037033026s
STEP: Saw pod success
Jan 17 05:07:59.706: INFO: Pod "pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898" satisfied condition "Succeeded or Failed"
Jan 17 05:07:59.709: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898 container env-test: <nil>
STEP: delete the pod
Jan 17 05:07:59.795: INFO: Waiting for pod pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898 to disappear
Jan 17 05:07:59.800: INFO: Pod pod-configmaps-08fef6ad-645b-4a89-862e-a07394cb7898 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:07:59.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1920" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":226,"skipped":4138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:07:59.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 17 05:07:59.887: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 05:08:59.950: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:08:59.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:09:00.038: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 17 05:09:00.045: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jan 17 05:09:00.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-5717" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:09:00.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1872" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:60.441 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":227,"skipped":4160,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:09:00.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3054
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3054
STEP: Waiting until pod test-pod will start running in namespace statefulset-3054
STEP: Creating statefulset with conflicting port in namespace statefulset-3054
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3054
Jan 17 05:09:02.434: INFO: Observed stateful pod in namespace: statefulset-3054, name: ss-0, uid: 5b92463c-c835-42ca-8895-72f2c844221f, status phase: Pending. Waiting for statefulset controller to delete.
Jan 17 05:09:02.459: INFO: Observed stateful pod in namespace: statefulset-3054, name: ss-0, uid: 5b92463c-c835-42ca-8895-72f2c844221f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 05:09:02.491: INFO: Observed stateful pod in namespace: statefulset-3054, name: ss-0, uid: 5b92463c-c835-42ca-8895-72f2c844221f, status phase: Failed. Waiting for statefulset controller to delete.
Jan 17 05:09:02.506: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3054
STEP: Removing pod with conflicting port in namespace statefulset-3054
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3054 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 05:09:04.571: INFO: Deleting all statefulset in ns statefulset-3054
Jan 17 05:09:04.577: INFO: Scaling statefulset ss to 0
Jan 17 05:09:14.609: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 05:09:14.613: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 05:09:14.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3054" for this suite.

• [SLOW TEST:14.407 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":228,"skipped":4180,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:09:14.673: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-770
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:09:14.808: INFO: Found 0 stateful pods, waiting for 1
Jan 17 05:09:24.818: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 17 05:09:24.853: INFO: Found 1 stateful pods, waiting for 2
Jan 17 05:09:34.862: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 17 05:09:34.862: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 05:09:34.896: INFO: Deleting all statefulset in ns statefulset-770
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 05:09:34.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-770" for this suite.

• [SLOW TEST:20.259 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":229,"skipped":4201,"failed":0}
SSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:09:34.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 17 05:09:55.335: INFO: EndpointSlice for Service endpointslice-4045/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 17 05:10:05.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4045" for this suite.

• [SLOW TEST:30.439 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":230,"skipped":4207,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:10:05.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 05:11:05.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2791" for this suite.

• [SLOW TEST:60.106 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":231,"skipped":4216,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:05.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-90e98947-3f80-44c4-a257-066a6f4aa5d7
STEP: Creating a pod to test consume configMaps
Jan 17 05:11:05.570: INFO: Waiting up to 5m0s for pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2" in namespace "configmap-4431" to be "Succeeded or Failed"
Jan 17 05:11:05.577: INFO: Pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.137052ms
Jan 17 05:11:07.587: INFO: Pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017550516s
Jan 17 05:11:09.598: INFO: Pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028584228s
Jan 17 05:11:11.609: INFO: Pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039415069s
STEP: Saw pod success
Jan 17 05:11:11.609: INFO: Pod "pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2" satisfied condition "Succeeded or Failed"
Jan 17 05:11:11.614: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:11:11.690: INFO: Waiting for pod pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2 to disappear
Jan 17 05:11:11.695: INFO: Pod pod-configmaps-37c1a471-0f95-44ed-b363-67dfa2e270e2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:11:11.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4431" for this suite.

• [SLOW TEST:6.234 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":232,"skipped":4228,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:11.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 17 05:11:11.774: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 17 05:11:11.786: INFO: starting watch
STEP: patching
STEP: updating
Jan 17 05:11:11.816: INFO: waiting for watch events with expected annotations
Jan 17 05:11:11.816: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 17 05:11:11.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9724" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":233,"skipped":4233,"failed":0}

------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:11.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 17 05:11:11.970: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 17 05:11:12.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1337" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":234,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:12.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-5cff0cb2-c69f-44ce-89e9-b0aae11d5c95
STEP: Creating a pod to test consume configMaps
Jan 17 05:11:12.086: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9" in namespace "projected-5170" to be "Succeeded or Failed"
Jan 17 05:11:12.092: INFO: Pod "pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.348571ms
Jan 17 05:11:14.099: INFO: Pod "pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013049771s
Jan 17 05:11:16.107: INFO: Pod "pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020942699s
STEP: Saw pod success
Jan 17 05:11:16.107: INFO: Pod "pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9" satisfied condition "Succeeded or Failed"
Jan 17 05:11:16.110: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:11:16.142: INFO: Waiting for pod pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9 to disappear
Jan 17 05:11:16.147: INFO: Pod pod-projected-configmaps-121dc8e4-7de3-48d2-bd77-f1ae8bfb80b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 05:11:16.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5170" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":235,"skipped":4291,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:16.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jan 17 05:11:16.218: INFO: Waiting up to 5m0s for pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d" in namespace "svcaccounts-7198" to be "Succeeded or Failed"
Jan 17 05:11:16.224: INFO: Pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.679079ms
Jan 17 05:11:18.243: INFO: Pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.024959313s
Jan 17 05:11:20.254: INFO: Pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d": Phase="Running", Reason="", readiness=false. Elapsed: 4.035916758s
Jan 17 05:11:22.313: INFO: Pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.094923983s
STEP: Saw pod success
Jan 17 05:11:22.313: INFO: Pod "test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d" satisfied condition "Succeeded or Failed"
Jan 17 05:11:22.317: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:11:22.420: INFO: Waiting for pod test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d to disappear
Jan 17 05:11:22.425: INFO: Pod test-pod-9fd822ca-71f8-41b9-80c0-9c028682af9d no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 05:11:22.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7198" for this suite.

• [SLOW TEST:6.276 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":236,"skipped":4322,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:22.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jan 17 05:11:22.493: INFO: Creating simple deployment test-deployment-jwttq
Jan 17 05:11:22.528: INFO: deployment "test-deployment-jwttq" doesn't have the required revision set
STEP: Getting /status
Jan 17 05:11:24.555: INFO: Deployment test-deployment-jwttq has Conditions: [{Available True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jwttq-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jan 17 05:11:24.572: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 11, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 11, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 11, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 11, 22, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jwttq-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 17 05:11:24.576: INFO: Observed &Deployment event: ADDED
Jan 17 05:11:24.576: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jwttq-688c4d6789"}
Jan 17 05:11:24.577: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jwttq-688c4d6789"}
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 05:11:24.577: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jwttq-688c4d6789" is progressing.}
Jan 17 05:11:24.577: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jwttq-688c4d6789" has successfully progressed.}
Jan 17 05:11:24.577: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 05:11:24.577: INFO: Observed Deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jwttq-688c4d6789" has successfully progressed.}
Jan 17 05:11:24.577: INFO: Found Deployment test-deployment-jwttq in namespace deployment-6586 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 05:11:24.577: INFO: Deployment test-deployment-jwttq has an updated status
STEP: patching the Statefulset Status
Jan 17 05:11:24.577: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 17 05:11:24.591: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 17 05:11:24.593: INFO: Observed &Deployment event: ADDED
Jan 17 05:11:24.593: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jwttq-688c4d6789"}
Jan 17 05:11:24.593: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.593: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jwttq-688c4d6789"}
Jan 17 05:11:24.593: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 05:11:24.594: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:22 +0000 UTC 2023-01-17 05:11:22 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jwttq-688c4d6789" is progressing.}
Jan 17 05:11:24.594: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jwttq-688c4d6789" has successfully progressed.}
Jan 17 05:11:24.594: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-17 05:11:24 +0000 UTC 2023-01-17 05:11:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jwttq-688c4d6789" has successfully progressed.}
Jan 17 05:11:24.594: INFO: Observed deployment test-deployment-jwttq in namespace deployment-6586 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 17 05:11:24.594: INFO: Observed &Deployment event: MODIFIED
Jan 17 05:11:24.594: INFO: Found deployment test-deployment-jwttq in namespace deployment-6586 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 17 05:11:24.594: INFO: Deployment test-deployment-jwttq has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 05:11:24.604: INFO: Deployment "test-deployment-jwttq":
&Deployment{ObjectMeta:{test-deployment-jwttq  deployment-6586  7f1bba2d-4c3e-45d5-84ea-28ec041bb34b 42035 1 2023-01-17 05:11:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-17 05:11:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-17 05:11:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-17 05:11:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bfe078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jwttq-688c4d6789",LastUpdateTime:2023-01-17 05:11:24 +0000 UTC,LastTransitionTime:2023-01-17 05:11:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 05:11:24.608: INFO: New ReplicaSet "test-deployment-jwttq-688c4d6789" of Deployment "test-deployment-jwttq":
&ReplicaSet{ObjectMeta:{test-deployment-jwttq-688c4d6789  deployment-6586  b8cf16f4-3e46-4038-ab76-f2986ee86833 42030 1 2023-01-17 05:11:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jwttq 7f1bba2d-4c3e-45d5-84ea-28ec041bb34b 0xc004bfe450 0xc004bfe451}] []  [{kube-controller-manager Update apps/v1 2023-01-17 05:11:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f1bba2d-4c3e-45d5-84ea-28ec041bb34b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:11:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bfe4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:11:24.613: INFO: Pod "test-deployment-jwttq-688c4d6789-rpk9z" is available:
&Pod{ObjectMeta:{test-deployment-jwttq-688c4d6789-rpk9z test-deployment-jwttq-688c4d6789- deployment-6586  96527026-54cd-463d-b178-ed5a31b260f7 42029 0 2023-01-17 05:11:22 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:dab094bf9efcc7b40badd8e198fc2e850d8ee0f0adbf8096a3cd777a212c1c72 cni.projectcalico.org/podIP:10.100.11.204/32 cni.projectcalico.org/podIPs:10.100.11.204/32] [{apps/v1 ReplicaSet test-deployment-jwttq-688c4d6789 b8cf16f4-3e46-4038-ab76-f2986ee86833 0xc003005410 0xc003005411}] []  [{kube-controller-manager Update v1 2023-01-17 05:11:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8cf16f4-3e46-4038-ab76-f2986ee86833\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:11:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:11:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4lqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4lqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:11:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:11:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:11:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.204,StartTime:2023-01-17 05:11:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:11:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2256cabeac4aa50ee2f517c719bdd02d3e6e949289d8ac5cd3122879c0fd85b9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 05:11:24.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6586" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":237,"skipped":4329,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:24.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8383.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8383.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8383.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8383.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:11:26.736: INFO: DNS probes using dns-8383/dns-test-967af8e5-217b-4a99-b077-79f3dab35b05 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 05:11:26.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8383" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":238,"skipped":4336,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:26.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 17 05:11:26.818: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 05:11:31.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8675" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":239,"skipped":4350,"failed":0}
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:11:31.320: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 17 05:12:00.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6997" for this suite.

• [SLOW TEST:29.458 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":240,"skipped":4357,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:00.779: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:12:01.992: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:12:05.030: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:12:05.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:12:08.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6479" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.590 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":241,"skipped":4370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-df19a8c6-32d1-452c-8502-51f0d804d2b9
STEP: Creating a pod to test consume configMaps
Jan 17 05:12:08.505: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1" in namespace "projected-7066" to be "Succeeded or Failed"
Jan 17 05:12:08.511: INFO: Pod "pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060603ms
Jan 17 05:12:10.524: INFO: Pod "pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018881824s
Jan 17 05:12:12.536: INFO: Pod "pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030281044s
STEP: Saw pod success
Jan 17 05:12:12.536: INFO: Pod "pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1" satisfied condition "Succeeded or Failed"
Jan 17 05:12:12.539: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-0 pod pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 17 05:12:12.564: INFO: Waiting for pod pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1 to disappear
Jan 17 05:12:12.569: INFO: Pod pod-projected-configmaps-9c9790d7-084f-440b-8453-6cf3d276fef1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 05:12:12.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7066" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4407,"failed":0}
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:12.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 17 05:12:15.179: INFO: Successfully updated pod "adopt-release-792b9"
STEP: Checking that the Job readopts the Pod
Jan 17 05:12:15.179: INFO: Waiting up to 15m0s for pod "adopt-release-792b9" in namespace "job-615" to be "adopted"
Jan 17 05:12:15.184: INFO: Pod "adopt-release-792b9": Phase="Running", Reason="", readiness=true. Elapsed: 5.145538ms
Jan 17 05:12:17.195: INFO: Pod "adopt-release-792b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016388851s
Jan 17 05:12:17.195: INFO: Pod "adopt-release-792b9" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 17 05:12:17.717: INFO: Successfully updated pod "adopt-release-792b9"
STEP: Checking that the Job releases the Pod
Jan 17 05:12:17.717: INFO: Waiting up to 15m0s for pod "adopt-release-792b9" in namespace "job-615" to be "released"
Jan 17 05:12:17.722: INFO: Pod "adopt-release-792b9": Phase="Running", Reason="", readiness=true. Elapsed: 5.258932ms
Jan 17 05:12:19.732: INFO: Pod "adopt-release-792b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.015026596s
Jan 17 05:12:19.732: INFO: Pod "adopt-release-792b9" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 17 05:12:19.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-615" for this suite.

• [SLOW TEST:7.157 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":243,"skipped":4410,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:19.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jan 17 05:12:19.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-1987 api-versions'
Jan 17 05:12:19.894: INFO: stderr: ""
Jan 17 05:12:19.894: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:12:19.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1987" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":244,"skipped":4430,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:19.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 17 05:12:20.019: INFO: Waiting up to 5m0s for pod "pod-75e58bdd-be00-46cf-9723-c75575520ce2" in namespace "emptydir-1733" to be "Succeeded or Failed"
Jan 17 05:12:20.025: INFO: Pod "pod-75e58bdd-be00-46cf-9723-c75575520ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.509693ms
Jan 17 05:12:22.035: INFO: Pod "pod-75e58bdd-be00-46cf-9723-c75575520ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016002847s
Jan 17 05:12:24.045: INFO: Pod "pod-75e58bdd-be00-46cf-9723-c75575520ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025451954s
STEP: Saw pod success
Jan 17 05:12:24.045: INFO: Pod "pod-75e58bdd-be00-46cf-9723-c75575520ce2" satisfied condition "Succeeded or Failed"
Jan 17 05:12:24.048: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-75e58bdd-be00-46cf-9723-c75575520ce2 container test-container: <nil>
STEP: delete the pod
Jan 17 05:12:24.087: INFO: Waiting for pod pod-75e58bdd-be00-46cf-9723-c75575520ce2 to disappear
Jan 17 05:12:24.093: INFO: Pod pod-75e58bdd-be00-46cf-9723-c75575520ce2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:12:24.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1733" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":245,"skipped":4439,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:24.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:12:24.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2093" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":246,"skipped":4445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:24.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 17 05:12:24.321: INFO: Waiting up to 5m0s for pod "downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a" in namespace "downward-api-2751" to be "Succeeded or Failed"
Jan 17 05:12:24.326: INFO: Pod "downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441543ms
Jan 17 05:12:26.335: INFO: Pod "downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01401036s
Jan 17 05:12:28.345: INFO: Pod "downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024456541s
STEP: Saw pod success
Jan 17 05:12:28.345: INFO: Pod "downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a" satisfied condition "Succeeded or Failed"
Jan 17 05:12:28.350: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:12:28.389: INFO: Waiting for pod downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a to disappear
Jan 17 05:12:28.394: INFO: Pod downward-api-38beaa5c-f605-4ff6-8c29-09b5f6466c4a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 17 05:12:28.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2751" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4501,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:12:28.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 17 05:12:28.459: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 05:13:28.508: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:13:28.514: INFO: Starting informer...
STEP: Starting pods...
Jan 17 05:13:28.748: INFO: Pod1 is running on cluster124-apihrjet4zqi-node-2. Tainting Node
Jan 17 05:13:30.984: INFO: Pod2 is running on cluster124-apihrjet4zqi-node-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 17 05:13:36.867: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 17 05:13:56.909: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:13:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9320" for this suite.

• [SLOW TEST:88.567 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":248,"skipped":4532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:13:56.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2905
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 17 05:13:57.096: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 05:13:57.177: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:13:59.188: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:01.184: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:03.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:05.188: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:07.186: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:09.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:11.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:13.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:15.189: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:17.187: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:14:19.188: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 17 05:14:19.197: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 17 05:14:19.205: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 17 05:14:21.241: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 05:14:21.241: INFO: Breadth first check of 10.100.106.124 on host 10.0.0.16...
Jan 17 05:14:21.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.245:9080/dial?request=hostname&protocol=udp&host=10.100.106.124&port=8081&tries=1'] Namespace:pod-network-test-2905 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:14:21.245: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:14:21.246: INFO: ExecWithOptions: Clientset creation
Jan 17 05:14:21.246: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.106.124%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 05:14:21.398: INFO: Waiting for responses: map[]
Jan 17 05:14:21.398: INFO: reached 10.100.106.124 after 0/1 tries
Jan 17 05:14:21.398: INFO: Breadth first check of 10.100.44.243 on host 10.0.0.21...
Jan 17 05:14:21.404: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.245:9080/dial?request=hostname&protocol=udp&host=10.100.44.243&port=8081&tries=1'] Namespace:pod-network-test-2905 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:14:21.404: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:14:21.404: INFO: ExecWithOptions: Clientset creation
Jan 17 05:14:21.404: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.44.243%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 05:14:21.580: INFO: Waiting for responses: map[]
Jan 17 05:14:21.580: INFO: reached 10.100.44.243 after 0/1 tries
Jan 17 05:14:21.580: INFO: Breadth first check of 10.100.11.230 on host 10.0.0.9...
Jan 17 05:14:21.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.11.245:9080/dial?request=hostname&protocol=udp&host=10.100.11.230&port=8081&tries=1'] Namespace:pod-network-test-2905 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:14:21.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:14:21.588: INFO: ExecWithOptions: Clientset creation
Jan 17 05:14:21.588: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2905/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.11.245%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.11.230%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 17 05:14:21.724: INFO: Waiting for responses: map[]
Jan 17 05:14:21.724: INFO: reached 10.100.11.230 after 0/1 tries
Jan 17 05:14:21.724: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 17 05:14:21.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2905" for this suite.

• [SLOW TEST:24.766 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":249,"skipped":4558,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:21.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-86f81926-6bb7-4df4-8f10-75d92820b45a
STEP: Creating the pod
Jan 17 05:14:21.824: INFO: The status of Pod pod-configmaps-cefb498c-9108-4f98-a121-d751a6b4066d is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:14:23.833: INFO: The status of Pod pod-configmaps-cefb498c-9108-4f98-a121-d751a6b4066d is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-86f81926-6bb7-4df4-8f10-75d92820b45a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:14:25.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8011" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":250,"skipped":4561,"failed":0}
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:25.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-ttc9
STEP: Creating a pod to test atomic-volume-subpath
Jan 17 05:14:26.088: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-ttc9" in namespace "subpath-4705" to be "Succeeded or Failed"
Jan 17 05:14:26.094: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.829986ms
Jan 17 05:14:28.109: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.020960543s
Jan 17 05:14:30.119: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 4.031050442s
Jan 17 05:14:32.128: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.039394848s
Jan 17 05:14:34.135: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 8.046990743s
Jan 17 05:14:36.149: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 10.061293805s
Jan 17 05:14:38.161: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.072695102s
Jan 17 05:14:40.170: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 14.082020842s
Jan 17 05:14:42.181: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 16.093045358s
Jan 17 05:14:44.189: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 18.100883783s
Jan 17 05:14:46.198: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=true. Elapsed: 20.109615128s
Jan 17 05:14:48.207: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Running", Reason="", readiness=false. Elapsed: 22.118838214s
Jan 17 05:14:50.218: INFO: Pod "pod-subpath-test-downwardapi-ttc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.130214856s
STEP: Saw pod success
Jan 17 05:14:50.219: INFO: Pod "pod-subpath-test-downwardapi-ttc9" satisfied condition "Succeeded or Failed"
Jan 17 05:14:50.222: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-subpath-test-downwardapi-ttc9 container test-container-subpath-downwardapi-ttc9: <nil>
STEP: delete the pod
Jan 17 05:14:50.305: INFO: Waiting for pod pod-subpath-test-downwardapi-ttc9 to disappear
Jan 17 05:14:50.310: INFO: Pod pod-subpath-test-downwardapi-ttc9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-ttc9
Jan 17 05:14:50.310: INFO: Deleting pod "pod-subpath-test-downwardapi-ttc9" in namespace "subpath-4705"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 17 05:14:50.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4705" for this suite.

• [SLOW TEST:24.352 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":251,"skipped":4566,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:50.337: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:14:50.405: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 17 05:14:55.417: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 17 05:14:55.417: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 05:14:55.458: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2974  76295d8c-d428-437a-b1ce-aaf153801fef 43398 1 2023-01-17 05:14:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2023-01-17 05:14:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00299de18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 17 05:14:55.462: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 17 05:14:55.462: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 17 05:14:55.462: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2974  a0c4f2dc-13bb-486c-b9d7-fd4da1e6de44 43400 1 2023-01-17 05:14:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 76295d8c-d428-437a-b1ce-aaf153801fef 0xc001028157 0xc001028158}] []  [{e2e.test Update apps/v1 2023-01-17 05:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:14:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-17 05:14:55 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"76295d8c-d428-437a-b1ce-aaf153801fef\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001028228 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:14:55.466: INFO: Pod "test-cleanup-controller-msxfj" is available:
&Pod{ObjectMeta:{test-cleanup-controller-msxfj test-cleanup-controller- deployment-2974  1c503a07-63cc-4f71-ae53-7a7fbfb0b85f 43386 0 2023-01-17 05:14:50 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:10f4a90d5eac3bdf8f6df071dd8150107f5ad672d13076e975a7756eef7097df cni.projectcalico.org/podIP:10.100.11.216/32 cni.projectcalico.org/podIPs:10.100.11.216/32] [{apps/v1 ReplicaSet test-cleanup-controller a0c4f2dc-13bb-486c-b9d7-fd4da1e6de44 0xc001028577 0xc001028578}] []  [{kube-controller-manager Update v1 2023-01-17 05:14:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0c4f2dc-13bb-486c-b9d7-fd4da1e6de44\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:14:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:14:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2fhtz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2fhtz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:14:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:14:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:14:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:14:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.216,StartTime:2023-01-17 05:14:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:14:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://6ad267a1bb9ec2089228b0360b95410748a9b03f38b69f774ab58d152b15455c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 05:14:55.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2974" for this suite.

• [SLOW TEST:5.149 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":252,"skipped":4594,"failed":0}
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:55.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 17 05:14:55.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9054" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":253,"skipped":4594,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:55.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jan 17 05:14:55.654: INFO: Major version: 1
STEP: Confirm minor version
Jan 17 05:14:55.654: INFO: cleanMinorVersion: 24
Jan 17 05:14:55.654: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jan 17 05:14:55.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-699" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":254,"skipped":4609,"failed":0}

------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:14:55.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:14:55.771: INFO: created pod
Jan 17 05:14:55.771: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8007" to be "Succeeded or Failed"
Jan 17 05:14:55.776: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.063047ms
Jan 17 05:14:57.785: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013590286s
Jan 17 05:14:59.795: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023548508s
STEP: Saw pod success
Jan 17 05:14:59.795: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 17 05:15:29.796: INFO: polling logs
Jan 17 05:15:29.809: INFO: Pod logs: 
I0117 05:14:56.777438       1 log.go:195] OK: Got token
I0117 05:14:56.777549       1 log.go:195] validating with in-cluster discovery
I0117 05:14:56.778056       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0117 05:14:56.778136       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8007:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673933095, NotBefore:1673932495, IssuedAt:1673932495, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8007", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e1cf73a0-fa9e-4dbb-a52e-edd2a9fdba6a"}}}
I0117 05:14:56.971091       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0117 05:14:57.002819       1 log.go:195] OK: Validated signature on JWT
I0117 05:14:57.003099       1 log.go:195] OK: Got valid claims from token!
I0117 05:14:57.003272       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8007:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1673933095, NotBefore:1673932495, IssuedAt:1673932495, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8007", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"e1cf73a0-fa9e-4dbb-a52e-edd2a9fdba6a"}}}

Jan 17 05:15:29.809: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 17 05:15:29.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8007" for this suite.

• [SLOW TEST:34.186 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":255,"skipped":4609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:15:29.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jan 17 05:15:29.933: INFO: Waiting up to 5m0s for pod "var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27" in namespace "var-expansion-907" to be "Succeeded or Failed"
Jan 17 05:15:29.939: INFO: Pod "var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27": Phase="Pending", Reason="", readiness=false. Elapsed: 6.227095ms
Jan 17 05:15:31.949: INFO: Pod "var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015648563s
Jan 17 05:15:33.957: INFO: Pod "var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02423033s
STEP: Saw pod success
Jan 17 05:15:33.957: INFO: Pod "var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27" satisfied condition "Succeeded or Failed"
Jan 17 05:15:33.961: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27 container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:15:34.003: INFO: Waiting for pod var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27 to disappear
Jan 17 05:15:34.009: INFO: Pod var-expansion-88f7e2bd-7411-4df5-9770-a1535958fa27 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 05:15:34.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-907" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":256,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:15:34.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:15:34.129: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 17 05:15:34.147: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:34.147: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:34.147: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:34.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:15:34.152: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 05:15:35.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:35.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:35.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:35.167: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:15:35.167: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 05:15:36.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:36.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:36.162: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:36.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 05:15:36.166: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:15:37.160: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:37.161: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:37.161: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:37.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 05:15:37.164: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 17 05:15:37.209: INFO: Wrong image for pod: daemon-set-6qg9x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:37.209: INFO: Wrong image for pod: daemon-set-7nm4b. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:37.209: INFO: Wrong image for pod: daemon-set-gqmmd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:37.215: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:37.215: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:37.215: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:38.225: INFO: Wrong image for pod: daemon-set-6qg9x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:38.225: INFO: Wrong image for pod: daemon-set-gqmmd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:38.230: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:38.230: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:38.230: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:39.222: INFO: Wrong image for pod: daemon-set-6qg9x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:39.222: INFO: Wrong image for pod: daemon-set-gqmmd. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:39.223: INFO: Pod daemon-set-j6xc9 is not available
Jan 17 05:15:39.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:39.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:39.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:40.222: INFO: Wrong image for pod: daemon-set-6qg9x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:40.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:40.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:40.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:41.223: INFO: Wrong image for pod: daemon-set-6qg9x. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 17 05:15:41.223: INFO: Pod daemon-set-gvskv is not available
Jan 17 05:15:41.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:41.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:41.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:42.253: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:42.253: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:42.253: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:43.229: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:43.229: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:43.229: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.221: INFO: Pod daemon-set-4fpsg is not available
Jan 17 05:15:44.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.227: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.228: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 17 05:15:44.236: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.236: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.236: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:44.241: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 05:15:44.241: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:15:45.251: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:45.251: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:45.251: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:45.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 05:15:45.260: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:15:46.285: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:46.285: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:46.285: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:15:46.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 05:15:46.291: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6455, will wait for the garbage collector to delete the pods
Jan 17 05:15:46.378: INFO: Deleting DaemonSet.extensions daemon-set took: 13.803648ms
Jan 17 05:15:46.479: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.815137ms
Jan 17 05:15:48.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:15:48.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 05:15:48.393: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43797"},"items":null}

Jan 17 05:15:48.396: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43797"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:15:48.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6455" for this suite.

• [SLOW TEST:14.391 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":257,"skipped":4759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:15:48.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4284
Jan 17 05:15:48.508: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:15:50.520: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 05:15:50.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 05:15:50.808: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 17 05:15:50.808: INFO: stdout: "iptables"
Jan 17 05:15:50.808: INFO: proxyMode: iptables
Jan 17 05:15:50.838: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 05:15:50.844: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4284
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4284
I0117 05:15:50.881865      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4284, replica count: 3
I0117 05:15:53.932307      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:15:53.949: INFO: Creating new exec pod
Jan 17 05:15:56.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec execpod-affinitypmqqt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 17 05:15:57.265: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout+  80\necho hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 05:15:57.265: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:15:57.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec execpod-affinitypmqqt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.219.225 80'
Jan 17 05:15:57.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.219.225 80\nConnection to 10.254.219.225 80 port [tcp/http] succeeded!\n"
Jan 17 05:15:57.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:15:57.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec execpod-affinitypmqqt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.219.225:80/ ; done'
Jan 17 05:15:57.930: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n"
Jan 17 05:15:57.930: INFO: stdout: "\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf\naffinity-clusterip-timeout-76txf"
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Received response from host: affinity-clusterip-timeout-76txf
Jan 17 05:15:57.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec execpod-affinitypmqqt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.219.225:80/'
Jan 17 05:15:58.154: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n"
Jan 17 05:15:58.154: INFO: stdout: "affinity-clusterip-timeout-76txf"
Jan 17 05:16:18.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4284 exec execpod-affinitypmqqt -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.219.225:80/'
Jan 17 05:16:18.409: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.219.225:80/\n"
Jan 17 05:16:18.409: INFO: stdout: "affinity-clusterip-timeout-xvpx6"
Jan 17 05:16:18.409: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4284, will wait for the garbage collector to delete the pods
Jan 17 05:16:18.511: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.781187ms
Jan 17 05:16:18.611: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.424726ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:16:21.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4284" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:32.841 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":258,"skipped":4783,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:16:21.278: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 17 05:18:01.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8580" for this suite.

• [SLOW TEST:100.137 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":259,"skipped":4796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:01.416: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 17 05:18:05.530: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 17 05:18:05.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-647" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":260,"skipped":4827,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:05.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:18:21.683: INFO: DNS probes using dns-4456/dns-test-d408e14b-d207-44a9-aba4-ac5a666abaf9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 05:18:21.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4456" for this suite.

• [SLOW TEST:16.158 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":261,"skipped":4836,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 17 05:18:21.806: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7507  e07b6113-7493-4418-977b-fa43d321cf99 44492 0 2023-01-17 05:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-17 05:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:18:21.807: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7507  e07b6113-7493-4418-977b-fa43d321cf99 44493 0 2023-01-17 05:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-17 05:18:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 17 05:18:21.833: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7507  e07b6113-7493-4418-977b-fa43d321cf99 44494 0 2023-01-17 05:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-17 05:18:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:18:21.834: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7507  e07b6113-7493-4418-977b-fa43d321cf99 44495 0 2023-01-17 05:18:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-17 05:18:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 17 05:18:21.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7507" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":262,"skipped":4854,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:18:21.908: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c" in namespace "downward-api-8368" to be "Succeeded or Failed"
Jan 17 05:18:21.913: INFO: Pod "downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.209216ms
Jan 17 05:18:23.920: INFO: Pod "downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011519825s
Jan 17 05:18:25.932: INFO: Pod "downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023634703s
STEP: Saw pod success
Jan 17 05:18:25.932: INFO: Pod "downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c" satisfied condition "Succeeded or Failed"
Jan 17 05:18:25.936: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c container client-container: <nil>
STEP: delete the pod
Jan 17 05:18:26.017: INFO: Waiting for pod downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c to disappear
Jan 17 05:18:26.022: INFO: Pod downwardapi-volume-4336311a-a201-4c49-8aab-8b75a38bf15c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 05:18:26.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8368" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":263,"skipped":4867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:26.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 17 05:18:26.103: INFO: Waiting up to 5m0s for pod "pod-56efc5e4-89a8-4321-9822-81018445ab2b" in namespace "emptydir-7605" to be "Succeeded or Failed"
Jan 17 05:18:26.114: INFO: Pod "pod-56efc5e4-89a8-4321-9822-81018445ab2b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.709092ms
Jan 17 05:18:28.121: INFO: Pod "pod-56efc5e4-89a8-4321-9822-81018445ab2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017464386s
Jan 17 05:18:30.132: INFO: Pod "pod-56efc5e4-89a8-4321-9822-81018445ab2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02894437s
STEP: Saw pod success
Jan 17 05:18:30.132: INFO: Pod "pod-56efc5e4-89a8-4321-9822-81018445ab2b" satisfied condition "Succeeded or Failed"
Jan 17 05:18:30.136: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-56efc5e4-89a8-4321-9822-81018445ab2b container test-container: <nil>
STEP: delete the pod
Jan 17 05:18:30.162: INFO: Waiting for pod pod-56efc5e4-89a8-4321-9822-81018445ab2b to disappear
Jan 17 05:18:30.168: INFO: Pod pod-56efc5e4-89a8-4321-9822-81018445ab2b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:18:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7605" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":4889,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:30.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:18:36.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2023" for this suite.
STEP: Destroying namespace "nsdeletetest-2989" for this suite.
Jan 17 05:18:36.442: INFO: Namespace nsdeletetest-2989 was already deleted
STEP: Destroying namespace "nsdeletetest-4888" for this suite.

• [SLOW TEST:6.264 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":265,"skipped":4890,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:36.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:18:37.121: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:18:40.155: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:18:40.164: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2558-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:18:43.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8768" for this suite.
STEP: Destroying namespace "webhook-8768-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.969 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":266,"skipped":4893,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:43.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:18:43.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:18:44.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8996" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":267,"skipped":4895,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:44.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 17 05:18:44.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 create -f -'
Jan 17 05:18:45.931: INFO: stderr: ""
Jan 17 05:18:45.931: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 17 05:18:45.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 05:18:46.036: INFO: stderr: ""
Jan 17 05:18:46.036: INFO: stdout: "update-demo-nautilus-6w4gl update-demo-nautilus-j6hkg "
Jan 17 05:18:46.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-6w4gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 05:18:46.134: INFO: stderr: ""
Jan 17 05:18:46.134: INFO: stdout: ""
Jan 17 05:18:46.134: INFO: update-demo-nautilus-6w4gl is created but not running
Jan 17 05:18:51.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 05:18:51.227: INFO: stderr: ""
Jan 17 05:18:51.228: INFO: stdout: "update-demo-nautilus-6w4gl update-demo-nautilus-j6hkg "
Jan 17 05:18:51.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-6w4gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 05:18:51.314: INFO: stderr: ""
Jan 17 05:18:51.314: INFO: stdout: ""
Jan 17 05:18:51.314: INFO: update-demo-nautilus-6w4gl is created but not running
Jan 17 05:18:56.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 17 05:18:56.420: INFO: stderr: ""
Jan 17 05:18:56.420: INFO: stdout: "update-demo-nautilus-6w4gl update-demo-nautilus-j6hkg "
Jan 17 05:18:56.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-6w4gl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 05:18:56.506: INFO: stderr: ""
Jan 17 05:18:56.506: INFO: stdout: "true"
Jan 17 05:18:56.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-6w4gl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 05:18:56.603: INFO: stderr: ""
Jan 17 05:18:56.603: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 05:18:56.603: INFO: validating pod update-demo-nautilus-6w4gl
Jan 17 05:18:56.609: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 05:18:56.610: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 05:18:56.610: INFO: update-demo-nautilus-6w4gl is verified up and running
Jan 17 05:18:56.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-j6hkg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 17 05:18:56.707: INFO: stderr: ""
Jan 17 05:18:56.707: INFO: stdout: "true"
Jan 17 05:18:56.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods update-demo-nautilus-j6hkg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 17 05:18:56.801: INFO: stderr: ""
Jan 17 05:18:56.801: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 17 05:18:56.801: INFO: validating pod update-demo-nautilus-j6hkg
Jan 17 05:18:56.810: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 17 05:18:56.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 17 05:18:56.810: INFO: update-demo-nautilus-j6hkg is verified up and running
STEP: using delete to clean up resources
Jan 17 05:18:56.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 delete --grace-period=0 --force -f -'
Jan 17 05:18:56.939: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 05:18:56.939: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 17 05:18:56.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get rc,svc -l name=update-demo --no-headers'
Jan 17 05:18:57.084: INFO: stderr: "No resources found in kubectl-8686 namespace.\n"
Jan 17 05:18:57.084: INFO: stdout: ""
Jan 17 05:18:57.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8686 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 05:18:57.178: INFO: stderr: ""
Jan 17 05:18:57.178: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:18:57.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8686" for this suite.

• [SLOW TEST:13.027 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":268,"skipped":4898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:18:57.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:18:57.249: INFO: Creating deployment "webserver-deployment"
Jan 17 05:18:57.260: INFO: Waiting for observed generation 1
Jan 17 05:18:59.273: INFO: Waiting for all required pods to come up
Jan 17 05:18:59.280: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 17 05:19:01.304: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 17 05:19:01.312: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 17 05:19:01.329: INFO: Updating deployment webserver-deployment
Jan 17 05:19:01.329: INFO: Waiting for observed generation 2
Jan 17 05:19:03.341: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 17 05:19:03.345: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 17 05:19:03.349: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 05:19:03.359: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 17 05:19:03.359: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 17 05:19:03.364: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 17 05:19:03.370: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 17 05:19:03.370: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 17 05:19:03.385: INFO: Updating deployment webserver-deployment
Jan 17 05:19:03.385: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 17 05:19:03.390: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 17 05:19:05.412: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 05:19:05.433: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7434  dbff9a3c-7721-41ef-8780-921af9495dda 45312 3 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b12f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-17 05:19:03 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-01-17 05:19:05 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,},},ReadyReplicas:9,CollisionCount:nil,},}

Jan 17 05:19:05.438: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-7434  222ce066-8434-4392-9bef-6498dda5bb0f 45208 3 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment dbff9a3c-7721-41ef-8780-921af9495dda 0xc0058b17e7 0xc0058b17e8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbff9a3c-7721-41ef-8780-921af9495dda\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b1888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:19:05.438: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 17 05:19:05.438: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-7434  b3fa9e35-213c-418b-afc5-b8f8e78166a6 45314 3 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment dbff9a3c-7721-41ef-8780-921af9495dda 0xc0058b16f7 0xc0058b16f8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dbff9a3c-7721-41ef-8780-921af9495dda\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b1788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:19:05.447: INFO: Pod "webserver-deployment-55df494869-2fq8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2fq8l webserver-deployment-55df494869- deployment-7434  9ade01f8-431f-489f-9d83-3ddc17e4715c 45279 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ba398f6806c3facb37b7ecadcaa6fa20d888bb52f58f37d4ffd2b53d58a508f9 cni.projectcalico.org/podIP:10.100.11.200/32 cni.projectcalico.org/podIPs:10.100.11.200/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fce157 0xc005fce158}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tr6z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tr6z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.448: INFO: Pod "webserver-deployment-55df494869-4wgvn" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4wgvn webserver-deployment-55df494869- deployment-7434  106a4c83-8f50-4c4c-b25f-0fb935abc5c8 45193 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fce340 0xc005fce341}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tshxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tshxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.448: INFO: Pod "webserver-deployment-55df494869-4zmvf" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4zmvf webserver-deployment-55df494869- deployment-7434  8a604d58-d1c0-45b0-9a8f-bfe5c50d0f25 45308 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:46fc241bb5322c21e194633f139db3ed699f23ab5b417c7b49cc2b4bceabc665 cni.projectcalico.org/podIP:10.100.11.207/32 cni.projectcalico.org/podIPs:10.100.11.207/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fce4e0 0xc005fce4e1}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9pccw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9pccw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.448: INFO: Pod "webserver-deployment-55df494869-7bp4h" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-7bp4h webserver-deployment-55df494869- deployment-7434  a69b23f5-4421-4153-9fef-297b6eac3064 44997 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c4a2cb01f0202f667f3553561a0448f93a6cd3a239bf307957cced6ff884b287 cni.projectcalico.org/podIP:10.100.11.195/32 cni.projectcalico.org/podIPs:10.100.11.195/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fce680 0xc005fce681}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fl7x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fl7x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.195,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1f7a17ff0e02f07aaf14c8f90d04697e24898505e6072ae6cdb505c391fc0d2f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.449: INFO: Pod "webserver-deployment-55df494869-8fvls" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8fvls webserver-deployment-55df494869- deployment-7434  a779a711-a555-4036-85f0-17612fb1dd34 45197 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fce880 0xc005fce881}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mp9nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mp9nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.449: INFO: Pod "webserver-deployment-55df494869-8gl8b" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8gl8b webserver-deployment-55df494869- deployment-7434  08020c6b-c5b7-425e-b02b-83cc5d2fc654 45296 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f66abc0e449c715a5673afca89e8b82be84ba85f9b542b44089e6610fc9704e3 cni.projectcalico.org/podIP:10.100.11.215/32 cni.projectcalico.org/podIPs:10.100.11.215/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcea00 0xc005fcea01}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v6cg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v6cg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.449: INFO: Pod "webserver-deployment-55df494869-9xlqb" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-9xlqb webserver-deployment-55df494869- deployment-7434  fb90ab90-5b70-46fc-b4be-73217916f20a 44999 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:62c762c0ad597574ff3699ffbaf89d3e1aff171e8f66c770ed8c7ac996ba41ab cni.projectcalico.org/podIP:10.100.44.214/32 cni.projectcalico.org/podIPs:10.100.44.214/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcec00 0xc005fcec01}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tnrnm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tnrnm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.214,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f43476354428458e65ec67b8ddf370ea08d01361c754d916ea17da0a9b1b656f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.450: INFO: Pod "webserver-deployment-55df494869-dh5zx" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dh5zx webserver-deployment-55df494869- deployment-7434  e9461fb6-db5e-4317-b03a-d4b9121fa945 45006 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:01967c7b9363aa46ddefa950dd93d2efa71ed03f597c072f5758cfec98444027 cni.projectcalico.org/podIP:10.100.106.126/32 cni.projectcalico.org/podIPs:10.100.106.126/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcee00 0xc005fcee01}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.106.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlxl7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlxl7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.106.126,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b40c36ba463298f12251a3e5e6fcbc397778f5668fa4d3415fbd286b226319c3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.106.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.450: INFO: Pod "webserver-deployment-55df494869-dk79j" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dk79j webserver-deployment-55df494869- deployment-7434  a08601bd-601e-43c0-8c14-94630fc82d2f 45290 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:d462a4df9a1902c32ba9ec2febf178755f50477ec5bdd85c28cb5463686bc7a8 cni.projectcalico.org/podIP:10.100.106.76/32 cni.projectcalico.org/podIPs:10.100.106.76/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcf020 0xc005fcf021}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvt7k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvt7k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.451: INFO: Pod "webserver-deployment-55df494869-dwgrx" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-dwgrx webserver-deployment-55df494869- deployment-7434  1d7db3fd-862a-4358-beed-4b7800735322 45005 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ec013d1c9bafa37e218e05c440a802a19e6e964362e6660e25c6e3eb537d4afb cni.projectcalico.org/podIP:10.100.44.255/32 cni.projectcalico.org/podIPs:10.100.44.255/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcf220 0xc005fcf221}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.255\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbjlf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbjlf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.255,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://874181931207dc7d03d30d5411cef30c2858dcf688487a5fae8dc1e300cf529f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.255,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.451: INFO: Pod "webserver-deployment-55df494869-fq96j" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-fq96j webserver-deployment-55df494869- deployment-7434  39386634-c99e-4aeb-ad8b-0af9a14955f9 45295 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcf420 0xc005fcf421}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkctw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkctw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.452: INFO: Pod "webserver-deployment-55df494869-gmhzg" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-gmhzg webserver-deployment-55df494869- deployment-7434  23cad030-9dc3-4951-adaa-7f4fa2a942fb 44996 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:6ba71d9fed94ec9b080d6ac59566b0f878a88a65f8ca075e806e20cbd3bf1503 cni.projectcalico.org/podIP:10.100.44.204/32 cni.projectcalico.org/podIPs:10.100.44.204/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcf5f0 0xc005fcf5f1}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.204\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hz69v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hz69v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.204,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://da6f61055ca16e55835a6a39e6bd9c9d5ae3981a545ef570a717dc43e94ac527,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.204,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.452: INFO: Pod "webserver-deployment-55df494869-gxnmj" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-gxnmj webserver-deployment-55df494869- deployment-7434  e726a9bb-7869-4cfa-85a4-f18a9551d103 45291 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:0fcb988e548c9d0e30591fa702c098ab0945d14b5392c80e93d7f8573a438c4d cni.projectcalico.org/podIP:10.100.44.209/32 cni.projectcalico.org/podIPs:10.100.44.209/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcf810 0xc005fcf811}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzltg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzltg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.452: INFO: Pod "webserver-deployment-55df494869-kp6fd" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-kp6fd webserver-deployment-55df494869- deployment-7434  0187c957-c6e9-49b1-80f2-dc09858b6936 45014 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:b4ada998a8609ac419ee9748d5b89847dea75897a8fe464e90529bf67062dc5d cni.projectcalico.org/podIP:10.100.106.73/32 cni.projectcalico.org/podIPs:10.100.106.73/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcfb20 0xc005fcfb21}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.106.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xg84s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xg84s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.106.73,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ec90681a231a72dc2e6e7f47d54d51c68974ce71bae2ba3c34cec6540346bc9c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.106.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.453: INFO: Pod "webserver-deployment-55df494869-ktqpv" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ktqpv webserver-deployment-55df494869- deployment-7434  8e1df702-4014-43a4-af43-8b00f576634e 45250 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:96ab4fff2ca3c4f0b1ac417fb21534d2e5512f2640702eae3967ebd5e3666ef5 cni.projectcalico.org/podIP:10.100.106.105/32 cni.projectcalico.org/podIPs:10.100.106.105/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcfd20 0xc005fcfd21}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxkw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxkw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.453: INFO: Pod "webserver-deployment-55df494869-m9rr4" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-m9rr4 webserver-deployment-55df494869- deployment-7434  18f32cd5-0453-4882-83f6-c75b5d9c9eff 45248 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:6cd4658141c58c4fb1b34666d4b6f6fd6989b33f1b376d6de5643e6d54ce2ea8 cni.projectcalico.org/podIP:10.100.11.229/32 cni.projectcalico.org/podIPs:10.100.11.229/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc005fcff90 0xc005fcff91}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kn79c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kn79c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.454: INFO: Pod "webserver-deployment-55df494869-smswn" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-smswn webserver-deployment-55df494869- deployment-7434  ec9e6bc1-0668-48aa-9b5e-5a8de66aa5c2 45311 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:4db4f78456feccca228bd6f43d4cba8f798a87e63209e89bdc8bdbae55faf38e cni.projectcalico.org/podIP:10.100.44.200/32 cni.projectcalico.org/podIPs:10.100.44.200/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc004ff8190 0xc004ff8191}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbjkm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbjkm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.200,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:19:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://91c5dec3705f66f90bd3a675f92648ff320b6804426d731f4d5e4f21ce46615b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.454: INFO: Pod "webserver-deployment-55df494869-sw6q2" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-sw6q2 webserver-deployment-55df494869- deployment-7434  c121aabb-2a06-4795-b347-01ee33883e15 45300 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c77124b8168fcee9729fbef39ce37468f4bfdc36eb6ae38a26d5d6f6244c0991 cni.projectcalico.org/podIP:10.100.44.201/32 cni.projectcalico.org/podIPs:10.100.44.201/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc004ff83b0 0xc004ff83b1}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:19:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5l2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5l2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:10.100.44.201,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:19:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ca56f3aff41b62e18e21bf40f17a46e6e506e98e8b69dbd99f82336129daee13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.44.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.455: INFO: Pod "webserver-deployment-55df494869-sznln" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-sznln webserver-deployment-55df494869- deployment-7434  5e25c85e-0bfc-48be-9864-4bee4fcc6aed 45010 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:efaf12f131dcd248ba6100ff31e0e54c57998f3bf03631255b9cbbe27fddf3cd cni.projectcalico.org/podIP:10.100.106.86/32 cni.projectcalico.org/podIPs:10.100.106.86/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc004ff85d0 0xc004ff85d1}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.106.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6skz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6skz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:10.100.106.86,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://83dda378aac56e093bd540ec1103b74d512875793e7a84e1f0267f679f607712,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.106.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.455: INFO: Pod "webserver-deployment-55df494869-xmhsl" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-xmhsl webserver-deployment-55df494869- deployment-7434  702f6cf1-8277-4dfd-96af-3e2cb560293a 45017 0 2023-01-17 05:18:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:c382440d06f714bb6e628c983defca5725a40ed8fd538f008c842c77ae9120b1 cni.projectcalico.org/podIP:10.100.11.198/32 cni.projectcalico.org/podIPs:10.100.11.198/32] [{apps/v1 ReplicaSet webserver-deployment-55df494869 b3fa9e35-213c-418b-afc5-b8f8e78166a6 0xc004ff8800 0xc004ff8801}] []  [{kube-controller-manager Update v1 2023-01-17 05:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b3fa9e35-213c-418b-afc5-b8f8e78166a6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-17 05:18:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-17 05:18:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qk5cp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qk5cp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:18:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.198,StartTime:2023-01-17 05:18:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:18:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7cac9c1a7666c21dc103ded5e1f035635a2022886b55c9f93f8dd12a39ce944c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.456: INFO: Pod "webserver-deployment-57ccb67bb8-4qnfh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-4qnfh webserver-deployment-57ccb67bb8- deployment-7434  8e6dafcc-8b41-4125-82aa-559690d501ce 45205 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff8a00 0xc004ff8a01}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8ktg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8ktg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.456: INFO: Pod "webserver-deployment-57ccb67bb8-7vlx4" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-7vlx4 webserver-deployment-57ccb67bb8- deployment-7434  9d259049-1639-4886-9649-4120246c5829 45259 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:57c1db7f1a51bb48c3472fdd18b41b8ca6c9046dfac6004fac025e1ab943df4c cni.projectcalico.org/podIP:10.100.106.109/32 cni.projectcalico.org/podIPs:10.100.106.109/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff8bd0 0xc004ff8bd1}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-thw5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-thw5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.457: INFO: Pod "webserver-deployment-57ccb67bb8-8c48h" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-8c48h webserver-deployment-57ccb67bb8- deployment-7434  94b9b425-7a5f-445b-8b93-7167254e727e 45090 0 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:78806622191ed867fceb0df9fe64efe5f873c509eff143d9a219c5e77e8057a2 cni.projectcalico.org/podIP:10.100.106.98/32 cni.projectcalico.org/podIPs:10.100.106.98/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff8e00 0xc004ff8e01}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nl2d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nl2d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.457: INFO: Pod "webserver-deployment-57ccb67bb8-8mf6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-8mf6k webserver-deployment-57ccb67bb8- deployment-7434  3cac8f9b-7ea8-4728-babc-57c06d570eb9 45283 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:0928b4dbccccaba409465837fc0dd8cf4b78ecf6d534875b3c7c71f569217954 cni.projectcalico.org/podIP:10.100.44.202/32 cni.projectcalico.org/podIPs:10.100.44.202/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9020 0xc004ff9021}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxtcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxtcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.457: INFO: Pod "webserver-deployment-57ccb67bb8-d6vqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-d6vqz webserver-deployment-57ccb67bb8- deployment-7434  ab018749-e07e-4a31-ba50-477795a6dfb7 45085 0 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:16fcca8fe3884ffc870b014fb77aa5dba5eac7990bfb4d63117069df5e1f8e2a cni.projectcalico.org/podIP:10.100.44.217/32 cni.projectcalico.org/podIPs:10.100.44.217/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9240 0xc004ff9241}] []  [{Go-http-client Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rx7bx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rx7bx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 05:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.458: INFO: Pod "webserver-deployment-57ccb67bb8-knl6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-knl6s webserver-deployment-57ccb67bb8- deployment-7434  2d16ef60-7c2e-4cab-8297-df0da6fd2a25 45243 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:b143d52317c2660ff7bad58f67fb6a934558a8e469466bca1e74af71e6045245 cni.projectcalico.org/podIP:10.100.106.79/32 cni.projectcalico.org/podIPs:10.100.106.79/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9450 0xc004ff9451}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kqnwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kqnwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.458: INFO: Pod "webserver-deployment-57ccb67bb8-mj6cb" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mj6cb webserver-deployment-57ccb67bb8- deployment-7434  09928d82-943e-4585-b493-12b6dbc4f406 45093 0 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7244fe7f7f8afabc0d0ae96dd735b0fad52e72f6d12982a29ca14ec0cdeda1ef cni.projectcalico.org/podIP:10.100.44.194/32 cni.projectcalico.org/podIPs:10.100.44.194/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9710 0xc004ff9711}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-crtr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-crtr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 05:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.458: INFO: Pod "webserver-deployment-57ccb67bb8-sq754" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-sq754 webserver-deployment-57ccb67bb8- deployment-7434  03e88270-42e1-4165-a593-f29cec21b908 45235 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9910 0xc004ff9911}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7kxg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7kxg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.16,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.459: INFO: Pod "webserver-deployment-57ccb67bb8-vs8qq" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-vs8qq webserver-deployment-57ccb67bb8- deployment-7434  e408b608-ec2e-42f3-9ed9-9594af64a60e 45190 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9c10 0xc004ff9c11}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ntxs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ntxs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.459: INFO: Pod "webserver-deployment-57ccb67bb8-wldkl" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wldkl webserver-deployment-57ccb67bb8- deployment-7434  ca806771-a32a-4693-9be7-cbae0fb84c04 45263 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:e84e1792cb8085d181240dd99694e00163a8f7c7abaab6190712eb7ea7aceda5 cni.projectcalico.org/podIP:10.100.44.207/32 cni.projectcalico.org/podIPs:10.100.44.207/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc004ff9f00 0xc004ff9f01}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8dv9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8dv9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.21,PodIP:,StartTime:2023-01-17 05:19:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.459: INFO: Pod "webserver-deployment-57ccb67bb8-x6sdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-x6sdg webserver-deployment-57ccb67bb8- deployment-7434  74908b84-9bf5-4b12-92d5-35a8b85c5bf8 45109 0 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:92ce9942a8a93d1896dc0648799af1894034083b0460331bb9c0bccbfd5606ad cni.projectcalico.org/podIP:10.100.11.197/32 cni.projectcalico.org/podIPs:10.100.11.197/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc0060d6130 0xc0060d6131}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9d5gt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9d5gt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.460: INFO: Pod "webserver-deployment-57ccb67bb8-x7z9q" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-x7z9q webserver-deployment-57ccb67bb8- deployment-7434  f176c225-55ae-4854-8564-05189bd686cc 45200 0 2023-01-17 05:19:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc0060d6330 0xc0060d6331}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qkvn6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qkvn6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 17 05:19:05.460: INFO: Pod "webserver-deployment-57ccb67bb8-zmvhk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-zmvhk webserver-deployment-57ccb67bb8- deployment-7434  e870c60f-76f5-4c15-8314-083e2dd8dad9 45086 0 2023-01-17 05:19:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:d280d5084094e75371f8fc76a8290baebe69ed2a4db70ee082368949bd741a27 cni.projectcalico.org/podIP:10.100.11.201/32 cni.projectcalico.org/podIPs:10.100.11.201/32] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 222ce066-8434-4392-9bef-6498dda5bb0f 0xc0060d64b0 0xc0060d64b1}] []  [{kube-controller-manager Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"222ce066-8434-4392-9bef-6498dda5bb0f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:19:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-17 05:19:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b4c75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4c75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:19:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:,StartTime:2023-01-17 05:19:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 05:19:05.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7434" for this suite.

• [SLOW TEST:8.277 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":269,"skipped":4924,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:19:05.482: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-aea0f05d-91fc-477c-8585-36a5fa8d9c28 in namespace container-probe-8008
Jan 17 05:19:13.572: INFO: Started pod liveness-aea0f05d-91fc-477c-8585-36a5fa8d9c28 in namespace container-probe-8008
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 05:19:13.579: INFO: Initial restart count of pod liveness-aea0f05d-91fc-477c-8585-36a5fa8d9c28 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 05:23:14.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8008" for this suite.

• [SLOW TEST:249.295 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":270,"skipped":4980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:14.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-2845f26f-9c23-4ccd-9aa7-c8f1966f6e58
STEP: Creating a pod to test consume configMaps
Jan 17 05:23:14.859: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a" in namespace "projected-9183" to be "Succeeded or Failed"
Jan 17 05:23:14.865: INFO: Pod "pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.475638ms
Jan 17 05:23:16.878: INFO: Pod "pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019519151s
Jan 17 05:23:18.887: INFO: Pod "pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028263805s
STEP: Saw pod success
Jan 17 05:23:18.887: INFO: Pod "pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a" satisfied condition "Succeeded or Failed"
Jan 17 05:23:18.890: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:23:18.975: INFO: Waiting for pod pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a to disappear
Jan 17 05:23:18.980: INFO: Pod pod-projected-configmaps-39b7c536-069d-4652-955f-ae10600b152a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 05:23:18.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9183" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":271,"skipped":5014,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:19.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-80d1a109-afc4-48dd-88db-192b798effee
STEP: Creating a pod to test consume secrets
Jan 17 05:23:19.120: INFO: Waiting up to 5m0s for pod "pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562" in namespace "secrets-3695" to be "Succeeded or Failed"
Jan 17 05:23:19.126: INFO: Pod "pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402863ms
Jan 17 05:23:21.134: INFO: Pod "pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01427869s
Jan 17 05:23:23.145: INFO: Pod "pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025054158s
STEP: Saw pod success
Jan 17 05:23:23.145: INFO: Pod "pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562" satisfied condition "Succeeded or Failed"
Jan 17 05:23:23.149: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562 container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:23:23.184: INFO: Waiting for pod pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562 to disappear
Jan 17 05:23:23.189: INFO: Pod pod-secrets-01a156ac-9685-4b0a-885f-e8aa1b8bb562 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:23:23.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3695" for this suite.
STEP: Destroying namespace "secret-namespace-52" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":272,"skipped":5019,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:23.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-8758f033-ed7a-43f4-bd68-a70e29db0f2c
STEP: Creating configMap with name cm-test-opt-upd-eaa374f4-a3e3-4eff-ba70-83a6c8733821
STEP: Creating the pod
Jan 17 05:23:23.315: INFO: The status of Pod pod-configmaps-abfebdc3-7aa1-4fe0-8f07-600e8876b28b is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:23:25.323: INFO: The status of Pod pod-configmaps-abfebdc3-7aa1-4fe0-8f07-600e8876b28b is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-8758f033-ed7a-43f4-bd68-a70e29db0f2c
STEP: Updating configmap cm-test-opt-upd-eaa374f4-a3e3-4eff-ba70-83a6c8733821
STEP: Creating configMap with name cm-test-opt-create-e081c76a-954c-4709-a58c-b402470246af
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:23:27.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9382" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":273,"skipped":5023,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:27.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 05:23:38.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8427" for this suite.

• [SLOW TEST:11.146 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":274,"skipped":5034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:38.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 17 05:23:42.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4787" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":275,"skipped":5072,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:23:42.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jan 17 05:23:54.394: INFO: 70 pods remaining
Jan 17 05:23:54.395: INFO: 70 pods has nil DeletionTimestamp
Jan 17 05:23:54.395: INFO: 
STEP: Gathering metrics
W0117 05:23:59.416382      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 05:23:59.416: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 17 05:23:59.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-28mtp" in namespace "gc-9474"
Jan 17 05:23:59.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-29f7q" in namespace "gc-9474"
Jan 17 05:23:59.484: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fpjd" in namespace "gc-9474"
Jan 17 05:23:59.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fqql" in namespace "gc-9474"
Jan 17 05:23:59.549: INFO: Deleting pod "simpletest-rc-to-be-deleted-2prmx" in namespace "gc-9474"
Jan 17 05:23:59.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-2t2cl" in namespace "gc-9474"
Jan 17 05:23:59.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-4b8n9" in namespace "gc-9474"
Jan 17 05:23:59.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bjvm" in namespace "gc-9474"
Jan 17 05:23:59.694: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fjp8" in namespace "gc-9474"
Jan 17 05:23:59.734: INFO: Deleting pod "simpletest-rc-to-be-deleted-55crb" in namespace "gc-9474"
Jan 17 05:23:59.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lptx" in namespace "gc-9474"
Jan 17 05:23:59.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-627cq" in namespace "gc-9474"
Jan 17 05:23:59.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hblc" in namespace "gc-9474"
Jan 17 05:23:59.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pmv4" in namespace "gc-9474"
Jan 17 05:23:59.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-75sv7" in namespace "gc-9474"
Jan 17 05:24:00.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hkwz" in namespace "gc-9474"
Jan 17 05:24:00.131: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pgrv" in namespace "gc-9474"
Jan 17 05:24:00.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sprq" in namespace "gc-9474"
Jan 17 05:24:00.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-8d24d" in namespace "gc-9474"
Jan 17 05:24:00.236: INFO: Deleting pod "simpletest-rc-to-be-deleted-8msbj" in namespace "gc-9474"
Jan 17 05:24:00.279: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qtgb" in namespace "gc-9474"
Jan 17 05:24:00.346: INFO: Deleting pod "simpletest-rc-to-be-deleted-95t6v" in namespace "gc-9474"
Jan 17 05:24:00.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-97ln7" in namespace "gc-9474"
Jan 17 05:24:00.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-98h5n" in namespace "gc-9474"
Jan 17 05:24:00.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-9m5wj" in namespace "gc-9474"
Jan 17 05:24:00.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zcp9" in namespace "gc-9474"
Jan 17 05:24:00.529: INFO: Deleting pod "simpletest-rc-to-be-deleted-b96qs" in namespace "gc-9474"
Jan 17 05:24:00.558: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdfdn" in namespace "gc-9474"
Jan 17 05:24:00.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfs6h" in namespace "gc-9474"
Jan 17 05:24:00.636: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvwnt" in namespace "gc-9474"
Jan 17 05:24:00.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz5ld" in namespace "gc-9474"
Jan 17 05:24:00.720: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdvrq" in namespace "gc-9474"
Jan 17 05:24:00.762: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmkd2" in namespace "gc-9474"
Jan 17 05:24:00.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-ct964" in namespace "gc-9474"
Jan 17 05:24:00.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-dfnz4" in namespace "gc-9474"
Jan 17 05:24:00.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkj2m" in namespace "gc-9474"
Jan 17 05:24:01.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-f4cxc" in namespace "gc-9474"
Jan 17 05:24:01.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-f79zn" in namespace "gc-9474"
Jan 17 05:24:01.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhqx6" in namespace "gc-9474"
Jan 17 05:24:01.144: INFO: Deleting pod "simpletest-rc-to-be-deleted-fm8lx" in namespace "gc-9474"
Jan 17 05:24:01.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsj7d" in namespace "gc-9474"
Jan 17 05:24:01.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2cbz" in namespace "gc-9474"
Jan 17 05:24:01.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-g2hp5" in namespace "gc-9474"
Jan 17 05:24:01.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggg2p" in namespace "gc-9474"
Jan 17 05:24:01.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-gplgl" in namespace "gc-9474"
Jan 17 05:24:01.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-hbcdn" in namespace "gc-9474"
Jan 17 05:24:01.414: INFO: Deleting pod "simpletest-rc-to-be-deleted-ht4df" in namespace "gc-9474"
Jan 17 05:24:01.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-hw828" in namespace "gc-9474"
Jan 17 05:24:01.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxqs8" in namespace "gc-9474"
Jan 17 05:24:01.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcks5" in namespace "gc-9474"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 05:24:01.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9474" for this suite.

• [SLOW TEST:18.775 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":276,"skipped":5073,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:01.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:24:01.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6325" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":277,"skipped":5076,"failed":0}
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:01.956: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:24:02.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6109
I0117 05:24:02.164718      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6109, replica count: 1
I0117 05:24:03.215791      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 05:24:04.216071      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 05:24:05.216912      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0117 05:24:06.217031      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:24:06.363: INFO: Created: latency-svc-x5wnp
Jan 17 05:24:06.386: INFO: Got endpoints: latency-svc-x5wnp [69.633234ms]
Jan 17 05:24:06.436: INFO: Created: latency-svc-gs2nk
Jan 17 05:24:06.440: INFO: Created: latency-svc-wvgw4
Jan 17 05:24:06.461: INFO: Created: latency-svc-rn88f
Jan 17 05:24:06.485: INFO: Created: latency-svc-4wjrk
Jan 17 05:24:06.499: INFO: Got endpoints: latency-svc-gs2nk [112.420498ms]
Jan 17 05:24:06.513: INFO: Created: latency-svc-htpkn
Jan 17 05:24:06.529: INFO: Created: latency-svc-psds2
Jan 17 05:24:06.549: INFO: Got endpoints: latency-svc-4wjrk [161.552016ms]
Jan 17 05:24:06.550: INFO: Got endpoints: latency-svc-rn88f [162.768803ms]
Jan 17 05:24:06.558: INFO: Created: latency-svc-h2pvz
Jan 17 05:24:06.560: INFO: Got endpoints: latency-svc-wvgw4 [172.306408ms]
Jan 17 05:24:06.576: INFO: Created: latency-svc-n6h8p
Jan 17 05:24:06.593: INFO: Created: latency-svc-rpfc8
Jan 17 05:24:06.619: INFO: Created: latency-svc-glrfh
Jan 17 05:24:06.634: INFO: Got endpoints: latency-svc-htpkn [247.333862ms]
Jan 17 05:24:06.639: INFO: Got endpoints: latency-svc-psds2 [251.766871ms]
Jan 17 05:24:06.641: INFO: Created: latency-svc-gf94r
Jan 17 05:24:06.643: INFO: Got endpoints: latency-svc-h2pvz [255.698012ms]
Jan 17 05:24:06.654: INFO: Created: latency-svc-jlllr
Jan 17 05:24:06.674: INFO: Created: latency-svc-7phgp
Jan 17 05:24:06.702: INFO: Created: latency-svc-xx857
Jan 17 05:24:06.702: INFO: Got endpoints: latency-svc-gf94r [314.475947ms]
Jan 17 05:24:06.702: INFO: Got endpoints: latency-svc-glrfh [314.809916ms]
Jan 17 05:24:06.702: INFO: Got endpoints: latency-svc-n6h8p [315.236293ms]
Jan 17 05:24:06.703: INFO: Got endpoints: latency-svc-rpfc8 [315.274259ms]
Jan 17 05:24:06.725: INFO: Created: latency-svc-m567r
Jan 17 05:24:06.742: INFO: Created: latency-svc-k5vp2
Jan 17 05:24:06.752: INFO: Got endpoints: latency-svc-jlllr [364.992469ms]
Jan 17 05:24:06.759: INFO: Got endpoints: latency-svc-7phgp [371.128089ms]
Jan 17 05:24:06.773: INFO: Created: latency-svc-xh8kh
Jan 17 05:24:06.779: INFO: Got endpoints: latency-svc-xx857 [390.983352ms]
Jan 17 05:24:06.779: INFO: Got endpoints: latency-svc-m567r [390.825013ms]
Jan 17 05:24:06.782: INFO: Got endpoints: latency-svc-k5vp2 [283.258048ms]
Jan 17 05:24:06.809: INFO: Created: latency-svc-xznff
Jan 17 05:24:06.824: INFO: Created: latency-svc-2h8mp
Jan 17 05:24:06.825: INFO: Got endpoints: latency-svc-xh8kh [275.154976ms]
Jan 17 05:24:06.849: INFO: Created: latency-svc-qfnvw
Jan 17 05:24:06.866: INFO: Created: latency-svc-lxzxt
Jan 17 05:24:06.883: INFO: Got endpoints: latency-svc-xznff [333.96821ms]
Jan 17 05:24:06.890: INFO: Created: latency-svc-9h5w8
Jan 17 05:24:06.905: INFO: Created: latency-svc-tf7xp
Jan 17 05:24:06.934: INFO: Got endpoints: latency-svc-qfnvw [299.477975ms]
Jan 17 05:24:06.934: INFO: Got endpoints: latency-svc-lxzxt [294.797677ms]
Jan 17 05:24:06.934: INFO: Got endpoints: latency-svc-2h8mp [373.986413ms]
Jan 17 05:24:06.947: INFO: Got endpoints: latency-svc-9h5w8 [304.006499ms]
Jan 17 05:24:06.948: INFO: Created: latency-svc-nb9dp
Jan 17 05:24:06.956: INFO: Got endpoints: latency-svc-tf7xp [254.27738ms]
Jan 17 05:24:06.964: INFO: Created: latency-svc-qg6zw
Jan 17 05:24:06.983: INFO: Created: latency-svc-c757q
Jan 17 05:24:07.002: INFO: Got endpoints: latency-svc-nb9dp [299.555466ms]
Jan 17 05:24:07.017: INFO: Created: latency-svc-rq79b
Jan 17 05:24:07.019: INFO: Got endpoints: latency-svc-qg6zw [316.999739ms]
Jan 17 05:24:07.025: INFO: Created: latency-svc-p6n98
Jan 17 05:24:07.040: INFO: Got endpoints: latency-svc-c757q [337.200335ms]
Jan 17 05:24:07.052: INFO: Got endpoints: latency-svc-rq79b [299.722288ms]
Jan 17 05:24:07.060: INFO: Got endpoints: latency-svc-p6n98 [301.123103ms]
Jan 17 05:24:07.064: INFO: Created: latency-svc-pfpcr
Jan 17 05:24:07.101: INFO: Created: latency-svc-56p9x
Jan 17 05:24:07.120: INFO: Got endpoints: latency-svc-56p9x [341.152293ms]
Jan 17 05:24:07.122: INFO: Created: latency-svc-n6cb2
Jan 17 05:24:07.135: INFO: Got endpoints: latency-svc-pfpcr [355.8024ms]
Jan 17 05:24:07.146: INFO: Created: latency-svc-svdfq
Jan 17 05:24:07.155: INFO: Got endpoints: latency-svc-n6cb2 [372.620324ms]
Jan 17 05:24:07.165: INFO: Created: latency-svc-sb6kb
Jan 17 05:24:07.179: INFO: Got endpoints: latency-svc-svdfq [354.151548ms]
Jan 17 05:24:07.194: INFO: Created: latency-svc-bgtng
Jan 17 05:24:07.199: INFO: Got endpoints: latency-svc-sb6kb [315.922326ms]
Jan 17 05:24:07.207: INFO: Created: latency-svc-5txt9
Jan 17 05:24:07.210: INFO: Got endpoints: latency-svc-bgtng [276.272993ms]
Jan 17 05:24:07.228: INFO: Got endpoints: latency-svc-5txt9 [293.678223ms]
Jan 17 05:24:07.235: INFO: Created: latency-svc-sk2rb
Jan 17 05:24:07.260: INFO: Created: latency-svc-cvhcm
Jan 17 05:24:07.264: INFO: Got endpoints: latency-svc-sk2rb [330.553429ms]
Jan 17 05:24:07.286: INFO: Got endpoints: latency-svc-cvhcm [339.141698ms]
Jan 17 05:24:07.288: INFO: Created: latency-svc-fhtwv
Jan 17 05:24:07.307: INFO: Got endpoints: latency-svc-fhtwv [350.575195ms]
Jan 17 05:24:07.308: INFO: Created: latency-svc-d7c4b
Jan 17 05:24:07.319: INFO: Created: latency-svc-shspl
Jan 17 05:24:07.335: INFO: Created: latency-svc-5gfc7
Jan 17 05:24:07.338: INFO: Got endpoints: latency-svc-d7c4b [335.844921ms]
Jan 17 05:24:07.341: INFO: Got endpoints: latency-svc-shspl [321.447752ms]
Jan 17 05:24:07.357: INFO: Created: latency-svc-x6xq5
Jan 17 05:24:07.361: INFO: Got endpoints: latency-svc-5gfc7 [320.620477ms]
Jan 17 05:24:07.387: INFO: Got endpoints: latency-svc-x6xq5 [335.403298ms]
Jan 17 05:24:07.405: INFO: Created: latency-svc-rhpz4
Jan 17 05:24:07.417: INFO: Got endpoints: latency-svc-rhpz4 [356.597157ms]
Jan 17 05:24:07.436: INFO: Created: latency-svc-xc7d5
Jan 17 05:24:07.440: INFO: Got endpoints: latency-svc-xc7d5 [320.156989ms]
Jan 17 05:24:07.454: INFO: Created: latency-svc-bgkxp
Jan 17 05:24:07.481: INFO: Got endpoints: latency-svc-bgkxp [346.729799ms]
Jan 17 05:24:07.497: INFO: Created: latency-svc-fd2pv
Jan 17 05:24:07.513: INFO: Created: latency-svc-n6kj6
Jan 17 05:24:07.514: INFO: Got endpoints: latency-svc-fd2pv [358.679818ms]
Jan 17 05:24:07.530: INFO: Got endpoints: latency-svc-n6kj6 [350.616545ms]
Jan 17 05:24:07.533: INFO: Created: latency-svc-pt9jw
Jan 17 05:24:07.544: INFO: Created: latency-svc-r95md
Jan 17 05:24:07.568: INFO: Created: latency-svc-r7fl2
Jan 17 05:24:07.578: INFO: Got endpoints: latency-svc-pt9jw [379.386864ms]
Jan 17 05:24:07.606: INFO: Got endpoints: latency-svc-r95md [395.548774ms]
Jan 17 05:24:07.606: INFO: Got endpoints: latency-svc-r7fl2 [378.002937ms]
Jan 17 05:24:07.609: INFO: Created: latency-svc-l8mkz
Jan 17 05:24:07.628: INFO: Created: latency-svc-xltbb
Jan 17 05:24:07.650: INFO: Got endpoints: latency-svc-l8mkz [385.156587ms]
Jan 17 05:24:07.655: INFO: Got endpoints: latency-svc-xltbb [368.554228ms]
Jan 17 05:24:07.669: INFO: Created: latency-svc-2hkfd
Jan 17 05:24:07.695: INFO: Got endpoints: latency-svc-2hkfd [388.084433ms]
Jan 17 05:24:07.706: INFO: Created: latency-svc-q4r2s
Jan 17 05:24:07.714: INFO: Created: latency-svc-xpdsh
Jan 17 05:24:07.729: INFO: Created: latency-svc-f86tz
Jan 17 05:24:07.749: INFO: Got endpoints: latency-svc-q4r2s [411.239358ms]
Jan 17 05:24:07.754: INFO: Got endpoints: latency-svc-xpdsh [413.374401ms]
Jan 17 05:24:07.766: INFO: Created: latency-svc-gd9lc
Jan 17 05:24:07.779: INFO: Got endpoints: latency-svc-f86tz [418.543908ms]
Jan 17 05:24:07.792: INFO: Created: latency-svc-kq4gt
Jan 17 05:24:07.814: INFO: Got endpoints: latency-svc-gd9lc [426.462448ms]
Jan 17 05:24:07.822: INFO: Created: latency-svc-4lsnn
Jan 17 05:24:07.845: INFO: Created: latency-svc-rd6n9
Jan 17 05:24:07.855: INFO: Created: latency-svc-c8s2z
Jan 17 05:24:07.869: INFO: Got endpoints: latency-svc-kq4gt [451.852168ms]
Jan 17 05:24:07.881: INFO: Created: latency-svc-bkgtc
Jan 17 05:24:07.898: INFO: Created: latency-svc-bjr8z
Jan 17 05:24:07.914: INFO: Created: latency-svc-gpq76
Jan 17 05:24:07.928: INFO: Got endpoints: latency-svc-4lsnn [487.399798ms]
Jan 17 05:24:07.931: INFO: Created: latency-svc-snhf8
Jan 17 05:24:07.948: INFO: Created: latency-svc-t8njl
Jan 17 05:24:07.956: INFO: Got endpoints: latency-svc-rd6n9 [474.596896ms]
Jan 17 05:24:07.962: INFO: Created: latency-svc-d86bx
Jan 17 05:24:07.978: INFO: Created: latency-svc-cxwsr
Jan 17 05:24:08.006: INFO: Created: latency-svc-9fjvz
Jan 17 05:24:08.012: INFO: Got endpoints: latency-svc-c8s2z [498.045247ms]
Jan 17 05:24:08.025: INFO: Created: latency-svc-z99wg
Jan 17 05:24:08.038: INFO: Created: latency-svc-c9whk
Jan 17 05:24:08.057: INFO: Got endpoints: latency-svc-bkgtc [527.007627ms]
Jan 17 05:24:08.058: INFO: Created: latency-svc-mx4d6
Jan 17 05:24:08.071: INFO: Created: latency-svc-m2dms
Jan 17 05:24:08.120: INFO: Created: latency-svc-v7ttn
Jan 17 05:24:08.124: INFO: Got endpoints: latency-svc-bjr8z [545.926123ms]
Jan 17 05:24:08.126: INFO: Created: latency-svc-vp5b2
Jan 17 05:24:08.126: INFO: Created: latency-svc-qw7fd
Jan 17 05:24:08.128: INFO: Created: latency-svc-527jf
Jan 17 05:24:08.150: INFO: Created: latency-svc-6h5bs
Jan 17 05:24:08.155: INFO: Got endpoints: latency-svc-gpq76 [549.362675ms]
Jan 17 05:24:08.177: INFO: Created: latency-svc-w7c47
Jan 17 05:24:08.235: INFO: Got endpoints: latency-svc-snhf8 [628.929985ms]
Jan 17 05:24:08.262: INFO: Got endpoints: latency-svc-t8njl [612.202887ms]
Jan 17 05:24:08.264: INFO: Created: latency-svc-jmrtb
Jan 17 05:24:08.284: INFO: Created: latency-svc-dh8nd
Jan 17 05:24:08.303: INFO: Got endpoints: latency-svc-d86bx [648.638374ms]
Jan 17 05:24:08.333: INFO: Created: latency-svc-qz78c
Jan 17 05:24:08.355: INFO: Got endpoints: latency-svc-cxwsr [659.575228ms]
Jan 17 05:24:08.388: INFO: Created: latency-svc-94wmp
Jan 17 05:24:08.411: INFO: Got endpoints: latency-svc-9fjvz [661.815101ms]
Jan 17 05:24:08.436: INFO: Created: latency-svc-nvkgt
Jan 17 05:24:08.480: INFO: Got endpoints: latency-svc-z99wg [725.669988ms]
Jan 17 05:24:08.513: INFO: Created: latency-svc-4nsvj
Jan 17 05:24:08.516: INFO: Got endpoints: latency-svc-c9whk [737.113819ms]
Jan 17 05:24:08.537: INFO: Created: latency-svc-5rhx6
Jan 17 05:24:08.559: INFO: Got endpoints: latency-svc-mx4d6 [745.126253ms]
Jan 17 05:24:08.586: INFO: Created: latency-svc-qbppc
Jan 17 05:24:08.606: INFO: Got endpoints: latency-svc-m2dms [737.217037ms]
Jan 17 05:24:08.624: INFO: Created: latency-svc-87h4k
Jan 17 05:24:08.655: INFO: Got endpoints: latency-svc-v7ttn [698.686589ms]
Jan 17 05:24:08.674: INFO: Created: latency-svc-xxg8s
Jan 17 05:24:08.710: INFO: Got endpoints: latency-svc-527jf [698.616294ms]
Jan 17 05:24:08.737: INFO: Created: latency-svc-bl6ct
Jan 17 05:24:08.760: INFO: Got endpoints: latency-svc-vp5b2 [832.368116ms]
Jan 17 05:24:08.783: INFO: Created: latency-svc-z4vfq
Jan 17 05:24:08.806: INFO: Got endpoints: latency-svc-qw7fd [749.11457ms]
Jan 17 05:24:08.837: INFO: Created: latency-svc-zwttw
Jan 17 05:24:08.868: INFO: Got endpoints: latency-svc-6h5bs [743.367137ms]
Jan 17 05:24:08.895: INFO: Created: latency-svc-vm7md
Jan 17 05:24:08.917: INFO: Got endpoints: latency-svc-w7c47 [762.115177ms]
Jan 17 05:24:08.941: INFO: Created: latency-svc-74d4g
Jan 17 05:24:08.989: INFO: Got endpoints: latency-svc-jmrtb [754.569478ms]
Jan 17 05:24:09.015: INFO: Created: latency-svc-db87f
Jan 17 05:24:09.022: INFO: Got endpoints: latency-svc-dh8nd [759.904682ms]
Jan 17 05:24:09.042: INFO: Created: latency-svc-9lv2j
Jan 17 05:24:09.082: INFO: Got endpoints: latency-svc-qz78c [778.52911ms]
Jan 17 05:24:09.102: INFO: Created: latency-svc-87vjm
Jan 17 05:24:09.118: INFO: Got endpoints: latency-svc-94wmp [763.173286ms]
Jan 17 05:24:09.139: INFO: Created: latency-svc-59s8t
Jan 17 05:24:09.162: INFO: Got endpoints: latency-svc-nvkgt [750.4994ms]
Jan 17 05:24:09.186: INFO: Created: latency-svc-qmscg
Jan 17 05:24:09.208: INFO: Got endpoints: latency-svc-4nsvj [728.058187ms]
Jan 17 05:24:09.234: INFO: Created: latency-svc-9xlh5
Jan 17 05:24:09.252: INFO: Got endpoints: latency-svc-5rhx6 [735.22786ms]
Jan 17 05:24:09.281: INFO: Created: latency-svc-zccwr
Jan 17 05:24:09.311: INFO: Got endpoints: latency-svc-qbppc [751.522842ms]
Jan 17 05:24:09.339: INFO: Created: latency-svc-z62hj
Jan 17 05:24:09.358: INFO: Got endpoints: latency-svc-87h4k [752.4797ms]
Jan 17 05:24:09.378: INFO: Created: latency-svc-vhtlh
Jan 17 05:24:09.417: INFO: Got endpoints: latency-svc-xxg8s [762.344323ms]
Jan 17 05:24:09.443: INFO: Created: latency-svc-6d8rk
Jan 17 05:24:09.459: INFO: Got endpoints: latency-svc-bl6ct [748.763109ms]
Jan 17 05:24:09.483: INFO: Created: latency-svc-vwg4w
Jan 17 05:24:09.530: INFO: Got endpoints: latency-svc-z4vfq [769.75033ms]
Jan 17 05:24:09.550: INFO: Created: latency-svc-xfh2l
Jan 17 05:24:09.562: INFO: Got endpoints: latency-svc-zwttw [755.95236ms]
Jan 17 05:24:09.583: INFO: Created: latency-svc-hpspt
Jan 17 05:24:09.612: INFO: Got endpoints: latency-svc-vm7md [744.124867ms]
Jan 17 05:24:09.627: INFO: Created: latency-svc-k7js4
Jan 17 05:24:09.657: INFO: Got endpoints: latency-svc-74d4g [740.030039ms]
Jan 17 05:24:09.679: INFO: Created: latency-svc-728z2
Jan 17 05:24:09.703: INFO: Got endpoints: latency-svc-db87f [713.43142ms]
Jan 17 05:24:09.728: INFO: Created: latency-svc-cfmn4
Jan 17 05:24:09.756: INFO: Got endpoints: latency-svc-9lv2j [734.570319ms]
Jan 17 05:24:09.781: INFO: Created: latency-svc-44gcr
Jan 17 05:24:09.830: INFO: Got endpoints: latency-svc-87vjm [747.787221ms]
Jan 17 05:24:09.857: INFO: Created: latency-svc-vr9z8
Jan 17 05:24:09.866: INFO: Got endpoints: latency-svc-59s8t [748.467ms]
Jan 17 05:24:09.887: INFO: Created: latency-svc-v5lcg
Jan 17 05:24:09.908: INFO: Got endpoints: latency-svc-qmscg [746.609455ms]
Jan 17 05:24:09.938: INFO: Created: latency-svc-tjks6
Jan 17 05:24:09.958: INFO: Got endpoints: latency-svc-9xlh5 [749.701631ms]
Jan 17 05:24:10.003: INFO: Created: latency-svc-2p7xn
Jan 17 05:24:10.010: INFO: Got endpoints: latency-svc-zccwr [758.209451ms]
Jan 17 05:24:10.035: INFO: Created: latency-svc-5vz4s
Jan 17 05:24:10.069: INFO: Got endpoints: latency-svc-z62hj [758.440182ms]
Jan 17 05:24:10.100: INFO: Created: latency-svc-5mjqf
Jan 17 05:24:10.106: INFO: Got endpoints: latency-svc-vhtlh [747.709333ms]
Jan 17 05:24:10.128: INFO: Created: latency-svc-h494h
Jan 17 05:24:10.157: INFO: Got endpoints: latency-svc-6d8rk [739.80235ms]
Jan 17 05:24:10.177: INFO: Created: latency-svc-lpxjq
Jan 17 05:24:10.212: INFO: Got endpoints: latency-svc-vwg4w [752.49813ms]
Jan 17 05:24:10.241: INFO: Created: latency-svc-vkwgk
Jan 17 05:24:10.271: INFO: Got endpoints: latency-svc-xfh2l [740.969819ms]
Jan 17 05:24:10.307: INFO: Got endpoints: latency-svc-hpspt [745.043531ms]
Jan 17 05:24:10.324: INFO: Created: latency-svc-ttwv8
Jan 17 05:24:10.343: INFO: Created: latency-svc-nlw54
Jan 17 05:24:10.357: INFO: Got endpoints: latency-svc-k7js4 [745.152402ms]
Jan 17 05:24:10.395: INFO: Created: latency-svc-sj4n8
Jan 17 05:24:10.414: INFO: Got endpoints: latency-svc-728z2 [756.760383ms]
Jan 17 05:24:10.439: INFO: Created: latency-svc-qjnqj
Jan 17 05:24:10.454: INFO: Got endpoints: latency-svc-cfmn4 [751.108355ms]
Jan 17 05:24:10.482: INFO: Created: latency-svc-dbzz2
Jan 17 05:24:10.504: INFO: Got endpoints: latency-svc-44gcr [748.03522ms]
Jan 17 05:24:10.530: INFO: Created: latency-svc-pb6vk
Jan 17 05:24:10.562: INFO: Got endpoints: latency-svc-vr9z8 [731.988287ms]
Jan 17 05:24:10.612: INFO: Created: latency-svc-9nk7g
Jan 17 05:24:10.616: INFO: Got endpoints: latency-svc-v5lcg [749.971772ms]
Jan 17 05:24:10.636: INFO: Created: latency-svc-5dncw
Jan 17 05:24:10.652: INFO: Got endpoints: latency-svc-tjks6 [744.077054ms]
Jan 17 05:24:10.676: INFO: Created: latency-svc-7cj2h
Jan 17 05:24:10.705: INFO: Got endpoints: latency-svc-2p7xn [746.735501ms]
Jan 17 05:24:10.723: INFO: Created: latency-svc-c7tm5
Jan 17 05:24:10.759: INFO: Got endpoints: latency-svc-5vz4s [748.799473ms]
Jan 17 05:24:10.779: INFO: Created: latency-svc-xjrtn
Jan 17 05:24:10.825: INFO: Got endpoints: latency-svc-5mjqf [755.718442ms]
Jan 17 05:24:10.849: INFO: Created: latency-svc-6w7hj
Jan 17 05:24:10.858: INFO: Got endpoints: latency-svc-h494h [751.527661ms]
Jan 17 05:24:10.880: INFO: Created: latency-svc-db9mh
Jan 17 05:24:10.903: INFO: Got endpoints: latency-svc-lpxjq [745.790708ms]
Jan 17 05:24:10.932: INFO: Created: latency-svc-m878r
Jan 17 05:24:10.978: INFO: Got endpoints: latency-svc-vkwgk [765.930619ms]
Jan 17 05:24:11.004: INFO: Created: latency-svc-42fqk
Jan 17 05:24:11.010: INFO: Got endpoints: latency-svc-ttwv8 [739.535739ms]
Jan 17 05:24:11.037: INFO: Created: latency-svc-2qmkf
Jan 17 05:24:11.049: INFO: Got endpoints: latency-svc-nlw54 [741.569919ms]
Jan 17 05:24:11.106: INFO: Got endpoints: latency-svc-sj4n8 [748.767223ms]
Jan 17 05:24:11.116: INFO: Created: latency-svc-gcpq5
Jan 17 05:24:11.139: INFO: Created: latency-svc-hwchc
Jan 17 05:24:11.154: INFO: Got endpoints: latency-svc-qjnqj [739.339879ms]
Jan 17 05:24:11.182: INFO: Created: latency-svc-zls47
Jan 17 05:24:11.200: INFO: Got endpoints: latency-svc-dbzz2 [745.904095ms]
Jan 17 05:24:11.226: INFO: Created: latency-svc-52gzf
Jan 17 05:24:11.252: INFO: Got endpoints: latency-svc-pb6vk [747.559224ms]
Jan 17 05:24:11.276: INFO: Created: latency-svc-fck6c
Jan 17 05:24:11.305: INFO: Got endpoints: latency-svc-9nk7g [743.211852ms]
Jan 17 05:24:11.323: INFO: Created: latency-svc-7dfx5
Jan 17 05:24:11.356: INFO: Got endpoints: latency-svc-5dncw [739.714755ms]
Jan 17 05:24:11.375: INFO: Created: latency-svc-6nrhn
Jan 17 05:24:11.437: INFO: Got endpoints: latency-svc-7cj2h [784.452876ms]
Jan 17 05:24:11.462: INFO: Created: latency-svc-nw5kp
Jan 17 05:24:11.504: INFO: Got endpoints: latency-svc-c7tm5 [799.429302ms]
Jan 17 05:24:11.523: INFO: Created: latency-svc-gpv78
Jan 17 05:24:11.548: INFO: Got endpoints: latency-svc-xjrtn [789.605506ms]
Jan 17 05:24:11.561: INFO: Got endpoints: latency-svc-6w7hj [735.774125ms]
Jan 17 05:24:11.583: INFO: Created: latency-svc-tdwnq
Jan 17 05:24:11.602: INFO: Created: latency-svc-8t6q6
Jan 17 05:24:11.611: INFO: Got endpoints: latency-svc-db9mh [753.380673ms]
Jan 17 05:24:11.636: INFO: Created: latency-svc-8zq88
Jan 17 05:24:11.654: INFO: Got endpoints: latency-svc-m878r [751.076599ms]
Jan 17 05:24:11.682: INFO: Created: latency-svc-gbqmr
Jan 17 05:24:11.703: INFO: Got endpoints: latency-svc-42fqk [724.915798ms]
Jan 17 05:24:11.730: INFO: Created: latency-svc-cmz8v
Jan 17 05:24:11.753: INFO: Got endpoints: latency-svc-2qmkf [742.979973ms]
Jan 17 05:24:11.796: INFO: Created: latency-svc-dbgk8
Jan 17 05:24:11.798: INFO: Got endpoints: latency-svc-gcpq5 [748.834455ms]
Jan 17 05:24:11.824: INFO: Created: latency-svc-ls8p8
Jan 17 05:24:11.856: INFO: Got endpoints: latency-svc-hwchc [750.38816ms]
Jan 17 05:24:11.880: INFO: Created: latency-svc-2js9d
Jan 17 05:24:11.905: INFO: Got endpoints: latency-svc-zls47 [751.451022ms]
Jan 17 05:24:11.936: INFO: Created: latency-svc-ls5mh
Jan 17 05:24:11.951: INFO: Got endpoints: latency-svc-52gzf [751.387039ms]
Jan 17 05:24:11.972: INFO: Created: latency-svc-wnnzw
Jan 17 05:24:12.010: INFO: Got endpoints: latency-svc-fck6c [757.194516ms]
Jan 17 05:24:12.031: INFO: Created: latency-svc-xkwqr
Jan 17 05:24:12.053: INFO: Got endpoints: latency-svc-7dfx5 [748.541875ms]
Jan 17 05:24:12.076: INFO: Created: latency-svc-4q92w
Jan 17 05:24:12.101: INFO: Got endpoints: latency-svc-6nrhn [744.789846ms]
Jan 17 05:24:12.125: INFO: Created: latency-svc-sldw2
Jan 17 05:24:12.154: INFO: Got endpoints: latency-svc-nw5kp [716.660943ms]
Jan 17 05:24:12.170: INFO: Created: latency-svc-42sxd
Jan 17 05:24:12.211: INFO: Got endpoints: latency-svc-gpv78 [707.061013ms]
Jan 17 05:24:12.253: INFO: Created: latency-svc-28lhg
Jan 17 05:24:12.258: INFO: Got endpoints: latency-svc-tdwnq [709.210674ms]
Jan 17 05:24:12.286: INFO: Created: latency-svc-lmdlz
Jan 17 05:24:12.312: INFO: Got endpoints: latency-svc-8t6q6 [750.86696ms]
Jan 17 05:24:12.341: INFO: Created: latency-svc-sdkhr
Jan 17 05:24:12.362: INFO: Got endpoints: latency-svc-8zq88 [750.467165ms]
Jan 17 05:24:12.399: INFO: Created: latency-svc-6mngs
Jan 17 05:24:12.417: INFO: Got endpoints: latency-svc-gbqmr [763.068797ms]
Jan 17 05:24:12.440: INFO: Created: latency-svc-2hvjg
Jan 17 05:24:12.454: INFO: Got endpoints: latency-svc-cmz8v [751.43308ms]
Jan 17 05:24:12.476: INFO: Created: latency-svc-7jbcd
Jan 17 05:24:12.503: INFO: Got endpoints: latency-svc-dbgk8 [749.746202ms]
Jan 17 05:24:12.531: INFO: Created: latency-svc-9t7kb
Jan 17 05:24:12.556: INFO: Got endpoints: latency-svc-ls8p8 [757.504535ms]
Jan 17 05:24:12.581: INFO: Created: latency-svc-x2958
Jan 17 05:24:12.610: INFO: Got endpoints: latency-svc-2js9d [753.82782ms]
Jan 17 05:24:12.635: INFO: Created: latency-svc-zqgtz
Jan 17 05:24:12.661: INFO: Got endpoints: latency-svc-ls5mh [756.069553ms]
Jan 17 05:24:12.682: INFO: Created: latency-svc-jh2cw
Jan 17 05:24:12.708: INFO: Got endpoints: latency-svc-wnnzw [756.563537ms]
Jan 17 05:24:12.730: INFO: Created: latency-svc-957p2
Jan 17 05:24:12.765: INFO: Got endpoints: latency-svc-xkwqr [754.939345ms]
Jan 17 05:24:12.790: INFO: Created: latency-svc-nhkql
Jan 17 05:24:12.807: INFO: Got endpoints: latency-svc-4q92w [753.786361ms]
Jan 17 05:24:12.833: INFO: Created: latency-svc-86vgg
Jan 17 05:24:12.859: INFO: Got endpoints: latency-svc-sldw2 [757.486419ms]
Jan 17 05:24:12.883: INFO: Created: latency-svc-wctsq
Jan 17 05:24:12.906: INFO: Got endpoints: latency-svc-42sxd [751.691729ms]
Jan 17 05:24:12.929: INFO: Created: latency-svc-4gq2q
Jan 17 05:24:12.950: INFO: Got endpoints: latency-svc-28lhg [738.98479ms]
Jan 17 05:24:12.977: INFO: Created: latency-svc-mhzvz
Jan 17 05:24:13.008: INFO: Got endpoints: latency-svc-lmdlz [750.094342ms]
Jan 17 05:24:13.028: INFO: Created: latency-svc-72s8l
Jan 17 05:24:13.057: INFO: Got endpoints: latency-svc-sdkhr [745.166112ms]
Jan 17 05:24:13.083: INFO: Created: latency-svc-d8rvt
Jan 17 05:24:13.105: INFO: Got endpoints: latency-svc-6mngs [743.614763ms]
Jan 17 05:24:13.131: INFO: Created: latency-svc-4bx8v
Jan 17 05:24:13.162: INFO: Got endpoints: latency-svc-2hvjg [744.211437ms]
Jan 17 05:24:13.187: INFO: Created: latency-svc-76jfm
Jan 17 05:24:13.204: INFO: Got endpoints: latency-svc-7jbcd [749.370953ms]
Jan 17 05:24:13.223: INFO: Created: latency-svc-dfknz
Jan 17 05:24:13.267: INFO: Got endpoints: latency-svc-9t7kb [763.692427ms]
Jan 17 05:24:13.285: INFO: Created: latency-svc-sjd2l
Jan 17 05:24:13.306: INFO: Got endpoints: latency-svc-x2958 [750.753735ms]
Jan 17 05:24:13.326: INFO: Created: latency-svc-hzzzg
Jan 17 05:24:13.355: INFO: Got endpoints: latency-svc-zqgtz [744.983813ms]
Jan 17 05:24:13.387: INFO: Created: latency-svc-854l5
Jan 17 05:24:13.419: INFO: Got endpoints: latency-svc-jh2cw [757.369413ms]
Jan 17 05:24:13.438: INFO: Created: latency-svc-pnczz
Jan 17 05:24:13.455: INFO: Got endpoints: latency-svc-957p2 [746.984152ms]
Jan 17 05:24:13.490: INFO: Created: latency-svc-rljvb
Jan 17 05:24:13.510: INFO: Got endpoints: latency-svc-nhkql [744.983288ms]
Jan 17 05:24:13.534: INFO: Created: latency-svc-m5xhp
Jan 17 05:24:13.562: INFO: Got endpoints: latency-svc-86vgg [755.089404ms]
Jan 17 05:24:13.587: INFO: Created: latency-svc-7pjs2
Jan 17 05:24:13.597: INFO: Got endpoints: latency-svc-wctsq [738.695962ms]
Jan 17 05:24:13.628: INFO: Created: latency-svc-b5r4v
Jan 17 05:24:13.658: INFO: Got endpoints: latency-svc-4gq2q [752.040738ms]
Jan 17 05:24:13.676: INFO: Created: latency-svc-m5zdr
Jan 17 05:24:13.703: INFO: Got endpoints: latency-svc-mhzvz [752.632385ms]
Jan 17 05:24:13.725: INFO: Created: latency-svc-5xzk6
Jan 17 05:24:13.757: INFO: Got endpoints: latency-svc-72s8l [749.411861ms]
Jan 17 05:24:13.777: INFO: Created: latency-svc-dk6jg
Jan 17 05:24:13.810: INFO: Got endpoints: latency-svc-d8rvt [753.190577ms]
Jan 17 05:24:13.832: INFO: Created: latency-svc-tglh8
Jan 17 05:24:13.861: INFO: Got endpoints: latency-svc-4bx8v [755.32683ms]
Jan 17 05:24:13.902: INFO: Created: latency-svc-kz4rt
Jan 17 05:24:13.912: INFO: Got endpoints: latency-svc-76jfm [750.502352ms]
Jan 17 05:24:13.945: INFO: Created: latency-svc-xcvbf
Jan 17 05:24:13.961: INFO: Got endpoints: latency-svc-dfknz [757.671479ms]
Jan 17 05:24:13.987: INFO: Created: latency-svc-sdp86
Jan 17 05:24:14.011: INFO: Got endpoints: latency-svc-sjd2l [744.050689ms]
Jan 17 05:24:14.033: INFO: Created: latency-svc-bdvdw
Jan 17 05:24:14.051: INFO: Got endpoints: latency-svc-hzzzg [744.904368ms]
Jan 17 05:24:14.077: INFO: Created: latency-svc-444zj
Jan 17 05:24:14.104: INFO: Got endpoints: latency-svc-854l5 [748.295074ms]
Jan 17 05:24:14.125: INFO: Created: latency-svc-p92gw
Jan 17 05:24:14.162: INFO: Got endpoints: latency-svc-pnczz [743.025311ms]
Jan 17 05:24:14.183: INFO: Created: latency-svc-7gkc9
Jan 17 05:24:14.208: INFO: Got endpoints: latency-svc-rljvb [752.682471ms]
Jan 17 05:24:14.241: INFO: Created: latency-svc-d8g8k
Jan 17 05:24:14.280: INFO: Got endpoints: latency-svc-m5xhp [770.054876ms]
Jan 17 05:24:14.319: INFO: Got endpoints: latency-svc-7pjs2 [756.984375ms]
Jan 17 05:24:14.358: INFO: Got endpoints: latency-svc-b5r4v [760.440172ms]
Jan 17 05:24:14.398: INFO: Got endpoints: latency-svc-m5zdr [740.6361ms]
Jan 17 05:24:14.455: INFO: Got endpoints: latency-svc-5xzk6 [752.150692ms]
Jan 17 05:24:14.507: INFO: Got endpoints: latency-svc-dk6jg [749.950164ms]
Jan 17 05:24:14.560: INFO: Got endpoints: latency-svc-tglh8 [749.503527ms]
Jan 17 05:24:14.611: INFO: Got endpoints: latency-svc-kz4rt [749.780387ms]
Jan 17 05:24:14.664: INFO: Got endpoints: latency-svc-xcvbf [751.802456ms]
Jan 17 05:24:14.698: INFO: Got endpoints: latency-svc-sdp86 [736.269904ms]
Jan 17 05:24:14.752: INFO: Got endpoints: latency-svc-bdvdw [741.243754ms]
Jan 17 05:24:14.806: INFO: Got endpoints: latency-svc-444zj [754.475808ms]
Jan 17 05:24:14.860: INFO: Got endpoints: latency-svc-p92gw [756.381864ms]
Jan 17 05:24:14.898: INFO: Got endpoints: latency-svc-7gkc9 [736.185894ms]
Jan 17 05:24:14.959: INFO: Got endpoints: latency-svc-d8g8k [750.73302ms]
Jan 17 05:24:14.959: INFO: Latencies: [112.420498ms 161.552016ms 162.768803ms 172.306408ms 247.333862ms 251.766871ms 254.27738ms 255.698012ms 275.154976ms 276.272993ms 283.258048ms 293.678223ms 294.797677ms 299.477975ms 299.555466ms 299.722288ms 301.123103ms 304.006499ms 314.475947ms 314.809916ms 315.236293ms 315.274259ms 315.922326ms 316.999739ms 320.156989ms 320.620477ms 321.447752ms 330.553429ms 333.96821ms 335.403298ms 335.844921ms 337.200335ms 339.141698ms 341.152293ms 346.729799ms 350.575195ms 350.616545ms 354.151548ms 355.8024ms 356.597157ms 358.679818ms 364.992469ms 368.554228ms 371.128089ms 372.620324ms 373.986413ms 378.002937ms 379.386864ms 385.156587ms 388.084433ms 390.825013ms 390.983352ms 395.548774ms 411.239358ms 413.374401ms 418.543908ms 426.462448ms 451.852168ms 474.596896ms 487.399798ms 498.045247ms 527.007627ms 545.926123ms 549.362675ms 612.202887ms 628.929985ms 648.638374ms 659.575228ms 661.815101ms 698.616294ms 698.686589ms 707.061013ms 709.210674ms 713.43142ms 716.660943ms 724.915798ms 725.669988ms 728.058187ms 731.988287ms 734.570319ms 735.22786ms 735.774125ms 736.185894ms 736.269904ms 737.113819ms 737.217037ms 738.695962ms 738.98479ms 739.339879ms 739.535739ms 739.714755ms 739.80235ms 740.030039ms 740.6361ms 740.969819ms 741.243754ms 741.569919ms 742.979973ms 743.025311ms 743.211852ms 743.367137ms 743.614763ms 744.050689ms 744.077054ms 744.124867ms 744.211437ms 744.789846ms 744.904368ms 744.983288ms 744.983813ms 745.043531ms 745.126253ms 745.152402ms 745.166112ms 745.790708ms 745.904095ms 746.609455ms 746.735501ms 746.984152ms 747.559224ms 747.709333ms 747.787221ms 748.03522ms 748.295074ms 748.467ms 748.541875ms 748.763109ms 748.767223ms 748.799473ms 748.834455ms 749.11457ms 749.370953ms 749.411861ms 749.503527ms 749.701631ms 749.746202ms 749.780387ms 749.950164ms 749.971772ms 750.094342ms 750.38816ms 750.467165ms 750.4994ms 750.502352ms 750.73302ms 750.753735ms 750.86696ms 751.076599ms 751.108355ms 751.387039ms 751.43308ms 751.451022ms 751.522842ms 751.527661ms 751.691729ms 751.802456ms 752.040738ms 752.150692ms 752.4797ms 752.49813ms 752.632385ms 752.682471ms 753.190577ms 753.380673ms 753.786361ms 753.82782ms 754.475808ms 754.569478ms 754.939345ms 755.089404ms 755.32683ms 755.718442ms 755.95236ms 756.069553ms 756.381864ms 756.563537ms 756.760383ms 756.984375ms 757.194516ms 757.369413ms 757.486419ms 757.504535ms 757.671479ms 758.209451ms 758.440182ms 759.904682ms 760.440172ms 762.115177ms 762.344323ms 763.068797ms 763.173286ms 763.692427ms 765.930619ms 769.75033ms 770.054876ms 778.52911ms 784.452876ms 789.605506ms 799.429302ms 832.368116ms]
Jan 17 05:24:14.959: INFO: 50 %ile: 743.367137ms
Jan 17 05:24:14.959: INFO: 90 %ile: 757.486419ms
Jan 17 05:24:14.959: INFO: 99 %ile: 799.429302ms
Jan 17 05:24:14.959: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jan 17 05:24:14.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6109" for this suite.

• [SLOW TEST:13.037 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":278,"skipped":5081,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 17 05:24:15.099: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:15.099: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:15.099: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:15.108: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:24:15.108: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 05:24:16.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:16.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:16.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:16.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:24:16.124: INFO: Node cluster124-apihrjet4zqi-node-0 is running 0 daemon pod, expected 1
Jan 17 05:24:17.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:17.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:17.119: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:17.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 17 05:24:17.126: INFO: Node cluster124-apihrjet4zqi-node-1 is running 0 daemon pod, expected 1
Jan 17 05:24:18.117: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:18.117: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:18.117: INFO: DaemonSet pods can't tolerate node cluster124-apihrjet4zqi-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 17 05:24:18.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 17 05:24:18.122: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 17 05:24:18.162: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"49640"},"items":null}

Jan 17 05:24:18.166: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"49640"},"items":[{"metadata":{"name":"daemon-set-97vxw","generateName":"daemon-set-","namespace":"daemonsets-8245","uid":"76717814-617c-4397-a63c-632d104cef27","resourceVersion":"49537","creationTimestamp":"2023-01-17T05:24:15Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"92d52b54683402755dd1e91039044dd31fb83f58fdd59a8f5e3b4632370801c6","cni.projectcalico.org/podIP":"10.100.11.244/32","cni.projectcalico.org/podIPs":"10.100.11.244/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"06548a0c-bc5c-4bcc-b400-99378838692d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06548a0c-bc5c-4bcc-b400-99378838692d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-q7r5p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-q7r5p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster124-apihrjet4zqi-node-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster124-apihrjet4zqi-node-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"}],"hostIP":"10.0.0.9","podIP":"10.100.11.244","podIPs":[{"ip":"10.100.11.244"}],"startTime":"2023-01-17T05:24:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T05:24:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f004296a9616450c57eca88ee9400774a6aa4612c7a661de07ef43681374713f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-r4rw9","generateName":"daemon-set-","namespace":"daemonsets-8245","uid":"8c3d7fbf-a51a-4d36-a249-19ba88e96407","resourceVersion":"49559","creationTimestamp":"2023-01-17T05:24:15Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ed72a9c6258363f2322b3ae5d11417c2195417d62cb9f46945299d3bac46f9b6","cni.projectcalico.org/podIP":"10.100.106.93/32","cni.projectcalico.org/podIPs":"10.100.106.93/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"06548a0c-bc5c-4bcc-b400-99378838692d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06548a0c-bc5c-4bcc-b400-99378838692d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.106.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-8rf4k","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-8rf4k","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster124-apihrjet4zqi-node-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster124-apihrjet4zqi-node-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:16Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:16Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"}],"hostIP":"10.0.0.16","podIP":"10.100.106.93","podIPs":[{"ip":"10.100.106.93"}],"startTime":"2023-01-17T05:24:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T05:24:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6347ec3b30f8fa1ec9bcc67853d823c9b293dfe3318d1237a851aaff0b86040c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-wsjt9","generateName":"daemon-set-","namespace":"daemonsets-8245","uid":"39248da7-9565-403c-8f1a-70acb241f0de","resourceVersion":"49604","creationTimestamp":"2023-01-17T05:24:15Z","labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"15f2955cbad21ab63a12ac8b00bddd00662b48fddcffe886801c8e4494562345","cni.projectcalico.org/podIP":"10.100.44.228/32","cni.projectcalico.org/podIPs":"10.100.44.228/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"06548a0c-bc5c-4bcc-b400-99378838692d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:15Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06548a0c-bc5c-4bcc-b400-99378838692d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:16Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-17T05:24:17Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.44.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9s7vc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9s7vc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"cluster124-apihrjet4zqi-node-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["cluster124-apihrjet4zqi-node-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:17Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:17Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-17T05:24:15Z"}],"hostIP":"10.0.0.21","podIP":"10.100.44.228","podIPs":[{"ip":"10.100.44.228"}],"startTime":"2023-01-17T05:24:15Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-17T05:24:16Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://73b86713ce979fb6926afc41486be107ed1b830748745a781b6d47e2db035fe6","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:24:18.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8245" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":279,"skipped":5092,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:18.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:24:18.320: INFO: Endpoints addresses: [10.0.0.12 10.0.0.17 10.0.0.19] , ports: [6443]
Jan 17 05:24:18.320: INFO: EndpointSlices addresses: [10.0.0.12 10.0.0.17 10.0.0.19] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 17 05:24:18.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2316" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":280,"skipped":5108,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:24:18.379: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:24:21.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5237" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":281,"skipped":5128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:21.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-133b9697-b28d-4f7f-9da4-913431fa86e8
STEP: Creating a pod to test consume configMaps
Jan 17 05:24:21.767: INFO: Waiting up to 5m0s for pod "pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf" in namespace "configmap-6761" to be "Succeeded or Failed"
Jan 17 05:24:21.775: INFO: Pod "pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.955009ms
Jan 17 05:24:23.789: INFO: Pod "pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022018745s
Jan 17 05:24:25.799: INFO: Pod "pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031329133s
STEP: Saw pod success
Jan 17 05:24:25.799: INFO: Pod "pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf" satisfied condition "Succeeded or Failed"
Jan 17 05:24:25.811: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:24:25.860: INFO: Waiting for pod pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf to disappear
Jan 17 05:24:25.875: INFO: Pod pod-configmaps-892a74c8-c564-41a7-b798-c31c3c36e6bf no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:24:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6761" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":282,"skipped":5162,"failed":0}

------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-4473
STEP: creating service affinity-nodeport in namespace services-4473
STEP: creating replication controller affinity-nodeport in namespace services-4473
I0117 05:24:26.020366      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4473, replica count: 3
I0117 05:24:29.071332      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:24:29.107: INFO: Creating new exec pod
Jan 17 05:24:36.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4473 exec execpod-affinityjsskj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 17 05:24:36.536: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 17 05:24:36.536: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:24:36.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4473 exec execpod-affinityjsskj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.170.136 80'
Jan 17 05:24:36.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.170.136 80\nConnection to 10.254.170.136 80 port [tcp/http] succeeded!\n"
Jan 17 05:24:36.805: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:24:36.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4473 exec execpod-affinityjsskj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.9 32024'
Jan 17 05:24:37.080: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.9 32024\nConnection to 10.0.0.9 32024 port [tcp/*] succeeded!\n"
Jan 17 05:24:37.080: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:24:37.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4473 exec execpod-affinityjsskj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 32024'
Jan 17 05:24:37.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 32024\nConnection to 10.0.0.16 32024 port [tcp/*] succeeded!\n"
Jan 17 05:24:37.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:24:37.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-4473 exec execpod-affinityjsskj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:32024/ ; done'
Jan 17 05:24:37.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:32024/\n"
Jan 17 05:24:37.781: INFO: stdout: "\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm\naffinity-nodeport-92pdm"
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Received response from host: affinity-nodeport-92pdm
Jan 17 05:24:37.781: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4473, will wait for the garbage collector to delete the pods
Jan 17 05:24:37.889: INFO: Deleting ReplicationController affinity-nodeport took: 11.371263ms
Jan 17 05:24:37.989: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.656667ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:24:40.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4473" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:14.573 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":283,"skipped":5162,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:40.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 17 05:24:42.618: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 17 05:24:44.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7246" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":284,"skipped":5168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:44.648: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7441
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7441
I0117 05:24:44.765396      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7441, replica count: 2
Jan 17 05:24:47.817: INFO: Creating new exec pod
I0117 05:24:47.817635      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:24:50.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7441 exec execpod4qxtg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 05:24:51.178: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 05:24:51.178: INFO: stdout: ""
Jan 17 05:24:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7441 exec execpod4qxtg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 05:24:52.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 05:24:52.428: INFO: stdout: "externalname-service-9kfj5"
Jan 17 05:24:52.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7441 exec execpod4qxtg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.207.189 80'
Jan 17 05:24:52.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.207.189 80\nConnection to 10.254.207.189 80 port [tcp/http] succeeded!\n"
Jan 17 05:24:52.702: INFO: stdout: "externalname-service-w4q8f"
Jan 17 05:24:52.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7441 exec execpod4qxtg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.16 31939'
Jan 17 05:24:53.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.16 31939\nConnection to 10.0.0.16 31939 port [tcp/*] succeeded!\n"
Jan 17 05:24:53.040: INFO: stdout: "externalname-service-9kfj5"
Jan 17 05:24:53.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-7441 exec execpod4qxtg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.9 31939'
Jan 17 05:24:53.274: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.9 31939\nConnection to 10.0.0.9 31939 port [tcp/*] succeeded!\n"
Jan 17 05:24:53.274: INFO: stdout: "externalname-service-w4q8f"
Jan 17 05:24:53.274: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:24:53.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7441" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.706 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":285,"skipped":5216,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:24:53.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jan 17 05:24:53.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 17 05:24:53.584: INFO: stderr: ""
Jan 17 05:24:53.584: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jan 17 05:24:53.584: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 17 05:24:53.584: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2276" to be "running and ready, or succeeded"
Jan 17 05:24:53.596: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.371403ms
Jan 17 05:24:55.603: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018806382s
Jan 17 05:24:57.611: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.027537924s
Jan 17 05:24:57.611: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 17 05:24:57.611: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 17 05:24:57.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator'
Jan 17 05:24:57.769: INFO: stderr: ""
Jan 17 05:24:57.769: INFO: stdout: "I0117 05:24:54.710906       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/b6x 452\nI0117 05:24:54.911070       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wk8 480\nI0117 05:24:55.111759       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/xvqq 238\nI0117 05:24:55.311083       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/vks 400\nI0117 05:24:55.511506       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/pmc 277\nI0117 05:24:55.711920       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/cbd 484\nI0117 05:24:55.911335       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/gqn 403\nI0117 05:24:56.111726       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/kjz 590\nI0117 05:24:56.311042       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/jjv 329\nI0117 05:24:56.511500       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9zk 587\nI0117 05:24:56.711990       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/ck9w 218\nI0117 05:24:56.911348       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/9cc7 252\nI0117 05:24:57.111746       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8xdf 410\nI0117 05:24:57.311952       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/88s 369\nI0117 05:24:57.511358       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gj8s 361\nI0117 05:24:57.711560       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dxq 577\n"
STEP: limiting log lines
Jan 17 05:24:57.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator --tail=1'
Jan 17 05:24:57.857: INFO: stderr: ""
Jan 17 05:24:57.857: INFO: stdout: "I0117 05:24:57.711560       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dxq 577\n"
Jan 17 05:24:57.857: INFO: got output "I0117 05:24:57.711560       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dxq 577\n"
STEP: limiting log bytes
Jan 17 05:24:57.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator --limit-bytes=1'
Jan 17 05:24:57.946: INFO: stderr: ""
Jan 17 05:24:57.946: INFO: stdout: "I"
Jan 17 05:24:57.946: INFO: got output "I"
STEP: exposing timestamps
Jan 17 05:24:57.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 17 05:24:58.041: INFO: stderr: ""
Jan 17 05:24:58.041: INFO: stdout: "2023-01-17T05:24:57.912239911Z I0117 05:24:57.912012       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/5jm2 402\n"
Jan 17 05:24:58.041: INFO: got output "2023-01-17T05:24:57.912239911Z I0117 05:24:57.912012       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/5jm2 402\n"
STEP: restricting to a time range
Jan 17 05:25:00.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator --since=1s'
Jan 17 05:25:00.798: INFO: stderr: ""
Jan 17 05:25:00.798: INFO: stdout: "I0117 05:24:59.911568       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/5gsq 573\nI0117 05:25:00.111692       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/kdf6 555\nI0117 05:25:00.312808       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/d4vj 555\nI0117 05:25:00.511146       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/jds2 369\nI0117 05:25:00.711500       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/jmt 309\n"
Jan 17 05:25:00.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 logs logs-generator logs-generator --since=24h'
Jan 17 05:25:00.916: INFO: stderr: ""
Jan 17 05:25:00.916: INFO: stdout: "I0117 05:24:54.710906       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/b6x 452\nI0117 05:24:54.911070       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/wk8 480\nI0117 05:24:55.111759       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/xvqq 238\nI0117 05:24:55.311083       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/vks 400\nI0117 05:24:55.511506       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/pmc 277\nI0117 05:24:55.711920       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/cbd 484\nI0117 05:24:55.911335       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/gqn 403\nI0117 05:24:56.111726       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/kjz 590\nI0117 05:24:56.311042       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/jjv 329\nI0117 05:24:56.511500       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/9zk 587\nI0117 05:24:56.711990       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/ck9w 218\nI0117 05:24:56.911348       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/default/pods/9cc7 252\nI0117 05:24:57.111746       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/8xdf 410\nI0117 05:24:57.311952       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/88s 369\nI0117 05:24:57.511358       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/gj8s 361\nI0117 05:24:57.711560       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/dxq 577\nI0117 05:24:57.912012       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/5jm2 402\nI0117 05:24:58.111483       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/6qp 530\nI0117 05:24:58.311906       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/gf7 221\nI0117 05:24:58.511312       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/j8mp 234\nI0117 05:24:58.711554       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/lcrx 400\nI0117 05:24:58.911807       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/dbxz 334\nI0117 05:24:59.111103       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/9tqf 596\nI0117 05:24:59.311506       1 logs_generator.go:76] 23 POST /api/v1/namespaces/ns/pods/ccs 342\nI0117 05:24:59.511967       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/7rc7 398\nI0117 05:24:59.711363       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/kube-system/pods/fch 403\nI0117 05:24:59.911568       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/5gsq 573\nI0117 05:25:00.111692       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/kdf6 555\nI0117 05:25:00.312808       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/default/pods/d4vj 555\nI0117 05:25:00.511146       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/jds2 369\nI0117 05:25:00.711500       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/jmt 309\nI0117 05:25:00.911903       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/default/pods/5d4 532\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jan 17 05:25:00.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2276 delete pod logs-generator'
Jan 17 05:25:01.696: INFO: stderr: ""
Jan 17 05:25:01.696: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:25:01.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2276" for this suite.

• [SLOW TEST:8.360 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":286,"skipped":5228,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:01.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 17 05:25:01.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6655" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":287,"skipped":5233,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:01.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:25:02.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:25:05.570: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:25:05.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4198" for this suite.
STEP: Destroying namespace "webhook-4198-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":288,"skipped":5241,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:05.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:25:06.579: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 17 05:25:08.597: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 17, 5, 25, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 25, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 25, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 25, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:25:11.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
Jan 17 05:25:12.719: INFO: Waiting for webhook configuration to be ready...
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:25:24.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7660" for this suite.
STEP: Destroying namespace "webhook-7660-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:18.176 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":289,"skipped":5255,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:24.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 17 05:25:24.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8198 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 05:25:24.437: INFO: stderr: ""
Jan 17 05:25:24.437: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 17 05:25:24.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8198 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 17 05:25:25.825: INFO: stderr: ""
Jan 17 05:25:25.825: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 17 05:25:25.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-8198 delete pods e2e-test-httpd-pod'
Jan 17 05:25:28.837: INFO: stderr: ""
Jan 17 05:25:28.837: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:25:28.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8198" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":290,"skipped":5274,"failed":0}

------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:28.864: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 17 05:25:28.940: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:25:30.947: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 17 05:25:30.969: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:25:32.978: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 17 05:25:33.066: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 05:25:33.071: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 05:25:35.071: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 05:25:35.078: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 17 05:25:37.073: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 17 05:25:37.078: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 17 05:25:37.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1767" for this suite.

• [SLOW TEST:8.233 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":291,"skipped":5274,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0117 05:25:38.243521      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 05:25:38.243: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 05:25:38.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-525" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":292,"skipped":5332,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:38.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-98775806-a3f4-4984-8c2e-3450aa593086
STEP: Creating a pod to test consume secrets
Jan 17 05:25:38.371: INFO: Waiting up to 5m0s for pod "pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501" in namespace "secrets-4535" to be "Succeeded or Failed"
Jan 17 05:25:38.379: INFO: Pod "pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501": Phase="Pending", Reason="", readiness=false. Elapsed: 7.52859ms
Jan 17 05:25:40.391: INFO: Pod "pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019654249s
Jan 17 05:25:42.397: INFO: Pod "pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025789081s
STEP: Saw pod success
Jan 17 05:25:42.397: INFO: Pod "pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501" satisfied condition "Succeeded or Failed"
Jan 17 05:25:42.409: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501 container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:25:42.443: INFO: Waiting for pod pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501 to disappear
Jan 17 05:25:42.451: INFO: Pod pod-secrets-01ea5f1c-9a84-4daf-bc37-e4d73c273501 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:25:42.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4535" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5340,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:42.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0117 05:25:43.606391      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 17 05:25:43.606: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 17 05:25:43.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3203" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":294,"skipped":5358,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:43.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 17 05:25:43.708: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:25:45.716: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 17 05:25:45.736: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:25:47.742: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 17 05:25:47.758: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 05:25:47.764: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 05:25:49.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 05:25:49.773: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 17 05:25:51.765: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 17 05:25:51.775: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 17 05:25:51.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-365" for this suite.

• [SLOW TEST:8.184 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":295,"skipped":5366,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:51.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:25:51.890: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0" in namespace "projected-3158" to be "Succeeded or Failed"
Jan 17 05:25:51.896: INFO: Pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.128469ms
Jan 17 05:25:53.907: INFO: Pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017077846s
Jan 17 05:25:55.918: INFO: Pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028334942s
Jan 17 05:25:57.926: INFO: Pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036433478s
STEP: Saw pod success
Jan 17 05:25:57.926: INFO: Pod "downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0" satisfied condition "Succeeded or Failed"
Jan 17 05:25:57.930: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0 container client-container: <nil>
STEP: delete the pod
Jan 17 05:25:57.967: INFO: Waiting for pod downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0 to disappear
Jan 17 05:25:57.973: INFO: Pod downwardapi-volume-b3ea8791-92ba-4343-9155-3fb308d4f0a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:25:57.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3158" for this suite.

• [SLOW TEST:6.193 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":296,"skipped":5368,"failed":0}
SSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:25:58.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 17 05:26:08.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4590" for this suite.

• [SLOW TEST:10.158 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":297,"skipped":5373,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:26:08.167: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jan 17 05:26:08.236: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 17 05:26:08.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4946" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":298,"skipped":5421,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:26:08.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 17 05:26:08.300: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 17 05:26:08.313: INFO: Waiting for terminating namespaces to be deleted...
Jan 17 05:26:08.318: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-0 before test
Jan 17 05:26:08.330: INFO: calico-node-xdhq9 from kube-system started at 2023-01-17 02:35:55 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 05:26:08.330: INFO: csi-cinder-nodeplugin-tbzrq from kube-system started at 2023-01-17 02:36:15 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 05:26:08.330: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 05:26:08.330: INFO: magnum-kube-prometheus-sta-operator-675d58f6dc-bc5jf from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
Jan 17 05:26:08.330: INFO: magnum-kube-state-metrics-7645bf695b-2vs67 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 17 05:26:08.330: INFO: magnum-metrics-server-d6ddcd656-4phlx from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container metrics-server ready: true, restart count 0
Jan 17 05:26:08.330: INFO: magnum-prometheus-node-exporter-6ln7n from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 05:26:08.330: INFO: npd-8pbfg from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 05:26:08.330: INFO: prometheus-magnum-kube-prometheus-sta-prometheus-0 from kube-system started at 2023-01-17 05:13:32 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container config-reloader ready: true, restart count 0
Jan 17 05:26:08.330: INFO: 	Container prometheus ready: true, restart count 0
Jan 17 05:26:08.330: INFO: sonobuoy from sonobuoy started at 2023-01-17 04:09:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 17 05:26:08.330: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-fcb54 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.330: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 05:26:08.330: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 05:26:08.330: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-1 before test
Jan 17 05:26:08.340: INFO: calico-node-9shzb from kube-system started at 2023-01-17 02:35:49 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 05:26:08.340: INFO: csi-cinder-nodeplugin-qhmg4 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 05:26:08.340: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 05:26:08.340: INFO: kube-dns-autoscaler-5b9649896b-427d6 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container autoscaler ready: true, restart count 0
Jan 17 05:26:08.340: INFO: magnum-grafana-d59cf7df4-shfs9 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (3 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container grafana ready: true, restart count 0
Jan 17 05:26:08.340: INFO: 	Container grafana-sc-dashboard ready: true, restart count 0
Jan 17 05:26:08.340: INFO: 	Container grafana-sc-datasources ready: true, restart count 0
Jan 17 05:26:08.340: INFO: magnum-prometheus-node-exporter-sr8z8 from kube-system started at 2023-01-17 02:36:30 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 05:26:08.340: INFO: npd-b7kb8 from kube-system started at 2023-01-17 02:36:09 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 05:26:08.340: INFO: sonobuoy-e2e-job-6c6d2af065634bb1 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container e2e ready: true, restart count 0
Jan 17 05:26:08.340: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 05:26:08.340: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-lnwl6 from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.340: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 05:26:08.341: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 17 05:26:08.341: INFO: 
Logging pods the apiserver thinks is on node cluster124-apihrjet4zqi-node-2 before test
Jan 17 05:26:08.354: INFO: indexed-job-0-q57xm from job-4590 started at 2023-01-17 05:25:58 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container c ready: false, restart count 0
Jan 17 05:26:08.354: INFO: indexed-job-1-jpjdf from job-4590 started at 2023-01-17 05:25:58 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container c ready: false, restart count 0
Jan 17 05:26:08.354: INFO: indexed-job-2-rzd7r from job-4590 started at 2023-01-17 05:26:02 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container c ready: false, restart count 0
Jan 17 05:26:08.354: INFO: indexed-job-3-56zzb from job-4590 started at 2023-01-17 05:26:02 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container c ready: false, restart count 0
Jan 17 05:26:08.354: INFO: calico-node-wwxlv from kube-system started at 2023-01-17 02:36:15 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container calico-node ready: true, restart count 0
Jan 17 05:26:08.354: INFO: csi-cinder-nodeplugin-vqs2k from kube-system started at 2023-01-17 05:13:56 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
Jan 17 05:26:08.354: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jan 17 05:26:08.354: INFO: magnum-prometheus-node-exporter-swjm2 from kube-system started at 2023-01-17 05:13:56 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container node-exporter ready: true, restart count 0
Jan 17 05:26:08.354: INFO: npd-c4sq2 from kube-system started at 2023-01-17 02:36:35 +0000 UTC (1 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 17 05:26:08.354: INFO: sonobuoy-systemd-logs-daemon-set-2095de961e3e4e09-9j7nc from sonobuoy started at 2023-01-17 04:09:37 +0000 UTC (2 container statuses recorded)
Jan 17 05:26:08.354: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 17 05:26:08.354: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-61ac22cb-fe22-4c82-9120-5b7d85e4523a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.9 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-61ac22cb-fe22-4c82-9120-5b7d85e4523a off the node cluster124-apihrjet4zqi-node-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-61ac22cb-fe22-4c82-9120-5b7d85e4523a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:31:12.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5429" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.319 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":299,"skipped":5425,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:31:12.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 17 05:31:14.675: INFO: pods: 0 < 3
Jan 17 05:31:16.686: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 17 05:31:22.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-49" for this suite.

• [SLOW TEST:10.318 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":300,"skipped":5445,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:31:22.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:31:22.998: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 17 05:31:28.012: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 17 05:31:28.012: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 17 05:31:30.025: INFO: Creating deployment "test-rollover-deployment"
Jan 17 05:31:30.041: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 17 05:31:32.055: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 17 05:31:32.063: INFO: Ensure that both replica sets have 1 created replica
Jan 17 05:31:32.069: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 17 05:31:32.085: INFO: Updating deployment test-rollover-deployment
Jan 17 05:31:32.085: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 17 05:31:34.104: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 17 05:31:34.112: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 17 05:31:34.118: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 05:31:34.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:36.130: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 05:31:36.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:38.132: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 05:31:38.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:40.134: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 05:31:40.134: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:42.133: INFO: all replica sets need to contain the pod-template-hash label
Jan 17 05:31:42.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:44.139: INFO: 
Jan 17 05:31:44.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 17, 5, 31, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 17, 5, 31, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 17 05:31:46.132: INFO: 
Jan 17 05:31:46.132: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 17 05:31:46.145: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8041  1d96754a-efa8-4192-9260-92230800bd05 53011 2 2023-01-17 05:31:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-17 05:31:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:31:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058a0a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-17 05:31:30 +0000 UTC,LastTransitionTime:2023-01-17 05:31:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-01-17 05:31:44 +0000 UTC,LastTransitionTime:2023-01-17 05:31:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 17 05:31:46.149: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-8041  886f6118-5e8e-41b2-96e5-6d5e84d1d61a 53001 2 2023-01-17 05:31:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1d96754a-efa8-4192-9260-92230800bd05 0xc0058a0ee7 0xc0058a0ee8}] []  [{kube-controller-manager Update apps/v1 2023-01-17 05:31:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d96754a-efa8-4192-9260-92230800bd05\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:31:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058a0f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:31:46.149: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 17 05:31:46.149: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8041  949fe6da-150f-42b4-95eb-39bf49d7bf46 53010 2 2023-01-17 05:31:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1d96754a-efa8-4192-9260-92230800bd05 0xc0058a0db7 0xc0058a0db8}] []  [{e2e.test Update apps/v1 2023-01-17 05:31:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:31:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d96754a-efa8-4192-9260-92230800bd05\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:31:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0058a0e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:31:46.149: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-8041  0157216b-fb8d-47b2-acab-8caa7942f7a1 52949 2 2023-01-17 05:31:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1d96754a-efa8-4192-9260-92230800bd05 0xc0058a1000 0xc0058a1001}] []  [{kube-controller-manager Update apps/v1 2023-01-17 05:31:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d96754a-efa8-4192-9260-92230800bd05\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-17 05:31:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058a10a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 17 05:31:46.153: INFO: Pod "test-rollover-deployment-779c67f4f8-ppzlg" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-ppzlg test-rollover-deployment-779c67f4f8- deployment-8041  f83eb191-9c53-4ab3-8bd5-c773c73c2162 52972 0 2023-01-17 05:31:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:021ba90ecd0e760e2eb528a6b35538b52cb9632ee4eb00c39c52aeff11a58333 cni.projectcalico.org/podIP:10.100.11.211/32 cni.projectcalico.org/podIPs:10.100.11.211/32] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 886f6118-5e8e-41b2-96e5-6d5e84d1d61a 0xc002a61ff7 0xc002a61ff8}] []  [{Go-http-client Update v1 2023-01-17 05:31:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-17 05:31:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"886f6118-5e8e-41b2-96e5-6d5e84d1d61a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-17 05:31:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.11.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps8bq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps8bq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:cluster124-apihrjet4zqi-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:31:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:31:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:31:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-17 05:31:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.9,PodIP:10.100.11.211,StartTime:2023-01-17 05:31:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-17 05:31:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://880a78dda1b373a8ba2f403e188994cb42d04bb0763bc54e0a130f590a33ea4b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.11.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 17 05:31:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8041" for this suite.

• [SLOW TEST:23.282 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":301,"skipped":5483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:31:46.175: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-981.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-981.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 250.85.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.85.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.85.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.85.250_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-981.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-981.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-981.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-981.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-981.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 250.85.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.85.250_udp@PTR;check="$$(dig +tcp +noall +answer +search 250.85.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.85.250_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:31:48.363: INFO: Unable to read wheezy_udp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.368: INFO: Unable to read wheezy_tcp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.372: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.397: INFO: Unable to read jessie_udp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.401: INFO: Unable to read jessie_tcp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.405: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.409: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:48.428: INFO: Lookups using dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2 failed for: [wheezy_udp@dns-test-service.dns-981.svc.cluster.local wheezy_tcp@dns-test-service.dns-981.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local jessie_udp@dns-test-service.dns-981.svc.cluster.local jessie_tcp@dns-test-service.dns-981.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local]

Jan 17 05:31:53.434: INFO: Unable to read wheezy_udp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:53.445: INFO: Unable to read wheezy_tcp@dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:53.449: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:53.472: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local from pod dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2: the server could not find the requested resource (get pods dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2)
Jan 17 05:31:53.540: INFO: Lookups using dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2 failed for: [wheezy_udp@dns-test-service.dns-981.svc.cluster.local wheezy_tcp@dns-test-service.dns-981.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-981.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-981.svc.cluster.local]

Jan 17 05:31:58.493: INFO: DNS probes using dns-981/dns-test-35dca8b5-0066-4a79-9443-a8cb720928d2 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 05:31:58.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-981" for this suite.

• [SLOW TEST:12.457 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":302,"skipped":5563,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:31:58.632: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-9621/secret-test-9f0f55d9-d71d-484f-b516-df64506af599
STEP: Creating a pod to test consume secrets
Jan 17 05:31:58.734: INFO: Waiting up to 5m0s for pod "pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53" in namespace "secrets-9621" to be "Succeeded or Failed"
Jan 17 05:31:58.740: INFO: Pod "pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.140465ms
Jan 17 05:32:00.749: INFO: Pod "pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015224796s
Jan 17 05:32:02.759: INFO: Pod "pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024814685s
STEP: Saw pod success
Jan 17 05:32:02.759: INFO: Pod "pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53" satisfied condition "Succeeded or Failed"
Jan 17 05:32:02.762: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53 container env-test: <nil>
STEP: delete the pod
Jan 17 05:32:02.857: INFO: Waiting for pod pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53 to disappear
Jan 17 05:32:02.862: INFO: Pod pod-configmaps-04e48923-35b2-4d70-926d-cc4831feed53 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 17 05:32:02.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9621" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5570,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:32:02.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 17 05:32:03.884: INFO: starting watch
STEP: patching
STEP: updating
Jan 17 05:32:03.917: INFO: waiting for watch events with expected annotations
Jan 17 05:32:03.917: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:32:04.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4977" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":304,"skipped":5597,"failed":0}

------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:32:04.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:32:04.161: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3eaf6969-9cb8-4966-af86-bf66e1a9c892" in namespace "security-context-test-73" to be "Succeeded or Failed"
Jan 17 05:32:04.179: INFO: Pod "busybox-readonly-false-3eaf6969-9cb8-4966-af86-bf66e1a9c892": Phase="Pending", Reason="", readiness=false. Elapsed: 17.273192ms
Jan 17 05:32:06.186: INFO: Pod "busybox-readonly-false-3eaf6969-9cb8-4966-af86-bf66e1a9c892": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024114852s
Jan 17 05:32:08.196: INFO: Pod "busybox-readonly-false-3eaf6969-9cb8-4966-af86-bf66e1a9c892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034443901s
Jan 17 05:32:08.196: INFO: Pod "busybox-readonly-false-3eaf6969-9cb8-4966-af86-bf66e1a9c892" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 17 05:32:08.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-73" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":305,"skipped":5597,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:32:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 17 05:32:08.286: INFO: Waiting up to 5m0s for pod "pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af" in namespace "emptydir-3354" to be "Succeeded or Failed"
Jan 17 05:32:08.292: INFO: Pod "pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.164224ms
Jan 17 05:32:10.303: INFO: Pod "pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01693776s
Jan 17 05:32:12.310: INFO: Pod "pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023354214s
STEP: Saw pod success
Jan 17 05:32:12.310: INFO: Pod "pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af" satisfied condition "Succeeded or Failed"
Jan 17 05:32:12.314: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af container test-container: <nil>
STEP: delete the pod
Jan 17 05:32:12.355: INFO: Waiting for pod pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af to disappear
Jan 17 05:32:12.361: INFO: Pod pod-ef0ef7f8-4534-4308-b5a8-8d7bf7cde1af no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:32:12.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3354" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":306,"skipped":5617,"failed":0}
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:32:12.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2869
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 17 05:32:12.446: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 17 05:32:12.554: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:32:14.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:32:16.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:32:18.563: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:32:20.562: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:32:22.564: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 17 05:32:24.564: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 17 05:32:24.576: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 17 05:32:26.583: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 17 05:32:28.588: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 17 05:32:30.595: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 17 05:32:32.584: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jan 17 05:32:34.587: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 17 05:32:34.595: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 17 05:32:36.651: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 17 05:32:36.651: INFO: Going to poll 10.100.106.117 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:32:36.654: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.106.117:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2869 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:32:36.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:32:36.655: INFO: ExecWithOptions: Clientset creation
Jan 17 05:32:36.655: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2869/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.106.117%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:32:36.834: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 17 05:32:36.834: INFO: Going to poll 10.100.44.233 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:32:36.839: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.44.233:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2869 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:32:36.839: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:32:36.839: INFO: ExecWithOptions: Clientset creation
Jan 17 05:32:36.839: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2869/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.44.233%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:32:37.051: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 17 05:32:37.051: INFO: Going to poll 10.100.11.237 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 17 05:32:37.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.11.237:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2869 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 17 05:32:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:32:37.057: INFO: ExecWithOptions: Clientset creation
Jan 17 05:32:37.057: INFO: ExecWithOptions: execute(POST https://10.254.0.1:443/api/v1/namespaces/pod-network-test-2869/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.11.237%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 17 05:32:37.186: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 17 05:32:37.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2869" for this suite.

• [SLOW TEST:24.826 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":307,"skipped":5618,"failed":0}
SSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:32:37.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-87682b62-ce5d-4f23-ba3a-db7683c822d9 in namespace container-probe-7140
Jan 17 05:32:39.309: INFO: Started pod busybox-87682b62-ce5d-4f23-ba3a-db7683c822d9 in namespace container-probe-7140
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 05:32:39.312: INFO: Initial restart count of pod busybox-87682b62-ce5d-4f23-ba3a-db7683c822d9 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 05:36:40.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7140" for this suite.

• [SLOW TEST:243.455 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":308,"skipped":5624,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:36:40.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:36:40.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jan 17 05:36:47.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 create -f -'
Jan 17 05:36:48.974: INFO: stderr: ""
Jan 17 05:36:48.974: INFO: stdout: "e2e-test-crd-publish-openapi-9972-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 05:36:48.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 delete e2e-test-crd-publish-openapi-9972-crds test-foo'
Jan 17 05:36:49.080: INFO: stderr: ""
Jan 17 05:36:49.080: INFO: stdout: "e2e-test-crd-publish-openapi-9972-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 17 05:36:49.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 apply -f -'
Jan 17 05:36:50.054: INFO: stderr: ""
Jan 17 05:36:50.054: INFO: stdout: "e2e-test-crd-publish-openapi-9972-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 17 05:36:50.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 delete e2e-test-crd-publish-openapi-9972-crds test-foo'
Jan 17 05:36:50.181: INFO: stderr: ""
Jan 17 05:36:50.181: INFO: stdout: "e2e-test-crd-publish-openapi-9972-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 17 05:36:50.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 create -f -'
Jan 17 05:36:50.461: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 17 05:36:50.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 create -f -'
Jan 17 05:36:50.728: INFO: rc: 1
Jan 17 05:36:50.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 apply -f -'
Jan 17 05:36:50.979: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jan 17 05:36:50.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 create -f -'
Jan 17 05:36:51.234: INFO: rc: 1
Jan 17 05:36:51.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 --namespace=crd-publish-openapi-1105 apply -f -'
Jan 17 05:36:52.085: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 17 05:36:52.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 explain e2e-test-crd-publish-openapi-9972-crds'
Jan 17 05:36:52.382: INFO: stderr: ""
Jan 17 05:36:52.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9972-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 17 05:36:52.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 explain e2e-test-crd-publish-openapi-9972-crds.metadata'
Jan 17 05:36:52.654: INFO: stderr: ""
Jan 17 05:36:52.654: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9972-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 17 05:36:52.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 explain e2e-test-crd-publish-openapi-9972-crds.spec'
Jan 17 05:36:52.948: INFO: stderr: ""
Jan 17 05:36:52.948: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9972-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 17 05:36:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 explain e2e-test-crd-publish-openapi-9972-crds.spec.bars'
Jan 17 05:36:53.209: INFO: stderr: ""
Jan 17 05:36:53.209: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9972-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 17 05:36:53.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=crd-publish-openapi-1105 explain e2e-test-crd-publish-openapi-9972-crds.spec.bars2'
Jan 17 05:36:53.509: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:36:59.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1105" for this suite.

• [SLOW TEST:18.782 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":309,"skipped":5639,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:36:59.445: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 17 05:36:59.552: INFO: Waiting up to 5m0s for pod "downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f" in namespace "downward-api-3433" to be "Succeeded or Failed"
Jan 17 05:36:59.557: INFO: Pod "downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.20066ms
Jan 17 05:37:01.569: INFO: Pod "downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016855537s
Jan 17 05:37:03.579: INFO: Pod "downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027175697s
STEP: Saw pod success
Jan 17 05:37:03.579: INFO: Pod "downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f" satisfied condition "Succeeded or Failed"
Jan 17 05:37:03.583: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:37:03.664: INFO: Waiting for pod downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f to disappear
Jan 17 05:37:03.669: INFO: Pod downward-api-129e4bce-e69e-40aa-afb2-57031ae70a4f no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 17 05:37:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3433" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":310,"skipped":5656,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:37:03.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 17 05:37:03.750: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:37:05.756: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 17 05:37:06.786: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 17 05:37:07.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9497" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":311,"skipped":5659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:37:07.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 17 05:37:07.899: INFO: Waiting up to 5m0s for pod "downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43" in namespace "downward-api-8900" to be "Succeeded or Failed"
Jan 17 05:37:07.913: INFO: Pod "downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866249ms
Jan 17 05:37:09.924: INFO: Pod "downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024655675s
Jan 17 05:37:11.931: INFO: Pod "downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031493454s
STEP: Saw pod success
Jan 17 05:37:11.931: INFO: Pod "downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43" satisfied condition "Succeeded or Failed"
Jan 17 05:37:11.935: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43 container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:37:11.969: INFO: Waiting for pod downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43 to disappear
Jan 17 05:37:11.975: INFO: Pod downward-api-be13dd6f-151a-4ac1-a2ee-a1bffa3aef43 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 17 05:37:11.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8900" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":312,"skipped":5702,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:37:11.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:37:12.056: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346" in namespace "downward-api-8544" to be "Succeeded or Failed"
Jan 17 05:37:12.062: INFO: Pod "downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346": Phase="Pending", Reason="", readiness=false. Elapsed: 5.530099ms
Jan 17 05:37:14.073: INFO: Pod "downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016273172s
Jan 17 05:37:16.081: INFO: Pod "downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025141177s
STEP: Saw pod success
Jan 17 05:37:16.082: INFO: Pod "downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346" satisfied condition "Succeeded or Failed"
Jan 17 05:37:16.086: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346 container client-container: <nil>
STEP: delete the pod
Jan 17 05:37:16.115: INFO: Waiting for pod downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346 to disappear
Jan 17 05:37:16.123: INFO: Pod downwardapi-volume-ba5bf6d8-7656-45bf-8542-f4670c121346 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 05:37:16.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8544" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":313,"skipped":5730,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:37:16.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 17 05:39:00.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1837" for this suite.

• [SLOW TEST:104.099 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":314,"skipped":5746,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:00.246: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-653a74a6-0d6b-41f4-a6ea-b750f576577d
STEP: Creating a pod to test consume secrets
Jan 17 05:39:00.366: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239" in namespace "projected-794" to be "Succeeded or Failed"
Jan 17 05:39:00.372: INFO: Pod "pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239": Phase="Pending", Reason="", readiness=false. Elapsed: 6.299836ms
Jan 17 05:39:02.383: INFO: Pod "pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017716685s
Jan 17 05:39:04.396: INFO: Pod "pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03000448s
STEP: Saw pod success
Jan 17 05:39:04.396: INFO: Pod "pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239" satisfied condition "Succeeded or Failed"
Jan 17 05:39:04.400: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239 container secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:39:04.506: INFO: Waiting for pod pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239 to disappear
Jan 17 05:39:04.511: INFO: Pod pod-projected-secrets-f1cbc218-6c25-4382-8f73-6637fc4e8239 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 05:39:04.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-794" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":315,"skipped":5760,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:04.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 17 05:39:04.591: INFO: The status of Pod pod-update-b80a3c40-aa1f-4c0c-8d67-afe502a98710 is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:39:06.601: INFO: The status of Pod pod-update-b80a3c40-aa1f-4c0c-8d67-afe502a98710 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 17 05:39:07.136: INFO: Successfully updated pod "pod-update-b80a3c40-aa1f-4c0c-8d67-afe502a98710"
STEP: verifying the updated pod is in kubernetes
Jan 17 05:39:07.146: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 17 05:39:07.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9486" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":316,"skipped":5776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:07.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 05:39:14.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2914" for this suite.

• [SLOW TEST:7.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":317,"skipped":5806,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:14.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5467
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-5467
I0117 05:39:14.433488      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5467, replica count: 2
I0117 05:39:17.484841      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:39:17.485: INFO: Creating new exec pod
Jan 17 05:39:20.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-5467 exec execpodk628p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 17 05:39:20.751: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 17 05:39:20.751: INFO: stdout: "externalname-service-dmjfj"
Jan 17 05:39:20.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-5467 exec execpodk628p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.0.121 80'
Jan 17 05:39:21.063: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.0.121 80\nConnection to 10.254.0.121 80 port [tcp/http] succeeded!\n"
Jan 17 05:39:21.063: INFO: stdout: "externalname-service-dmjfj"
Jan 17 05:39:21.063: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:39:21.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5467" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:6.820 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":318,"skipped":5843,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:21.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2103
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-2103
Jan 17 05:39:21.243: INFO: Found 0 stateful pods, waiting for 1
Jan 17 05:39:31.259: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 17 05:39:31.321: INFO: Deleting all statefulset in ns statefulset-2103
Jan 17 05:39:31.326: INFO: Scaling statefulset ss to 0
Jan 17 05:39:41.362: INFO: Waiting for statefulset status.replicas updated to 0
Jan 17 05:39:41.366: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 17 05:39:41.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2103" for this suite.

• [SLOW TEST:20.276 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":319,"skipped":5873,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:41.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:39:41.509: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 17 05:39:41.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:41.523: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 17 05:39:41.566: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:41.566: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:42.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:42.575: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:43.573: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 05:39:43.573: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 17 05:39:43.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 05:39:43.612: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 17 05:39:44.619: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:44.619: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 17 05:39:44.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:44.649: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:45.655: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:45.655: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:46.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:46.657: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:47.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:47.658: INFO: Node cluster124-apihrjet4zqi-node-2 is running 0 daemon pod, expected 1
Jan 17 05:39:48.657: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 17 05:39:48.657: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3900, will wait for the garbage collector to delete the pods
Jan 17 05:39:48.732: INFO: Deleting DaemonSet.extensions daemon-set took: 12.257678ms
Jan 17 05:39:48.832: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.638217ms
Jan 17 05:39:51.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 17 05:39:51.544: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 17 05:39:51.547: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55147"},"items":null}

Jan 17 05:39:51.550: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55147"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:39:51.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3900" for this suite.

• [SLOW TEST:10.192 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":320,"skipped":5890,"failed":0}
SSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:51.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 17 05:39:51.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9463" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":321,"skipped":5893,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:51.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 17 05:39:51.777: INFO: Waiting up to 5m0s for pod "pod-d7f1b327-cc10-40f3-a693-c103a49bffb6" in namespace "emptydir-1170" to be "Succeeded or Failed"
Jan 17 05:39:51.782: INFO: Pod "pod-d7f1b327-cc10-40f3-a693-c103a49bffb6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458982ms
Jan 17 05:39:53.790: INFO: Pod "pod-d7f1b327-cc10-40f3-a693-c103a49bffb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013399381s
Jan 17 05:39:55.797: INFO: Pod "pod-d7f1b327-cc10-40f3-a693-c103a49bffb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01963387s
STEP: Saw pod success
Jan 17 05:39:55.797: INFO: Pod "pod-d7f1b327-cc10-40f3-a693-c103a49bffb6" satisfied condition "Succeeded or Failed"
Jan 17 05:39:55.800: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-d7f1b327-cc10-40f3-a693-c103a49bffb6 container test-container: <nil>
STEP: delete the pod
Jan 17 05:39:55.830: INFO: Waiting for pod pod-d7f1b327-cc10-40f3-a693-c103a49bffb6 to disappear
Jan 17 05:39:55.835: INFO: Pod pod-d7f1b327-cc10-40f3-a693-c103a49bffb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 17 05:39:55.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1170" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":5899,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:39:55.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-cdacbd39-8bbb-4a0b-ac97-40339bb64c1a
STEP: Creating a pod to test consume secrets
Jan 17 05:39:55.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085" in namespace "projected-4150" to be "Succeeded or Failed"
Jan 17 05:39:55.931: INFO: Pod "pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085": Phase="Pending", Reason="", readiness=false. Elapsed: 6.325447ms
Jan 17 05:39:57.941: INFO: Pod "pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016634026s
Jan 17 05:39:59.949: INFO: Pod "pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024434389s
STEP: Saw pod success
Jan 17 05:39:59.949: INFO: Pod "pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085" satisfied condition "Succeeded or Failed"
Jan 17 05:39:59.952: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:39:59.982: INFO: Waiting for pod pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085 to disappear
Jan 17 05:39:59.987: INFO: Pod pod-projected-secrets-98315595-1b9d-4bef-a087-ca14d5b21085 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 05:39:59.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4150" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":5901,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:40:00.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 17 05:40:00.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-132 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 17 05:40:00.169: INFO: stderr: ""
Jan 17 05:40:00.169: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 17 05:40:05.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-132 get pod e2e-test-httpd-pod -o json'
Jan 17 05:40:05.333: INFO: stderr: ""
Jan 17 05:40:05.333: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"4b8f9ac53ccdbceb6638a5f7f6fe6763c4945f6e264dadbdb80aa8e3227bb24f\",\n            \"cni.projectcalico.org/podIP\": \"10.100.11.209/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.11.209/32\"\n        },\n        \"creationTimestamp\": \"2023-01-17T05:40:00Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-132\",\n        \"resourceVersion\": \"55270\",\n        \"uid\": \"08faae2b-bc81-4760-b81c-0f3156c3912c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-2ms6v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"cluster124-apihrjet4zqi-node-2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-2ms6v\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T05:40:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T05:40:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T05:40:01Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-17T05:40:00Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a74d0b99b61e153d87d634e33b634519a004d4544aac8c40be6bf3e92aec3fbe\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-17T05:40:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.9\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.11.209\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.11.209\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-17T05:40:00Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 17 05:40:05.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-132 replace -f -'
Jan 17 05:40:06.422: INFO: stderr: ""
Jan 17 05:40:06.422: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jan 17 05:40:06.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-132 delete pods e2e-test-httpd-pod'
Jan 17 05:40:08.571: INFO: stderr: ""
Jan 17 05:40:08.571: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:40:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-132" for this suite.

• [SLOW TEST:8.589 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":324,"skipped":5902,"failed":0}
S
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:40:08.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-a1448694-cd09-4902-b06c-9718448c38ea in namespace container-probe-8261
Jan 17 05:40:10.678: INFO: Started pod liveness-a1448694-cd09-4902-b06c-9718448c38ea in namespace container-probe-8261
STEP: checking the pod's current state and verifying that restartCount is present
Jan 17 05:40:10.681: INFO: Initial restart count of pod liveness-a1448694-cd09-4902-b06c-9718448c38ea is 0
Jan 17 05:40:30.784: INFO: Restart count of pod container-probe-8261/liveness-a1448694-cd09-4902-b06c-9718448c38ea is now 1 (20.103032792s elapsed)
Jan 17 05:40:50.878: INFO: Restart count of pod container-probe-8261/liveness-a1448694-cd09-4902-b06c-9718448c38ea is now 2 (40.197260113s elapsed)
Jan 17 05:41:10.973: INFO: Restart count of pod container-probe-8261/liveness-a1448694-cd09-4902-b06c-9718448c38ea is now 3 (1m0.291310033s elapsed)
Jan 17 05:41:31.071: INFO: Restart count of pod container-probe-8261/liveness-a1448694-cd09-4902-b06c-9718448c38ea is now 4 (1m20.38933242s elapsed)
Jan 17 05:42:41.417: INFO: Restart count of pod container-probe-8261/liveness-a1448694-cd09-4902-b06c-9718448c38ea is now 5 (2m30.735873406s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 17 05:42:41.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8261" for this suite.

• [SLOW TEST:152.869 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":5903,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:42:41.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:42:42.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:42:45.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 17 05:42:47.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=webhook-1886 attach --namespace=webhook-1886 to-be-attached-pod -i -c=container1'
Jan 17 05:42:47.644: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:42:47.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1886" for this suite.
STEP: Destroying namespace "webhook-1886-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.283 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":326,"skipped":5915,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:42:47.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 17 05:42:51.888: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 17 05:42:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4028" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":327,"skipped":5933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:42:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-7ab9411f-c331-4327-ba69-ad96760d4168
STEP: Creating a pod to test consume configMaps
Jan 17 05:42:52.010: INFO: Waiting up to 5m0s for pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e" in namespace "configmap-9735" to be "Succeeded or Failed"
Jan 17 05:42:52.017: INFO: Pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.867276ms
Jan 17 05:42:54.027: INFO: Pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e": Phase="Running", Reason="", readiness=true. Elapsed: 2.017229302s
Jan 17 05:42:56.033: INFO: Pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e": Phase="Running", Reason="", readiness=false. Elapsed: 4.023288068s
Jan 17 05:42:58.045: INFO: Pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035121914s
STEP: Saw pod success
Jan 17 05:42:58.045: INFO: Pod "pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e" satisfied condition "Succeeded or Failed"
Jan 17 05:42:58.050: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:42:58.160: INFO: Waiting for pod pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e to disappear
Jan 17 05:42:58.165: INFO: Pod pod-configmaps-a64bd5be-c069-4d06-a272-69af3c2ea63e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 17 05:42:58.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9735" for this suite.

• [SLOW TEST:6.248 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":328,"skipped":5960,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:42:58.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 17 05:42:58.244: INFO: namespace kubectl-7783
Jan 17 05:42:58.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7783 create -f -'
Jan 17 05:42:58.586: INFO: stderr: ""
Jan 17 05:42:58.586: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 17 05:42:59.596: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 05:42:59.596: INFO: Found 0 / 1
Jan 17 05:43:00.595: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 05:43:00.595: INFO: Found 1 / 1
Jan 17 05:43:00.595: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 17 05:43:00.599: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 17 05:43:00.599: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 17 05:43:00.599: INFO: wait on agnhost-primary startup in kubectl-7783 
Jan 17 05:43:00.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7783 logs agnhost-primary-xl4zg agnhost-primary'
Jan 17 05:43:00.760: INFO: stderr: ""
Jan 17 05:43:00.760: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 17 05:43:00.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7783 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 17 05:43:00.873: INFO: stderr: ""
Jan 17 05:43:00.873: INFO: stdout: "service/rm2 exposed\n"
Jan 17 05:43:00.882: INFO: Service rm2 in namespace kubectl-7783 found.
STEP: exposing service
Jan 17 05:43:02.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-7783 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 17 05:43:03.003: INFO: stderr: ""
Jan 17 05:43:03.003: INFO: stdout: "service/rm3 exposed\n"
Jan 17 05:43:03.014: INFO: Service rm3 in namespace kubectl-7783 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:43:05.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7783" for this suite.

• [SLOW TEST:6.861 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":329,"skipped":6046,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:05.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:43:06.187: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:43:09.223: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:43:09.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9984" for this suite.
STEP: Destroying namespace "webhook-9984-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":330,"skipped":6077,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:09.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:43:09.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2" in namespace "downward-api-1463" to be "Succeeded or Failed"
Jan 17 05:43:09.569: INFO: Pod "downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.913594ms
Jan 17 05:43:11.576: INFO: Pod "downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011413208s
Jan 17 05:43:13.587: INFO: Pod "downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02247258s
STEP: Saw pod success
Jan 17 05:43:13.587: INFO: Pod "downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2" satisfied condition "Succeeded or Failed"
Jan 17 05:43:13.590: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2 container client-container: <nil>
STEP: delete the pod
Jan 17 05:43:13.627: INFO: Waiting for pod downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2 to disappear
Jan 17 05:43:13.632: INFO: Pod downwardapi-volume-34404324-906f-455d-9f49-19e603a858d2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 05:43:13.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1463" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":331,"skipped":6096,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:13.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 17 05:43:13.746: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 17 05:43:13.788: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 17 05:43:13.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-882" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":332,"skipped":6097,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:13.841: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:43:13.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9210" for this suite.
STEP: Destroying namespace "nspatchtest-40a1b32d-594b-475d-ae36-005c08c1904f-3392" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":333,"skipped":6106,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:13.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 17 05:43:14.101: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 17 05:43:14.113: INFO: starting watch
STEP: patching
STEP: updating
Jan 17 05:43:14.146: INFO: waiting for watch events with expected annotations
Jan 17 05:43:14.146: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 17 05:43:14.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5977" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":334,"skipped":6114,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:14.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:43:14.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d" in namespace "downward-api-5560" to be "Succeeded or Failed"
Jan 17 05:43:14.318: INFO: Pod "downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.974806ms
Jan 17 05:43:16.324: INFO: Pod "downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012277295s
Jan 17 05:43:18.334: INFO: Pod "downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022117267s
STEP: Saw pod success
Jan 17 05:43:18.334: INFO: Pod "downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d" satisfied condition "Succeeded or Failed"
Jan 17 05:43:18.338: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d container client-container: <nil>
STEP: delete the pod
Jan 17 05:43:18.376: INFO: Waiting for pod downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d to disappear
Jan 17 05:43:18.382: INFO: Pod downwardapi-volume-278f1be8-c032-42d6-8bd3-8068ba6b540d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 17 05:43:18.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5560" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":335,"skipped":6131,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:18.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:43:20.544: INFO: DNS probes using dns-test-be01ef32-026b-4c15-b3c6-cdccb2933bb9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:43:22.657: INFO: DNS probes using dns-test-8b65a74e-ed57-4188-bd98-de2602c4edf0 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6975.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6975.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 17 05:43:24.780: INFO: DNS probes using dns-test-17d2b835-3642-4e93-9469-d4a289d9c8b7 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 17 05:43:24.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6975" for this suite.

• [SLOW TEST:6.470 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":336,"skipped":6138,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:24.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jan 17 05:43:24.953: INFO: Waiting up to 5m0s for pod "client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8" in namespace "containers-1930" to be "Succeeded or Failed"
Jan 17 05:43:24.958: INFO: Pod "client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025824ms
Jan 17 05:43:26.968: INFO: Pod "client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014505609s
Jan 17 05:43:28.977: INFO: Pod "client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024150535s
STEP: Saw pod success
Jan 17 05:43:28.977: INFO: Pod "client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8" satisfied condition "Succeeded or Failed"
Jan 17 05:43:28.981: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8 container agnhost-container: <nil>
STEP: delete the pod
Jan 17 05:43:29.017: INFO: Waiting for pod client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8 to disappear
Jan 17 05:43:29.026: INFO: Pod client-containers-9c7e162a-1fc5-4167-ba99-e9f292d063e8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 17 05:43:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1930" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":337,"skipped":6228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:43:29.045: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-3773
Jan 17 05:43:29.109: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:43:31.121: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 17 05:43:31.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 17 05:43:31.392: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 17 05:43:31.392: INFO: stdout: "iptables"
Jan 17 05:43:31.392: INFO: proxyMode: iptables
Jan 17 05:43:31.418: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 17 05:43:31.423: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-3773
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3773
I0117 05:43:31.481020      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3773, replica count: 3
I0117 05:43:34.531707      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 17 05:43:34.548: INFO: Creating new exec pod
Jan 17 05:43:37.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 17 05:43:37.811: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 17 05:43:37.811: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:43:37.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.254.201.57 80'
Jan 17 05:43:38.047: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.254.201.57 80\nConnection to 10.254.201.57 80 port [tcp/http] succeeded!\n"
Jan 17 05:43:38.047: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:43:38.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.9 30169'
Jan 17 05:43:38.324: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.9 30169\nConnection to 10.0.0.9 30169 port [tcp/*] succeeded!\n"
Jan 17 05:43:38.324: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:43:38.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.0.21 30169'
Jan 17 05:43:38.609: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.0.21 30169\nConnection to 10.0.0.21 30169 port [tcp/*] succeeded!\n"
Jan 17 05:43:38.609: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 17 05:43:38.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.16:30169/ ; done'
Jan 17 05:43:39.032: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n"
Jan 17 05:43:39.032: INFO: stdout: "\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw\naffinity-nodeport-timeout-shrpw"
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Received response from host: affinity-nodeport-timeout-shrpw
Jan 17 05:43:39.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30169/'
Jan 17 05:43:39.271: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n"
Jan 17 05:43:39.271: INFO: stdout: "affinity-nodeport-timeout-shrpw"
Jan 17 05:43:59.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=services-3773 exec execpod-affinityzmbtd -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.16:30169/'
Jan 17 05:43:59.511: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.16:30169/\n"
Jan 17 05:43:59.511: INFO: stdout: "affinity-nodeport-timeout-6wfhr"
Jan 17 05:43:59.511: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3773, will wait for the garbage collector to delete the pods
Jan 17 05:43:59.612: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.275349ms
Jan 17 05:43:59.713: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.450065ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 17 05:44:01.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3773" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:32.840 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":338,"skipped":6253,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:01.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 05:44:18.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5729" for this suite.

• [SLOW TEST:16.173 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":339,"skipped":6270,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:18.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 17 05:44:18.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152" in namespace "projected-579" to be "Succeeded or Failed"
Jan 17 05:44:18.140: INFO: Pod "downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152": Phase="Pending", Reason="", readiness=false. Elapsed: 15.896274ms
Jan 17 05:44:20.152: INFO: Pod "downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028138902s
Jan 17 05:44:22.160: INFO: Pod "downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03586768s
STEP: Saw pod success
Jan 17 05:44:22.160: INFO: Pod "downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152" satisfied condition "Succeeded or Failed"
Jan 17 05:44:22.164: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152 container client-container: <nil>
STEP: delete the pod
Jan 17 05:44:22.203: INFO: Waiting for pod downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152 to disappear
Jan 17 05:44:22.210: INFO: Pod downwardapi-volume-4f7ec590-5907-480e-96ad-49ce0ee30152 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 17 05:44:22.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-579" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":340,"skipped":6288,"failed":0}
SSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:22.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jan 17 05:44:22.368: INFO: Waiting up to 5m0s for pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61" in namespace "var-expansion-2625" to be "Succeeded or Failed"
Jan 17 05:44:22.381: INFO: Pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61": Phase="Pending", Reason="", readiness=false. Elapsed: 13.561806ms
Jan 17 05:44:24.393: INFO: Pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61": Phase="Running", Reason="", readiness=true. Elapsed: 2.025247465s
Jan 17 05:44:26.401: INFO: Pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61": Phase="Running", Reason="", readiness=false. Elapsed: 4.033402192s
Jan 17 05:44:28.411: INFO: Pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043195771s
STEP: Saw pod success
Jan 17 05:44:28.411: INFO: Pod "var-expansion-f068f835-6a57-4ae7-9816-29acb503be61" satisfied condition "Succeeded or Failed"
Jan 17 05:44:28.415: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod var-expansion-f068f835-6a57-4ae7-9816-29acb503be61 container dapi-container: <nil>
STEP: delete the pod
Jan 17 05:44:28.450: INFO: Waiting for pod var-expansion-f068f835-6a57-4ae7-9816-29acb503be61 to disappear
Jan 17 05:44:28.454: INFO: Pod var-expansion-f068f835-6a57-4ae7-9816-29acb503be61 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 17 05:44:28.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2625" for this suite.

• [SLOW TEST:6.235 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":341,"skipped":6292,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:28.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:44:29.005: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:44:32.048: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:44:42.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7569" for this suite.
STEP: Destroying namespace "webhook-7569-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:14.089 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":342,"skipped":6294,"failed":0}
SSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:42.562: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:44:43.421: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 17 05:44:43.424: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 17 05:44:43.424: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 17 05:44:43.424: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 17 05:44:43.424: INFO: Checking APIGroup: apps
Jan 17 05:44:43.426: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 17 05:44:43.426: INFO: Versions found [{apps/v1 v1}]
Jan 17 05:44:43.426: INFO: apps/v1 matches apps/v1
Jan 17 05:44:43.426: INFO: Checking APIGroup: events.k8s.io
Jan 17 05:44:43.428: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 17 05:44:43.428: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.428: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 17 05:44:43.428: INFO: Checking APIGroup: authentication.k8s.io
Jan 17 05:44:43.431: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 17 05:44:43.431: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 17 05:44:43.431: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 17 05:44:43.431: INFO: Checking APIGroup: authorization.k8s.io
Jan 17 05:44:43.433: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 17 05:44:43.433: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 17 05:44:43.433: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 17 05:44:43.433: INFO: Checking APIGroup: autoscaling
Jan 17 05:44:43.435: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 17 05:44:43.435: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 17 05:44:43.435: INFO: autoscaling/v2 matches autoscaling/v2
Jan 17 05:44:43.435: INFO: Checking APIGroup: batch
Jan 17 05:44:43.437: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 17 05:44:43.437: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 17 05:44:43.437: INFO: batch/v1 matches batch/v1
Jan 17 05:44:43.437: INFO: Checking APIGroup: certificates.k8s.io
Jan 17 05:44:43.439: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 17 05:44:43.439: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 17 05:44:43.439: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 17 05:44:43.439: INFO: Checking APIGroup: networking.k8s.io
Jan 17 05:44:43.441: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 17 05:44:43.441: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 17 05:44:43.441: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 17 05:44:43.442: INFO: Checking APIGroup: policy
Jan 17 05:44:43.444: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 17 05:44:43.444: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 17 05:44:43.444: INFO: policy/v1 matches policy/v1
Jan 17 05:44:43.444: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 17 05:44:43.446: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 17 05:44:43.446: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 17 05:44:43.446: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 17 05:44:43.446: INFO: Checking APIGroup: storage.k8s.io
Jan 17 05:44:43.448: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 17 05:44:43.448: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.448: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 17 05:44:43.448: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 17 05:44:43.451: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 17 05:44:43.451: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 17 05:44:43.451: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 17 05:44:43.451: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 17 05:44:43.455: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 17 05:44:43.455: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 17 05:44:43.455: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 17 05:44:43.455: INFO: Checking APIGroup: scheduling.k8s.io
Jan 17 05:44:43.457: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 17 05:44:43.457: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 17 05:44:43.457: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 17 05:44:43.457: INFO: Checking APIGroup: coordination.k8s.io
Jan 17 05:44:43.460: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 17 05:44:43.460: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 17 05:44:43.460: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 17 05:44:43.460: INFO: Checking APIGroup: node.k8s.io
Jan 17 05:44:43.462: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 17 05:44:43.462: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.462: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 17 05:44:43.462: INFO: Checking APIGroup: discovery.k8s.io
Jan 17 05:44:43.465: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 17 05:44:43.465: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.465: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 17 05:44:43.465: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 17 05:44:43.467: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 05:44:43.467: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.467: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 17 05:44:43.467: INFO: Checking APIGroup: internal.apiserver.k8s.io
Jan 17 05:44:43.469: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Jan 17 05:44:43.469: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jan 17 05:44:43.469: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Jan 17 05:44:43.469: INFO: Checking APIGroup: crd.projectcalico.org
Jan 17 05:44:43.471: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 17 05:44:43.471: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 17 05:44:43.471: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 17 05:44:43.471: INFO: Checking APIGroup: monitoring.coreos.com
Jan 17 05:44:43.473: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 17 05:44:43.473: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 17 05:44:43.473: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 17 05:44:43.473: INFO: Checking APIGroup: metrics.k8s.io
Jan 17 05:44:43.476: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 17 05:44:43.476: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 17 05:44:43.476: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jan 17 05:44:43.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-6513" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":343,"skipped":6297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:43.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-c78be243-e820-4d60-8d3c-d81e04740653
STEP: Creating a pod to test consume secrets
Jan 17 05:44:43.560: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429" in namespace "projected-625" to be "Succeeded or Failed"
Jan 17 05:44:43.566: INFO: Pod "pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429": Phase="Pending", Reason="", readiness=false. Elapsed: 6.115996ms
Jan 17 05:44:45.576: INFO: Pod "pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016045616s
Jan 17 05:44:47.584: INFO: Pod "pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023952925s
STEP: Saw pod success
Jan 17 05:44:47.584: INFO: Pod "pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429" satisfied condition "Succeeded or Failed"
Jan 17 05:44:47.587: INFO: Trying to get logs from node cluster124-apihrjet4zqi-node-2 pod pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 17 05:44:47.622: INFO: Waiting for pod pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429 to disappear
Jan 17 05:44:47.628: INFO: Pod pod-projected-secrets-36bff881-ff61-49e2-bab9-d1f48103a429 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 17 05:44:47.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-625" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6322,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:47.645: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-7196d53f-90a2-4313-ab43-ff1b861f217a
STEP: Creating configMap with name cm-test-opt-upd-b4f82382-5a27-48bd-8421-89f6ee6087c8
STEP: Creating the pod
Jan 17 05:44:47.761: INFO: The status of Pod pod-projected-configmaps-72073a14-0c85-441f-bb4f-5897442a367a is Pending, waiting for it to be Running (with Ready = true)
Jan 17 05:44:49.773: INFO: The status of Pod pod-projected-configmaps-72073a14-0c85-441f-bb4f-5897442a367a is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-7196d53f-90a2-4313-ab43-ff1b861f217a
STEP: Updating configmap cm-test-opt-upd-b4f82382-5a27-48bd-8421-89f6ee6087c8
STEP: Creating configMap with name cm-test-opt-create-17723289-2cec-47b0-9256-0bd7a1911387
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 17 05:44:51.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9393" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":345,"skipped":6326,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:51.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 17 05:44:53.986: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 17 05:44:56.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1755" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":346,"skipped":6345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:56.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:44:56.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-2084 version'
Jan 17 05:44:56.197: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 17 05:44:56.197: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.9\", GitCommit:\"9710807c82740b9799453677c977758becf0acbb\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:09Z\", GoVersion:\"go1.18.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.9\", GitCommit:\"9710807c82740b9799453677c977758becf0acbb\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:06Z\", GoVersion:\"go1.18.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:44:56.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2084" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":347,"skipped":6370,"failed":0}
SSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:56.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 17 05:44:56.301: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 17 05:44:56.307: INFO: starting watch
STEP: patching
STEP: updating
Jan 17 05:44:56.330: INFO: waiting for watch events with expected annotations
Jan 17 05:44:56.330: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jan 17 05:44:56.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6996" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":348,"skipped":6373,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:44:56.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 17 05:44:56.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
Jan 17 05:45:02.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:45:24.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9154" for this suite.

• [SLOW TEST:28.136 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":349,"skipped":6390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:45:24.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 17 05:45:40.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3249" for this suite.

• [SLOW TEST:16.274 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":350,"skipped":6454,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:45:40.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jan 17 05:45:40.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 create -f -'
Jan 17 05:45:41.782: INFO: stderr: ""
Jan 17 05:45:41.782: INFO: stdout: "pod/pause created\n"
Jan 17 05:45:41.782: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 17 05:45:41.782: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-542" to be "running and ready"
Jan 17 05:45:41.793: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.664117ms
Jan 17 05:45:43.802: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019348133s
Jan 17 05:45:43.802: INFO: Pod "pause" satisfied condition "running and ready"
Jan 17 05:45:43.802: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 17 05:45:43.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 label pods pause testing-label=testing-label-value'
Jan 17 05:45:43.918: INFO: stderr: ""
Jan 17 05:45:43.918: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 17 05:45:43.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 get pod pause -L testing-label'
Jan 17 05:45:44.005: INFO: stderr: ""
Jan 17 05:45:44.005: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 17 05:45:44.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 label pods pause testing-label-'
Jan 17 05:45:44.102: INFO: stderr: ""
Jan 17 05:45:44.102: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 17 05:45:44.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 get pod pause -L testing-label'
Jan 17 05:45:44.217: INFO: stderr: ""
Jan 17 05:45:44.218: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jan 17 05:45:44.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 delete --grace-period=0 --force -f -'
Jan 17 05:45:44.344: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 17 05:45:44.344: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 17 05:45:44.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 get rc,svc -l name=pause --no-headers'
Jan 17 05:45:44.436: INFO: stderr: "No resources found in kubectl-542 namespace.\n"
Jan 17 05:45:44.436: INFO: stdout: ""
Jan 17 05:45:44.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-542 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 17 05:45:44.526: INFO: stderr: ""
Jan 17 05:45:44.526: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:45:44.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-542" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":351,"skipped":6552,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:45:44.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 17 05:45:44.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57304 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:44.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57304 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 17 05:45:44.612: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57305 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:44.612: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57305 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 17 05:45:44.624: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57306 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:44.624: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57306 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 17 05:45:44.639: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57307 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:44.639: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3450  0d8786c8-d276-4224-a3fe-e532811d9bd9 57307 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 17 05:45:44.648: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3450  055e7ec1-0de9-459b-bc08-f2c3bc4a9fe7 57309 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:44.648: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3450  055e7ec1-0de9-459b-bc08-f2c3bc4a9fe7 57309 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 17 05:45:54.667: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3450  055e7ec1-0de9-459b-bc08-f2c3bc4a9fe7 57355 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 17 05:45:54.667: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3450  055e7ec1-0de9-459b-bc08-f2c3bc4a9fe7 57355 0 2023-01-17 05:45:44 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-17 05:45:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 17 05:46:04.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3450" for this suite.

• [SLOW TEST:20.158 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":352,"skipped":6553,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:46:04.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 17 05:46:04.753: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 17 05:47:04.806: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 17 05:47:04.811: INFO: Starting informer...
STEP: Starting pod...
Jan 17 05:47:05.041: INFO: Pod is running on cluster124-apihrjet4zqi-node-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 17 05:47:05.076: INFO: Pod wasn't evicted. Proceeding
Jan 17 05:47:05.076: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 17 05:48:20.126: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jan 17 05:48:20.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7920" for this suite.

• [SLOW TEST:135.461 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":353,"skipped":6567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:48:20.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 17 05:48:20.874: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 17 05:48:23.917: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:48:24.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2053" for this suite.
STEP: Destroying namespace "webhook-2053-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":354,"skipped":6595,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:48:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 17 05:48:24.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 17 05:48:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3823" for this suite.

• [SLOW TEST:27.780 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":355,"skipped":6596,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 17 05:48:51.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1302063267
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jan 17 05:48:52.033: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1302063267 --namespace=kubectl-1427 proxy --unix-socket=/tmp/kubectl-proxy-unix3809952297/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 17 05:48:52.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1427" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":356,"skipped":6600,"failed":0}
SSSSSSSSSSSSSSSSSJan 17 05:48:52.105: INFO: Running AfterSuite actions on all nodes
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 17 05:48:52.105: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 17 05:48:52.105: INFO: Running AfterSuite actions on node 1
Jan 17 05:48:52.105: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 5912.869 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h38m34.846530739s
Test Suite Passed
