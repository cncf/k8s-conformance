I0128 00:13:30.421471      26 e2e.go:129] Starting e2e run "5914f3c0-36a5-47cc-acf4-af23a2cbf1e8" on Ginkgo node 1
{"msg":"Test Suite starting","total":356,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1674864810 - Will randomize all specs
Will run 356 of 6973 specs

Jan 28 00:13:32.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
E0128 00:13:32.789647      26 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 28 00:13:32.789: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 28 00:13:32.845: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 28 00:13:32.980: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 28 00:13:32.980: INFO: expected 18 pod replicas in namespace 'kube-system', 18 are Running and Ready.
Jan 28 00:13:32.980: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 28 00:13:33.007: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
Jan 28 00:13:33.007: INFO: e2e test version: v1.24.10
Jan 28 00:13:33.011: INFO: kube-apiserver version: v1.24.10+IKS
Jan 28 00:13:33.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:13:33.027: INFO: Cluster IP family: ipv4
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:33.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
W0128 00:13:33.118670      26 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 28 00:13:33.119: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Jan 28 00:13:33.168: INFO: PSP annotation exists on dry run pod: "ibm-privileged-psp"; assuming PodSecurityPolicy is enabled
W0128 00:13:33.184013      26 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
W0128 00:13:33.202254      26 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jan 28 00:13:33.231: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7696
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:13:33.431: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf" in namespace "projected-7696" to be "Succeeded or Failed"
Jan 28 00:13:33.449: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.650915ms
Jan 28 00:13:35.480: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048056802s
Jan 28 00:13:37.513: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080463243s
Jan 28 00:13:39.543: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf": Phase="Running", Reason="", readiness=false. Elapsed: 6.111199608s
Jan 28 00:13:41.575: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.142803347s
STEP: Saw pod success
Jan 28 00:13:41.575: INFO: Pod "downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf" satisfied condition "Succeeded or Failed"
Jan 28 00:13:41.594: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf container client-container: <nil>
STEP: delete the pod
Jan 28 00:13:41.743: INFO: Waiting for pod downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf to disappear
Jan 28 00:13:41.761: INFO: Pod downwardapi-volume-2b052229-845e-4b1d-8f33-486bd52b4dcf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 00:13:41.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7696" for this suite.

• [SLOW TEST:8.769 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":1,"skipped":7,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:41.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2776
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 28 00:13:42.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:13:42.142: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:13:43.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:13:43.177: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:13:44.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:13:44.199: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:13:45.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:13:45.186: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:13:46.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:13:46.187: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:13:47.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:13:47.186: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 28 00:13:47.311: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18167"},"items":null}

Jan 28 00:13:47.330: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18171"},"items":[{"metadata":{"name":"daemon-set-btfvl","generateName":"daemon-set-","namespace":"daemonsets-2776","uid":"6dd9f273-8865-428d-850e-fe0f25c9a1e6","resourceVersion":"18169","creationTimestamp":"2023-01-28T00:13:42Z","deletionTimestamp":"2023-01-28T00:14:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7c4442f9dfd498a147c07a8f5e175a9eb8c400b219888d91da7e7d336d293928","cni.projectcalico.org/podIP":"172.30.102.101/32","cni.projectcalico.org/podIPs":"172.30.102.101/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f9589926-1df6-4770-872c-eb99af116a0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9589926-1df6-4770-872c-eb99af116a0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.101\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s6b9j","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s6b9j","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.187.128.27","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.187.128.27"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"}],"hostIP":"10.187.128.27","podIP":"172.30.102.101","podIPs":[{"ip":"172.30.102.101"}],"startTime":"2023-01-28T00:13:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:13:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://eea700330815aaf54d9cc06005ebb8448949c39d664cc27690a4a3a2ea789680","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-k5q78","generateName":"daemon-set-","namespace":"daemonsets-2776","uid":"a766d5c2-c0d3-45a6-8c5f-ec7ae3968da0","resourceVersion":"18170","creationTimestamp":"2023-01-28T00:13:42Z","deletionTimestamp":"2023-01-28T00:14:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7bc3d6157b1c2b8a629e71a6668cba5fadddd0203ea5059c2a0ca0cebb102d07","cni.projectcalico.org/podIP":"172.30.253.239/32","cni.projectcalico.org/podIPs":"172.30.253.239/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f9589926-1df6-4770-872c-eb99af116a0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9589926-1df6-4770-872c-eb99af116a0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:47Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.253.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-n2dt5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-n2dt5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.187.128.30","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.187.128.30"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:47Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:47Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"}],"hostIP":"10.187.128.30","podIP":"172.30.253.239","podIPs":[{"ip":"172.30.253.239"}],"startTime":"2023-01-28T00:13:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:13:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://cb5e205b7d433e027dfcde9b4c6a1effa9b4166d2fe65e9ff62e96eb85b9ad0a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-pkcxz","generateName":"daemon-set-","namespace":"daemonsets-2776","uid":"3731025f-47e3-4d01-b8a7-6bb327aafe7f","resourceVersion":"18168","creationTimestamp":"2023-01-28T00:13:42Z","deletionTimestamp":"2023-01-28T00:14:17Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6df8db488c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"dd1c5a73a109b50351a39d4a20535bd5c6cb5dd9e603b0c62e0b940800e74f72","cni.projectcalico.org/podIP":"172.30.90.108/32","cni.projectcalico.org/podIPs":"172.30.90.108/32","kubernetes.io/psp":"e2e-test-privileged-psp"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"f9589926-1df6-4770-872c-eb99af116a0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:42Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9589926-1df6-4770-872c-eb99af116a0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:43Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-28T00:13:46Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2scxz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2scxz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.187.128.43","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.187.128.43"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:46Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:46Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-28T00:13:42Z"}],"hostIP":"10.187.128.43","podIP":"172.30.90.108","podIPs":[{"ip":"172.30.90.108"}],"startTime":"2023-01-28T00:13:42Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-28T00:13:46Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://584191487c6bebb967b1e76bb35e4b76f57c33df279b4cd6d1f017f2450badf8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:13:47.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2776" for this suite.

• [SLOW TEST:5.640 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":356,"completed":2,"skipped":22,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:47.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3322
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check is all data is printed  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:13:47.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3322 version'
Jan 28 00:13:47.708: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 28 00:13:47.708: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.10\", GitCommit:\"5c1d2d4295f9b4eb12bfbf6429fdf989f2ca8a02\", GitTreeState:\"clean\", BuildDate:\"2023-01-18T19:15:31Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.4\nServer Version: version.Info{Major:\"1\", Minor:\"24\", GitVersion:\"v1.24.10+IKS\", GitCommit:\"9319cb2a310085ab72ab684994c8d9bb03903426\", GitTreeState:\"clean\", BuildDate:\"2023-01-19T22:49:34Z\", GoVersion:\"go1.19.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:13:47.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3322" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":356,"completed":3,"skipped":29,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:47.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1858
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-7d5b717a-0f7f-4d0d-9968-b3e79da520d3
STEP: Creating a pod to test consume secrets
Jan 28 00:13:48.047: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9" in namespace "projected-1858" to be "Succeeded or Failed"
Jan 28 00:13:48.065: INFO: Pod "pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.404278ms
Jan 28 00:13:50.109: INFO: Pod "pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062067031s
Jan 28 00:13:52.135: INFO: Pod "pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087942955s
STEP: Saw pod success
Jan 28 00:13:52.135: INFO: Pod "pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9" satisfied condition "Succeeded or Failed"
Jan 28 00:13:52.153: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:13:52.250: INFO: Waiting for pod pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9 to disappear
Jan 28 00:13:52.267: INFO: Pod pod-projected-secrets-e146bd00-95db-43b6-8366-ee5ba576eeb9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 00:13:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1858" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":4,"skipped":62,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:52.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-7574
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jan 28 00:13:52.531: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jan 28 00:13:52.562: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 28 00:13:52.562: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jan 28 00:13:52.603: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 28 00:13:52.603: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jan 28 00:13:52.645: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 28 00:13:52.645: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jan 28 00:13:59.799: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:188
Jan 28 00:13:59.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7574" for this suite.

• [SLOW TEST:7.595 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":356,"completed":5,"skipped":75,"failed":0}
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:13:59.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8548
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 28 00:14:00.169: INFO: The status of Pod annotationupdateefffc6ac-ab31-4dda-b3b7-144fcb33a00a is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:14:02.201: INFO: The status of Pod annotationupdateefffc6ac-ab31-4dda-b3b7-144fcb33a00a is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:14:04.192: INFO: The status of Pod annotationupdateefffc6ac-ab31-4dda-b3b7-144fcb33a00a is Running (Ready = true)
Jan 28 00:14:04.871: INFO: Successfully updated pod "annotationupdateefffc6ac-ab31-4dda-b3b7-144fcb33a00a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:14:06.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8548" for this suite.

• [SLOW TEST:7.098 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":6,"skipped":77,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:07.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4582
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 00:14:07.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4582" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":356,"completed":7,"skipped":79,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:07.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5513
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:14:08.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 14, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 14, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 14, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 14, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:14:11.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jan 28 00:14:13.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=webhook-5513 attach --namespace=webhook-5513 to-be-attached-pod -i -c=container1'
Jan 28 00:14:13.562: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:14:13.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5513" for this suite.
STEP: Destroying namespace "webhook-5513-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.454 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":356,"completed":8,"skipped":104,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:13.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8084
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:14:14.096: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33" in namespace "downward-api-8084" to be "Succeeded or Failed"
Jan 28 00:14:14.115: INFO: Pod "downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33": Phase="Pending", Reason="", readiness=false. Elapsed: 18.415719ms
Jan 28 00:14:16.137: INFO: Pod "downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04117043s
Jan 28 00:14:18.158: INFO: Pod "downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062121455s
STEP: Saw pod success
Jan 28 00:14:18.158: INFO: Pod "downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33" satisfied condition "Succeeded or Failed"
Jan 28 00:14:18.174: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33 container client-container: <nil>
STEP: delete the pod
Jan 28 00:14:18.267: INFO: Waiting for pod downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33 to disappear
Jan 28 00:14:18.285: INFO: Pod downwardapi-volume-6d345362-9a4e-4387-b2f6-0ce2eb496a33 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:14:18.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8084" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":9,"skipped":115,"failed":0}
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:18.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4968
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-tv598 in namespace proxy-4968
I0128 00:14:18.641106      26 runners.go:193] Created replication controller with name: proxy-service-tv598, namespace: proxy-4968, replica count: 1
I0128 00:14:19.693141      26 runners.go:193] proxy-service-tv598 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:14:20.693341      26 runners.go:193] proxy-service-tv598 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:14:21.693553      26 runners.go:193] proxy-service-tv598 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0128 00:14:22.693944      26 runners.go:193] proxy-service-tv598 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 00:14:22.713: INFO: setup took 4.162854518s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 28 00:14:22.880: INFO: (0) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 165.953744ms)
Jan 28 00:14:22.880: INFO: (0) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 166.292283ms)
Jan 28 00:14:22.896: INFO: (0) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 183.038196ms)
Jan 28 00:14:22.897: INFO: (0) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 184.152675ms)
Jan 28 00:14:22.899: INFO: (0) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 185.164048ms)
Jan 28 00:14:22.915: INFO: (0) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 201.490188ms)
Jan 28 00:14:22.916: INFO: (0) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 202.350371ms)
Jan 28 00:14:22.917: INFO: (0) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 203.472533ms)
Jan 28 00:14:22.919: INFO: (0) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 205.440525ms)
Jan 28 00:14:22.919: INFO: (0) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 205.680135ms)
Jan 28 00:14:22.920: INFO: (0) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 207.356965ms)
Jan 28 00:14:22.922: INFO: (0) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 208.69916ms)
Jan 28 00:14:22.923: INFO: (0) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 209.55386ms)
Jan 28 00:14:22.924: INFO: (0) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 210.536169ms)
Jan 28 00:14:22.927: INFO: (0) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 212.669782ms)
Jan 28 00:14:22.927: INFO: (0) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 213.314158ms)
Jan 28 00:14:22.950: INFO: (1) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 23.037971ms)
Jan 28 00:14:22.957: INFO: (1) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 30.540056ms)
Jan 28 00:14:22.958: INFO: (1) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 31.634784ms)
Jan 28 00:14:22.958: INFO: (1) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 30.962683ms)
Jan 28 00:14:22.959: INFO: (1) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 31.949056ms)
Jan 28 00:14:22.959: INFO: (1) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 32.007484ms)
Jan 28 00:14:22.959: INFO: (1) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 31.776316ms)
Jan 28 00:14:22.959: INFO: (1) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 32.250995ms)
Jan 28 00:14:22.960: INFO: (1) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.433506ms)
Jan 28 00:14:22.960: INFO: (1) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 32.56012ms)
Jan 28 00:14:22.963: INFO: (1) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 36.199638ms)
Jan 28 00:14:22.963: INFO: (1) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 35.936012ms)
Jan 28 00:14:22.966: INFO: (1) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 38.985155ms)
Jan 28 00:14:22.967: INFO: (1) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 39.036567ms)
Jan 28 00:14:22.968: INFO: (1) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 40.639624ms)
Jan 28 00:14:22.968: INFO: (1) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 41.244743ms)
Jan 28 00:14:22.989: INFO: (2) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 20.477963ms)
Jan 28 00:14:22.999: INFO: (2) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 29.877732ms)
Jan 28 00:14:23.001: INFO: (2) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 32.717769ms)
Jan 28 00:14:23.002: INFO: (2) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 33.519287ms)
Jan 28 00:14:23.002: INFO: (2) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 33.218403ms)
Jan 28 00:14:23.002: INFO: (2) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 33.836143ms)
Jan 28 00:14:23.002: INFO: (2) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 33.503993ms)
Jan 28 00:14:23.002: INFO: (2) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 33.249312ms)
Jan 28 00:14:23.004: INFO: (2) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 34.479163ms)
Jan 28 00:14:23.004: INFO: (2) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 35.644898ms)
Jan 28 00:14:23.006: INFO: (2) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 37.685776ms)
Jan 28 00:14:23.009: INFO: (2) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 40.438122ms)
Jan 28 00:14:23.010: INFO: (2) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 41.112289ms)
Jan 28 00:14:23.011: INFO: (2) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 42.058259ms)
Jan 28 00:14:23.011: INFO: (2) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 42.04584ms)
Jan 28 00:14:23.011: INFO: (2) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 42.409526ms)
Jan 28 00:14:23.036: INFO: (3) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 23.903976ms)
Jan 28 00:14:23.044: INFO: (3) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 31.990912ms)
Jan 28 00:14:23.045: INFO: (3) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 33.005683ms)
Jan 28 00:14:23.046: INFO: (3) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 34.292264ms)
Jan 28 00:14:23.046: INFO: (3) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 34.177981ms)
Jan 28 00:14:23.046: INFO: (3) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 34.341556ms)
Jan 28 00:14:23.050: INFO: (3) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 38.547385ms)
Jan 28 00:14:23.051: INFO: (3) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 39.182671ms)
Jan 28 00:14:23.061: INFO: (3) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 49.259705ms)
Jan 28 00:14:23.062: INFO: (3) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 50.055926ms)
Jan 28 00:14:23.062: INFO: (3) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 50.578648ms)
Jan 28 00:14:23.062: INFO: (3) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 50.343874ms)
Jan 28 00:14:23.063: INFO: (3) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 50.605131ms)
Jan 28 00:14:23.063: INFO: (3) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 50.898348ms)
Jan 28 00:14:23.063: INFO: (3) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 50.982802ms)
Jan 28 00:14:23.063: INFO: (3) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 51.14957ms)
Jan 28 00:14:23.087: INFO: (4) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 23.722212ms)
Jan 28 00:14:23.093: INFO: (4) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 29.80705ms)
Jan 28 00:14:23.093: INFO: (4) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 30.074505ms)
Jan 28 00:14:23.094: INFO: (4) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 30.281794ms)
Jan 28 00:14:23.094: INFO: (4) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 30.447083ms)
Jan 28 00:14:23.094: INFO: (4) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 31.631271ms)
Jan 28 00:14:23.095: INFO: (4) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 32.477956ms)
Jan 28 00:14:23.097: INFO: (4) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 33.231448ms)
Jan 28 00:14:23.097: INFO: (4) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 33.501453ms)
Jan 28 00:14:23.097: INFO: (4) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 34.087656ms)
Jan 28 00:14:23.100: INFO: (4) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 36.804199ms)
Jan 28 00:14:23.100: INFO: (4) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 37.281278ms)
Jan 28 00:14:23.102: INFO: (4) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 38.622433ms)
Jan 28 00:14:23.102: INFO: (4) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 39.053328ms)
Jan 28 00:14:23.102: INFO: (4) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 38.888224ms)
Jan 28 00:14:23.102: INFO: (4) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 38.873966ms)
Jan 28 00:14:23.125: INFO: (5) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 22.762497ms)
Jan 28 00:14:23.131: INFO: (5) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 28.173743ms)
Jan 28 00:14:23.131: INFO: (5) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 28.627132ms)
Jan 28 00:14:23.132: INFO: (5) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 29.440245ms)
Jan 28 00:14:23.135: INFO: (5) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.224502ms)
Jan 28 00:14:23.135: INFO: (5) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 32.688801ms)
Jan 28 00:14:23.135: INFO: (5) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 32.425913ms)
Jan 28 00:14:23.135: INFO: (5) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 32.650017ms)
Jan 28 00:14:23.136: INFO: (5) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 32.975146ms)
Jan 28 00:14:23.136: INFO: (5) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 33.069543ms)
Jan 28 00:14:23.136: INFO: (5) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.952753ms)
Jan 28 00:14:23.147: INFO: (5) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 44.132028ms)
Jan 28 00:14:23.148: INFO: (5) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 44.859361ms)
Jan 28 00:14:23.148: INFO: (5) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 44.717781ms)
Jan 28 00:14:23.149: INFO: (5) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 46.67804ms)
Jan 28 00:14:23.149: INFO: (5) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 46.205671ms)
Jan 28 00:14:23.174: INFO: (6) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 24.332821ms)
Jan 28 00:14:23.182: INFO: (6) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 31.97876ms)
Jan 28 00:14:23.182: INFO: (6) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 32.486984ms)
Jan 28 00:14:23.182: INFO: (6) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 32.682613ms)
Jan 28 00:14:23.183: INFO: (6) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 33.466183ms)
Jan 28 00:14:23.183: INFO: (6) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 34.032991ms)
Jan 28 00:14:23.184: INFO: (6) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 34.758938ms)
Jan 28 00:14:23.184: INFO: (6) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 34.502973ms)
Jan 28 00:14:23.184: INFO: (6) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 34.578425ms)
Jan 28 00:14:23.185: INFO: (6) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 35.294448ms)
Jan 28 00:14:23.188: INFO: (6) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 37.884656ms)
Jan 28 00:14:23.188: INFO: (6) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 38.339916ms)
Jan 28 00:14:23.188: INFO: (6) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 38.729106ms)
Jan 28 00:14:23.188: INFO: (6) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 38.80672ms)
Jan 28 00:14:23.191: INFO: (6) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 41.129856ms)
Jan 28 00:14:23.191: INFO: (6) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 41.316281ms)
Jan 28 00:14:23.213: INFO: (7) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 21.433827ms)
Jan 28 00:14:23.221: INFO: (7) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 28.459128ms)
Jan 28 00:14:23.222: INFO: (7) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 30.139831ms)
Jan 28 00:14:23.222: INFO: (7) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 29.737471ms)
Jan 28 00:14:23.222: INFO: (7) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 29.778109ms)
Jan 28 00:14:23.223: INFO: (7) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 30.343593ms)
Jan 28 00:14:23.223: INFO: (7) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 29.571962ms)
Jan 28 00:14:23.223: INFO: (7) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 29.538624ms)
Jan 28 00:14:23.223: INFO: (7) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 30.324701ms)
Jan 28 00:14:23.226: INFO: (7) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 32.597403ms)
Jan 28 00:14:23.226: INFO: (7) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 34.203912ms)
Jan 28 00:14:23.227: INFO: (7) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 35.800669ms)
Jan 28 00:14:23.228: INFO: (7) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 36.210409ms)
Jan 28 00:14:23.231: INFO: (7) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 38.698491ms)
Jan 28 00:14:23.231: INFO: (7) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 38.887857ms)
Jan 28 00:14:23.233: INFO: (7) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 39.657284ms)
Jan 28 00:14:23.255: INFO: (8) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.508663ms)
Jan 28 00:14:23.261: INFO: (8) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 27.203891ms)
Jan 28 00:14:23.262: INFO: (8) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 28.218732ms)
Jan 28 00:14:23.262: INFO: (8) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 28.83758ms)
Jan 28 00:14:23.263: INFO: (8) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 29.208747ms)
Jan 28 00:14:23.263: INFO: (8) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 29.536306ms)
Jan 28 00:14:23.263: INFO: (8) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 29.864283ms)
Jan 28 00:14:23.264: INFO: (8) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 30.281858ms)
Jan 28 00:14:23.264: INFO: (8) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 30.777161ms)
Jan 28 00:14:23.264: INFO: (8) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 30.374038ms)
Jan 28 00:14:23.266: INFO: (8) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 33.194991ms)
Jan 28 00:14:23.267: INFO: (8) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 33.129959ms)
Jan 28 00:14:23.270: INFO: (8) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 36.355884ms)
Jan 28 00:14:23.270: INFO: (8) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 36.339544ms)
Jan 28 00:14:23.270: INFO: (8) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 36.986094ms)
Jan 28 00:14:23.274: INFO: (8) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 40.767364ms)
Jan 28 00:14:23.297: INFO: (9) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.505416ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 35.88056ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 36.275234ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 35.914675ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 36.421565ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 36.167549ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 36.488674ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 37.635206ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 36.901638ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 37.024758ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 36.508518ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 36.835523ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 36.506282ms)
Jan 28 00:14:23.312: INFO: (9) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 36.812186ms)
Jan 28 00:14:23.323: INFO: (9) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 47.32006ms)
Jan 28 00:14:23.323: INFO: (9) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 47.818055ms)
Jan 28 00:14:23.346: INFO: (10) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 23.179561ms)
Jan 28 00:14:23.356: INFO: (10) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 32.878504ms)
Jan 28 00:14:23.357: INFO: (10) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 33.021544ms)
Jan 28 00:14:23.357: INFO: (10) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 33.492218ms)
Jan 28 00:14:23.357: INFO: (10) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 33.880053ms)
Jan 28 00:14:23.357: INFO: (10) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 33.969745ms)
Jan 28 00:14:23.358: INFO: (10) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 34.524457ms)
Jan 28 00:14:23.358: INFO: (10) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 34.205168ms)
Jan 28 00:14:23.358: INFO: (10) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 34.189328ms)
Jan 28 00:14:23.359: INFO: (10) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 35.870333ms)
Jan 28 00:14:23.363: INFO: (10) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 39.587913ms)
Jan 28 00:14:23.364: INFO: (10) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 40.392367ms)
Jan 28 00:14:23.364: INFO: (10) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 40.729049ms)
Jan 28 00:14:23.364: INFO: (10) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 40.622927ms)
Jan 28 00:14:23.364: INFO: (10) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 40.747683ms)
Jan 28 00:14:23.367: INFO: (10) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 43.762868ms)
Jan 28 00:14:23.388: INFO: (11) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 20.387101ms)
Jan 28 00:14:23.392: INFO: (11) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 24.79408ms)
Jan 28 00:14:23.394: INFO: (11) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 25.964355ms)
Jan 28 00:14:23.395: INFO: (11) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 27.53361ms)
Jan 28 00:14:23.396: INFO: (11) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 27.730309ms)
Jan 28 00:14:23.396: INFO: (11) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 28.159147ms)
Jan 28 00:14:23.397: INFO: (11) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 29.104926ms)
Jan 28 00:14:23.397: INFO: (11) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 29.432196ms)
Jan 28 00:14:23.397: INFO: (11) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 29.627555ms)
Jan 28 00:14:23.397: INFO: (11) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 29.445265ms)
Jan 28 00:14:23.403: INFO: (11) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 35.011491ms)
Jan 28 00:14:23.403: INFO: (11) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 34.713961ms)
Jan 28 00:14:23.403: INFO: (11) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 35.247105ms)
Jan 28 00:14:23.403: INFO: (11) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 35.002707ms)
Jan 28 00:14:23.405: INFO: (11) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 37.560191ms)
Jan 28 00:14:23.405: INFO: (11) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 37.671398ms)
Jan 28 00:14:23.427: INFO: (12) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 21.747337ms)
Jan 28 00:14:23.436: INFO: (12) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 30.129603ms)
Jan 28 00:14:23.437: INFO: (12) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 31.640891ms)
Jan 28 00:14:23.437: INFO: (12) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 31.571534ms)
Jan 28 00:14:23.438: INFO: (12) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.558826ms)
Jan 28 00:14:23.438: INFO: (12) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.174731ms)
Jan 28 00:14:23.438: INFO: (12) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 32.665917ms)
Jan 28 00:14:23.439: INFO: (12) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 32.853609ms)
Jan 28 00:14:23.441: INFO: (12) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 35.232878ms)
Jan 28 00:14:23.444: INFO: (12) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 38.636501ms)
Jan 28 00:14:23.445: INFO: (12) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 38.866558ms)
Jan 28 00:14:23.447: INFO: (12) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 41.220793ms)
Jan 28 00:14:23.447: INFO: (12) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 41.539367ms)
Jan 28 00:14:23.447: INFO: (12) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 42.092144ms)
Jan 28 00:14:23.449: INFO: (12) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 43.596921ms)
Jan 28 00:14:23.449: INFO: (12) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 43.841915ms)
Jan 28 00:14:23.564: INFO: (13) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 113.421568ms)
Jan 28 00:14:23.564: INFO: (13) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 113.962741ms)
Jan 28 00:14:23.564: INFO: (13) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 114.07685ms)
Jan 28 00:14:23.565: INFO: (13) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 115.143825ms)
Jan 28 00:14:23.565: INFO: (13) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 114.791586ms)
Jan 28 00:14:23.566: INFO: (13) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 115.994655ms)
Jan 28 00:14:23.569: INFO: (13) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 119.227253ms)
Jan 28 00:14:23.574: INFO: (13) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 123.715012ms)
Jan 28 00:14:23.574: INFO: (13) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 123.994475ms)
Jan 28 00:14:23.575: INFO: (13) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 124.843692ms)
Jan 28 00:14:23.575: INFO: (13) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 124.199831ms)
Jan 28 00:14:23.582: INFO: (13) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 131.201781ms)
Jan 28 00:14:23.586: INFO: (13) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 136.677182ms)
Jan 28 00:14:23.587: INFO: (13) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 136.375518ms)
Jan 28 00:14:23.587: INFO: (13) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 137.145937ms)
Jan 28 00:14:23.587: INFO: (13) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 136.73017ms)
Jan 28 00:14:23.608: INFO: (14) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 21.154992ms)
Jan 28 00:14:23.609: INFO: (14) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 21.632936ms)
Jan 28 00:14:23.609: INFO: (14) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 22.025133ms)
Jan 28 00:14:23.609: INFO: (14) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 21.729147ms)
Jan 28 00:14:23.610: INFO: (14) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 22.04851ms)
Jan 28 00:14:23.610: INFO: (14) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 22.266434ms)
Jan 28 00:14:23.610: INFO: (14) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 22.434834ms)
Jan 28 00:14:23.620: INFO: (14) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.543311ms)
Jan 28 00:14:23.620: INFO: (14) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 32.650417ms)
Jan 28 00:14:23.620: INFO: (14) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 32.908175ms)
Jan 28 00:14:23.622: INFO: (14) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 34.592599ms)
Jan 28 00:14:23.626: INFO: (14) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 38.670852ms)
Jan 28 00:14:23.626: INFO: (14) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 38.617447ms)
Jan 28 00:14:23.627: INFO: (14) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 39.289328ms)
Jan 28 00:14:23.627: INFO: (14) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 39.881578ms)
Jan 28 00:14:23.627: INFO: (14) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 39.508057ms)
Jan 28 00:14:23.645: INFO: (15) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 17.147252ms)
Jan 28 00:14:23.648: INFO: (15) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 20.172822ms)
Jan 28 00:14:23.648: INFO: (15) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 20.561338ms)
Jan 28 00:14:23.649: INFO: (15) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.340215ms)
Jan 28 00:14:23.649: INFO: (15) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.642928ms)
Jan 28 00:14:23.650: INFO: (15) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 22.504909ms)
Jan 28 00:14:23.650: INFO: (15) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 22.564754ms)
Jan 28 00:14:23.651: INFO: (15) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 23.762184ms)
Jan 28 00:14:23.652: INFO: (15) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 23.927006ms)
Jan 28 00:14:23.652: INFO: (15) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 24.596705ms)
Jan 28 00:14:23.653: INFO: (15) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 26.09794ms)
Jan 28 00:14:23.658: INFO: (15) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 30.109042ms)
Jan 28 00:14:23.658: INFO: (15) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 30.618572ms)
Jan 28 00:14:23.659: INFO: (15) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 31.207915ms)
Jan 28 00:14:23.659: INFO: (15) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 31.943031ms)
Jan 28 00:14:23.660: INFO: (15) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 32.233007ms)
Jan 28 00:14:23.682: INFO: (16) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.15953ms)
Jan 28 00:14:23.682: INFO: (16) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 21.910363ms)
Jan 28 00:14:23.682: INFO: (16) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 21.993353ms)
Jan 28 00:14:23.683: INFO: (16) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 22.665111ms)
Jan 28 00:14:23.684: INFO: (16) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 23.242846ms)
Jan 28 00:14:23.684: INFO: (16) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 23.163258ms)
Jan 28 00:14:23.684: INFO: (16) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 23.485408ms)
Jan 28 00:14:23.686: INFO: (16) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 25.478802ms)
Jan 28 00:14:23.686: INFO: (16) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 26.31829ms)
Jan 28 00:14:23.686: INFO: (16) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 26.409807ms)
Jan 28 00:14:23.689: INFO: (16) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 28.326627ms)
Jan 28 00:14:23.690: INFO: (16) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 29.257876ms)
Jan 28 00:14:23.690: INFO: (16) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 29.418534ms)
Jan 28 00:14:23.696: INFO: (16) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 35.405718ms)
Jan 28 00:14:23.696: INFO: (16) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 35.174191ms)
Jan 28 00:14:23.697: INFO: (16) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 36.491482ms)
Jan 28 00:14:23.712: INFO: (17) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 15.219639ms)
Jan 28 00:14:23.719: INFO: (17) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 21.547042ms)
Jan 28 00:14:23.719: INFO: (17) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 21.687338ms)
Jan 28 00:14:23.720: INFO: (17) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 21.720706ms)
Jan 28 00:14:23.720: INFO: (17) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 21.952224ms)
Jan 28 00:14:23.720: INFO: (17) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 22.501451ms)
Jan 28 00:14:23.720: INFO: (17) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 22.591683ms)
Jan 28 00:14:23.720: INFO: (17) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 22.356728ms)
Jan 28 00:14:23.721: INFO: (17) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 22.63479ms)
Jan 28 00:14:23.721: INFO: (17) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 23.498782ms)
Jan 28 00:14:23.727: INFO: (17) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 29.142408ms)
Jan 28 00:14:23.727: INFO: (17) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 29.407168ms)
Jan 28 00:14:23.727: INFO: (17) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 29.69673ms)
Jan 28 00:14:23.727: INFO: (17) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 29.044913ms)
Jan 28 00:14:23.727: INFO: (17) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 29.720395ms)
Jan 28 00:14:23.740: INFO: (17) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 42.381305ms)
Jan 28 00:14:23.758: INFO: (18) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 17.739725ms)
Jan 28 00:14:23.760: INFO: (18) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 19.600433ms)
Jan 28 00:14:23.760: INFO: (18) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 19.169122ms)
Jan 28 00:14:23.760: INFO: (18) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 20.414257ms)
Jan 28 00:14:23.760: INFO: (18) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 20.262365ms)
Jan 28 00:14:23.761: INFO: (18) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 20.451486ms)
Jan 28 00:14:23.761: INFO: (18) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 20.95191ms)
Jan 28 00:14:23.761: INFO: (18) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 21.41058ms)
Jan 28 00:14:23.762: INFO: (18) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 21.239435ms)
Jan 28 00:14:23.762: INFO: (18) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 21.85611ms)
Jan 28 00:14:23.762: INFO: (18) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 21.600811ms)
Jan 28 00:14:23.765: INFO: (18) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 24.485374ms)
Jan 28 00:14:23.775: INFO: (18) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 34.905156ms)
Jan 28 00:14:23.775: INFO: (18) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 35.166387ms)
Jan 28 00:14:23.776: INFO: (18) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 35.354683ms)
Jan 28 00:14:23.776: INFO: (18) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 35.535014ms)
Jan 28 00:14:23.798: INFO: (19) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:443/proxy/tlsrewritem... (200; 21.976776ms)
Jan 28 00:14:23.799: INFO: (19) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:1080/proxy/rewriteme">... (200; 22.424451ms)
Jan 28 00:14:23.799: INFO: (19) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:162/proxy/: bar (200; 23.125884ms)
Jan 28 00:14:23.799: INFO: (19) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7/proxy/rewriteme">test</a> (200; 23.308939ms)
Jan 28 00:14:23.800: INFO: (19) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:462/proxy/: tls qux (200; 23.554594ms)
Jan 28 00:14:23.800: INFO: (19) /api/v1/namespaces/proxy-4968/pods/https:proxy-service-tv598-lhkt7:460/proxy/: tls baz (200; 23.895154ms)
Jan 28 00:14:23.800: INFO: (19) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/: <a href="/api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:1080/proxy/rewriteme">test<... (200; 24.109839ms)
Jan 28 00:14:23.801: INFO: (19) /api/v1/namespaces/proxy-4968/pods/http:proxy-service-tv598-lhkt7:160/proxy/: foo (200; 24.551437ms)
Jan 28 00:14:23.801: INFO: (19) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:160/proxy/: foo (200; 24.67962ms)
Jan 28 00:14:23.801: INFO: (19) /api/v1/namespaces/proxy-4968/pods/proxy-service-tv598-lhkt7:162/proxy/: bar (200; 25.1313ms)
Jan 28 00:14:23.807: INFO: (19) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname2/proxy/: tls qux (200; 30.528849ms)
Jan 28 00:14:23.807: INFO: (19) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname1/proxy/: foo (200; 31.344496ms)
Jan 28 00:14:23.808: INFO: (19) /api/v1/namespaces/proxy-4968/services/https:proxy-service-tv598:tlsportname1/proxy/: tls baz (200; 32.007849ms)
Jan 28 00:14:23.809: INFO: (19) /api/v1/namespaces/proxy-4968/services/http:proxy-service-tv598:portname2/proxy/: bar (200; 32.615168ms)
Jan 28 00:14:23.809: INFO: (19) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname2/proxy/: bar (200; 32.733471ms)
Jan 28 00:14:23.809: INFO: (19) /api/v1/namespaces/proxy-4968/services/proxy-service-tv598:portname1/proxy/: foo (200; 32.775363ms)
STEP: deleting ReplicationController proxy-service-tv598 in namespace proxy-4968, will wait for the garbage collector to delete the pods
Jan 28 00:14:23.921: INFO: Deleting ReplicationController proxy-service-tv598 took: 24.788072ms
Jan 28 00:14:24.022: INFO: Terminating ReplicationController proxy-service-tv598 pods took: 100.828736ms
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 28 00:14:24.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4968" for this suite.

• [SLOW TEST:6.653 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":356,"completed":10,"skipped":120,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3953
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1540
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 28 00:14:25.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3953 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jan 28 00:14:25.300: INFO: stderr: ""
Jan 28 00:14:25.300: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1544
Jan 28 00:14:25.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3953 delete pods e2e-test-httpd-pod'
Jan 28 00:14:28.572: INFO: stderr: ""
Jan 28 00:14:28.572: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:14:28.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3953" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":356,"completed":11,"skipped":125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:28.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3137
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:14:28.959: INFO: Create a RollingUpdate DaemonSet
Jan 28 00:14:29.007: INFO: Check that daemon pods launch on every node of the cluster
Jan 28 00:14:29.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:14:29.045: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:14:30.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:14:30.090: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:14:31.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 00:14:31.124: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:14:32.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:14:32.093: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan 28 00:14:32.093: INFO: Update the DaemonSet to trigger a rollout
Jan 28 00:14:32.136: INFO: Updating DaemonSet daemon-set
Jan 28 00:14:35.266: INFO: Roll back the DaemonSet before rollout is complete
Jan 28 00:14:35.319: INFO: Updating DaemonSet daemon-set
Jan 28 00:14:35.319: INFO: Make sure DaemonSet rollback is complete
Jan 28 00:14:35.341: INFO: Wrong image for pod: daemon-set-2jt8d. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 28 00:14:35.341: INFO: Pod daemon-set-2jt8d is not available
Jan 28 00:14:38.380: INFO: Pod daemon-set-9bv6d is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3137, will wait for the garbage collector to delete the pods
Jan 28 00:14:38.527: INFO: Deleting DaemonSet.extensions daemon-set took: 25.852643ms
Jan 28 00:14:38.628: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.097902ms
Jan 28 00:14:41.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:14:41.357: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:14:41.378: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18815"},"items":null}

Jan 28 00:14:41.392: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18815"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:14:41.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3137" for this suite.

• [SLOW TEST:12.874 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":356,"completed":12,"skipped":169,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:41.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9689
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:14:41.815: INFO: The status of Pod busybox-readonly-fsb94aebaa-a16c-4452-a82e-4f15355234a8 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:14:43.843: INFO: The status of Pod busybox-readonly-fsb94aebaa-a16c-4452-a82e-4f15355234a8 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:14:45.841: INFO: The status of Pod busybox-readonly-fsb94aebaa-a16c-4452-a82e-4f15355234a8 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 28 00:14:45.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9689" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":13,"skipped":178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1086
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:14:46.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 28 00:14:51.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-1086 --namespace=crd-publish-openapi-1086 create -f -'
Jan 28 00:14:52.051: INFO: stderr: ""
Jan 28 00:14:52.051: INFO: stdout: "e2e-test-crd-publish-openapi-9545-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 28 00:14:52.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-1086 --namespace=crd-publish-openapi-1086 delete e2e-test-crd-publish-openapi-9545-crds test-cr'
Jan 28 00:14:52.242: INFO: stderr: ""
Jan 28 00:14:52.243: INFO: stdout: "e2e-test-crd-publish-openapi-9545-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 28 00:14:52.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-1086 --namespace=crd-publish-openapi-1086 apply -f -'
Jan 28 00:14:52.474: INFO: stderr: ""
Jan 28 00:14:52.474: INFO: stdout: "e2e-test-crd-publish-openapi-9545-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 28 00:14:52.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-1086 --namespace=crd-publish-openapi-1086 delete e2e-test-crd-publish-openapi-9545-crds test-cr'
Jan 28 00:14:52.574: INFO: stderr: ""
Jan 28 00:14:52.574: INFO: stdout: "e2e-test-crd-publish-openapi-9545-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 28 00:14:52.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-1086 explain e2e-test-crd-publish-openapi-9545-crds'
Jan 28 00:14:53.181: INFO: stderr: ""
Jan 28 00:14:53.181: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9545-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:14:56.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1086" for this suite.

• [SLOW TEST:10.507 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":356,"completed":14,"skipped":204,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:14:56.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8439
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 28 00:15:01.979: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 28 00:15:02.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8439" for this suite.

• [SLOW TEST:5.582 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":15,"skipped":214,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:15:02.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6106
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6106
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 28 00:15:02.384: INFO: Found 0 stateful pods, waiting for 3
Jan 28 00:15:12.408: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:15:12.408: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:15:12.408: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:15:12.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6106 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:15:12.754: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:15:12.754: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:15:12.754: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 28 00:15:22.917: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 28 00:15:33.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6106 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:15:33.306: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 00:15:33.306: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:15:33.306: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Jan 28 00:15:53.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6106 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:15:53.702: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:15:53.702: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:15:53.702: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:16:03.852: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 28 00:16:13.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6106 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:16:14.254: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 00:16:14.254: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:16:14.254: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:16:24.361: INFO: Waiting for StatefulSet statefulset-6106/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:16:34.404: INFO: Deleting all statefulset in ns statefulset-6106
Jan 28 00:16:34.420: INFO: Scaling statefulset ss2 to 0
Jan 28 00:16:44.512: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:16:44.526: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 00:16:44.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6106" for this suite.

• [SLOW TEST:102.550 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":356,"completed":16,"skipped":214,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:16:44.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2036
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support proxy with --port 0  [Conformance]
  test/e2e/framework/framework.go:652
STEP: starting the proxy server
Jan 28 00:16:44.900: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-2036 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:16:44.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2036" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":356,"completed":17,"skipped":216,"failed":0}
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:16:45.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-8116
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 00:16:49.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-8116" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":18,"skipped":222,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:16:49.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3657
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:16:49.775: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729" in namespace "security-context-test-3657" to be "Succeeded or Failed"
Jan 28 00:16:49.792: INFO: Pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729": Phase="Pending", Reason="", readiness=false. Elapsed: 17.09089ms
Jan 28 00:16:51.823: INFO: Pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047435262s
Jan 28 00:16:53.844: INFO: Pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068819624s
Jan 28 00:16:55.874: INFO: Pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098515216s
Jan 28 00:16:55.874: INFO: Pod "alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 00:16:55.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3657" for this suite.

• [SLOW TEST:6.449 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:298
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":19,"skipped":240,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:16:55.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2874
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-7965c5a2-fb83-49d0-9dde-a7588c59a067
STEP: Creating a pod to test consume secrets
Jan 28 00:16:56.255: INFO: Waiting up to 5m0s for pod "pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b" in namespace "secrets-2874" to be "Succeeded or Failed"
Jan 28 00:16:56.272: INFO: Pod "pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.334475ms
Jan 28 00:16:58.296: INFO: Pod "pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041357577s
Jan 28 00:17:00.340: INFO: Pod "pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084857687s
STEP: Saw pod success
Jan 28 00:17:00.340: INFO: Pod "pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b" satisfied condition "Succeeded or Failed"
Jan 28 00:17:00.362: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:17:00.438: INFO: Waiting for pod pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b to disappear
Jan 28 00:17:00.454: INFO: Pod pod-secrets-d9f34668-1bb8-4a01-93c7-60aef3d2ec6b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 00:17:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2874" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":20,"skipped":245,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:00.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2846
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 00:17:00.734: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 00:17:00.778: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 00:17:00.799: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.27 before test
Jan 28 00:17:00.841: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.841: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 00:17:00.841: INFO: calico-node-9md4k from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.841: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:17:00.841: INFO: calico-typha-987c59c59-r2mxh from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.841: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:17:00.841: INFO: coredns-649f45bb5-4l67p from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.841: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:17:00.842: INFO: ibm-keepalived-watcher-qxwck from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:17:00.842: INFO: ibm-master-proxy-static-10.187.128.27 from kube-system started at 2023-01-27 22:02:32 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:17:00.842: INFO: ibmcloud-block-storage-driver-mfqhd from kube-system started at 2023-01-27 22:02:42 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:17:00.842: INFO: ingress-cluster-healthcheck-56756684f7-qbpsl from kube-system started at 2023-01-27 22:43:38 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 00:17:00.842: INFO: konnectivity-agent-8ws48 from kube-system started at 2023-01-27 22:37:37 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:17:00.842: INFO: metrics-server-666774474b-hfjbk from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:17:00.842: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:17:00.842: INFO: sonobuoy-e2e-job-68fa4b4566bd42ca from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container e2e ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:17:00.842: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:17:00.842: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.30 before test
Jan 28 00:17:00.880: INFO: calico-node-lg9x8 from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.880: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:17:00.880: INFO: calico-typha-987c59c59-qs9ww from kube-system started at 2023-01-27 22:03:10 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:17:00.881: INFO: coredns-649f45bb5-9gxmz from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:17:00.881: INFO: ibm-keepalived-watcher-p94jf from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:17:00.881: INFO: ibm-master-proxy-static-10.187.128.30 from kube-system started at 2023-01-27 22:02:39 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:17:00.881: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:17:00.881: INFO: ibmcloud-block-storage-driver-mqmrs from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:17:00.881: INFO: konnectivity-agent-nbg9q from kube-system started at 2023-01-27 22:37:30 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:17:00.881: INFO: metrics-server-666774474b-x2b75 from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:17:00.881: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:17:00.881: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:17:00.881: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-grkj5 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:17:00.881: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-2j8w6 from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:17:00.881: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:17:00.881: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:33:03 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.881: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 00:17:00.881: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.43 before test
Jan 28 00:17:00.933: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-h27j5 from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 00:17:00.933: INFO: calico-kube-controllers-6c58444bc9-tvxlr from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 00:17:00.933: INFO: calico-node-tfsm6 from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:17:00.933: INFO: calico-typha-987c59c59-lzxm5 from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:17:00.933: INFO: coredns-649f45bb5-9w6d2 from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:17:00.933: INFO: coredns-autoscaler-64db77d767-4bnrk from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 00:17:00.933: INFO: dashboard-metrics-scraper-f74668d5f-n58pz from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibm-file-plugin-6b4fdfc7d8-6p4mp from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibm-keepalived-watcher-9flqg from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibm-master-proxy-static-10.187.128.43 from kube-system started at 2023-01-27 22:02:29 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:17:00.933: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibm-storage-watcher-6cd9b56547-klq4v from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibmcloud-block-storage-driver-d4h67 from kube-system started at 2023-01-27 22:02:49 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:17:00.933: INFO: ibmcloud-block-storage-plugin-5f9c77dc8c-dwb8r from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 00:17:00.933: INFO: konnectivity-agent-zrrn4 from kube-system started at 2023-01-27 22:37:33 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:17:00.933: INFO: kubernetes-dashboard-65d4f9ccbf-ncq24 from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 00:17:00.933: INFO: alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729 from security-context-test-3657 started at 2023-01-28 00:16:49 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container alpine-nnp-false-b2156a32-f111-4853-8686-5fe1f20e4729 ready: false, restart count 0
Jan 28 00:17:00.933: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:19 +0000 UTC (1 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 00:17:00.933: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-m6nkr from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:17:00.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:17:00.933: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
STEP: verifying the node has the label node 10.187.128.27
STEP: verifying the node has the label node 10.187.128.30
STEP: verifying the node has the label node 10.187.128.43
Jan 28 00:17:01.216: INFO: Pod ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx requesting resource cpu=5m on Node 10.187.128.27
Jan 28 00:17:01.216: INFO: Pod ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-h27j5 requesting resource cpu=5m on Node 10.187.128.43
Jan 28 00:17:01.216: INFO: Pod calico-kube-controllers-6c58444bc9-tvxlr requesting resource cpu=10m on Node 10.187.128.43
Jan 28 00:17:01.216: INFO: Pod calico-node-9md4k requesting resource cpu=250m on Node 10.187.128.27
Jan 28 00:17:01.216: INFO: Pod calico-node-lg9x8 requesting resource cpu=250m on Node 10.187.128.30
Jan 28 00:17:01.216: INFO: Pod calico-node-tfsm6 requesting resource cpu=250m on Node 10.187.128.43
Jan 28 00:17:01.216: INFO: Pod calico-typha-987c59c59-lzxm5 requesting resource cpu=250m on Node 10.187.128.43
Jan 28 00:17:01.216: INFO: Pod calico-typha-987c59c59-qs9ww requesting resource cpu=250m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod calico-typha-987c59c59-r2mxh requesting resource cpu=250m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod coredns-649f45bb5-4l67p requesting resource cpu=100m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod coredns-649f45bb5-9gxmz requesting resource cpu=100m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod coredns-649f45bb5-9w6d2 requesting resource cpu=100m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod coredns-autoscaler-64db77d767-4bnrk requesting resource cpu=5m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod dashboard-metrics-scraper-f74668d5f-n58pz requesting resource cpu=1m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibm-file-plugin-6b4fdfc7d8-6p4mp requesting resource cpu=50m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibm-keepalived-watcher-9flqg requesting resource cpu=5m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibm-keepalived-watcher-p94jf requesting resource cpu=5m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod ibm-keepalived-watcher-qxwck requesting resource cpu=5m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod ibm-master-proxy-static-10.187.128.27 requesting resource cpu=25m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod ibm-master-proxy-static-10.187.128.30 requesting resource cpu=25m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod ibm-master-proxy-static-10.187.128.43 requesting resource cpu=25m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibm-storage-watcher-6cd9b56547-klq4v requesting resource cpu=50m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibmcloud-block-storage-driver-d4h67 requesting resource cpu=50m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ibmcloud-block-storage-driver-mfqhd requesting resource cpu=50m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod ibmcloud-block-storage-driver-mqmrs requesting resource cpu=50m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod ibmcloud-block-storage-plugin-5f9c77dc8c-dwb8r requesting resource cpu=50m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod ingress-cluster-healthcheck-56756684f7-qbpsl requesting resource cpu=10m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod konnectivity-agent-8ws48 requesting resource cpu=10m on Node 10.187.128.27
Jan 28 00:17:01.217: INFO: Pod konnectivity-agent-nbg9q requesting resource cpu=10m on Node 10.187.128.30
Jan 28 00:17:01.217: INFO: Pod konnectivity-agent-zrrn4 requesting resource cpu=10m on Node 10.187.128.43
Jan 28 00:17:01.217: INFO: Pod kubernetes-dashboard-65d4f9ccbf-ncq24 requesting resource cpu=50m on Node 10.187.128.43
Jan 28 00:17:01.218: INFO: Pod metrics-server-666774474b-hfjbk requesting resource cpu=126m on Node 10.187.128.27
Jan 28 00:17:01.218: INFO: Pod metrics-server-666774474b-x2b75 requesting resource cpu=126m on Node 10.187.128.30
Jan 28 00:17:01.218: INFO: Pod public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-grkj5 requesting resource cpu=20m on Node 10.187.128.30
Jan 28 00:17:01.218: INFO: Pod public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8 requesting resource cpu=20m on Node 10.187.128.27
Jan 28 00:17:01.218: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.187.128.43
Jan 28 00:17:01.218: INFO: Pod sonobuoy-e2e-job-68fa4b4566bd42ca requesting resource cpu=0m on Node 10.187.128.27
Jan 28 00:17:01.218: INFO: Pod sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-2j8w6 requesting resource cpu=0m on Node 10.187.128.30
Jan 28 00:17:01.218: INFO: Pod sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-m6nkr requesting resource cpu=0m on Node 10.187.128.43
Jan 28 00:17:01.218: INFO: Pod sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d requesting resource cpu=0m on Node 10.187.128.27
Jan 28 00:17:01.218: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.187.128.30
STEP: Starting Pods to consume most of the cluster CPU.
Jan 28 00:17:01.218: INFO: Creating a pod which consumes cpu=2141m on Node 10.187.128.27
Jan 28 00:17:01.251: INFO: Creating a pod which consumes cpu=2151m on Node 10.187.128.30
Jan 28 00:17:01.283: INFO: Creating a pod which consumes cpu=2099m on Node 10.187.128.43
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb.173e50dcf276a38b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb to 10.187.128.43]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb.173e50dd448c8ae6], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.7"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb.173e50dd51283b2e], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.7" in 211.450437ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb.173e50dd54eea41d], Reason = [Created], Message = [Created container filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb.173e50dd5db22082], Reason = [Started], Message = [Started container filler-pod-23577e5d-2c9b-48be-81d9-486f99524afb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7.173e50dceec8bfb3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7 to 10.187.128.27]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7.173e50dd3f4023f8], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.7"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7.173e50dd49ceb54f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.7" in 177.094824ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7.173e50dd4c8c7c55], Reason = [Created], Message = [Created container filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7.173e50dd5749b6fc], Reason = [Started], Message = [Started container filler-pod-43c9dcc7-c41d-4c66-95d7-e1e4c6e1a8f7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d.173e50dcf03d73a3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2846/filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d to 10.187.128.30]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d.173e50dd4a527296], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.7" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d.173e50dd4d847d37], Reason = [Created], Message = [Created container filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d.173e50dd562c60d5], Reason = [Started], Message = [Started container filler-pod-7d7b972f-e0ce-44d7-b8db-3c8053232e8d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173e50dde8333c0d], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.]
STEP: removing the label node off the node 10.187.128.27
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.187.128.30
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.187.128.43
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:17:06.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2846" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.244 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":356,"completed":21,"skipped":259,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:06.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2185
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 28 00:17:06.992: INFO: Waiting up to 5m0s for pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450" in namespace "emptydir-2185" to be "Succeeded or Failed"
Jan 28 00:17:07.008: INFO: Pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450": Phase="Pending", Reason="", readiness=false. Elapsed: 16.821684ms
Jan 28 00:17:09.036: INFO: Pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044120458s
Jan 28 00:17:11.069: INFO: Pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077266521s
Jan 28 00:17:13.091: INFO: Pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098971088s
STEP: Saw pod success
Jan 28 00:17:13.091: INFO: Pod "pod-cb0e36e6-2c80-48ea-9ec7-613455138450" satisfied condition "Succeeded or Failed"
Jan 28 00:17:13.114: INFO: Trying to get logs from node 10.187.128.43 pod pod-cb0e36e6-2c80-48ea-9ec7-613455138450 container test-container: <nil>
STEP: delete the pod
Jan 28 00:17:13.201: INFO: Waiting for pod pod-cb0e36e6-2c80-48ea-9ec7-613455138450 to disappear
Jan 28 00:17:13.219: INFO: Pod pod-cb0e36e6-2c80-48ea-9ec7-613455138450 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 00:17:13.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2185" for this suite.

• [SLOW TEST:6.524 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":22,"skipped":261,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:13.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8317
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:79
Jan 28 00:17:13.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the sample API server.
Jan 28 00:17:14.077: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 28 00:17:16.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:17:18.309: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:17:20.314: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 17, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-bd4454f8c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:17:22.671: INFO: Waited 309.033434ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jan 28 00:17:22.974: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:69
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:188
Jan 28 00:17:23.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8317" for this suite.

• [SLOW TEST:10.332 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":356,"completed":23,"skipped":268,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:23.610: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8833
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jan 28 00:17:23.878: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:17:25.906: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:17:27.916: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jan 28 00:17:27.974: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:17:30.010: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 28 00:17:30.028: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:30.028: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:30.028: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 00:17:30.254: INFO: Exec stderr: ""
Jan 28 00:17:30.254: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:30.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:30.255: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:30.255: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 00:17:30.485: INFO: Exec stderr: ""
Jan 28 00:17:30.485: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:30.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:30.487: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:30.487: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 00:17:30.661: INFO: Exec stderr: ""
Jan 28 00:17:30.661: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:30.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:30.662: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:30.662: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 00:17:30.826: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 28 00:17:30.826: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:30.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:30.827: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:30.827: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 28 00:17:31.012: INFO: Exec stderr: ""
Jan 28 00:17:31.012: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:31.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:31.013: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:31.013: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 28 00:17:31.200: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 28 00:17:31.200: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:31.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:31.201: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:31.201: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 00:17:31.361: INFO: Exec stderr: ""
Jan 28 00:17:31.362: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:31.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:31.362: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:31.363: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 28 00:17:31.635: INFO: Exec stderr: ""
Jan 28 00:17:31.635: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:31.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:31.636: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:31.636: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 00:17:31.832: INFO: Exec stderr: ""
Jan 28 00:17:31.833: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8833 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:17:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:17:31.834: INFO: ExecWithOptions: Clientset creation
Jan 28 00:17:31.834: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8833/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 28 00:17:32.006: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:188
Jan 28 00:17:32.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8833" for this suite.

• [SLOW TEST:8.475 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":24,"skipped":288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6136
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 28 00:17:39.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6136" for this suite.

• [SLOW TEST:7.102 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":356,"completed":25,"skipped":321,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:39.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6248
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-47137859-9eda-49a0-9df7-ef6e07a15fa4
STEP: Creating a pod to test consume secrets
Jan 28 00:17:39.462: INFO: Waiting up to 5m0s for pod "pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44" in namespace "secrets-6248" to be "Succeeded or Failed"
Jan 28 00:17:39.479: INFO: Pod "pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44": Phase="Pending", Reason="", readiness=false. Elapsed: 16.809602ms
Jan 28 00:17:41.518: INFO: Pod "pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05556697s
Jan 28 00:17:43.538: INFO: Pod "pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076162924s
STEP: Saw pod success
Jan 28 00:17:43.538: INFO: Pod "pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44" satisfied condition "Succeeded or Failed"
Jan 28 00:17:43.556: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44 container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:17:43.677: INFO: Waiting for pod pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44 to disappear
Jan 28 00:17:43.694: INFO: Pod pod-secrets-0294ae0e-d9ea-4925-92c8-1be542434b44 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 00:17:43.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6248" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":26,"skipped":346,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4646
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 28 00:17:44.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4646" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":27,"skipped":376,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:44.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1572
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 28 00:17:44.481: INFO: Waiting up to 5m0s for pod "pod-4cdff1a2-3140-47ee-98ee-72121b6982ad" in namespace "emptydir-1572" to be "Succeeded or Failed"
Jan 28 00:17:44.497: INFO: Pod "pod-4cdff1a2-3140-47ee-98ee-72121b6982ad": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025261ms
Jan 28 00:17:46.523: INFO: Pod "pod-4cdff1a2-3140-47ee-98ee-72121b6982ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042541088s
Jan 28 00:17:48.545: INFO: Pod "pod-4cdff1a2-3140-47ee-98ee-72121b6982ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064269704s
STEP: Saw pod success
Jan 28 00:17:48.545: INFO: Pod "pod-4cdff1a2-3140-47ee-98ee-72121b6982ad" satisfied condition "Succeeded or Failed"
Jan 28 00:17:48.563: INFO: Trying to get logs from node 10.187.128.43 pod pod-4cdff1a2-3140-47ee-98ee-72121b6982ad container test-container: <nil>
STEP: delete the pod
Jan 28 00:17:48.659: INFO: Waiting for pod pod-4cdff1a2-3140-47ee-98ee-72121b6982ad to disappear
Jan 28 00:17:48.675: INFO: Pod pod-4cdff1a2-3140-47ee-98ee-72121b6982ad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 00:17:48.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1572" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":28,"skipped":394,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:48.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5677
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating api versions
Jan 28 00:17:48.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5677 api-versions'
Jan 28 00:17:49.054: INFO: stderr: ""
Jan 28 00:17:49.054: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nibm.com/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:17:49.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5677" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":356,"completed":29,"skipped":448,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:17:49.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4703
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should scale a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 28 00:17:49.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 create -f -'
Jan 28 00:17:50.459: INFO: stderr: ""
Jan 28 00:17:50.459: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 28 00:17:50.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:17:50.551: INFO: stderr: ""
Jan 28 00:17:50.551: INFO: stdout: "update-demo-nautilus-9l4cd update-demo-nautilus-z6wzb "
Jan 28 00:17:50.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-9l4cd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:17:50.650: INFO: stderr: ""
Jan 28 00:17:50.650: INFO: stdout: ""
Jan 28 00:17:50.650: INFO: update-demo-nautilus-9l4cd is created but not running
Jan 28 00:17:55.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:17:55.733: INFO: stderr: ""
Jan 28 00:17:55.733: INFO: stdout: "update-demo-nautilus-9l4cd update-demo-nautilus-z6wzb "
Jan 28 00:17:55.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-9l4cd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:17:55.808: INFO: stderr: ""
Jan 28 00:17:55.808: INFO: stdout: "true"
Jan 28 00:17:55.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-9l4cd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 00:17:55.901: INFO: stderr: ""
Jan 28 00:17:55.901: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 00:17:55.901: INFO: validating pod update-demo-nautilus-9l4cd
Jan 28 00:17:55.972: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 00:17:55.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 00:17:55.972: INFO: update-demo-nautilus-9l4cd is verified up and running
Jan 28 00:17:55.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:17:56.075: INFO: stderr: ""
Jan 28 00:17:56.075: INFO: stdout: "true"
Jan 28 00:17:56.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 00:17:56.154: INFO: stderr: ""
Jan 28 00:17:56.154: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 00:17:56.154: INFO: validating pod update-demo-nautilus-z6wzb
Jan 28 00:17:56.208: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 00:17:56.208: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 00:17:56.208: INFO: update-demo-nautilus-z6wzb is verified up and running
STEP: scaling down the replication controller
Jan 28 00:17:56.210: INFO: scanned /root for discovery docs: <nil>
Jan 28 00:17:56.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 28 00:17:57.365: INFO: stderr: ""
Jan 28 00:17:57.365: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 28 00:17:57.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:17:57.453: INFO: stderr: ""
Jan 28 00:17:57.453: INFO: stdout: "update-demo-nautilus-9l4cd update-demo-nautilus-z6wzb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 28 00:18:02.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:18:02.544: INFO: stderr: ""
Jan 28 00:18:02.544: INFO: stdout: "update-demo-nautilus-z6wzb "
Jan 28 00:18:02.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:18:02.650: INFO: stderr: ""
Jan 28 00:18:02.650: INFO: stdout: "true"
Jan 28 00:18:02.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 00:18:02.725: INFO: stderr: ""
Jan 28 00:18:02.725: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 00:18:02.725: INFO: validating pod update-demo-nautilus-z6wzb
Jan 28 00:18:02.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 00:18:02.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 00:18:02.752: INFO: update-demo-nautilus-z6wzb is verified up and running
STEP: scaling up the replication controller
Jan 28 00:18:02.753: INFO: scanned /root for discovery docs: <nil>
Jan 28 00:18:02.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 28 00:18:03.954: INFO: stderr: ""
Jan 28 00:18:03.954: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 28 00:18:03.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:18:04.062: INFO: stderr: ""
Jan 28 00:18:04.062: INFO: stdout: "update-demo-nautilus-55kf5 update-demo-nautilus-z6wzb "
Jan 28 00:18:04.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-55kf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:18:04.145: INFO: stderr: ""
Jan 28 00:18:04.145: INFO: stdout: ""
Jan 28 00:18:04.145: INFO: update-demo-nautilus-55kf5 is created but not running
Jan 28 00:18:09.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 00:18:09.252: INFO: stderr: ""
Jan 28 00:18:09.252: INFO: stdout: "update-demo-nautilus-55kf5 update-demo-nautilus-z6wzb "
Jan 28 00:18:09.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-55kf5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:18:09.338: INFO: stderr: ""
Jan 28 00:18:09.338: INFO: stdout: "true"
Jan 28 00:18:09.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-55kf5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 00:18:09.433: INFO: stderr: ""
Jan 28 00:18:09.433: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 00:18:09.433: INFO: validating pod update-demo-nautilus-55kf5
Jan 28 00:18:09.502: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 00:18:09.502: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 00:18:09.502: INFO: update-demo-nautilus-55kf5 is verified up and running
Jan 28 00:18:09.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 00:18:09.587: INFO: stderr: ""
Jan 28 00:18:09.587: INFO: stdout: "true"
Jan 28 00:18:09.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods update-demo-nautilus-z6wzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 00:18:09.672: INFO: stderr: ""
Jan 28 00:18:09.672: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 00:18:09.672: INFO: validating pod update-demo-nautilus-z6wzb
Jan 28 00:18:09.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 00:18:09.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 00:18:09.696: INFO: update-demo-nautilus-z6wzb is verified up and running
STEP: using delete to clean up resources
Jan 28 00:18:09.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 delete --grace-period=0 --force -f -'
Jan 28 00:18:09.794: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 00:18:09.794: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 28 00:18:09.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get rc,svc -l name=update-demo --no-headers'
Jan 28 00:18:09.902: INFO: stderr: "No resources found in kubectl-4703 namespace.\n"
Jan 28 00:18:09.902: INFO: stdout: ""
Jan 28 00:18:09.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4703 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 00:18:10.035: INFO: stderr: ""
Jan 28 00:18:10.035: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:18:10.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4703" for this suite.

• [SLOW TEST:20.978 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should scale a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":356,"completed":30,"skipped":450,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:10.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1996
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 28 00:18:10.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:18:10.476: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:18:11.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:18:11.538: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:18:12.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 00:18:12.521: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:18:13.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:18:13.524: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 28 00:18:13.641: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:18:13.641: INFO: Node 10.187.128.43 is running 0 daemon pod, expected 1
Jan 28 00:18:14.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:18:14.693: INFO: Node 10.187.128.43 is running 0 daemon pod, expected 1
Jan 28 00:18:15.687: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:18:15.687: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1996, will wait for the garbage collector to delete the pods
Jan 28 00:18:15.819: INFO: Deleting DaemonSet.extensions daemon-set took: 33.67183ms
Jan 28 00:18:15.920: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.048895ms
Jan 28 00:18:18.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:18:18.744: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:18:18.758: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20465"},"items":null}

Jan 28 00:18:18.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20465"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:18:18.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1996" for this suite.

• [SLOW TEST:8.821 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":356,"completed":31,"skipped":471,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8056
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:18:19.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8056" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":356,"completed":32,"skipped":475,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:19.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6025
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jan 28 00:18:23.521: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6025 PodName:pod-sharedvolume-33825357-49fc-49ba-9df8-c3e09537765f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:18:23.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:18:23.522: INFO: ExecWithOptions: Clientset creation
Jan 28 00:18:23.522: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-6025/pods/pod-sharedvolume-33825357-49fc-49ba-9df8-c3e09537765f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 28 00:18:23.746: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 00:18:23.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6025" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":356,"completed":33,"skipped":522,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:23.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6653
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:18:24.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 28 00:18:24.090: INFO: The status of Pod pod-logs-websocket-99c90b97-c0f4-400d-aea9-8eacf69cb222 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:18:26.118: INFO: The status of Pod pod-logs-websocket-99c90b97-c0f4-400d-aea9-8eacf69cb222 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 00:18:26.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6653" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":356,"completed":34,"skipped":555,"failed":0}
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:26.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1398
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test service account token: 
Jan 28 00:18:26.567: INFO: Waiting up to 5m0s for pod "test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde" in namespace "svcaccounts-1398" to be "Succeeded or Failed"
Jan 28 00:18:26.584: INFO: Pod "test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde": Phase="Pending", Reason="", readiness=false. Elapsed: 16.683607ms
Jan 28 00:18:28.609: INFO: Pod "test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04225585s
Jan 28 00:18:30.644: INFO: Pod "test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077222496s
STEP: Saw pod success
Jan 28 00:18:30.645: INFO: Pod "test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde" satisfied condition "Succeeded or Failed"
Jan 28 00:18:30.661: INFO: Trying to get logs from node 10.187.128.43 pod test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:18:30.753: INFO: Waiting for pod test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde to disappear
Jan 28 00:18:30.771: INFO: Pod test-pod-7b0803dc-7bfa-4678-800c-3b4a3768efde no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 00:18:30.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1398" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":356,"completed":35,"skipped":562,"failed":0}
S
------------------------------
[sig-node] Containers 
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:30.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5176
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override command
Jan 28 00:18:31.084: INFO: Waiting up to 5m0s for pod "client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6" in namespace "containers-5176" to be "Succeeded or Failed"
Jan 28 00:18:31.101: INFO: Pod "client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.497296ms
Jan 28 00:18:33.127: INFO: Pod "client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042632628s
Jan 28 00:18:35.163: INFO: Pod "client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078532707s
STEP: Saw pod success
Jan 28 00:18:35.163: INFO: Pod "client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6" satisfied condition "Succeeded or Failed"
Jan 28 00:18:35.182: INFO: Trying to get logs from node 10.187.128.30 pod client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:18:35.324: INFO: Waiting for pod client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6 to disappear
Jan 28 00:18:35.342: INFO: Pod client-containers-31fee6da-b202-403b-b715-0d1eeb6a07f6 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 28 00:18:35.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5176" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","total":356,"completed":36,"skipped":563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:35.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:18:36.244: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 28 00:18:38.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 18, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 18, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 18, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:18:41.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:18:52.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2374" for this suite.
STEP: Destroying namespace "webhook-2374-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:16.914 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":356,"completed":37,"skipped":609,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:52.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1298
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 28 00:18:52.557: INFO: Waiting up to 5m0s for pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671" in namespace "downward-api-1298" to be "Succeeded or Failed"
Jan 28 00:18:52.573: INFO: Pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671": Phase="Pending", Reason="", readiness=false. Elapsed: 16.098639ms
Jan 28 00:18:54.594: INFO: Pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037279888s
Jan 28 00:18:56.622: INFO: Pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064911402s
Jan 28 00:18:58.646: INFO: Pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089226103s
STEP: Saw pod success
Jan 28 00:18:58.646: INFO: Pod "downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671" satisfied condition "Succeeded or Failed"
Jan 28 00:18:58.664: INFO: Trying to get logs from node 10.187.128.30 pod downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671 container dapi-container: <nil>
STEP: delete the pod
Jan 28 00:18:58.757: INFO: Waiting for pod downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671 to disappear
Jan 28 00:18:58.773: INFO: Pod downward-api-13829bc7-f787-4e20-8424-7f8d7f8d8671 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 28 00:18:58.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1298" for this suite.

• [SLOW TEST:6.523 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":356,"completed":38,"skipped":645,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:58.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1823
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 28 00:18:59.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1823  e31fb468-d6c8-416a-93ec-84b60a0ee2b2 20813 0 2023-01-28 00:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-28 00:18:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:18:59.124: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1823  e31fb468-d6c8-416a-93ec-84b60a0ee2b2 20814 0 2023-01-28 00:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-28 00:18:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 28 00:18:59.201: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1823  e31fb468-d6c8-416a-93ec-84b60a0ee2b2 20815 0 2023-01-28 00:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-28 00:18:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 00:18:59.202: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1823  e31fb468-d6c8-416a-93ec-84b60a0ee2b2 20816 0 2023-01-28 00:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2023-01-28 00:18:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 28 00:18:59.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1823" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":356,"completed":39,"skipped":654,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:18:59.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1889
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-46f8f793-a0d8-4616-92f0-17cf5fcfc9b0
STEP: Creating a pod to test consume secrets
Jan 28 00:18:59.543: INFO: Waiting up to 5m0s for pod "pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1" in namespace "secrets-1889" to be "Succeeded or Failed"
Jan 28 00:18:59.582: INFO: Pod "pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.828524ms
Jan 28 00:19:01.610: INFO: Pod "pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06694593s
Jan 28 00:19:03.639: INFO: Pod "pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096119577s
STEP: Saw pod success
Jan 28 00:19:03.639: INFO: Pod "pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1" satisfied condition "Succeeded or Failed"
Jan 28 00:19:03.653: INFO: Trying to get logs from node 10.187.128.30 pod pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:19:03.735: INFO: Waiting for pod pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1 to disappear
Jan 28 00:19:03.752: INFO: Pod pod-secrets-d6ff3ce6-d497-46fd-b6fb-88f444ad87c1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 00:19:03.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1889" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":40,"skipped":659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:19:03.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-2413
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 28 00:19:04.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2413" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":356,"completed":41,"skipped":687,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:19:04.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2585
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 in namespace container-probe-2585
Jan 28 00:19:08.556: INFO: Started pod liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 in namespace container-probe-2585
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 00:19:08.574: INFO: Initial restart count of pod liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is 0
Jan 28 00:19:26.876: INFO: Restart count of pod container-probe-2585/liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is now 1 (18.302006331s elapsed)
Jan 28 00:19:47.133: INFO: Restart count of pod container-probe-2585/liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is now 2 (38.558947327s elapsed)
Jan 28 00:20:07.441: INFO: Restart count of pod container-probe-2585/liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is now 3 (58.867770551s elapsed)
Jan 28 00:20:27.718: INFO: Restart count of pod container-probe-2585/liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is now 4 (1m19.144245058s elapsed)
Jan 28 00:21:34.698: INFO: Restart count of pod container-probe-2585/liveness-b3d2c3c3-f1e2-451b-baf2-017d3640ca38 is now 5 (2m26.12432276s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 00:21:34.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2585" for this suite.

• [SLOW TEST:150.540 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":356,"completed":42,"skipped":689,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:21:34.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3205
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-5f5012f2-3385-4fc2-b230-ffe8c2d043b1
STEP: Creating a pod to test consume secrets
Jan 28 00:21:35.082: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de" in namespace "projected-3205" to be "Succeeded or Failed"
Jan 28 00:21:35.099: INFO: Pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de": Phase="Pending", Reason="", readiness=false. Elapsed: 17.195356ms
Jan 28 00:21:37.128: INFO: Pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de": Phase="Running", Reason="", readiness=true. Elapsed: 2.046546117s
Jan 28 00:21:39.159: INFO: Pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de": Phase="Running", Reason="", readiness=false. Elapsed: 4.07694588s
Jan 28 00:21:41.202: INFO: Pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.119961129s
STEP: Saw pod success
Jan 28 00:21:41.202: INFO: Pod "pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de" satisfied condition "Succeeded or Failed"
Jan 28 00:21:41.221: INFO: Trying to get logs from node 10.187.128.30 pod pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:21:41.390: INFO: Waiting for pod pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de to disappear
Jan 28 00:21:41.408: INFO: Pod pod-projected-secrets-9a490178-6e71-4f7e-960f-a78136b5e6de no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 00:21:41.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3205" for this suite.

• [SLOW TEST:6.657 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":43,"skipped":696,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:21:41.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4789
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 00:22:09.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4789" for this suite.

• [SLOW TEST:28.521 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":356,"completed":44,"skipped":709,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:09.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6994
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 28 00:22:10.243: INFO: Waiting up to 5m0s for pod "pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b" in namespace "emptydir-6994" to be "Succeeded or Failed"
Jan 28 00:22:10.260: INFO: Pod "pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.738972ms
Jan 28 00:22:12.289: INFO: Pod "pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046176487s
Jan 28 00:22:14.322: INFO: Pod "pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079288942s
STEP: Saw pod success
Jan 28 00:22:14.323: INFO: Pod "pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b" satisfied condition "Succeeded or Failed"
Jan 28 00:22:14.340: INFO: Trying to get logs from node 10.187.128.43 pod pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b container test-container: <nil>
STEP: delete the pod
Jan 28 00:22:14.490: INFO: Waiting for pod pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b to disappear
Jan 28 00:22:14.508: INFO: Pod pod-bdaa9f26-f9f0-49a0-96e5-355ce556695b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 00:22:14.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6994" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":45,"skipped":718,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:14.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2661
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:22:15.328: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:22:17.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 22, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 22, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:22:20.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:22:20.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2661" for this suite.
STEP: Destroying namespace "webhook-2661-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.425 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":356,"completed":46,"skipped":720,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:21.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7441
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-09aaef53-b613-4ee9-9724-1105c2299559
STEP: Creating a pod to test consume configMaps
Jan 28 00:22:21.290: INFO: Waiting up to 5m0s for pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d" in namespace "configmap-7441" to be "Succeeded or Failed"
Jan 28 00:22:21.304: INFO: Pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.308248ms
Jan 28 00:22:23.347: INFO: Pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057152449s
Jan 28 00:22:25.378: INFO: Pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087625148s
Jan 28 00:22:27.404: INFO: Pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.114121527s
STEP: Saw pod success
Jan 28 00:22:27.404: INFO: Pod "pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d" satisfied condition "Succeeded or Failed"
Jan 28 00:22:27.421: INFO: Trying to get logs from node 10.187.128.30 pod pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:22:27.518: INFO: Waiting for pod pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d to disappear
Jan 28 00:22:27.534: INFO: Pod pod-configmaps-9391fc31-5a15-40f8-a605-e22fd909354d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 00:22:27.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7441" for this suite.

• [SLOW TEST:6.591 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":47,"skipped":730,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:27.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8636
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-8636
STEP: creating service affinity-nodeport in namespace services-8636
STEP: creating replication controller affinity-nodeport in namespace services-8636
I0128 00:22:27.901739      26 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8636, replica count: 3
I0128 00:22:30.955070      26 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:22:33.955272      26 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 00:22:34.040: INFO: Creating new exec pod
Jan 28 00:22:37.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-8636 exec execpod-affinityzvh8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 28 00:22:37.460: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 28 00:22:37.460: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:22:37.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-8636 exec execpod-affinityzvh8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.129.66 80'
Jan 28 00:22:37.758: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.129.66 80\nConnection to 172.21.129.66 80 port [tcp/http] succeeded!\n"
Jan 28 00:22:37.758: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:22:37.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-8636 exec execpod-affinityzvh8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.30 30677'
Jan 28 00:22:38.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.30 30677\nConnection to 10.187.128.30 30677 port [tcp/*] succeeded!\n"
Jan 28 00:22:38.056: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:22:38.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-8636 exec execpod-affinityzvh8f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 30677'
Jan 28 00:22:38.306: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 30677\nConnection to 10.187.128.27 30677 port [tcp/*] succeeded!\n"
Jan 28 00:22:38.306: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:22:38.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-8636 exec execpod-affinityzvh8f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.187.128.27:30677/ ; done'
Jan 28 00:22:38.730: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:30677/\n"
Jan 28 00:22:38.730: INFO: stdout: "\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7\naffinity-nodeport-cj8f7"
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.730: INFO: Received response from host: affinity-nodeport-cj8f7
Jan 28 00:22:38.731: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8636, will wait for the garbage collector to delete the pods
Jan 28 00:22:38.879: INFO: Deleting ReplicationController affinity-nodeport took: 23.11146ms
Jan 28 00:22:38.980: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.060965ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:22:41.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8636" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:14.247 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":48,"skipped":734,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:41.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-6099
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Deleting RuntimeClass runtimeclass-6099-delete-me
STEP: Waiting for the RuntimeClass to disappear
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 28 00:22:42.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6099" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":49,"skipped":735,"failed":0}
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:42.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8732
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:22:42.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8732
I0128 00:22:42.437425      26 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8732, replica count: 1
I0128 00:22:43.488629      26 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 00:22:44.489084      26 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 00:22:44.635: INFO: Created: latency-svc-fkx8r
Jan 28 00:22:44.652: INFO: Got endpoints: latency-svc-fkx8r [62.625676ms]
Jan 28 00:22:44.699: INFO: Created: latency-svc-tln8x
Jan 28 00:22:44.720: INFO: Got endpoints: latency-svc-tln8x [66.713432ms]
Jan 28 00:22:44.740: INFO: Created: latency-svc-5kq9v
Jan 28 00:22:44.755: INFO: Got endpoints: latency-svc-5kq9v [101.688383ms]
Jan 28 00:22:44.769: INFO: Created: latency-svc-g5hfw
Jan 28 00:22:44.780: INFO: Got endpoints: latency-svc-g5hfw [126.997351ms]
Jan 28 00:22:44.799: INFO: Created: latency-svc-2zjmz
Jan 28 00:22:44.811: INFO: Got endpoints: latency-svc-2zjmz [157.054731ms]
Jan 28 00:22:44.822: INFO: Created: latency-svc-4b8gv
Jan 28 00:22:44.836: INFO: Got endpoints: latency-svc-4b8gv [182.312676ms]
Jan 28 00:22:44.848: INFO: Created: latency-svc-htkrc
Jan 28 00:22:44.867: INFO: Got endpoints: latency-svc-htkrc [213.784836ms]
Jan 28 00:22:44.895: INFO: Created: latency-svc-4fst4
Jan 28 00:22:44.924: INFO: Got endpoints: latency-svc-4fst4 [270.78328ms]
Jan 28 00:22:44.926: INFO: Created: latency-svc-d65q7
Jan 28 00:22:44.937: INFO: Got endpoints: latency-svc-d65q7 [284.983372ms]
Jan 28 00:22:44.946: INFO: Created: latency-svc-24zfg
Jan 28 00:22:44.958: INFO: Got endpoints: latency-svc-24zfg [304.59969ms]
Jan 28 00:22:44.972: INFO: Created: latency-svc-bd9sw
Jan 28 00:22:44.983: INFO: Got endpoints: latency-svc-bd9sw [329.911955ms]
Jan 28 00:22:44.998: INFO: Created: latency-svc-ls2rw
Jan 28 00:22:45.010: INFO: Got endpoints: latency-svc-ls2rw [356.715301ms]
Jan 28 00:22:45.033: INFO: Created: latency-svc-p5bqw
Jan 28 00:22:45.043: INFO: Got endpoints: latency-svc-p5bqw [389.219889ms]
Jan 28 00:22:45.069: INFO: Created: latency-svc-qp5gw
Jan 28 00:22:45.082: INFO: Got endpoints: latency-svc-qp5gw [428.969551ms]
Jan 28 00:22:45.093: INFO: Created: latency-svc-6lgsf
Jan 28 00:22:45.107: INFO: Got endpoints: latency-svc-6lgsf [453.277043ms]
Jan 28 00:22:45.116: INFO: Created: latency-svc-n85n5
Jan 28 00:22:45.128: INFO: Got endpoints: latency-svc-n85n5 [474.935302ms]
Jan 28 00:22:45.141: INFO: Created: latency-svc-bn8jh
Jan 28 00:22:45.162: INFO: Got endpoints: latency-svc-bn8jh [441.864734ms]
Jan 28 00:22:45.171: INFO: Created: latency-svc-g7bgw
Jan 28 00:22:45.179: INFO: Got endpoints: latency-svc-g7bgw [423.622211ms]
Jan 28 00:22:45.191: INFO: Created: latency-svc-rlkgq
Jan 28 00:22:45.203: INFO: Got endpoints: latency-svc-rlkgq [422.335067ms]
Jan 28 00:22:45.215: INFO: Created: latency-svc-lg5z8
Jan 28 00:22:45.228: INFO: Got endpoints: latency-svc-lg5z8 [417.447819ms]
Jan 28 00:22:45.242: INFO: Created: latency-svc-ptlwk
Jan 28 00:22:45.254: INFO: Got endpoints: latency-svc-ptlwk [417.84058ms]
Jan 28 00:22:45.272: INFO: Created: latency-svc-l4x57
Jan 28 00:22:45.284: INFO: Got endpoints: latency-svc-l4x57 [416.511955ms]
Jan 28 00:22:45.313: INFO: Created: latency-svc-28vh9
Jan 28 00:22:45.327: INFO: Got endpoints: latency-svc-28vh9 [402.564543ms]
Jan 28 00:22:45.350: INFO: Created: latency-svc-tt8z4
Jan 28 00:22:45.366: INFO: Got endpoints: latency-svc-tt8z4 [427.996288ms]
Jan 28 00:22:45.378: INFO: Created: latency-svc-6pgpl
Jan 28 00:22:45.388: INFO: Got endpoints: latency-svc-6pgpl [428.918684ms]
Jan 28 00:22:45.400: INFO: Created: latency-svc-kbpgs
Jan 28 00:22:45.413: INFO: Got endpoints: latency-svc-kbpgs [429.228749ms]
Jan 28 00:22:45.424: INFO: Created: latency-svc-sjm4z
Jan 28 00:22:45.438: INFO: Got endpoints: latency-svc-sjm4z [428.410036ms]
Jan 28 00:22:45.456: INFO: Created: latency-svc-4wtwj
Jan 28 00:22:45.487: INFO: Got endpoints: latency-svc-4wtwj [443.606559ms]
Jan 28 00:22:45.496: INFO: Created: latency-svc-zcg7x
Jan 28 00:22:45.518: INFO: Created: latency-svc-nhrks
Jan 28 00:22:45.526: INFO: Got endpoints: latency-svc-zcg7x [443.176621ms]
Jan 28 00:22:45.534: INFO: Got endpoints: latency-svc-nhrks [426.539497ms]
Jan 28 00:22:45.554: INFO: Created: latency-svc-tkwkw
Jan 28 00:22:45.560: INFO: Got endpoints: latency-svc-tkwkw [431.948683ms]
Jan 28 00:22:45.591: INFO: Created: latency-svc-pn5f6
Jan 28 00:22:45.602: INFO: Got endpoints: latency-svc-pn5f6 [440.567611ms]
Jan 28 00:22:45.612: INFO: Created: latency-svc-jcld6
Jan 28 00:22:45.623: INFO: Got endpoints: latency-svc-jcld6 [444.294834ms]
Jan 28 00:22:45.637: INFO: Created: latency-svc-pxt7r
Jan 28 00:22:45.652: INFO: Got endpoints: latency-svc-pxt7r [448.824147ms]
Jan 28 00:22:45.662: INFO: Created: latency-svc-b2tzl
Jan 28 00:22:45.675: INFO: Got endpoints: latency-svc-b2tzl [447.264143ms]
Jan 28 00:22:45.687: INFO: Created: latency-svc-49sq5
Jan 28 00:22:45.700: INFO: Got endpoints: latency-svc-49sq5 [445.769928ms]
Jan 28 00:22:45.711: INFO: Created: latency-svc-cr27t
Jan 28 00:22:45.726: INFO: Got endpoints: latency-svc-cr27t [442.18952ms]
Jan 28 00:22:45.741: INFO: Created: latency-svc-j894k
Jan 28 00:22:45.756: INFO: Got endpoints: latency-svc-j894k [429.117652ms]
Jan 28 00:22:45.767: INFO: Created: latency-svc-wfwxx
Jan 28 00:22:45.777: INFO: Got endpoints: latency-svc-wfwxx [411.580913ms]
Jan 28 00:22:45.803: INFO: Created: latency-svc-cf7dc
Jan 28 00:22:45.820: INFO: Got endpoints: latency-svc-cf7dc [432.369419ms]
Jan 28 00:22:45.841: INFO: Created: latency-svc-2xgmj
Jan 28 00:22:45.852: INFO: Got endpoints: latency-svc-2xgmj [438.611981ms]
Jan 28 00:22:45.865: INFO: Created: latency-svc-prnrg
Jan 28 00:22:45.877: INFO: Got endpoints: latency-svc-prnrg [439.137112ms]
Jan 28 00:22:45.890: INFO: Created: latency-svc-ghh64
Jan 28 00:22:45.902: INFO: Got endpoints: latency-svc-ghh64 [414.94661ms]
Jan 28 00:22:45.929: INFO: Created: latency-svc-lqmb7
Jan 28 00:22:45.942: INFO: Got endpoints: latency-svc-lqmb7 [416.666101ms]
Jan 28 00:22:45.954: INFO: Created: latency-svc-ptjsn
Jan 28 00:22:45.967: INFO: Got endpoints: latency-svc-ptjsn [433.247189ms]
Jan 28 00:22:45.981: INFO: Created: latency-svc-5nrqn
Jan 28 00:22:46.004: INFO: Got endpoints: latency-svc-5nrqn [444.70508ms]
Jan 28 00:22:46.008: INFO: Created: latency-svc-8wv7v
Jan 28 00:22:46.053: INFO: Got endpoints: latency-svc-8wv7v [451.045429ms]
Jan 28 00:22:46.065: INFO: Created: latency-svc-nxt8x
Jan 28 00:22:46.075: INFO: Got endpoints: latency-svc-nxt8x [451.154395ms]
Jan 28 00:22:46.088: INFO: Created: latency-svc-q8mxl
Jan 28 00:22:46.099: INFO: Got endpoints: latency-svc-q8mxl [447.025784ms]
Jan 28 00:22:46.111: INFO: Created: latency-svc-fz5jh
Jan 28 00:22:46.124: INFO: Got endpoints: latency-svc-fz5jh [448.041497ms]
Jan 28 00:22:46.137: INFO: Created: latency-svc-8h96k
Jan 28 00:22:46.152: INFO: Got endpoints: latency-svc-8h96k [452.042356ms]
Jan 28 00:22:46.168: INFO: Created: latency-svc-9zk2g
Jan 28 00:22:46.182: INFO: Got endpoints: latency-svc-9zk2g [455.906101ms]
Jan 28 00:22:46.190: INFO: Created: latency-svc-qfrbp
Jan 28 00:22:46.203: INFO: Got endpoints: latency-svc-qfrbp [447.104512ms]
Jan 28 00:22:46.215: INFO: Created: latency-svc-bhcrv
Jan 28 00:22:46.227: INFO: Got endpoints: latency-svc-bhcrv [449.096115ms]
Jan 28 00:22:46.241: INFO: Created: latency-svc-wtz9s
Jan 28 00:22:46.254: INFO: Got endpoints: latency-svc-wtz9s [433.775179ms]
Jan 28 00:22:46.270: INFO: Created: latency-svc-pj5vx
Jan 28 00:22:46.285: INFO: Got endpoints: latency-svc-pj5vx [433.42198ms]
Jan 28 00:22:46.298: INFO: Created: latency-svc-2zgjz
Jan 28 00:22:46.312: INFO: Got endpoints: latency-svc-2zgjz [434.262804ms]
Jan 28 00:22:46.321: INFO: Created: latency-svc-mtrsh
Jan 28 00:22:46.337: INFO: Got endpoints: latency-svc-mtrsh [434.852584ms]
Jan 28 00:22:46.362: INFO: Created: latency-svc-bl9hk
Jan 28 00:22:46.373: INFO: Got endpoints: latency-svc-bl9hk [430.921874ms]
Jan 28 00:22:46.387: INFO: Created: latency-svc-tjqn2
Jan 28 00:22:46.401: INFO: Got endpoints: latency-svc-tjqn2 [434.117936ms]
Jan 28 00:22:46.420: INFO: Created: latency-svc-r7j6v
Jan 28 00:22:46.420: INFO: Got endpoints: latency-svc-r7j6v [415.392967ms]
Jan 28 00:22:46.433: INFO: Created: latency-svc-t6whw
Jan 28 00:22:46.453: INFO: Got endpoints: latency-svc-t6whw [399.508103ms]
Jan 28 00:22:46.461: INFO: Created: latency-svc-vcd2b
Jan 28 00:22:46.472: INFO: Got endpoints: latency-svc-vcd2b [397.182841ms]
Jan 28 00:22:46.482: INFO: Created: latency-svc-rsjdm
Jan 28 00:22:46.495: INFO: Got endpoints: latency-svc-rsjdm [395.667123ms]
Jan 28 00:22:46.538: INFO: Created: latency-svc-l9c4n
Jan 28 00:22:46.548: INFO: Got endpoints: latency-svc-l9c4n [423.810649ms]
Jan 28 00:22:46.584: INFO: Created: latency-svc-95t7j
Jan 28 00:22:46.584: INFO: Created: latency-svc-jzm7x
Jan 28 00:22:46.584: INFO: Got endpoints: latency-svc-jzm7x [431.755938ms]
Jan 28 00:22:46.588: INFO: Got endpoints: latency-svc-95t7j [405.882436ms]
Jan 28 00:22:46.603: INFO: Created: latency-svc-wmtv5
Jan 28 00:22:46.627: INFO: Got endpoints: latency-svc-wmtv5 [424.01439ms]
Jan 28 00:22:46.637: INFO: Created: latency-svc-zttj5
Jan 28 00:22:46.649: INFO: Got endpoints: latency-svc-zttj5 [422.156549ms]
Jan 28 00:22:46.659: INFO: Created: latency-svc-s74mp
Jan 28 00:22:46.672: INFO: Got endpoints: latency-svc-s74mp [417.661766ms]
Jan 28 00:22:46.682: INFO: Created: latency-svc-nsz8g
Jan 28 00:22:46.695: INFO: Got endpoints: latency-svc-nsz8g [409.511797ms]
Jan 28 00:22:46.710: INFO: Created: latency-svc-5gltx
Jan 28 00:22:46.730: INFO: Got endpoints: latency-svc-5gltx [416.490806ms]
Jan 28 00:22:46.739: INFO: Created: latency-svc-jmbk8
Jan 28 00:22:46.752: INFO: Got endpoints: latency-svc-jmbk8 [415.067222ms]
Jan 28 00:22:46.766: INFO: Created: latency-svc-lgskh
Jan 28 00:22:46.779: INFO: Got endpoints: latency-svc-lgskh [406.156729ms]
Jan 28 00:22:46.790: INFO: Created: latency-svc-vwzsn
Jan 28 00:22:46.802: INFO: Got endpoints: latency-svc-vwzsn [401.080328ms]
Jan 28 00:22:46.823: INFO: Created: latency-svc-mnw5c
Jan 28 00:22:46.835: INFO: Got endpoints: latency-svc-mnw5c [415.284307ms]
Jan 28 00:22:46.847: INFO: Created: latency-svc-5g4m8
Jan 28 00:22:46.856: INFO: Got endpoints: latency-svc-5g4m8 [403.315334ms]
Jan 28 00:22:46.871: INFO: Created: latency-svc-n9llh
Jan 28 00:22:46.885: INFO: Got endpoints: latency-svc-n9llh [411.624604ms]
Jan 28 00:22:46.902: INFO: Created: latency-svc-h6k86
Jan 28 00:22:46.914: INFO: Got endpoints: latency-svc-h6k86 [419.02678ms]
Jan 28 00:22:46.944: INFO: Created: latency-svc-99t5w
Jan 28 00:22:46.953: INFO: Got endpoints: latency-svc-99t5w [405.356691ms]
Jan 28 00:22:46.975: INFO: Created: latency-svc-lxbk8
Jan 28 00:22:46.989: INFO: Got endpoints: latency-svc-lxbk8 [405.240762ms]
Jan 28 00:22:47.001: INFO: Created: latency-svc-vwv7f
Jan 28 00:22:47.012: INFO: Got endpoints: latency-svc-vwv7f [423.318001ms]
Jan 28 00:22:47.022: INFO: Created: latency-svc-4bl5s
Jan 28 00:22:47.033: INFO: Got endpoints: latency-svc-4bl5s [405.469643ms]
Jan 28 00:22:47.076: INFO: Created: latency-svc-6l6rv
Jan 28 00:22:47.095: INFO: Got endpoints: latency-svc-6l6rv [446.484717ms]
Jan 28 00:22:47.112: INFO: Created: latency-svc-p7x6l
Jan 28 00:22:47.128: INFO: Got endpoints: latency-svc-p7x6l [456.754344ms]
Jan 28 00:22:47.140: INFO: Created: latency-svc-7h74d
Jan 28 00:22:47.154: INFO: Got endpoints: latency-svc-7h74d [459.385792ms]
Jan 28 00:22:47.169: INFO: Created: latency-svc-s4vks
Jan 28 00:22:47.181: INFO: Got endpoints: latency-svc-s4vks [450.977746ms]
Jan 28 00:22:47.194: INFO: Created: latency-svc-ctdvd
Jan 28 00:22:47.205: INFO: Got endpoints: latency-svc-ctdvd [452.723046ms]
Jan 28 00:22:47.221: INFO: Created: latency-svc-t2vv4
Jan 28 00:22:47.238: INFO: Got endpoints: latency-svc-t2vv4 [458.903657ms]
Jan 28 00:22:47.246: INFO: Created: latency-svc-242jr
Jan 28 00:22:47.256: INFO: Got endpoints: latency-svc-242jr [453.634536ms]
Jan 28 00:22:47.272: INFO: Created: latency-svc-9rpkf
Jan 28 00:22:47.283: INFO: Got endpoints: latency-svc-9rpkf [447.514648ms]
Jan 28 00:22:47.299: INFO: Created: latency-svc-hswrr
Jan 28 00:22:47.309: INFO: Got endpoints: latency-svc-hswrr [452.424264ms]
Jan 28 00:22:47.333: INFO: Created: latency-svc-x942n
Jan 28 00:22:47.333: INFO: Got endpoints: latency-svc-x942n [448.439644ms]
Jan 28 00:22:47.359: INFO: Created: latency-svc-l4w8q
Jan 28 00:22:47.374: INFO: Got endpoints: latency-svc-l4w8q [460.345124ms]
Jan 28 00:22:47.384: INFO: Created: latency-svc-qlll9
Jan 28 00:22:47.398: INFO: Got endpoints: latency-svc-qlll9 [444.439196ms]
Jan 28 00:22:47.413: INFO: Created: latency-svc-tl2kr
Jan 28 00:22:47.422: INFO: Got endpoints: latency-svc-tl2kr [433.318371ms]
Jan 28 00:22:47.434: INFO: Created: latency-svc-4c89m
Jan 28 00:22:47.451: INFO: Got endpoints: latency-svc-4c89m [438.97022ms]
Jan 28 00:22:47.468: INFO: Created: latency-svc-msrzf
Jan 28 00:22:47.478: INFO: Got endpoints: latency-svc-msrzf [445.285375ms]
Jan 28 00:22:47.496: INFO: Created: latency-svc-55hp2
Jan 28 00:22:47.506: INFO: Got endpoints: latency-svc-55hp2 [410.297783ms]
Jan 28 00:22:47.517: INFO: Created: latency-svc-nwxjk
Jan 28 00:22:47.530: INFO: Got endpoints: latency-svc-nwxjk [401.853381ms]
Jan 28 00:22:47.539: INFO: Created: latency-svc-vqhnw
Jan 28 00:22:47.552: INFO: Got endpoints: latency-svc-vqhnw [397.302851ms]
Jan 28 00:22:47.566: INFO: Created: latency-svc-5b2ct
Jan 28 00:22:47.576: INFO: Got endpoints: latency-svc-5b2ct [395.109059ms]
Jan 28 00:22:47.599: INFO: Created: latency-svc-clphb
Jan 28 00:22:47.613: INFO: Got endpoints: latency-svc-clphb [408.642881ms]
Jan 28 00:22:47.624: INFO: Created: latency-svc-mr8t5
Jan 28 00:22:47.636: INFO: Got endpoints: latency-svc-mr8t5 [397.258454ms]
Jan 28 00:22:47.652: INFO: Created: latency-svc-rxhnr
Jan 28 00:22:47.666: INFO: Got endpoints: latency-svc-rxhnr [409.922105ms]
Jan 28 00:22:47.684: INFO: Created: latency-svc-z2ktt
Jan 28 00:22:47.697: INFO: Got endpoints: latency-svc-z2ktt [414.601386ms]
Jan 28 00:22:47.709: INFO: Created: latency-svc-wxx7b
Jan 28 00:22:47.721: INFO: Got endpoints: latency-svc-wxx7b [412.13471ms]
Jan 28 00:22:47.740: INFO: Created: latency-svc-kc9fn
Jan 28 00:22:47.754: INFO: Got endpoints: latency-svc-kc9fn [421.257887ms]
Jan 28 00:22:47.764: INFO: Created: latency-svc-942z6
Jan 28 00:22:47.777: INFO: Got endpoints: latency-svc-942z6 [403.01291ms]
Jan 28 00:22:47.791: INFO: Created: latency-svc-9wzkn
Jan 28 00:22:47.801: INFO: Got endpoints: latency-svc-9wzkn [403.28689ms]
Jan 28 00:22:47.816: INFO: Created: latency-svc-zlglf
Jan 28 00:22:47.830: INFO: Got endpoints: latency-svc-zlglf [407.498387ms]
Jan 28 00:22:47.856: INFO: Created: latency-svc-cztfh
Jan 28 00:22:47.865: INFO: Got endpoints: latency-svc-cztfh [413.807251ms]
Jan 28 00:22:47.879: INFO: Created: latency-svc-p2mbw
Jan 28 00:22:47.891: INFO: Got endpoints: latency-svc-p2mbw [413.515965ms]
Jan 28 00:22:47.908: INFO: Created: latency-svc-6mvnb
Jan 28 00:22:47.917: INFO: Got endpoints: latency-svc-6mvnb [411.458399ms]
Jan 28 00:22:47.937: INFO: Created: latency-svc-bd75c
Jan 28 00:22:47.950: INFO: Got endpoints: latency-svc-bd75c [419.589831ms]
Jan 28 00:22:47.980: INFO: Created: latency-svc-r8768
Jan 28 00:22:47.993: INFO: Got endpoints: latency-svc-r8768 [441.375477ms]
Jan 28 00:22:48.030: INFO: Created: latency-svc-h6tfr
Jan 28 00:22:48.040: INFO: Got endpoints: latency-svc-h6tfr [464.153416ms]
Jan 28 00:22:48.073: INFO: Created: latency-svc-dgvpk
Jan 28 00:22:48.096: INFO: Got endpoints: latency-svc-dgvpk [482.724088ms]
Jan 28 00:22:48.109: INFO: Created: latency-svc-rxplp
Jan 28 00:22:48.118: INFO: Got endpoints: latency-svc-rxplp [481.680708ms]
Jan 28 00:22:48.147: INFO: Created: latency-svc-vsnpx
Jan 28 00:22:48.160: INFO: Got endpoints: latency-svc-vsnpx [493.632706ms]
Jan 28 00:22:48.178: INFO: Created: latency-svc-wsfm2
Jan 28 00:22:48.194: INFO: Got endpoints: latency-svc-wsfm2 [496.028823ms]
Jan 28 00:22:48.209: INFO: Created: latency-svc-dzvgs
Jan 28 00:22:48.217: INFO: Got endpoints: latency-svc-dzvgs [495.873126ms]
Jan 28 00:22:48.260: INFO: Created: latency-svc-8v62n
Jan 28 00:22:48.260: INFO: Got endpoints: latency-svc-8v62n [506.064458ms]
Jan 28 00:22:48.270: INFO: Created: latency-svc-lm2c5
Jan 28 00:22:48.280: INFO: Got endpoints: latency-svc-lm2c5 [503.367987ms]
Jan 28 00:22:48.288: INFO: Created: latency-svc-2vq8m
Jan 28 00:22:48.299: INFO: Got endpoints: latency-svc-2vq8m [498.286879ms]
Jan 28 00:22:48.311: INFO: Created: latency-svc-br6fm
Jan 28 00:22:48.326: INFO: Got endpoints: latency-svc-br6fm [496.479179ms]
Jan 28 00:22:48.337: INFO: Created: latency-svc-bcwrf
Jan 28 00:22:48.348: INFO: Got endpoints: latency-svc-bcwrf [482.643053ms]
Jan 28 00:22:48.358: INFO: Created: latency-svc-6rppg
Jan 28 00:22:48.374: INFO: Got endpoints: latency-svc-6rppg [482.505002ms]
Jan 28 00:22:48.384: INFO: Created: latency-svc-bhqcn
Jan 28 00:22:48.396: INFO: Got endpoints: latency-svc-bhqcn [478.942186ms]
Jan 28 00:22:48.409: INFO: Created: latency-svc-wvtdp
Jan 28 00:22:48.422: INFO: Got endpoints: latency-svc-wvtdp [472.213121ms]
Jan 28 00:22:48.438: INFO: Created: latency-svc-tz9fb
Jan 28 00:22:48.447: INFO: Got endpoints: latency-svc-tz9fb [453.976392ms]
Jan 28 00:22:48.462: INFO: Created: latency-svc-tq6n2
Jan 28 00:22:48.479: INFO: Got endpoints: latency-svc-tq6n2 [438.307061ms]
Jan 28 00:22:48.488: INFO: Created: latency-svc-t764x
Jan 28 00:22:48.500: INFO: Got endpoints: latency-svc-t764x [403.912346ms]
Jan 28 00:22:48.515: INFO: Created: latency-svc-bwk8k
Jan 28 00:22:48.533: INFO: Got endpoints: latency-svc-bwk8k [414.72592ms]
Jan 28 00:22:48.544: INFO: Created: latency-svc-wwcwx
Jan 28 00:22:48.556: INFO: Got endpoints: latency-svc-wwcwx [396.22031ms]
Jan 28 00:22:48.568: INFO: Created: latency-svc-ktzf5
Jan 28 00:22:48.580: INFO: Got endpoints: latency-svc-ktzf5 [386.405638ms]
Jan 28 00:22:48.590: INFO: Created: latency-svc-g9d88
Jan 28 00:22:48.606: INFO: Got endpoints: latency-svc-g9d88 [388.301216ms]
Jan 28 00:22:48.616: INFO: Created: latency-svc-9nhhl
Jan 28 00:22:48.629: INFO: Got endpoints: latency-svc-9nhhl [368.281169ms]
Jan 28 00:22:48.639: INFO: Created: latency-svc-qwqpj
Jan 28 00:22:48.650: INFO: Got endpoints: latency-svc-qwqpj [369.71741ms]
Jan 28 00:22:48.662: INFO: Created: latency-svc-m9f4k
Jan 28 00:22:48.681: INFO: Got endpoints: latency-svc-m9f4k [382.074951ms]
Jan 28 00:22:48.693: INFO: Created: latency-svc-t4spl
Jan 28 00:22:48.703: INFO: Got endpoints: latency-svc-t4spl [376.482678ms]
Jan 28 00:22:48.714: INFO: Created: latency-svc-jm4nv
Jan 28 00:22:48.725: INFO: Got endpoints: latency-svc-jm4nv [377.155467ms]
Jan 28 00:22:48.737: INFO: Created: latency-svc-cg7nf
Jan 28 00:22:48.750: INFO: Got endpoints: latency-svc-cg7nf [376.053889ms]
Jan 28 00:22:48.762: INFO: Created: latency-svc-zkmbs
Jan 28 00:22:48.773: INFO: Got endpoints: latency-svc-zkmbs [376.556896ms]
Jan 28 00:22:48.785: INFO: Created: latency-svc-9zvxj
Jan 28 00:22:48.797: INFO: Got endpoints: latency-svc-9zvxj [374.345637ms]
Jan 28 00:22:48.808: INFO: Created: latency-svc-dd55f
Jan 28 00:22:48.819: INFO: Got endpoints: latency-svc-dd55f [371.773086ms]
Jan 28 00:22:48.833: INFO: Created: latency-svc-7xwfr
Jan 28 00:22:48.848: INFO: Got endpoints: latency-svc-7xwfr [369.066932ms]
Jan 28 00:22:48.856: INFO: Created: latency-svc-m9rbv
Jan 28 00:22:48.869: INFO: Got endpoints: latency-svc-m9rbv [369.201906ms]
Jan 28 00:22:48.881: INFO: Created: latency-svc-knzbp
Jan 28 00:22:48.894: INFO: Got endpoints: latency-svc-knzbp [361.222057ms]
Jan 28 00:22:48.905: INFO: Created: latency-svc-xdlz2
Jan 28 00:22:48.917: INFO: Got endpoints: latency-svc-xdlz2 [360.713794ms]
Jan 28 00:22:48.931: INFO: Created: latency-svc-mgwgp
Jan 28 00:22:48.941: INFO: Got endpoints: latency-svc-mgwgp [361.149653ms]
Jan 28 00:22:48.953: INFO: Created: latency-svc-c47b2
Jan 28 00:22:48.964: INFO: Got endpoints: latency-svc-c47b2 [358.220658ms]
Jan 28 00:22:48.977: INFO: Created: latency-svc-fnwkg
Jan 28 00:22:48.994: INFO: Got endpoints: latency-svc-fnwkg [365.428582ms]
Jan 28 00:22:49.004: INFO: Created: latency-svc-qbc96
Jan 28 00:22:49.014: INFO: Got endpoints: latency-svc-qbc96 [363.676229ms]
Jan 28 00:22:49.032: INFO: Created: latency-svc-nqpzp
Jan 28 00:22:49.056: INFO: Got endpoints: latency-svc-nqpzp [374.269059ms]
Jan 28 00:22:49.066: INFO: Created: latency-svc-sz5vq
Jan 28 00:22:49.078: INFO: Got endpoints: latency-svc-sz5vq [374.957297ms]
Jan 28 00:22:49.091: INFO: Created: latency-svc-rvrdn
Jan 28 00:22:49.102: INFO: Got endpoints: latency-svc-rvrdn [376.596363ms]
Jan 28 00:22:49.114: INFO: Created: latency-svc-756k2
Jan 28 00:22:49.126: INFO: Got endpoints: latency-svc-756k2 [375.905082ms]
Jan 28 00:22:49.148: INFO: Created: latency-svc-rgppx
Jan 28 00:22:49.150: INFO: Got endpoints: latency-svc-rgppx [377.015161ms]
Jan 28 00:22:49.159: INFO: Created: latency-svc-tm252
Jan 28 00:22:49.179: INFO: Got endpoints: latency-svc-tm252 [381.787879ms]
Jan 28 00:22:49.184: INFO: Created: latency-svc-59hj5
Jan 28 00:22:49.199: INFO: Got endpoints: latency-svc-59hj5 [379.628888ms]
Jan 28 00:22:49.210: INFO: Created: latency-svc-gjhxq
Jan 28 00:22:49.224: INFO: Got endpoints: latency-svc-gjhxq [375.930981ms]
Jan 28 00:22:49.232: INFO: Created: latency-svc-cvs47
Jan 28 00:22:49.242: INFO: Got endpoints: latency-svc-cvs47 [372.158604ms]
Jan 28 00:22:49.253: INFO: Created: latency-svc-lng82
Jan 28 00:22:49.267: INFO: Got endpoints: latency-svc-lng82 [372.534668ms]
Jan 28 00:22:49.291: INFO: Created: latency-svc-jwt6b
Jan 28 00:22:49.296: INFO: Got endpoints: latency-svc-jwt6b [378.537105ms]
Jan 28 00:22:49.305: INFO: Created: latency-svc-w8klk
Jan 28 00:22:49.322: INFO: Got endpoints: latency-svc-w8klk [380.063796ms]
Jan 28 00:22:49.330: INFO: Created: latency-svc-vznzl
Jan 28 00:22:49.342: INFO: Got endpoints: latency-svc-vznzl [377.747934ms]
Jan 28 00:22:49.354: INFO: Created: latency-svc-s6zmf
Jan 28 00:22:49.367: INFO: Got endpoints: latency-svc-s6zmf [372.659861ms]
Jan 28 00:22:49.377: INFO: Created: latency-svc-5ptqr
Jan 28 00:22:49.393: INFO: Got endpoints: latency-svc-5ptqr [378.846146ms]
Jan 28 00:22:49.408: INFO: Created: latency-svc-9f9ss
Jan 28 00:22:49.421: INFO: Got endpoints: latency-svc-9f9ss [364.631527ms]
Jan 28 00:22:49.436: INFO: Created: latency-svc-xlw5j
Jan 28 00:22:49.450: INFO: Got endpoints: latency-svc-xlw5j [371.123194ms]
Jan 28 00:22:49.465: INFO: Created: latency-svc-q96lw
Jan 28 00:22:49.485: INFO: Got endpoints: latency-svc-q96lw [383.134273ms]
Jan 28 00:22:49.496: INFO: Created: latency-svc-h22bl
Jan 28 00:22:49.501: INFO: Got endpoints: latency-svc-h22bl [374.167861ms]
Jan 28 00:22:49.512: INFO: Created: latency-svc-5gglx
Jan 28 00:22:49.524: INFO: Got endpoints: latency-svc-5gglx [373.764465ms]
Jan 28 00:22:49.537: INFO: Created: latency-svc-xg6kc
Jan 28 00:22:49.553: INFO: Got endpoints: latency-svc-xg6kc [373.991952ms]
Jan 28 00:22:49.562: INFO: Created: latency-svc-2hvhr
Jan 28 00:22:49.584: INFO: Got endpoints: latency-svc-2hvhr [384.742251ms]
Jan 28 00:22:49.591: INFO: Created: latency-svc-hmsnv
Jan 28 00:22:49.599: INFO: Got endpoints: latency-svc-hmsnv [375.367799ms]
Jan 28 00:22:49.619: INFO: Created: latency-svc-6d7gd
Jan 28 00:22:49.628: INFO: Got endpoints: latency-svc-6d7gd [386.622595ms]
Jan 28 00:22:49.642: INFO: Created: latency-svc-dmks2
Jan 28 00:22:49.655: INFO: Got endpoints: latency-svc-dmks2 [388.084104ms]
Jan 28 00:22:49.668: INFO: Created: latency-svc-xvrss
Jan 28 00:22:49.682: INFO: Got endpoints: latency-svc-xvrss [386.251345ms]
Jan 28 00:22:49.708: INFO: Created: latency-svc-rw7jn
Jan 28 00:22:49.720: INFO: Got endpoints: latency-svc-rw7jn [397.979668ms]
Jan 28 00:22:49.734: INFO: Created: latency-svc-9lrwz
Jan 28 00:22:49.747: INFO: Got endpoints: latency-svc-9lrwz [404.668058ms]
Jan 28 00:22:49.761: INFO: Created: latency-svc-p7vr7
Jan 28 00:22:49.782: INFO: Got endpoints: latency-svc-p7vr7 [415.077084ms]
Jan 28 00:22:49.792: INFO: Created: latency-svc-w4c6h
Jan 28 00:22:49.810: INFO: Got endpoints: latency-svc-w4c6h [416.958507ms]
Jan 28 00:22:49.837: INFO: Created: latency-svc-lcxnn
Jan 28 00:22:49.837: INFO: Created: latency-svc-pncwt
Jan 28 00:22:49.837: INFO: Got endpoints: latency-svc-pncwt [416.339014ms]
Jan 28 00:22:49.845: INFO: Got endpoints: latency-svc-lcxnn [395.020101ms]
Jan 28 00:22:49.860: INFO: Created: latency-svc-g448z
Jan 28 00:22:49.872: INFO: Got endpoints: latency-svc-g448z [386.886375ms]
Jan 28 00:22:49.893: INFO: Created: latency-svc-g9vph
Jan 28 00:22:49.904: INFO: Created: latency-svc-hxgm4
Jan 28 00:22:49.904: INFO: Got endpoints: latency-svc-g9vph [403.502915ms]
Jan 28 00:22:49.918: INFO: Got endpoints: latency-svc-hxgm4 [394.154498ms]
Jan 28 00:22:49.961: INFO: Created: latency-svc-g5t7d
Jan 28 00:22:49.970: INFO: Got endpoints: latency-svc-g5t7d [417.145539ms]
Jan 28 00:22:49.986: INFO: Created: latency-svc-hb2qj
Jan 28 00:22:50.028: INFO: Got endpoints: latency-svc-hb2qj [443.193822ms]
Jan 28 00:22:50.040: INFO: Created: latency-svc-bspdh
Jan 28 00:22:50.050: INFO: Got endpoints: latency-svc-bspdh [450.996573ms]
Jan 28 00:22:50.069: INFO: Created: latency-svc-qphfq
Jan 28 00:22:50.082: INFO: Got endpoints: latency-svc-qphfq [453.09953ms]
Jan 28 00:22:50.091: INFO: Created: latency-svc-rx578
Jan 28 00:22:50.109: INFO: Got endpoints: latency-svc-rx578 [454.378615ms]
Jan 28 00:22:50.120: INFO: Created: latency-svc-c45x6
Jan 28 00:22:50.133: INFO: Got endpoints: latency-svc-c45x6 [450.927998ms]
Jan 28 00:22:50.152: INFO: Created: latency-svc-p4lw8
Jan 28 00:22:50.162: INFO: Got endpoints: latency-svc-p4lw8 [442.408773ms]
Jan 28 00:22:50.171: INFO: Created: latency-svc-jpn7j
Jan 28 00:22:50.182: INFO: Got endpoints: latency-svc-jpn7j [435.489927ms]
Jan 28 00:22:50.196: INFO: Created: latency-svc-2gb6f
Jan 28 00:22:50.210: INFO: Got endpoints: latency-svc-2gb6f [427.398181ms]
Jan 28 00:22:50.224: INFO: Created: latency-svc-d8t4n
Jan 28 00:22:50.236: INFO: Got endpoints: latency-svc-d8t4n [424.674893ms]
Jan 28 00:22:50.247: INFO: Created: latency-svc-mwhth
Jan 28 00:22:50.260: INFO: Got endpoints: latency-svc-mwhth [422.24199ms]
Jan 28 00:22:50.276: INFO: Created: latency-svc-mjfzd
Jan 28 00:22:50.286: INFO: Got endpoints: latency-svc-mjfzd [441.077953ms]
Jan 28 00:22:50.286: INFO: Latencies: [66.713432ms 101.688383ms 126.997351ms 157.054731ms 182.312676ms 213.784836ms 270.78328ms 284.983372ms 304.59969ms 329.911955ms 356.715301ms 358.220658ms 360.713794ms 361.149653ms 361.222057ms 363.676229ms 364.631527ms 365.428582ms 368.281169ms 369.066932ms 369.201906ms 369.71741ms 371.123194ms 371.773086ms 372.158604ms 372.534668ms 372.659861ms 373.764465ms 373.991952ms 374.167861ms 374.269059ms 374.345637ms 374.957297ms 375.367799ms 375.905082ms 375.930981ms 376.053889ms 376.482678ms 376.556896ms 376.596363ms 377.015161ms 377.155467ms 377.747934ms 378.537105ms 378.846146ms 379.628888ms 380.063796ms 381.787879ms 382.074951ms 383.134273ms 384.742251ms 386.251345ms 386.405638ms 386.622595ms 386.886375ms 388.084104ms 388.301216ms 389.219889ms 394.154498ms 395.020101ms 395.109059ms 395.667123ms 396.22031ms 397.182841ms 397.258454ms 397.302851ms 397.979668ms 399.508103ms 401.080328ms 401.853381ms 402.564543ms 403.01291ms 403.28689ms 403.315334ms 403.502915ms 403.912346ms 404.668058ms 405.240762ms 405.356691ms 405.469643ms 405.882436ms 406.156729ms 407.498387ms 408.642881ms 409.511797ms 409.922105ms 410.297783ms 411.458399ms 411.580913ms 411.624604ms 412.13471ms 413.515965ms 413.807251ms 414.601386ms 414.72592ms 414.94661ms 415.067222ms 415.077084ms 415.284307ms 415.392967ms 416.339014ms 416.490806ms 416.511955ms 416.666101ms 416.958507ms 417.145539ms 417.447819ms 417.661766ms 417.84058ms 419.02678ms 419.589831ms 421.257887ms 422.156549ms 422.24199ms 422.335067ms 423.318001ms 423.622211ms 423.810649ms 424.01439ms 424.674893ms 426.539497ms 427.398181ms 427.996288ms 428.410036ms 428.918684ms 428.969551ms 429.117652ms 429.228749ms 430.921874ms 431.755938ms 431.948683ms 432.369419ms 433.247189ms 433.318371ms 433.42198ms 433.775179ms 434.117936ms 434.262804ms 434.852584ms 435.489927ms 438.307061ms 438.611981ms 438.97022ms 439.137112ms 440.567611ms 441.077953ms 441.375477ms 441.864734ms 442.18952ms 442.408773ms 443.176621ms 443.193822ms 443.606559ms 444.294834ms 444.439196ms 444.70508ms 445.285375ms 445.769928ms 446.484717ms 447.025784ms 447.104512ms 447.264143ms 447.514648ms 448.041497ms 448.439644ms 448.824147ms 449.096115ms 450.927998ms 450.977746ms 450.996573ms 451.045429ms 451.154395ms 452.042356ms 452.424264ms 452.723046ms 453.09953ms 453.277043ms 453.634536ms 453.976392ms 454.378615ms 455.906101ms 456.754344ms 458.903657ms 459.385792ms 460.345124ms 464.153416ms 472.213121ms 474.935302ms 478.942186ms 481.680708ms 482.505002ms 482.643053ms 482.724088ms 493.632706ms 495.873126ms 496.028823ms 496.479179ms 498.286879ms 503.367987ms 506.064458ms]
Jan 28 00:22:50.286: INFO: 50 %ile: 416.339014ms
Jan 28 00:22:50.286: INFO: 90 %ile: 455.906101ms
Jan 28 00:22:50.286: INFO: 99 %ile: 503.367987ms
Jan 28 00:22:50.286: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:188
Jan 28 00:22:50.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8732" for this suite.

• [SLOW TEST:8.142 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":356,"completed":50,"skipped":742,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:22:50.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3827
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-3827
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 28 00:22:50.563: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 00:22:50.712: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:22:52.736: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:22:54.744: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:22:56.738: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:22:58.738: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:00.741: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:02.742: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:04.743: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:06.733: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:08.736: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:10.746: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:23:12.747: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 28 00:23:12.780: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 28 00:23:12.818: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 28 00:23:16.924: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 00:23:16.924: INFO: Breadth first check of 172.30.102.112 on host 10.187.128.27...
Jan 28 00:23:16.942: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.81:9080/dial?request=hostname&protocol=http&host=172.30.102.112&port=8083&tries=1'] Namespace:pod-network-test-3827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:23:16.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:23:16.943: INFO: ExecWithOptions: Clientset creation
Jan 28 00:23:16.943: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3827/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.102.112%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:23:17.141: INFO: Waiting for responses: map[]
Jan 28 00:23:17.141: INFO: reached 172.30.102.112 after 0/1 tries
Jan 28 00:23:17.141: INFO: Breadth first check of 172.30.253.208 on host 10.187.128.30...
Jan 28 00:23:17.166: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.81:9080/dial?request=hostname&protocol=http&host=172.30.253.208&port=8083&tries=1'] Namespace:pod-network-test-3827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:23:17.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:23:17.168: INFO: ExecWithOptions: Clientset creation
Jan 28 00:23:17.168: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3827/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.253.208%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:23:17.360: INFO: Waiting for responses: map[]
Jan 28 00:23:17.360: INFO: reached 172.30.253.208 after 0/1 tries
Jan 28 00:23:17.360: INFO: Breadth first check of 172.30.90.80 on host 10.187.128.43...
Jan 28 00:23:17.380: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.81:9080/dial?request=hostname&protocol=http&host=172.30.90.80&port=8083&tries=1'] Namespace:pod-network-test-3827 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:23:17.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:23:17.380: INFO: ExecWithOptions: Clientset creation
Jan 28 00:23:17.380: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3827/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.81%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.90.80%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:23:17.582: INFO: Waiting for responses: map[]
Jan 28 00:23:17.582: INFO: reached 172.30.90.80 after 0/1 tries
Jan 28 00:23:17.582: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 28 00:23:17.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3827" for this suite.

• [SLOW TEST:27.294 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":356,"completed":51,"skipped":757,"failed":0}
[sig-node] Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:17.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9811
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 28 00:23:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9811" for this suite.
•{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":356,"completed":52,"skipped":757,"failed":0}

------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:20.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3037
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:23:20.422: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 28 00:23:25.451: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 28 00:23:25.451: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 28 00:23:27.475: INFO: Creating deployment "test-rollover-deployment"
Jan 28 00:23:27.524: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 28 00:23:29.571: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 28 00:23:29.609: INFO: Ensure that both replica sets have 1 created replica
Jan 28 00:23:29.644: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 28 00:23:29.682: INFO: Updating deployment test-rollover-deployment
Jan 28 00:23:29.682: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 28 00:23:31.736: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 28 00:23:31.771: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 28 00:23:31.875: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:23:31.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:23:33.917: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:23:33.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:23:35.920: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:23:35.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:23:37.922: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:23:37.923: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:23:39.925: INFO: all replica sets need to contain the pod-template-hash label
Jan 28 00:23:39.926: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 23, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 23, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-779c67f4f8\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 00:23:41.923: INFO: 
Jan 28 00:23:41.923: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 00:23:41.973: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3037  4d8f3946-db4f-45b3-aa20-475e66630dc8 23880 2 2023-01-28 00:23:27 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-28 00:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:23:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003637b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 00:23:27 +0000 UTC,LastTransitionTime:2023-01-28 00:23:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-779c67f4f8" has successfully progressed.,LastUpdateTime:2023-01-28 00:23:41 +0000 UTC,LastTransitionTime:2023-01-28 00:23:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 00:23:41.993: INFO: New ReplicaSet "test-rollover-deployment-779c67f4f8" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-779c67f4f8  deployment-3037  6f218276-2053-45b1-b569-3fabd1a5b6a0 23871 2 2023-01-28 00:23:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 4d8f3946-db4f-45b3-aa20-475e66630dc8 0xc003748007 0xc003748008}] []  [{kube-controller-manager Update apps/v1 2023-01-28 00:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8f3946-db4f-45b3-aa20-475e66630dc8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:23:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 779c67f4f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037480b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:23:41.993: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 28 00:23:41.993: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3037  9637d56e-5427-486d-8f16-f7cdbf8534f7 23879 2 2023-01-28 00:23:20 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 4d8f3946-db4f-45b3-aa20-475e66630dc8 0xc003637ed7 0xc003637ed8}] []  [{e2e.test Update apps/v1 2023-01-28 00:23:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:23:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8f3946-db4f-45b3-aa20-475e66630dc8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:23:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003637f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:23:41.993: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-87f8f6dcf  deployment-3037  714ead8d-afb6-4f11-9e97-216f6e08f798 23835 2 2023-01-28 00:23:27 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 4d8f3946-db4f-45b3-aa20-475e66630dc8 0xc003748120 0xc003748121}] []  [{kube-controller-manager Update apps/v1 2023-01-28 00:23:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d8f3946-db4f-45b3-aa20-475e66630dc8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 00:23:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 87f8f6dcf,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:87f8f6dcf] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037481c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 00:23:42.011: INFO: Pod "test-rollover-deployment-779c67f4f8-dhrh9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-779c67f4f8-dhrh9 test-rollover-deployment-779c67f4f8- deployment-3037  c70c1494-bc60-40b9-aea7-af7f0a10ee44 23856 0 2023-01-28 00:23:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:779c67f4f8] map[cni.projectcalico.org/containerID:9732f48575c7ea56d1491dc5fd969747482c3291603489ca4681ea057dd91243 cni.projectcalico.org/podIP:172.30.90.85/32 cni.projectcalico.org/podIPs:172.30.90.85/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-779c67f4f8 6f218276-2053-45b1-b569-3fabd1a5b6a0 0xc0038696f7 0xc0038696f8}] []  [{kube-controller-manager Update v1 2023-01-28 00:23:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f218276-2053-45b1-b569-3fabd1a5b6a0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 00:23:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 00:23:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gfgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gfgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:23:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:23:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 00:23:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.85,StartTime:2023-01-28 00:23:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 00:23:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://4831da3815373a98c833bf09477cc81f3a9170493ad11f29781e491b148bdf5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 00:23:42.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3037" for this suite.

• [SLOW TEST:21.898 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":356,"completed":53,"skipped":757,"failed":0}
SSSS
------------------------------
[sig-node] RuntimeClass 
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:42.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-5269
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 28 00:23:42.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5269" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","total":356,"completed":54,"skipped":761,"failed":0}
S
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:42.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6846
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jan 28 00:23:42.683: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 28 00:23:42.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6846" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":356,"completed":55,"skipped":762,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:42.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4597
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:23:43.463: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:23:46.575: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:23:46.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4597" for this suite.
STEP: Destroying namespace "webhook-4597-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":356,"completed":56,"skipped":776,"failed":0}

------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:47.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-4068
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jan 28 00:23:49.478: INFO: running pods: 0 < 1
Jan 28 00:23:51.512: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 28 00:23:53.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4068" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":356,"completed":57,"skipped":776,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:53.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9900
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:23:53.973: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Jan 28 00:23:53.973: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 28 00:23:53.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9900" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":356,"completed":58,"skipped":801,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:23:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8964
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8964
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-8964
Jan 28 00:23:54.332: INFO: Found 0 stateful pods, waiting for 1
Jan 28 00:24:04.353: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jan 28 00:24:04.446: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jan 28 00:24:04.481: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jan 28 00:24:04.488: INFO: Observed &StatefulSet event: ADDED
Jan 28 00:24:04.488: INFO: Found Statefulset ss in namespace statefulset-8964 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 00:24:04.488: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jan 28 00:24:04.488: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 00:24:04.510: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jan 28 00:24:04.518: INFO: Observed &StatefulSet event: ADDED
Jan 28 00:24:04.518: INFO: Observed Statefulset ss in namespace statefulset-8964 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 00:24:04.518: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:24:04.518: INFO: Deleting all statefulset in ns statefulset-8964
Jan 28 00:24:04.533: INFO: Scaling statefulset ss to 0
Jan 28 00:24:14.619: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:24:14.634: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 00:24:14.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8964" for this suite.

• [SLOW TEST:20.723 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":356,"completed":59,"skipped":809,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:24:14.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3238
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:24:15.373: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:24:17.434: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 24, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:24:20.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:24:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3238" for this suite.
STEP: Destroying namespace "webhook-3238-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.388 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":356,"completed":60,"skipped":818,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:24:21.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6613
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 28 00:24:21.422: INFO: Waiting up to 5m0s for pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6" in namespace "emptydir-6613" to be "Succeeded or Failed"
Jan 28 00:24:21.439: INFO: Pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.82338ms
Jan 28 00:24:23.461: INFO: Pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039310167s
Jan 28 00:24:25.526: INFO: Pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103570098s
Jan 28 00:24:27.551: INFO: Pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.12873644s
STEP: Saw pod success
Jan 28 00:24:27.551: INFO: Pod "pod-469d0a08-01a0-4ca2-bd02-155f091383e6" satisfied condition "Succeeded or Failed"
Jan 28 00:24:27.567: INFO: Trying to get logs from node 10.187.128.30 pod pod-469d0a08-01a0-4ca2-bd02-155f091383e6 container test-container: <nil>
STEP: delete the pod
Jan 28 00:24:27.712: INFO: Waiting for pod pod-469d0a08-01a0-4ca2-bd02-155f091383e6 to disappear
Jan 28 00:24:27.731: INFO: Pod pod-469d0a08-01a0-4ca2-bd02-155f091383e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 00:24:27.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6613" for this suite.

• [SLOW TEST:6.638 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":61,"skipped":848,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:24:27.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-961
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 28 00:24:28.060: INFO: The status of Pod labelsupdate9172e355-1ac8-48ae-a8bd-067747a27fb8 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:24:30.078: INFO: The status of Pod labelsupdate9172e355-1ac8-48ae-a8bd-067747a27fb8 is Running (Ready = true)
Jan 28 00:24:30.719: INFO: Successfully updated pod "labelsupdate9172e355-1ac8-48ae-a8bd-067747a27fb8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 00:24:34.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-961" for this suite.

• [SLOW TEST:7.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":62,"skipped":855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:24:34.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4014
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 00:24:46.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4014" for this suite.

• [SLOW TEST:11.393 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":356,"completed":63,"skipped":899,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:24:46.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9588
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-9cf558cb-caac-4a21-81fe-9f8b2606bf55
STEP: Creating configMap with name cm-test-opt-upd-960711d2-0911-451b-8178-3517a38e2dd7
STEP: Creating the pod
Jan 28 00:24:46.565: INFO: The status of Pod pod-projected-configmaps-fe49797d-b0c8-481f-87fe-c93ceaaca48b is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:24:48.584: INFO: The status of Pod pod-projected-configmaps-fe49797d-b0c8-481f-87fe-c93ceaaca48b is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:24:50.582: INFO: The status of Pod pod-projected-configmaps-fe49797d-b0c8-481f-87fe-c93ceaaca48b is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-9cf558cb-caac-4a21-81fe-9f8b2606bf55
STEP: Updating configmap cm-test-opt-upd-960711d2-0911-451b-8178-3517a38e2dd7
STEP: Creating configMap with name cm-test-opt-create-761ffaae-3254-45af-9bc0-dea78c497b07
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 00:26:12.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9588" for this suite.

• [SLOW TEST:86.221 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":64,"skipped":982,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:12.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7274
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-7274/configmap-test-a9c29e61-230f-44a2-a8ab-5ad95cb34a49
STEP: Creating a pod to test consume configMaps
Jan 28 00:26:12.728: INFO: Waiting up to 5m0s for pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800" in namespace "configmap-7274" to be "Succeeded or Failed"
Jan 28 00:26:12.739: INFO: Pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800": Phase="Pending", Reason="", readiness=false. Elapsed: 11.374415ms
Jan 28 00:26:14.765: INFO: Pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037628724s
Jan 28 00:26:16.783: INFO: Pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055425994s
Jan 28 00:26:18.802: INFO: Pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074105329s
STEP: Saw pod success
Jan 28 00:26:18.802: INFO: Pod "pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800" satisfied condition "Succeeded or Failed"
Jan 28 00:26:18.817: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800 container env-test: <nil>
STEP: delete the pod
Jan 28 00:26:18.924: INFO: Waiting for pod pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800 to disappear
Jan 28 00:26:18.935: INFO: Pod pod-configmaps-67ef42c9-b652-450b-b309-0ec48bcfa800 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 00:26:18.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7274" for this suite.

• [SLOW TEST:6.483 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":356,"completed":65,"skipped":1003,"failed":0}
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:18.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1467
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 28 00:26:19.230: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:26:21.248: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 28 00:26:21.287: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:26:23.302: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:26:25.299: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 28 00:26:25.359: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 28 00:26:25.371: INFO: Pod pod-with-poststart-http-hook still exists
Jan 28 00:26:27.371: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 28 00:26:27.388: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 28 00:26:27.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1467" for this suite.

• [SLOW TEST:8.454 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":356,"completed":66,"skipped":1006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity 
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:27.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename csistoragecapacity
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in csistoragecapacity-7102
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/storage.k8s.io
STEP: getting /apis/storage.k8s.io/v1
STEP: creating
STEP: watching
Jan 28 00:26:27.719: INFO: starting watch
STEP: getting
STEP: listing in namespace
STEP: listing across namespaces
STEP: patching
STEP: updating
Jan 28 00:26:27.837: INFO: waiting for watch events with expected annotations in namespace
Jan 28 00:26:27.837: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:188
Jan 28 00:26:27.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-7102" for this suite.
•{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","total":356,"completed":67,"skipped":1038,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:28.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5568
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 00:26:39.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5568" for this suite.

• [SLOW TEST:11.662 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":356,"completed":68,"skipped":1050,"failed":0}
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:39.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7434
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching services
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:26:39.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7434" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":356,"completed":69,"skipped":1050,"failed":0}

------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:39.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-6065
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 28 00:26:40.906: INFO: starting watch
STEP: patching
STEP: updating
Jan 28 00:26:40.952: INFO: waiting for watch events with expected annotations
Jan 28 00:26:40.952: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:26:41.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6065" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":356,"completed":70,"skipped":1050,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:26:41.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9028
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-downwardapi-g7dv
STEP: Creating a pod to test atomic-volume-subpath
Jan 28 00:26:41.533: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g7dv" in namespace "subpath-9028" to be "Succeeded or Failed"
Jan 28 00:26:41.546: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Pending", Reason="", readiness=false. Elapsed: 12.452155ms
Jan 28 00:26:43.578: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 2.044721708s
Jan 28 00:26:45.591: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 4.057606293s
Jan 28 00:26:47.632: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 6.098421082s
Jan 28 00:26:49.650: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 8.116611976s
Jan 28 00:26:51.676: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 10.14267214s
Jan 28 00:26:53.694: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 12.160195439s
Jan 28 00:26:55.707: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 14.173763404s
Jan 28 00:26:57.725: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 16.191409555s
Jan 28 00:26:59.742: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 18.209078491s
Jan 28 00:27:01.774: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=true. Elapsed: 20.240715282s
Jan 28 00:27:03.792: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Running", Reason="", readiness=false. Elapsed: 22.259061576s
Jan 28 00:27:05.806: INFO: Pod "pod-subpath-test-downwardapi-g7dv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.272409708s
STEP: Saw pod success
Jan 28 00:27:05.806: INFO: Pod "pod-subpath-test-downwardapi-g7dv" satisfied condition "Succeeded or Failed"
Jan 28 00:27:05.817: INFO: Trying to get logs from node 10.187.128.30 pod pod-subpath-test-downwardapi-g7dv container test-container-subpath-downwardapi-g7dv: <nil>
STEP: delete the pod
Jan 28 00:27:05.894: INFO: Waiting for pod pod-subpath-test-downwardapi-g7dv to disappear
Jan 28 00:27:05.904: INFO: Pod pod-subpath-test-downwardapi-g7dv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-g7dv
Jan 28 00:27:05.905: INFO: Deleting pod "pod-subpath-test-downwardapi-g7dv" in namespace "subpath-9028"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 28 00:27:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9028" for this suite.

• [SLOW TEST:24.675 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","total":356,"completed":71,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8764
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should apply changes to a job status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensure pods equal to paralellism count is attached to the job
STEP: patching /status
STEP: updating /status
STEP: get /status
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 28 00:27:10.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8764" for this suite.
•{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","total":356,"completed":72,"skipped":1110,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9544
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:27:11.044: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:27:13.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 27, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 27, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 27, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 27, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:27:16.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:27:16.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9544" for this suite.
STEP: Destroying namespace "webhook-9544-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":356,"completed":73,"skipped":1113,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:17.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2339
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:27:17.378: INFO: created pod pod-service-account-defaultsa
Jan 28 00:27:17.378: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 28 00:27:17.390: INFO: created pod pod-service-account-mountsa
Jan 28 00:27:17.391: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 28 00:27:17.404: INFO: created pod pod-service-account-nomountsa
Jan 28 00:27:17.404: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 28 00:27:17.419: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 28 00:27:17.419: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 28 00:27:17.435: INFO: created pod pod-service-account-mountsa-mountspec
Jan 28 00:27:17.435: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 28 00:27:17.449: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 28 00:27:17.449: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 28 00:27:17.462: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 28 00:27:17.462: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 28 00:27:17.484: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 28 00:27:17.484: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 28 00:27:17.501: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 28 00:27:17.501: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 00:27:17.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2339" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":356,"completed":74,"skipped":1146,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:17.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2280
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:27:17.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:27:18.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2280" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":356,"completed":75,"skipped":1156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:18.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7958
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 28 00:27:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7958" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":356,"completed":76,"skipped":1188,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:19.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2459
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:27:19.392: INFO: Creating pod...
Jan 28 00:27:21.449: INFO: Creating service...
Jan 28 00:27:21.482: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/DELETE
Jan 28 00:27:21.535: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 00:27:21.536: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/GET
Jan 28 00:27:21.557: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 28 00:27:21.557: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/HEAD
Jan 28 00:27:21.575: INFO: http.Client request:HEAD | StatusCode:200
Jan 28 00:27:21.576: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 28 00:27:21.594: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 00:27:21.594: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/PATCH
Jan 28 00:27:21.611: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 00:27:21.611: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/POST
Jan 28 00:27:21.630: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 00:27:21.630: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/pods/agnhost/proxy/some/path/with/PUT
Jan 28 00:27:21.660: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 00:27:21.660: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/DELETE
Jan 28 00:27:21.684: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 00:27:21.684: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/GET
Jan 28 00:27:21.707: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 28 00:27:21.707: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/HEAD
Jan 28 00:27:21.735: INFO: http.Client request:HEAD | StatusCode:200
Jan 28 00:27:21.735: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/OPTIONS
Jan 28 00:27:21.760: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 00:27:21.760: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/PATCH
Jan 28 00:27:21.785: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 00:27:21.785: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/POST
Jan 28 00:27:21.809: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 00:27:21.809: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-2459/services/test-service/proxy/some/path/with/PUT
Jan 28 00:27:21.833: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 28 00:27:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2459" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":356,"completed":77,"skipped":1201,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:21.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6089
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:27:22.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b" in namespace "downward-api-6089" to be "Succeeded or Failed"
Jan 28 00:27:22.170: INFO: Pod "downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.602809ms
Jan 28 00:27:24.186: INFO: Pod "downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041143487s
Jan 28 00:27:26.200: INFO: Pod "downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055785665s
STEP: Saw pod success
Jan 28 00:27:26.200: INFO: Pod "downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b" satisfied condition "Succeeded or Failed"
Jan 28 00:27:26.212: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b container client-container: <nil>
STEP: delete the pod
Jan 28 00:27:26.278: INFO: Waiting for pod downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b to disappear
Jan 28 00:27:26.289: INFO: Pod downwardapi-volume-b1c39b51-7e61-45bd-b2fb-ba28ab7b782b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:27:26.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6089" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":78,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:26.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8394
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:27:26.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:27:30.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:27:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7068-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:27:33.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8394" for this suite.
STEP: Destroying namespace "webhook-8394-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":356,"completed":79,"skipped":1333,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:27:33.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4158
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:27:34.022: INFO: created pod
Jan 28 00:27:34.023: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4158" to be "Succeeded or Failed"
Jan 28 00:27:34.037: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.000403ms
Jan 28 00:27:36.051: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028410875s
Jan 28 00:27:38.068: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045106456s
Jan 28 00:27:40.090: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.067395906s
STEP: Saw pod success
Jan 28 00:27:40.090: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 28 00:28:10.091: INFO: polling logs
Jan 28 00:28:10.169: INFO: Pod logs: 
I0128 00:27:35.782943       1 log.go:195] OK: Got token
I0128 00:27:35.782990       1 log.go:195] validating with in-cluster discovery
I0128 00:27:35.783566       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0128 00:27:35.783608       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4158:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674866254, NotBefore:1674865654, IssuedAt:1674865654, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4158", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4dacca35-d653-48b5-b425-bdde5c46d0b6"}}}
I0128 00:27:35.816541       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0128 00:27:35.838541       1 log.go:195] OK: Validated signature on JWT
I0128 00:27:35.838641       1 log.go:195] OK: Got valid claims from token!
I0128 00:27:35.838666       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-4158:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674866254, NotBefore:1674865654, IssuedAt:1674865654, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4158", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4dacca35-d653-48b5-b425-bdde5c46d0b6"}}}

Jan 28 00:28:10.169: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 00:28:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4158" for this suite.

• [SLOW TEST:36.456 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":356,"completed":80,"skipped":1345,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:28:10.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2927
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jan 28 00:28:10.470: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 00:28:15.491: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 00:28:15.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2927" for this suite.

• [SLOW TEST:5.381 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":356,"completed":81,"skipped":1383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:28:15.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-6135
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:28:15.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Creating first CR 
Jan 28 00:28:18.505: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:18Z]] name:name1 resourceVersion:25510 uid:dbce1be3-e1f8-4414-b1c8-ad48e5d32505] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jan 28 00:28:28.531: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:28Z]] name:name2 resourceVersion:25550 uid:bb3944a4-d138-48ea-8862-a801e4ad50e2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jan 28 00:28:38.590: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:38Z]] name:name1 resourceVersion:25560 uid:dbce1be3-e1f8-4414-b1c8-ad48e5d32505] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jan 28 00:28:48.617: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:48Z]] name:name2 resourceVersion:25569 uid:bb3944a4-d138-48ea-8862-a801e4ad50e2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jan 28 00:28:58.655: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:38Z]] name:name1 resourceVersion:25578 uid:dbce1be3-e1f8-4414-b1c8-ad48e5d32505] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jan 28 00:29:08.694: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-28T00:28:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-28T00:28:48Z]] name:name2 resourceVersion:25588 uid:bb3944a4-d138-48ea-8862-a801e4ad50e2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:29:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6135" for this suite.

• [SLOW TEST:63.659 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":356,"completed":82,"skipped":1418,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:29:19.303: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4084
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-3f781e83-efc7-4687-9a19-a8c52ae34c15
STEP: Creating a pod to test consume configMaps
Jan 28 00:29:19.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04" in namespace "projected-4084" to be "Succeeded or Failed"
Jan 28 00:29:19.564: INFO: Pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04": Phase="Pending", Reason="", readiness=false. Elapsed: 12.108325ms
Jan 28 00:29:21.580: INFO: Pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028118071s
Jan 28 00:29:23.599: INFO: Pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047414838s
Jan 28 00:29:25.616: INFO: Pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064237972s
STEP: Saw pod success
Jan 28 00:29:25.616: INFO: Pod "pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04" satisfied condition "Succeeded or Failed"
Jan 28 00:29:25.627: INFO: Trying to get logs from node 10.187.128.30 pod pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:29:25.741: INFO: Waiting for pod pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04 to disappear
Jan 28 00:29:25.754: INFO: Pod pod-projected-configmaps-b4a4975e-55f8-4809-bc48-08e68fcfda04 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 00:29:25.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4084" for this suite.

• [SLOW TEST:6.494 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":83,"skipped":1474,"failed":0}
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:29:25.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-8323
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 28 00:34:26.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8323" for this suite.

• [SLOW TEST:300.349 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":356,"completed":84,"skipped":1474,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:34:26.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5231
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Service
STEP: watching for the Service to be added
Jan 28 00:34:26.403: INFO: Found Service test-service-jq8lx in namespace services-5231 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 28 00:34:26.403: INFO: Service test-service-jq8lx created
STEP: Getting /status
Jan 28 00:34:26.419: INFO: Service test-service-jq8lx has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jan 28 00:34:26.447: INFO: observed Service test-service-jq8lx in namespace services-5231 with annotations: map[] & LoadBalancer: {[]}
Jan 28 00:34:26.447: INFO: Found Service test-service-jq8lx in namespace services-5231 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 28 00:34:26.447: INFO: Service test-service-jq8lx has service status patched
STEP: updating the ServiceStatus
Jan 28 00:34:26.484: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jan 28 00:34:26.490: INFO: Observed Service test-service-jq8lx in namespace services-5231 with annotations: map[] & Conditions: {[]}
Jan 28 00:34:26.490: INFO: Observed event: &Service{ObjectMeta:{test-service-jq8lx  services-5231  2533f4e6-6cce-4b04-b7af-12d97a6bd766 25948 0 2023-01-28 00:34:26 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2023-01-28 00:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-28 00:34:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.185.99,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.185.99],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 28 00:34:26.490: INFO: Found Service test-service-jq8lx in namespace services-5231 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 00:34:26.491: INFO: Service test-service-jq8lx has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jan 28 00:34:26.524: INFO: observed Service test-service-jq8lx in namespace services-5231 with labels: map[test-service-static:true]
Jan 28 00:34:26.524: INFO: observed Service test-service-jq8lx in namespace services-5231 with labels: map[test-service-static:true]
Jan 28 00:34:26.524: INFO: observed Service test-service-jq8lx in namespace services-5231 with labels: map[test-service-static:true]
Jan 28 00:34:26.524: INFO: Found Service test-service-jq8lx in namespace services-5231 with labels: map[test-service:patched test-service-static:true]
Jan 28 00:34:26.524: INFO: Service test-service-jq8lx patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jan 28 00:34:26.589: INFO: Observed event: ADDED
Jan 28 00:34:26.589: INFO: Observed event: MODIFIED
Jan 28 00:34:26.589: INFO: Observed event: MODIFIED
Jan 28 00:34:26.590: INFO: Observed event: MODIFIED
Jan 28 00:34:26.590: INFO: Found Service test-service-jq8lx in namespace services-5231 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 28 00:34:26.590: INFO: Service test-service-jq8lx deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:34:26.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5231" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":356,"completed":85,"skipped":1493,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:34:26.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1117
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 00:34:26.837: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 00:34:26.897: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 00:34:26.910: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.27 before test
Jan 28 00:34:26.961: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.961: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 00:34:26.962: INFO: calico-node-9md4k from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.962: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:34:26.962: INFO: calico-typha-987c59c59-r2mxh from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.962: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:34:26.962: INFO: coredns-649f45bb5-4l67p from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.962: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:34:26.962: INFO: ibm-keepalived-watcher-qxwck from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.962: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:34:26.962: INFO: ibm-master-proxy-static-10.187.128.27 from kube-system started at 2023-01-27 22:02:32 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:26.962: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:34:26.962: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:34:26.962: INFO: ibmcloud-block-storage-driver-mfqhd from kube-system started at 2023-01-27 22:02:42 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.963: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:34:26.963: INFO: ingress-cluster-healthcheck-56756684f7-qbpsl from kube-system started at 2023-01-27 22:43:38 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.963: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 00:34:26.963: INFO: konnectivity-agent-8ws48 from kube-system started at 2023-01-27 22:37:37 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.963: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:34:26.963: INFO: metrics-server-666774474b-hfjbk from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 00:34:26.963: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:34:26.963: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:34:26.963: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:34:26.963: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:26.963: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:34:26.964: INFO: sonobuoy-e2e-job-68fa4b4566bd42ca from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:26.964: INFO: 	Container e2e ready: true, restart count 0
Jan 28 00:34:26.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:34:26.964: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:26.964: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:34:26.964: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:34:26.964: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.30 before test
Jan 28 00:34:27.002: INFO: calico-node-lg9x8 from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.002: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:34:27.002: INFO: calico-typha-987c59c59-qs9ww from kube-system started at 2023-01-27 22:03:10 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.002: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:34:27.002: INFO: coredns-649f45bb5-9gxmz from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.002: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:34:27.002: INFO: ibm-keepalived-watcher-p94jf from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.002: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:34:27.002: INFO: ibm-master-proxy-static-10.187.128.30 from kube-system started at 2023-01-27 22:02:39 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:27.003: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:34:27.003: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:34:27.003: INFO: ibmcloud-block-storage-driver-mqmrs from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.003: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:34:27.003: INFO: konnectivity-agent-nbg9q from kube-system started at 2023-01-27 22:37:30 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.003: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:34:27.003: INFO: metrics-server-666774474b-x2b75 from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 00:34:27.003: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 00:34:27.003: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 00:34:27.003: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 00:34:27.003: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-grkj5 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.003: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 00:34:27.004: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-2j8w6 from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:27.004: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:34:27.004: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 00:34:27.004: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:33:03 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.004: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 00:34:27.004: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.43 before test
Jan 28 00:34:27.041: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-h27j5 from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.042: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 00:34:27.042: INFO: calico-kube-controllers-6c58444bc9-tvxlr from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.042: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 00:34:27.042: INFO: calico-node-tfsm6 from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.042: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 00:34:27.042: INFO: calico-typha-987c59c59-lzxm5 from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.042: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 00:34:27.042: INFO: coredns-649f45bb5-9w6d2 from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.042: INFO: 	Container coredns ready: true, restart count 0
Jan 28 00:34:27.043: INFO: coredns-autoscaler-64db77d767-4bnrk from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.043: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 00:34:27.043: INFO: dashboard-metrics-scraper-f74668d5f-n58pz from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.043: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 00:34:27.043: INFO: ibm-file-plugin-6b4fdfc7d8-6p4mp from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.043: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 00:34:27.043: INFO: ibm-keepalived-watcher-9flqg from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.043: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 00:34:27.043: INFO: ibm-master-proxy-static-10.187.128.43 from kube-system started at 2023-01-27 22:02:29 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:27.043: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 00:34:27.044: INFO: 	Container pause ready: true, restart count 0
Jan 28 00:34:27.044: INFO: ibm-storage-watcher-6cd9b56547-klq4v from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 00:34:27.044: INFO: ibmcloud-block-storage-driver-d4h67 from kube-system started at 2023-01-27 22:02:49 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 00:34:27.044: INFO: ibmcloud-block-storage-plugin-5f9c77dc8c-dwb8r from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 00:34:27.044: INFO: konnectivity-agent-zrrn4 from kube-system started at 2023-01-27 22:37:33 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 00:34:27.044: INFO: kubernetes-dashboard-65d4f9ccbf-ncq24 from kube-system started at 2023-01-27 22:02:51 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 00:34:27.044: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:19 +0000 UTC (1 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 00:34:27.044: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-m6nkr from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 00:34:27.044: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 00:34:27.045: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173e51d0712c143c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:34:28.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1117" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":356,"completed":86,"skipped":1503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:34:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-561
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 28 00:34:28.383: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 00:35:28.474: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:35:28.487: INFO: Starting informer...
STEP: Starting pod...
Jan 28 00:35:28.739: INFO: Pod is running on 10.187.128.43. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jan 28 00:35:28.795: INFO: Pod wasn't evicted. Proceeding
Jan 28 00:35:28.795: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jan 28 00:36:43.856: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:36:43.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-561" for this suite.

• [SLOW TEST:135.727 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":356,"completed":87,"skipped":1534,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:36:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9266
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:36:46.157: INFO: Deleting pod "var-expansion-587583b6-ddd6-4f38-bb45-b238db35853c" in namespace "var-expansion-9266"
Jan 28 00:36:46.187: INFO: Wait up to 5m0s for pod "var-expansion-587583b6-ddd6-4f38-bb45-b238db35853c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 00:36:50.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9266" for this suite.

• [SLOW TEST:6.343 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":356,"completed":88,"skipped":1554,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:36:50.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5489
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:36:50.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4" in namespace "projected-5489" to be "Succeeded or Failed"
Jan 28 00:36:50.502: INFO: Pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.187347ms
Jan 28 00:36:52.518: INFO: Pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042478172s
Jan 28 00:36:54.538: INFO: Pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062509486s
Jan 28 00:36:56.558: INFO: Pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.082509376s
STEP: Saw pod success
Jan 28 00:36:56.559: INFO: Pod "downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4" satisfied condition "Succeeded or Failed"
Jan 28 00:36:56.571: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4 container client-container: <nil>
STEP: delete the pod
Jan 28 00:36:56.682: INFO: Waiting for pod downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4 to disappear
Jan 28 00:36:56.693: INFO: Pod downwardapi-volume-866a7edc-a2d9-4484-9dcf-ac0024f17ef4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 00:36:56.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5489" for this suite.

• [SLOW TEST:6.478 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":89,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:36:56.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4188
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-4188
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 28 00:36:56.928: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 00:36:57.036: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:36:59.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:01.052: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:03.056: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:05.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:07.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:09.062: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:11.053: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:13.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:15.054: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:17.055: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:37:19.056: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 28 00:37:19.079: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 28 00:37:19.102: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 28 00:37:21.167: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 00:37:21.167: INFO: Breadth first check of 172.30.102.119 on host 10.187.128.27...
Jan 28 00:37:21.178: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.101:9080/dial?request=hostname&protocol=udp&host=172.30.102.119&port=8081&tries=1'] Namespace:pod-network-test-4188 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:37:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:37:21.179: INFO: ExecWithOptions: Clientset creation
Jan 28 00:37:21.179: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4188/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.101%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.102.119%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:37:21.397: INFO: Waiting for responses: map[]
Jan 28 00:37:21.397: INFO: reached 172.30.102.119 after 0/1 tries
Jan 28 00:37:21.397: INFO: Breadth first check of 172.30.253.223 on host 10.187.128.30...
Jan 28 00:37:21.410: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.101:9080/dial?request=hostname&protocol=udp&host=172.30.253.223&port=8081&tries=1'] Namespace:pod-network-test-4188 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:37:21.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:37:21.411: INFO: ExecWithOptions: Clientset creation
Jan 28 00:37:21.411: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4188/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.101%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.253.223%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:37:21.591: INFO: Waiting for responses: map[]
Jan 28 00:37:21.592: INFO: reached 172.30.253.223 after 0/1 tries
Jan 28 00:37:21.592: INFO: Breadth first check of 172.30.90.100 on host 10.187.128.43...
Jan 28 00:37:21.605: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.90.101:9080/dial?request=hostname&protocol=udp&host=172.30.90.100&port=8081&tries=1'] Namespace:pod-network-test-4188 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:37:21.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:37:21.606: INFO: ExecWithOptions: Clientset creation
Jan 28 00:37:21.606: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4188/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.90.101%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.90.100%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 28 00:37:21.827: INFO: Waiting for responses: map[]
Jan 28 00:37:21.827: INFO: reached 172.30.90.100 after 0/1 tries
Jan 28 00:37:21.827: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 28 00:37:21.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4188" for this suite.

• [SLOW TEST:25.134 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":356,"completed":90,"skipped":1598,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:37:21.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5461
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5461
STEP: creating service affinity-clusterip-transition in namespace services-5461
STEP: creating replication controller affinity-clusterip-transition in namespace services-5461
I0128 00:37:22.115888      26 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5461, replica count: 3
I0128 00:37:25.166220      26 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 00:37:25.204: INFO: Creating new exec pod
Jan 28 00:37:28.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5461 exec execpod-affinityfwvmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 28 00:37:28.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 28 00:37:28.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:37:28.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5461 exec execpod-affinityfwvmd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.173.85 80'
Jan 28 00:37:28.796: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.173.85 80\nConnection to 172.21.173.85 80 port [tcp/http] succeeded!\n"
Jan 28 00:37:28.796: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 00:37:28.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5461 exec execpod-affinityfwvmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.85:80/ ; done'
Jan 28 00:37:29.186: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n"
Jan 28 00:37:29.186: INFO: stdout: "\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp"
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.186: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:29.187: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5461 exec execpod-affinityfwvmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.85:80/ ; done'
Jan 28 00:37:59.585: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n"
Jan 28 00:37:59.585: INFO: stdout: "\naffinity-clusterip-transition-l8dj8\naffinity-clusterip-transition-l8dj8\naffinity-clusterip-transition-l8dj8\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-l8dj8\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-f46dp\naffinity-clusterip-transition-l8dj8"
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-l8dj8
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-l8dj8
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-l8dj8
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-l8dj8
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-f46dp
Jan 28 00:37:59.585: INFO: Received response from host: affinity-clusterip-transition-l8dj8
Jan 28 00:37:59.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5461 exec execpod-affinityfwvmd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.173.85:80/ ; done'
Jan 28 00:38:00.017: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.173.85:80/\n"
Jan 28 00:38:00.017: INFO: stdout: "\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm\naffinity-clusterip-transition-hxpxm"
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Received response from host: affinity-clusterip-transition-hxpxm
Jan 28 00:38:00.017: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5461, will wait for the garbage collector to delete the pods
Jan 28 00:38:00.150: INFO: Deleting ReplicationController affinity-clusterip-transition took: 29.305935ms
Jan 28 00:38:00.251: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.121224ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:38:03.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5461" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:41.284 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":91,"skipped":1613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:38:03.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6404
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-50b28b6d-e780-4a4b-9fc0-e37c9a3fb712
STEP: Creating a pod to test consume secrets
Jan 28 00:38:03.391: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05" in namespace "projected-6404" to be "Succeeded or Failed"
Jan 28 00:38:03.403: INFO: Pod "pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041048ms
Jan 28 00:38:05.419: INFO: Pod "pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027713688s
Jan 28 00:38:07.464: INFO: Pod "pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073021754s
STEP: Saw pod success
Jan 28 00:38:07.464: INFO: Pod "pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05" satisfied condition "Succeeded or Failed"
Jan 28 00:38:07.476: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:38:07.545: INFO: Waiting for pod pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05 to disappear
Jan 28 00:38:07.556: INFO: Pod pod-projected-secrets-62c448f6-5c99-4a27-97cb-36c8e0caef05 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 00:38:07.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6404" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":92,"skipped":1669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:38:07.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-1498
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 00:38:07.844: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 00:39:07.953: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:39:07.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1285
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jan 28 00:39:10.296: INFO: found a healthy node: 10.187.128.43
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:39:26.674: INFO: pods created so far: [1 1 1]
Jan 28 00:39:26.674: INFO: length of pods created so far: 3
Jan 28 00:39:30.750: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:188
Jan 28 00:39:37.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1285" for this suite.
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:39:37.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1498" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:90.548 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":356,"completed":93,"skipped":1705,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:39:38.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-3780
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 00:39:38.437: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 00:40:38.597: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 28 00:40:38.720: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 28 00:40:38.744: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 28 00:40:38.797: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 28 00:40:38.822: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 28 00:40:38.881: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 28 00:40:38.905: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:40:57.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3780" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:79.322 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":356,"completed":94,"skipped":1719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:40:57.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9289
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:40:57.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9289" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":356,"completed":95,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:40:57.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6543
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod test-webserver-7391c3e2-f74a-4b79-807b-0f2d0a0b01f5 in namespace container-probe-6543
Jan 28 00:41:02.262: INFO: Started pod test-webserver-7391c3e2-f74a-4b79-807b-0f2d0a0b01f5 in namespace container-probe-6543
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 00:41:02.285: INFO: Initial restart count of pod test-webserver-7391c3e2-f74a-4b79-807b-0f2d0a0b01f5 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 00:45:04.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6543" for this suite.

• [SLOW TEST:246.187 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":96,"skipped":1864,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:45:04.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1226
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-1226
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 28 00:45:04.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 00:45:04.640: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:45:06.666: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:08.674: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:10.668: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:12.673: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:14.683: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:16.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:18.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:20.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:22.672: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:24.670: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:45:26.679: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 28 00:45:26.713: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 28 00:45:26.754: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 28 00:45:30.950: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 00:45:30.950: INFO: Going to poll 172.30.102.125 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:45:30.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.102.125:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:45:30.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:45:30.981: INFO: ExecWithOptions: Clientset creation
Jan 28 00:45:30.981: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.102.125%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:45:31.233: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 28 00:45:31.233: INFO: Going to poll 172.30.253.227 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:45:31.254: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.253.227:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:45:31.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:45:31.255: INFO: ExecWithOptions: Clientset creation
Jan 28 00:45:31.255: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.253.227%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:45:31.459: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 28 00:45:31.459: INFO: Going to poll 172.30.90.114 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:45:31.475: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.90.114:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1226 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:45:31.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:45:31.475: INFO: ExecWithOptions: Clientset creation
Jan 28 00:45:31.476: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-1226/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.90.114%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:45:31.698: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 28 00:45:31.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1226" for this suite.

• [SLOW TEST:27.616 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":97,"skipped":1872,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:45:31.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1471
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1471
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:45:32.026: INFO: Found 0 stateful pods, waiting for 1
Jan 28 00:45:42.057: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jan 28 00:45:42.160: INFO: Found 1 stateful pods, waiting for 2
Jan 28 00:45:52.191: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:45:52.191: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:45:52.291: INFO: Deleting all statefulset in ns statefulset-1471
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 00:45:52.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1471" for this suite.

• [SLOW TEST:20.632 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":356,"completed":98,"skipped":1900,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:45:52.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2574
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2574
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2574
STEP: creating replication controller externalsvc in namespace services-2574
I0128 00:45:52.817401      26 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2574, replica count: 2
I0128 00:45:55.869117      26 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jan 28 00:45:55.982: INFO: Creating new exec pod
Jan 28 00:46:00.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2574 exec execpodwrt4w -- /bin/sh -x -c nslookup nodeport-service.services-2574.svc.cluster.local'
Jan 28 00:46:00.387: INFO: stderr: "+ nslookup nodeport-service.services-2574.svc.cluster.local\n"
Jan 28 00:46:00.387: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2574.svc.cluster.local\tcanonical name = externalsvc.services-2574.svc.cluster.local.\nName:\texternalsvc.services-2574.svc.cluster.local\nAddress: 172.21.246.37\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2574, will wait for the garbage collector to delete the pods
Jan 28 00:46:00.482: INFO: Deleting ReplicationController externalsvc took: 26.697097ms
Jan 28 00:46:00.582: INFO: Terminating ReplicationController externalsvc pods took: 100.104305ms
Jan 28 00:46:03.457: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 00:46:03.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2574" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:11.162 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":356,"completed":99,"skipped":1916,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5352
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 28 00:46:08.972: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 28 00:46:09.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5352" for this suite.

• [SLOW TEST:5.546 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":356,"completed":100,"skipped":1927,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:09.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2251
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-f7d28185-794a-41e6-9afe-8fbfbdcbfc5b
STEP: Creating a pod to test consume configMaps
Jan 28 00:46:09.387: INFO: Waiting up to 5m0s for pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4" in namespace "configmap-2251" to be "Succeeded or Failed"
Jan 28 00:46:09.403: INFO: Pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.440604ms
Jan 28 00:46:11.429: INFO: Pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4": Phase="Running", Reason="", readiness=true. Elapsed: 2.041579861s
Jan 28 00:46:13.459: INFO: Pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4": Phase="Running", Reason="", readiness=false. Elapsed: 4.072145445s
Jan 28 00:46:15.482: INFO: Pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.09473473s
STEP: Saw pod success
Jan 28 00:46:15.482: INFO: Pod "pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4" satisfied condition "Succeeded or Failed"
Jan 28 00:46:15.498: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:46:15.640: INFO: Waiting for pod pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4 to disappear
Jan 28 00:46:15.658: INFO: Pod pod-configmaps-8197c831-4d6e-4693-a703-e71483f45df4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 00:46:15.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2251" for this suite.

• [SLOW TEST:6.606 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":101,"skipped":1940,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:15.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1323
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 28 00:46:15.957: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 28 00:46:20.976: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 28 00:46:21.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1323" for this suite.

• [SLOW TEST:5.416 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":356,"completed":102,"skipped":1980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:21.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6008
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jan 28 00:46:21.460: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 00:46:26.487: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jan 28 00:46:26.506: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jan 28 00:46:26.606: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jan 28 00:46:26.613: INFO: Observed &ReplicaSet event: ADDED
Jan 28 00:46:26.613: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.614: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.614: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.614: INFO: Found replicaset test-rs in namespace replicaset-6008 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 00:46:26.614: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jan 28 00:46:26.614: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 00:46:26.638: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jan 28 00:46:26.644: INFO: Observed &ReplicaSet event: ADDED
Jan 28 00:46:26.644: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.645: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.645: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.645: INFO: Observed replicaset test-rs in namespace replicaset-6008 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 00:46:26.645: INFO: Observed &ReplicaSet event: MODIFIED
Jan 28 00:46:26.645: INFO: Found replicaset test-rs in namespace replicaset-6008 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 28 00:46:26.645: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 00:46:26.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6008" for this suite.

• [SLOW TEST:5.565 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":356,"completed":103,"skipped":2004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6212
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1334
STEP: creating the pod
Jan 28 00:46:26.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 create -f -'
Jan 28 00:46:27.132: INFO: stderr: ""
Jan 28 00:46:27.132: INFO: stdout: "pod/pause created\n"
Jan 28 00:46:27.132: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 28 00:46:27.132: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6212" to be "running and ready"
Jan 28 00:46:27.151: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.432431ms
Jan 28 00:46:29.208: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075825515s
Jan 28 00:46:31.229: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.096892595s
Jan 28 00:46:31.229: INFO: Pod "pause" satisfied condition "running and ready"
Jan 28 00:46:31.229: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/framework/framework.go:652
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 28 00:46:31.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 label pods pause testing-label=testing-label-value'
Jan 28 00:46:31.331: INFO: stderr: ""
Jan 28 00:46:31.331: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 28 00:46:31.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 get pod pause -L testing-label'
Jan 28 00:46:31.423: INFO: stderr: ""
Jan 28 00:46:31.423: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 28 00:46:31.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 label pods pause testing-label-'
Jan 28 00:46:31.527: INFO: stderr: ""
Jan 28 00:46:31.527: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 28 00:46:31.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 get pod pause -L testing-label'
Jan 28 00:46:31.641: INFO: stderr: ""
Jan 28 00:46:31.641: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Jan 28 00:46:31.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 delete --grace-period=0 --force -f -'
Jan 28 00:46:31.783: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 00:46:31.783: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 28 00:46:31.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 get rc,svc -l name=pause --no-headers'
Jan 28 00:46:31.883: INFO: stderr: "No resources found in kubectl-6212 namespace.\n"
Jan 28 00:46:31.883: INFO: stdout: ""
Jan 28 00:46:31.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-6212 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 00:46:31.976: INFO: stderr: ""
Jan 28 00:46:31.976: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 00:46:31.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6212" for this suite.

• [SLOW TEST:5.332 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1332
    should update the label on a resource  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":356,"completed":104,"skipped":2044,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:32.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3435
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 00:46:39.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3435" for this suite.

• [SLOW TEST:7.355 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":356,"completed":105,"skipped":2049,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:39.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8756
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 28 00:46:43.766: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 28 00:46:43.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8756" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":106,"skipped":2064,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:43.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9225
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-c3e1ec6b-91d6-4c34-bc37-758d7630f704
STEP: Creating a pod to test consume configMaps
Jan 28 00:46:44.178: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1" in namespace "projected-9225" to be "Succeeded or Failed"
Jan 28 00:46:44.197: INFO: Pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.972505ms
Jan 28 00:46:46.228: INFO: Pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049784217s
Jan 28 00:46:48.259: INFO: Pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080250223s
Jan 28 00:46:50.286: INFO: Pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107349259s
STEP: Saw pod success
Jan 28 00:46:50.286: INFO: Pod "pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1" satisfied condition "Succeeded or Failed"
Jan 28 00:46:50.304: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:46:50.401: INFO: Waiting for pod pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1 to disappear
Jan 28 00:46:50.419: INFO: Pod pod-projected-configmaps-5a243b66-3214-4718-ae4b-733e733c63a1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 00:46:50.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9225" for this suite.

• [SLOW TEST:6.570 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":107,"skipped":2069,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:50.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9516
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4185
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4164
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:46:57.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9516" for this suite.
STEP: Destroying namespace "nsdeletetest-4185" for this suite.
Jan 28 00:46:57.261: INFO: Namespace nsdeletetest-4185 was already deleted
STEP: Destroying namespace "nsdeletetest-4164" for this suite.

• [SLOW TEST:6.815 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":356,"completed":108,"skipped":2128,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:46:57.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2903
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 28 00:46:57.558: INFO: The status of Pod annotationupdate73bcb145-b719-4334-b90b-603f3c6e29af is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:46:59.589: INFO: The status of Pod annotationupdate73bcb145-b719-4334-b90b-603f3c6e29af is Running (Ready = true)
Jan 28 00:47:00.184: INFO: Successfully updated pod "annotationupdate73bcb145-b719-4334-b90b-603f3c6e29af"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 00:47:02.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2903" for this suite.

• [SLOW TEST:5.053 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":356,"completed":109,"skipped":2141,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:02.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1471
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name projected-secret-test-318b0b78-6f1d-4788-a09d-95c31c2a8e6e
STEP: Creating a pod to test consume secrets
Jan 28 00:47:02.634: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d" in namespace "projected-1471" to be "Succeeded or Failed"
Jan 28 00:47:02.657: INFO: Pod "pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d": Phase="Pending", Reason="", readiness=false. Elapsed: 23.099983ms
Jan 28 00:47:04.687: INFO: Pod "pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053407911s
Jan 28 00:47:06.719: INFO: Pod "pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.085363448s
STEP: Saw pod success
Jan 28 00:47:06.719: INFO: Pod "pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d" satisfied condition "Succeeded or Failed"
Jan 28 00:47:06.738: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 00:47:06.847: INFO: Waiting for pod pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d to disappear
Jan 28 00:47:06.865: INFO: Pod pod-projected-secrets-28d467fb-7058-45ab-8fe7-b1172621a24d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 00:47:06.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1471" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":110,"skipped":2154,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8263
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-f25ae29f-1545-45d9-a60a-7b86dea5cafd
STEP: Creating a pod to test consume configMaps
Jan 28 00:47:07.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604" in namespace "projected-8263" to be "Succeeded or Failed"
Jan 28 00:47:07.206: INFO: Pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604": Phase="Pending", Reason="", readiness=false. Elapsed: 18.348023ms
Jan 28 00:47:09.239: INFO: Pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051763303s
Jan 28 00:47:11.261: INFO: Pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073982054s
Jan 28 00:47:13.287: INFO: Pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100089299s
STEP: Saw pod success
Jan 28 00:47:13.288: INFO: Pod "pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604" satisfied condition "Succeeded or Failed"
Jan 28 00:47:13.306: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 28 00:47:13.404: INFO: Waiting for pod pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604 to disappear
Jan 28 00:47:13.423: INFO: Pod pod-projected-configmaps-5e91d9ff-9556-47a7-8608-6df429bc8604 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 00:47:13.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8263" for this suite.

• [SLOW TEST:6.567 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":111,"skipped":2198,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:13.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-600
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 28 00:47:14.853: INFO: Pod name wrapped-volume-race-e2b87467-5a15-4551-8cbf-bc6302748b7b: Found 0 pods out of 5
Jan 28 00:47:19.893: INFO: Pod name wrapped-volume-race-e2b87467-5a15-4551-8cbf-bc6302748b7b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e2b87467-5a15-4551-8cbf-bc6302748b7b in namespace emptydir-wrapper-600, will wait for the garbage collector to delete the pods
Jan 28 00:47:20.135: INFO: Deleting ReplicationController wrapped-volume-race-e2b87467-5a15-4551-8cbf-bc6302748b7b took: 64.303146ms
Jan 28 00:47:20.236: INFO: Terminating ReplicationController wrapped-volume-race-e2b87467-5a15-4551-8cbf-bc6302748b7b pods took: 100.65414ms
STEP: Creating RC which spawns configmap-volume pods
Jan 28 00:47:22.823: INFO: Pod name wrapped-volume-race-1618096e-fc66-4260-a6ca-9a1b3348ae0c: Found 0 pods out of 5
Jan 28 00:47:27.872: INFO: Pod name wrapped-volume-race-1618096e-fc66-4260-a6ca-9a1b3348ae0c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1618096e-fc66-4260-a6ca-9a1b3348ae0c in namespace emptydir-wrapper-600, will wait for the garbage collector to delete the pods
Jan 28 00:47:28.064: INFO: Deleting ReplicationController wrapped-volume-race-1618096e-fc66-4260-a6ca-9a1b3348ae0c took: 31.546505ms
Jan 28 00:47:28.164: INFO: Terminating ReplicationController wrapped-volume-race-1618096e-fc66-4260-a6ca-9a1b3348ae0c pods took: 100.108369ms
STEP: Creating RC which spawns configmap-volume pods
Jan 28 00:47:31.047: INFO: Pod name wrapped-volume-race-64ba9766-e3c7-4dd5-82eb-d1bad5857182: Found 0 pods out of 5
Jan 28 00:47:36.099: INFO: Pod name wrapped-volume-race-64ba9766-e3c7-4dd5-82eb-d1bad5857182: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-64ba9766-e3c7-4dd5-82eb-d1bad5857182 in namespace emptydir-wrapper-600, will wait for the garbage collector to delete the pods
Jan 28 00:47:36.295: INFO: Deleting ReplicationController wrapped-volume-race-64ba9766-e3c7-4dd5-82eb-d1bad5857182 took: 36.555066ms
Jan 28 00:47:36.396: INFO: Terminating ReplicationController wrapped-volume-race-64ba9766-e3c7-4dd5-82eb-d1bad5857182 pods took: 100.725279ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 28 00:47:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-600" for this suite.

• [SLOW TEST:27.607 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":356,"completed":112,"skipped":2210,"failed":0}
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:41.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9046
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:47:41.345: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1" in namespace "security-context-test-9046" to be "Succeeded or Failed"
Jan 28 00:47:41.365: INFO: Pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.035074ms
Jan 28 00:47:43.389: INFO: Pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043499958s
Jan 28 00:47:45.418: INFO: Pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072639611s
Jan 28 00:47:47.449: INFO: Pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103704142s
Jan 28 00:47:47.449: INFO: Pod "busybox-user-65534-c5173779-f65a-4669-b81c-0464b488d6f1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 00:47:47.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9046" for this suite.

• [SLOW TEST:6.417 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:52
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":113,"skipped":2210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:47.508: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3909
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:47:47.834: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-34dcaae1-14f4-4b71-b707-eaa7baf9475d" in namespace "security-context-test-3909" to be "Succeeded or Failed"
Jan 28 00:47:47.852: INFO: Pod "busybox-readonly-false-34dcaae1-14f4-4b71-b707-eaa7baf9475d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.969516ms
Jan 28 00:47:49.874: INFO: Pod "busybox-readonly-false-34dcaae1-14f4-4b71-b707-eaa7baf9475d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040886927s
Jan 28 00:47:51.931: INFO: Pod "busybox-readonly-false-34dcaae1-14f4-4b71-b707-eaa7baf9475d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097140965s
Jan 28 00:47:51.931: INFO: Pod "busybox-readonly-false-34dcaae1-14f4-4b71-b707-eaa7baf9475d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 00:47:51.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3909" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":356,"completed":114,"skipped":2266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:51.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1485
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 28 00:47:52.256: INFO: Waiting up to 5m0s for pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22" in namespace "downward-api-1485" to be "Succeeded or Failed"
Jan 28 00:47:52.288: INFO: Pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22": Phase="Pending", Reason="", readiness=false. Elapsed: 31.756716ms
Jan 28 00:47:54.314: INFO: Pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058494824s
Jan 28 00:47:56.346: INFO: Pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090354284s
Jan 28 00:47:58.369: INFO: Pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.113189493s
STEP: Saw pod success
Jan 28 00:47:58.369: INFO: Pod "downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22" satisfied condition "Succeeded or Failed"
Jan 28 00:47:58.386: INFO: Trying to get logs from node 10.187.128.43 pod downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22 container dapi-container: <nil>
STEP: delete the pod
Jan 28 00:47:58.541: INFO: Waiting for pod downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22 to disappear
Jan 28 00:47:58.558: INFO: Pod downward-api-eefb6457-e75b-468e-94bf-f5613ea80e22 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 28 00:47:58.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1485" for this suite.

• [SLOW TEST:6.642 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":356,"completed":115,"skipped":2296,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:47:58.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7805
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:47:58.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df" in namespace "downward-api-7805" to be "Succeeded or Failed"
Jan 28 00:47:58.920: INFO: Pod "downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df": Phase="Pending", Reason="", readiness=false. Elapsed: 18.809412ms
Jan 28 00:48:00.944: INFO: Pod "downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042868282s
Jan 28 00:48:02.967: INFO: Pod "downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065033029s
STEP: Saw pod success
Jan 28 00:48:02.967: INFO: Pod "downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df" satisfied condition "Succeeded or Failed"
Jan 28 00:48:03.009: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df container client-container: <nil>
STEP: delete the pod
Jan 28 00:48:03.117: INFO: Waiting for pod downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df to disappear
Jan 28 00:48:03.134: INFO: Pod downwardapi-volume-790f1d2e-89fa-4cfd-b4f9-6c745b9ec1df no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:48:03.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7805" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":116,"skipped":2312,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:48:03.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3603
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's command
Jan 28 00:48:03.450: INFO: Waiting up to 5m0s for pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711" in namespace "var-expansion-3603" to be "Succeeded or Failed"
Jan 28 00:48:03.471: INFO: Pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711": Phase="Pending", Reason="", readiness=false. Elapsed: 20.642525ms
Jan 28 00:48:05.501: INFO: Pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050875802s
Jan 28 00:48:07.530: INFO: Pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079949795s
Jan 28 00:48:09.559: INFO: Pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108839502s
STEP: Saw pod success
Jan 28 00:48:09.559: INFO: Pod "var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711" satisfied condition "Succeeded or Failed"
Jan 28 00:48:09.595: INFO: Trying to get logs from node 10.187.128.43 pod var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711 container dapi-container: <nil>
STEP: delete the pod
Jan 28 00:48:09.710: INFO: Waiting for pod var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711 to disappear
Jan 28 00:48:09.725: INFO: Pod var-expansion-cf69d5a6-5012-48ad-89cc-967efb50c711 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 00:48:09.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3603" for this suite.

• [SLOW TEST:6.593 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":356,"completed":117,"skipped":2333,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:48:09.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3849
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0128 00:48:50.266532      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 00:48:50.266: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 28 00:48:50.266: INFO: Deleting pod "simpletest.rc-228t4" in namespace "gc-3849"
Jan 28 00:48:50.311: INFO: Deleting pod "simpletest.rc-2sw9z" in namespace "gc-3849"
Jan 28 00:48:50.362: INFO: Deleting pod "simpletest.rc-2wxkv" in namespace "gc-3849"
Jan 28 00:48:50.410: INFO: Deleting pod "simpletest.rc-2x2mb" in namespace "gc-3849"
Jan 28 00:48:50.454: INFO: Deleting pod "simpletest.rc-4hx5j" in namespace "gc-3849"
Jan 28 00:48:50.501: INFO: Deleting pod "simpletest.rc-5hcm9" in namespace "gc-3849"
Jan 28 00:48:50.595: INFO: Deleting pod "simpletest.rc-5lk8l" in namespace "gc-3849"
Jan 28 00:48:50.647: INFO: Deleting pod "simpletest.rc-5n89d" in namespace "gc-3849"
Jan 28 00:48:50.702: INFO: Deleting pod "simpletest.rc-65cb8" in namespace "gc-3849"
Jan 28 00:48:50.762: INFO: Deleting pod "simpletest.rc-6j9tg" in namespace "gc-3849"
Jan 28 00:48:50.802: INFO: Deleting pod "simpletest.rc-6lcnh" in namespace "gc-3849"
Jan 28 00:48:50.862: INFO: Deleting pod "simpletest.rc-75cll" in namespace "gc-3849"
Jan 28 00:48:50.914: INFO: Deleting pod "simpletest.rc-7n5bf" in namespace "gc-3849"
Jan 28 00:48:50.954: INFO: Deleting pod "simpletest.rc-7qwms" in namespace "gc-3849"
Jan 28 00:48:51.070: INFO: Deleting pod "simpletest.rc-7s6fq" in namespace "gc-3849"
Jan 28 00:48:51.171: INFO: Deleting pod "simpletest.rc-7xr75" in namespace "gc-3849"
Jan 28 00:48:51.273: INFO: Deleting pod "simpletest.rc-884xt" in namespace "gc-3849"
Jan 28 00:48:51.336: INFO: Deleting pod "simpletest.rc-8f6sx" in namespace "gc-3849"
Jan 28 00:48:51.398: INFO: Deleting pod "simpletest.rc-8ffg4" in namespace "gc-3849"
Jan 28 00:48:51.460: INFO: Deleting pod "simpletest.rc-8sm9l" in namespace "gc-3849"
Jan 28 00:48:51.525: INFO: Deleting pod "simpletest.rc-98cfh" in namespace "gc-3849"
Jan 28 00:48:51.596: INFO: Deleting pod "simpletest.rc-b42v7" in namespace "gc-3849"
Jan 28 00:48:51.696: INFO: Deleting pod "simpletest.rc-b6l88" in namespace "gc-3849"
Jan 28 00:48:51.777: INFO: Deleting pod "simpletest.rc-bd2bj" in namespace "gc-3849"
Jan 28 00:48:51.837: INFO: Deleting pod "simpletest.rc-bkx8d" in namespace "gc-3849"
Jan 28 00:48:51.908: INFO: Deleting pod "simpletest.rc-c7lc6" in namespace "gc-3849"
Jan 28 00:48:51.977: INFO: Deleting pod "simpletest.rc-cbm8q" in namespace "gc-3849"
Jan 28 00:48:52.043: INFO: Deleting pod "simpletest.rc-cdqc6" in namespace "gc-3849"
Jan 28 00:48:52.092: INFO: Deleting pod "simpletest.rc-d9sg8" in namespace "gc-3849"
Jan 28 00:48:52.141: INFO: Deleting pod "simpletest.rc-dmgcp" in namespace "gc-3849"
Jan 28 00:48:52.212: INFO: Deleting pod "simpletest.rc-fb2b4" in namespace "gc-3849"
Jan 28 00:48:52.272: INFO: Deleting pod "simpletest.rc-fmllf" in namespace "gc-3849"
Jan 28 00:48:52.325: INFO: Deleting pod "simpletest.rc-fvkxb" in namespace "gc-3849"
Jan 28 00:48:52.409: INFO: Deleting pod "simpletest.rc-gdgks" in namespace "gc-3849"
Jan 28 00:48:52.469: INFO: Deleting pod "simpletest.rc-ggl9t" in namespace "gc-3849"
Jan 28 00:48:52.520: INFO: Deleting pod "simpletest.rc-gjf8h" in namespace "gc-3849"
Jan 28 00:48:52.596: INFO: Deleting pod "simpletest.rc-gpg2b" in namespace "gc-3849"
Jan 28 00:48:52.658: INFO: Deleting pod "simpletest.rc-grkqr" in namespace "gc-3849"
Jan 28 00:48:52.718: INFO: Deleting pod "simpletest.rc-grl7n" in namespace "gc-3849"
Jan 28 00:48:52.788: INFO: Deleting pod "simpletest.rc-h8rvw" in namespace "gc-3849"
Jan 28 00:48:52.835: INFO: Deleting pod "simpletest.rc-hf4wv" in namespace "gc-3849"
Jan 28 00:48:52.895: INFO: Deleting pod "simpletest.rc-hndqm" in namespace "gc-3849"
Jan 28 00:48:52.963: INFO: Deleting pod "simpletest.rc-hnh7c" in namespace "gc-3849"
Jan 28 00:48:53.033: INFO: Deleting pod "simpletest.rc-ht6bk" in namespace "gc-3849"
Jan 28 00:48:53.114: INFO: Deleting pod "simpletest.rc-j4hcn" in namespace "gc-3849"
Jan 28 00:48:53.166: INFO: Deleting pod "simpletest.rc-jjqgw" in namespace "gc-3849"
Jan 28 00:48:53.229: INFO: Deleting pod "simpletest.rc-k2rhp" in namespace "gc-3849"
Jan 28 00:48:53.288: INFO: Deleting pod "simpletest.rc-k8l9t" in namespace "gc-3849"
Jan 28 00:48:53.347: INFO: Deleting pod "simpletest.rc-k9g7j" in namespace "gc-3849"
Jan 28 00:48:53.392: INFO: Deleting pod "simpletest.rc-lcjbc" in namespace "gc-3849"
Jan 28 00:48:53.476: INFO: Deleting pod "simpletest.rc-lhb5g" in namespace "gc-3849"
Jan 28 00:48:53.529: INFO: Deleting pod "simpletest.rc-ljqs9" in namespace "gc-3849"
Jan 28 00:48:53.592: INFO: Deleting pod "simpletest.rc-lnk7d" in namespace "gc-3849"
Jan 28 00:48:53.664: INFO: Deleting pod "simpletest.rc-lrds7" in namespace "gc-3849"
Jan 28 00:48:53.730: INFO: Deleting pod "simpletest.rc-mfkjp" in namespace "gc-3849"
Jan 28 00:48:53.798: INFO: Deleting pod "simpletest.rc-mlgxf" in namespace "gc-3849"
Jan 28 00:48:53.856: INFO: Deleting pod "simpletest.rc-mqnms" in namespace "gc-3849"
Jan 28 00:48:53.922: INFO: Deleting pod "simpletest.rc-mv4lt" in namespace "gc-3849"
Jan 28 00:48:53.982: INFO: Deleting pod "simpletest.rc-mv8t7" in namespace "gc-3849"
Jan 28 00:48:54.043: INFO: Deleting pod "simpletest.rc-n2rj5" in namespace "gc-3849"
Jan 28 00:48:54.111: INFO: Deleting pod "simpletest.rc-nkmkk" in namespace "gc-3849"
Jan 28 00:48:54.171: INFO: Deleting pod "simpletest.rc-nlkcn" in namespace "gc-3849"
Jan 28 00:48:54.231: INFO: Deleting pod "simpletest.rc-nxrbx" in namespace "gc-3849"
Jan 28 00:48:54.290: INFO: Deleting pod "simpletest.rc-p4qxn" in namespace "gc-3849"
Jan 28 00:48:54.358: INFO: Deleting pod "simpletest.rc-pczkl" in namespace "gc-3849"
Jan 28 00:48:54.440: INFO: Deleting pod "simpletest.rc-phjrz" in namespace "gc-3849"
Jan 28 00:48:54.539: INFO: Deleting pod "simpletest.rc-prvft" in namespace "gc-3849"
Jan 28 00:48:54.618: INFO: Deleting pod "simpletest.rc-q6gf4" in namespace "gc-3849"
Jan 28 00:48:54.667: INFO: Deleting pod "simpletest.rc-qdkhj" in namespace "gc-3849"
Jan 28 00:48:54.732: INFO: Deleting pod "simpletest.rc-qlpfl" in namespace "gc-3849"
Jan 28 00:48:54.802: INFO: Deleting pod "simpletest.rc-rjcxr" in namespace "gc-3849"
Jan 28 00:48:54.868: INFO: Deleting pod "simpletest.rc-rtrbk" in namespace "gc-3849"
Jan 28 00:48:54.934: INFO: Deleting pod "simpletest.rc-s6rg5" in namespace "gc-3849"
Jan 28 00:48:54.993: INFO: Deleting pod "simpletest.rc-s7nrx" in namespace "gc-3849"
Jan 28 00:48:55.077: INFO: Deleting pod "simpletest.rc-s9hj7" in namespace "gc-3849"
Jan 28 00:48:55.148: INFO: Deleting pod "simpletest.rc-sb2wp" in namespace "gc-3849"
Jan 28 00:48:55.213: INFO: Deleting pod "simpletest.rc-shp6m" in namespace "gc-3849"
Jan 28 00:48:55.287: INFO: Deleting pod "simpletest.rc-t49wx" in namespace "gc-3849"
Jan 28 00:48:55.359: INFO: Deleting pod "simpletest.rc-t4gcx" in namespace "gc-3849"
Jan 28 00:48:55.437: INFO: Deleting pod "simpletest.rc-tb76d" in namespace "gc-3849"
Jan 28 00:48:55.515: INFO: Deleting pod "simpletest.rc-thmz5" in namespace "gc-3849"
Jan 28 00:48:55.566: INFO: Deleting pod "simpletest.rc-tsrhs" in namespace "gc-3849"
Jan 28 00:48:55.648: INFO: Deleting pod "simpletest.rc-ttprv" in namespace "gc-3849"
Jan 28 00:48:55.703: INFO: Deleting pod "simpletest.rc-tx7wg" in namespace "gc-3849"
Jan 28 00:48:55.766: INFO: Deleting pod "simpletest.rc-v6mgx" in namespace "gc-3849"
Jan 28 00:48:55.826: INFO: Deleting pod "simpletest.rc-vd2cz" in namespace "gc-3849"
Jan 28 00:48:55.878: INFO: Deleting pod "simpletest.rc-w7mvz" in namespace "gc-3849"
Jan 28 00:48:55.944: INFO: Deleting pod "simpletest.rc-wb9xk" in namespace "gc-3849"
Jan 28 00:48:56.012: INFO: Deleting pod "simpletest.rc-wn95m" in namespace "gc-3849"
Jan 28 00:48:56.066: INFO: Deleting pod "simpletest.rc-wr7z4" in namespace "gc-3849"
Jan 28 00:48:56.124: INFO: Deleting pod "simpletest.rc-wz2fn" in namespace "gc-3849"
Jan 28 00:48:56.232: INFO: Deleting pod "simpletest.rc-xbbpg" in namespace "gc-3849"
Jan 28 00:48:56.297: INFO: Deleting pod "simpletest.rc-xf4zt" in namespace "gc-3849"
Jan 28 00:48:56.366: INFO: Deleting pod "simpletest.rc-xgzk7" in namespace "gc-3849"
Jan 28 00:48:56.432: INFO: Deleting pod "simpletest.rc-xpxzt" in namespace "gc-3849"
Jan 28 00:48:56.504: INFO: Deleting pod "simpletest.rc-xvljb" in namespace "gc-3849"
Jan 28 00:48:56.585: INFO: Deleting pod "simpletest.rc-z5tdp" in namespace "gc-3849"
Jan 28 00:48:56.649: INFO: Deleting pod "simpletest.rc-z7v2w" in namespace "gc-3849"
Jan 28 00:48:56.712: INFO: Deleting pod "simpletest.rc-zptnp" in namespace "gc-3849"
Jan 28 00:48:56.788: INFO: Deleting pod "simpletest.rc-zzmrz" in namespace "gc-3849"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 00:48:56.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3849" for this suite.

• [SLOW TEST:47.120 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":356,"completed":118,"skipped":2338,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:48:56.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6823
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:48:57.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b" in namespace "downward-api-6823" to be "Succeeded or Failed"
Jan 28 00:48:57.191: INFO: Pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.052753ms
Jan 28 00:48:59.221: INFO: Pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046983607s
Jan 28 00:49:01.251: INFO: Pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076799722s
Jan 28 00:49:03.280: INFO: Pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105280159s
STEP: Saw pod success
Jan 28 00:49:03.280: INFO: Pod "downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b" satisfied condition "Succeeded or Failed"
Jan 28 00:49:03.298: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b container client-container: <nil>
STEP: delete the pod
Jan 28 00:49:03.434: INFO: Waiting for pod downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b to disappear
Jan 28 00:49:03.450: INFO: Pod downwardapi-volume-7fa0206b-7e52-43c2-ac7c-09c21ac60a0b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:49:03.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6823" for this suite.

• [SLOW TEST:6.611 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":119,"skipped":2343,"failed":0}
SSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:03.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename hostport
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostport-3075
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jan 28 00:49:03.868: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:05.897: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:07.896: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.187.128.43 on the node which pod1 resides and expect scheduled
Jan 28 00:49:07.950: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:09.982: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.187.128.43 but use UDP protocol on the node which pod2 resides
Jan 28 00:49:10.030: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:12.053: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:14.053: INFO: The status of Pod pod3 is Running (Ready = true)
Jan 28 00:49:14.096: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:49:16.127: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jan 28 00:49:16.151: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.187.128.43 http://127.0.0.1:54323/hostname] Namespace:hostport-3075 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:49:16.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:49:16.151: INFO: ExecWithOptions: Clientset creation
Jan 28 00:49:16.152: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-3075/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.187.128.43+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.187.128.43, port: 54323
Jan 28 00:49:16.359: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.187.128.43:54323/hostname] Namespace:hostport-3075 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:49:16.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:49:16.360: INFO: ExecWithOptions: Clientset creation
Jan 28 00:49:16.360: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-3075/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.187.128.43%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.187.128.43, port: 54323 UDP
Jan 28 00:49:16.575: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.187.128.43 54323] Namespace:hostport-3075 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:49:16.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:49:16.576: INFO: ExecWithOptions: Clientset creation
Jan 28 00:49:16.576: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-3075/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+10.187.128.43+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:188
Jan 28 00:49:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3075" for this suite.

• [SLOW TEST:18.300 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":356,"completed":120,"skipped":2348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:21.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4596
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:49:22.073: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98" in namespace "downward-api-4596" to be "Succeeded or Failed"
Jan 28 00:49:22.092: INFO: Pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98": Phase="Pending", Reason="", readiness=false. Elapsed: 19.580476ms
Jan 28 00:49:24.121: INFO: Pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048081357s
Jan 28 00:49:26.156: INFO: Pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98": Phase="Pending", Reason="", readiness=false. Elapsed: 4.083232331s
Jan 28 00:49:28.186: INFO: Pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.113539922s
STEP: Saw pod success
Jan 28 00:49:28.186: INFO: Pod "downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98" satisfied condition "Succeeded or Failed"
Jan 28 00:49:28.204: INFO: Trying to get logs from node 10.187.128.30 pod downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98 container client-container: <nil>
STEP: delete the pod
Jan 28 00:49:28.439: INFO: Waiting for pod downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98 to disappear
Jan 28 00:49:28.458: INFO: Pod downwardapi-volume-c1ac4707-45d4-4ca5-a8c3-a66de0f3ca98 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:49:28.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4596" for this suite.

• [SLOW TEST:6.689 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":121,"skipped":2399,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:28.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-511
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-511
[It] should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating statefulset ss in namespace statefulset-511
Jan 28 00:49:28.777: INFO: Found 0 stateful pods, waiting for 1
Jan 28 00:49:38.824: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:49:38.968: INFO: Deleting all statefulset in ns statefulset-511
Jan 28 00:49:38.982: INFO: Scaling statefulset ss to 0
Jan 28 00:49:49.073: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:49:49.088: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 00:49:49.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-511" for this suite.

• [SLOW TEST:20.674 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":356,"completed":122,"skipped":2413,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:49.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6966
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-4f9c4165-a14c-422e-a2f5-014d660767d8
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 00:49:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6966" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":123,"skipped":2421,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:53.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6363
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace a pod template [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a pod template
STEP: Replace a pod template
Jan 28 00:49:53.899: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 28 00:49:53.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6363" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","total":356,"completed":124,"skipped":2431,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:53.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5489
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jan 28 00:49:54.244: INFO: observed Pod pod-test in namespace pods-5489 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 28 00:49:54.247: INFO: observed Pod pod-test in namespace pods-5489 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  }]
Jan 28 00:49:54.358: INFO: observed Pod pod-test in namespace pods-5489 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  }]
Jan 28 00:49:55.313: INFO: observed Pod pod-test in namespace pods-5489 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  }]
Jan 28 00:49:56.589: INFO: Found Pod pod-test in namespace pods-5489 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:49:54 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jan 28 00:49:56.811: INFO: observed event type MODIFIED
Jan 28 00:49:58.605: INFO: observed event type MODIFIED
Jan 28 00:49:58.884: INFO: observed event type MODIFIED
Jan 28 00:49:59.609: INFO: observed event type MODIFIED
Jan 28 00:49:59.650: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 00:49:59.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5489" for this suite.

• [SLOW TEST:5.803 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":356,"completed":125,"skipped":2437,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:49:59.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-3205
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 28 00:55:02.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3205" for this suite.

• [SLOW TEST:302.436 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":356,"completed":126,"skipped":2453,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:55:02.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2810
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jan 28 00:55:02.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:55:07.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:55:22.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2810" for this suite.

• [SLOW TEST:20.008 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":356,"completed":127,"skipped":2490,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:55:22.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6528
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 00:55:22.450: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 00:56:22.578: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:56:22.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-1753
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:56:22.884: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jan 28 00:56:22.899: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:188
Jan 28 00:56:22.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1753" for this suite.
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:56:23.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6528" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:61.092 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":356,"completed":128,"skipped":2496,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:56:23.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2211
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:56:23.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13" in namespace "projected-2211" to be "Succeeded or Failed"
Jan 28 00:56:23.528: INFO: Pod "downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13": Phase="Pending", Reason="", readiness=false. Elapsed: 16.070501ms
Jan 28 00:56:25.549: INFO: Pod "downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037587035s
Jan 28 00:56:27.584: INFO: Pod "downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07218843s
STEP: Saw pod success
Jan 28 00:56:27.584: INFO: Pod "downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13" satisfied condition "Succeeded or Failed"
Jan 28 00:56:27.602: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13 container client-container: <nil>
STEP: delete the pod
Jan 28 00:56:27.804: INFO: Waiting for pod downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13 to disappear
Jan 28 00:56:27.824: INFO: Pod downwardapi-volume-cd8afd34-7d46-4369-9735-c1044735ed13 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 00:56:27.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2211" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":129,"skipped":2502,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:56:27.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6743
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6743
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating stateful set ss in namespace statefulset-6743
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6743
Jan 28 00:56:28.162: INFO: Found 0 stateful pods, waiting for 1
Jan 28 00:56:38.187: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 28 00:56:38.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:56:38.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:56:38.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:56:38.530: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:56:38.555: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 28 00:56:48.591: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:56:48.592: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:56:48.664: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 28 00:56:48.664: INFO: ss-0  10.187.128.43  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  }]
Jan 28 00:56:48.664: INFO: 
Jan 28 00:56:48.664: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 28 00:56:49.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9815127s
Jan 28 00:56:50.712: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95563384s
Jan 28 00:56:51.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.933260156s
Jan 28 00:56:52.764: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.907847876s
Jan 28 00:56:53.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.881778114s
Jan 28 00:56:54.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.85713597s
Jan 28 00:56:55.839: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.82642034s
Jan 28 00:56:56.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.806555692s
Jan 28 00:56:57.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 778.90569ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6743
Jan 28 00:56:58.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:56:59.243: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 00:56:59.243: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:56:59.243: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:56:59.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:56:59.577: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 28 00:56:59.577: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:56:59.577: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:56:59.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 00:56:59.897: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 28 00:56:59.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 00:56:59.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 00:56:59.939: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:56:59.939: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 00:56:59.939: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 28 00:56:59.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:57:00.290: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:57:00.290: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:57:00.290: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:57:00.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:57:00.567: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:57:00.567: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:57:00.567: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:57:00.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-6743 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 00:57:00.847: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 00:57:00.847: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 00:57:00.847: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 00:57:00.847: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:57:00.864: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 28 00:57:10.921: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:57:10.921: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:57:10.921: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 00:57:10.999: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 28 00:57:11.000: INFO: ss-0  10.187.128.43  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  }]
Jan 28 00:57:11.000: INFO: ss-1  10.187.128.27  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  }]
Jan 28 00:57:11.000: INFO: ss-2  10.187.128.30  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  }]
Jan 28 00:57:11.000: INFO: 
Jan 28 00:57:11.000: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 28 00:57:12.019: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jan 28 00:57:12.019: INFO: ss-0  10.187.128.43  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:28 +0000 UTC  }]
Jan 28 00:57:12.019: INFO: ss-1  10.187.128.27  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  }]
Jan 28 00:57:12.019: INFO: ss-2  10.187.128.30  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:57:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 00:56:48 +0000 UTC  }]
Jan 28 00:57:12.019: INFO: 
Jan 28 00:57:12.019: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 28 00:57:13.048: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.939060484s
Jan 28 00:57:14.071: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.911082418s
Jan 28 00:57:15.097: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.888018739s
Jan 28 00:57:16.122: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.862088476s
Jan 28 00:57:17.152: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.837023399s
Jan 28 00:57:18.177: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.806768275s
Jan 28 00:57:19.199: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.781634762s
Jan 28 00:57:20.223: INFO: Verifying statefulset ss doesn't scale past 0 for another 760.162736ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6743
Jan 28 00:57:21.254: INFO: Scaling statefulset ss to 0
Jan 28 00:57:21.310: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 00:57:21.323: INFO: Deleting all statefulset in ns statefulset-6743
Jan 28 00:57:21.337: INFO: Scaling statefulset ss to 0
Jan 28 00:57:21.387: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 00:57:21.400: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 00:57:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6743" for this suite.

• [SLOW TEST:53.619 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":356,"completed":130,"skipped":2511,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:57:21.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-346
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 00:57:22.097: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 00:57:24.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 0, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 57, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 0, 57, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 0, 57, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 00:57:27.234: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 00:57:27.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-346" for this suite.
STEP: Destroying namespace "webhook-346-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":356,"completed":131,"skipped":2514,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:57:27.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9217
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 00:57:28.074: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 28 00:57:28.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:57:28.134: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:57:29.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:57:29.185: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 00:57:30.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:57:30.175: INFO: Node 10.187.128.43 is running 0 daemon pod, expected 1
Jan 28 00:57:31.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:57:31.185: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 28 00:57:31.310: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:31.310: INFO: Wrong image for pod: daemon-set-tkwsj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:32.353: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:32.353: INFO: Wrong image for pod: daemon-set-tkwsj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:33.350: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:33.350: INFO: Wrong image for pod: daemon-set-tkwsj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:34.356: INFO: Pod daemon-set-bml62 is not available
Jan 28 00:57:34.356: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:34.356: INFO: Wrong image for pod: daemon-set-tkwsj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:35.357: INFO: Pod daemon-set-bml62 is not available
Jan 28 00:57:35.357: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:35.357: INFO: Wrong image for pod: daemon-set-tkwsj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:36.352: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:37.350: INFO: Pod daemon-set-8n46h is not available
Jan 28 00:57:37.350: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:38.357: INFO: Pod daemon-set-8n46h is not available
Jan 28 00:57:38.357: INFO: Wrong image for pod: daemon-set-dx2js. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jan 28 00:57:40.356: INFO: Pod daemon-set-4x8hf is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 28 00:57:40.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:57:40.420: INFO: Node 10.187.128.30 is running 0 daemon pod, expected 1
Jan 28 00:57:41.467: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 00:57:41.467: INFO: Node 10.187.128.30 is running 0 daemon pod, expected 1
Jan 28 00:57:42.465: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 00:57:42.465: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9217, will wait for the garbage collector to delete the pods
Jan 28 00:57:42.634: INFO: Deleting DaemonSet.extensions daemon-set took: 26.48443ms
Jan 28 00:57:42.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.821139ms
Jan 28 00:57:45.076: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 00:57:45.076: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 00:57:45.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33457"},"items":null}

Jan 28 00:57:45.106: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33457"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 00:57:45.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9217" for this suite.

• [SLOW TEST:17.426 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":356,"completed":132,"skipped":2523,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:57:45.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3177
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name cm-test-opt-del-3dda965c-8e98-4d0b-9f41-e44622f56697
STEP: Creating configMap with name cm-test-opt-upd-460f5ccc-c0d1-4d0d-89bb-97781e78dd79
STEP: Creating the pod
Jan 28 00:57:45.528: INFO: The status of Pod pod-configmaps-5393b1fe-8232-4724-8677-e63c11334d2c is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:57:47.561: INFO: The status of Pod pod-configmaps-5393b1fe-8232-4724-8677-e63c11334d2c is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:57:49.583: INFO: The status of Pod pod-configmaps-5393b1fe-8232-4724-8677-e63c11334d2c is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-3dda965c-8e98-4d0b-9f41-e44622f56697
STEP: Updating configmap cm-test-opt-upd-460f5ccc-c0d1-4d0d-89bb-97781e78dd79
STEP: Creating configMap with name cm-test-opt-create-5396ab4e-4b42-478b-a487-f04e4f71c636
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 00:58:57.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3177" for this suite.

• [SLOW TEST:72.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":133,"skipped":2561,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:58:57.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3474
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 00:58:57.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca" in namespace "downward-api-3474" to be "Succeeded or Failed"
Jan 28 00:58:57.543: INFO: Pod "downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca": Phase="Pending", Reason="", readiness=false. Elapsed: 16.736228ms
Jan 28 00:58:59.575: INFO: Pod "downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048176216s
Jan 28 00:59:01.608: INFO: Pod "downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081821197s
STEP: Saw pod success
Jan 28 00:59:01.609: INFO: Pod "downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca" satisfied condition "Succeeded or Failed"
Jan 28 00:59:01.629: INFO: Trying to get logs from node 10.187.128.30 pod downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca container client-container: <nil>
STEP: delete the pod
Jan 28 00:59:01.831: INFO: Waiting for pod downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca to disappear
Jan 28 00:59:01.850: INFO: Pod downwardapi-volume-078ff3be-cf65-4c3e-adea-c33d5cf432ca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 00:59:01.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3474" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":356,"completed":134,"skipped":2565,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:59:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6301
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 28 00:59:02.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6301" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":356,"completed":135,"skipped":2584,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:59:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2671
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Performing setup for networking test in namespace pod-network-test-2671
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 28 00:59:02.465: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 28 00:59:02.628: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 00:59:04.651: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:06.649: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:08.657: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:10.663: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:12.657: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:14.653: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:16.663: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:18.662: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:20.665: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:22.645: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jan 28 00:59:24.654: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jan 28 00:59:24.690: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jan 28 00:59:24.730: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jan 28 00:59:28.895: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan 28 00:59:28.895: INFO: Going to poll 172.30.102.111 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:59:28.911: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.102.111 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:59:28.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:59:28.911: INFO: ExecWithOptions: Clientset creation
Jan 28 00:59:28.911: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.102.111+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:59:30.111: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 28 00:59:30.111: INFO: Going to poll 172.30.253.215 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:59:30.136: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.253.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:59:30.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:59:30.137: INFO: ExecWithOptions: Clientset creation
Jan 28 00:59:30.137: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.253.215+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:59:31.349: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 28 00:59:31.349: INFO: Going to poll 172.30.90.71 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan 28 00:59:31.365: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.90.71 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 00:59:31.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 00:59:31.366: INFO: ExecWithOptions: Clientset creation
Jan 28 00:59:31.366: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-2671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.90.71+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 00:59:32.550: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:188
Jan 28 00:59:32.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2671" for this suite.

• [SLOW TEST:30.346 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":136,"skipped":2605,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:59:32.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7575
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-45043fb7-6d5a-4987-9ff4-c9d01a4ca2c7
STEP: Creating a pod to test consume configMaps
Jan 28 00:59:32.867: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894" in namespace "projected-7575" to be "Succeeded or Failed"
Jan 28 00:59:32.883: INFO: Pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894": Phase="Pending", Reason="", readiness=false. Elapsed: 16.136022ms
Jan 28 00:59:34.904: INFO: Pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037252133s
Jan 28 00:59:36.922: INFO: Pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055537712s
Jan 28 00:59:38.953: INFO: Pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085895447s
STEP: Saw pod success
Jan 28 00:59:38.953: INFO: Pod "pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894" satisfied condition "Succeeded or Failed"
Jan 28 00:59:38.976: INFO: Trying to get logs from node 10.187.128.27 pod pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 00:59:39.112: INFO: Waiting for pod pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894 to disappear
Jan 28 00:59:39.131: INFO: Pod pod-projected-configmaps-3b169f67-18d9-4e80-b138-a23932969894 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 00:59:39.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7575" for this suite.

• [SLOW TEST:6.568 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":356,"completed":137,"skipped":2619,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 00:59:39.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4972
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-secret-zcb8
STEP: Creating a pod to test atomic-volume-subpath
Jan 28 00:59:39.478: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zcb8" in namespace "subpath-4972" to be "Succeeded or Failed"
Jan 28 00:59:39.502: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.156938ms
Jan 28 00:59:41.531: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.052567795s
Jan 28 00:59:43.563: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 4.084595219s
Jan 28 00:59:45.586: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 6.107757274s
Jan 28 00:59:47.631: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 8.152626322s
Jan 28 00:59:49.662: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 10.183718437s
Jan 28 00:59:51.720: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 12.24205278s
Jan 28 00:59:53.750: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 14.271201255s
Jan 28 00:59:55.777: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 16.298570037s
Jan 28 00:59:57.803: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 18.324933085s
Jan 28 00:59:59.831: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=true. Elapsed: 20.352802253s
Jan 28 01:00:01.868: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Running", Reason="", readiness=false. Elapsed: 22.389973495s
Jan 28 01:00:03.900: INFO: Pod "pod-subpath-test-secret-zcb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.42145725s
STEP: Saw pod success
Jan 28 01:00:03.900: INFO: Pod "pod-subpath-test-secret-zcb8" satisfied condition "Succeeded or Failed"
Jan 28 01:00:03.919: INFO: Trying to get logs from node 10.187.128.43 pod pod-subpath-test-secret-zcb8 container test-container-subpath-secret-zcb8: <nil>
STEP: delete the pod
Jan 28 01:00:04.021: INFO: Waiting for pod pod-subpath-test-secret-zcb8 to disappear
Jan 28 01:00:04.038: INFO: Pod pod-subpath-test-secret-zcb8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-zcb8
Jan 28 01:00:04.038: INFO: Deleting pod "pod-subpath-test-secret-zcb8" in namespace "subpath-4972"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 28 01:00:04.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4972" for this suite.

• [SLOW TEST:24.928 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","total":356,"completed":138,"skipped":2662,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:00:04.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9451
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:00:04.364: INFO: Waiting up to 5m0s for pod "downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3" in namespace "downward-api-9451" to be "Succeeded or Failed"
Jan 28 01:00:04.383: INFO: Pod "downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.362618ms
Jan 28 01:00:06.399: INFO: Pod "downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035032039s
Jan 28 01:00:08.424: INFO: Pod "downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059333973s
STEP: Saw pod success
Jan 28 01:00:08.424: INFO: Pod "downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3" satisfied condition "Succeeded or Failed"
Jan 28 01:00:08.439: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3 container client-container: <nil>
STEP: delete the pod
Jan 28 01:00:08.518: INFO: Waiting for pod downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3 to disappear
Jan 28 01:00:08.534: INFO: Pod downwardapi-volume-729d92c4-62dc-4225-92c1-8b292f9484e3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 01:00:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9451" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":356,"completed":139,"skipped":2678,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:00:08.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-48
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 28 01:00:08.775: INFO: PodSpec: initContainers in spec.initContainers
Jan 28 01:00:50.284: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d964d7b8-e7a7-4cfc-a2c5-fd78c33e49f4", GenerateName:"", Namespace:"init-container-48", SelfLink:"", UID:"ac879299-3c50-4383-8837-9fe3737c0ec4", ResourceVersion:"34049", Generation:0, CreationTimestamp:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"775565814"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"726063025940e63180469c63f4e796dff78aaf1272add17e8bf16d46160bd3e2", "cni.projectcalico.org/podIP":"172.30.90.72/32", "cni.projectcalico.org/podIPs":"172.30.90.72/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002098ee8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 1, 0, 9, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002098f18), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 28, 1, 0, 11, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002098f48), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-l86xr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0039a8640), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l86xr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l86xr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-l86xr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002b657b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.187.128.43", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002c2a7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b65840)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b65860)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002b65868), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002b6586c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0019aec30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.187.128.43", PodIP:"172.30.90.72", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.90.72"}}, StartTime:time.Date(2023, time.January, 28, 1, 0, 8, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c2a8c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002c2a930)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://0deb06b8c48a6e6ff148a2e57d09afd11dc18c43b30d8a9287aeb6450ad51f4c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039a86c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0039a86a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.7", ImageID:"", ContainerID:"", Started:(*bool)(0xc002b658ef)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 01:00:50.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-48" for this suite.

• [SLOW TEST:41.775 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":356,"completed":140,"skipped":2685,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:00:50.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6070
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6070.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6070.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 229.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.229_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6070.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6070.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6070.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6070.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6070.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 229.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.229_udp@PTR;check="$$(dig +tcp +noall +answer +search 229.39.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.39.229_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:00:58.790: INFO: Unable to read wheezy_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:58.813: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:58.836: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:58.860: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:58.973: INFO: Unable to read jessie_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:58.998: INFO: Unable to read jessie_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:59.021: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:59.046: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:00:59.139: INFO: Lookups using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b failed for: [wheezy_udp@dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_udp@dns-test-service.dns-6070.svc.cluster.local jessie_tcp@dns-test-service.dns-6070.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local]

Jan 28 01:01:04.165: INFO: Unable to read wheezy_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.188: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.232: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.354: INFO: Unable to read jessie_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.378: INFO: Unable to read jessie_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.401: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.425: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:04.520: INFO: Lookups using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b failed for: [wheezy_udp@dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_udp@dns-test-service.dns-6070.svc.cluster.local jessie_tcp@dns-test-service.dns-6070.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local]

Jan 28 01:01:09.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.203: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.231: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.255: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.372: INFO: Unable to read jessie_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.393: INFO: Unable to read jessie_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.413: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.437: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:09.532: INFO: Lookups using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b failed for: [wheezy_udp@dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_udp@dns-test-service.dns-6070.svc.cluster.local jessie_tcp@dns-test-service.dns-6070.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local]

Jan 28 01:01:14.167: INFO: Unable to read wheezy_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.189: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.212: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.237: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.357: INFO: Unable to read jessie_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.384: INFO: Unable to read jessie_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.408: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.431: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:14.531: INFO: Lookups using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b failed for: [wheezy_udp@dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_udp@dns-test-service.dns-6070.svc.cluster.local jessie_tcp@dns-test-service.dns-6070.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local]

Jan 28 01:01:19.162: INFO: Unable to read wheezy_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.186: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.232: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.356: INFO: Unable to read jessie_udp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.379: INFO: Unable to read jessie_tcp@dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.404: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.429: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local from pod dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b: the server could not find the requested resource (get pods dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b)
Jan 28 01:01:19.519: INFO: Lookups using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b failed for: [wheezy_udp@dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@dns-test-service.dns-6070.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_udp@dns-test-service.dns-6070.svc.cluster.local jessie_tcp@dns-test-service.dns-6070.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6070.svc.cluster.local]

Jan 28 01:01:24.525: INFO: DNS probes using dns-6070/dns-test-ad769b2e-bcb3-4837-882f-77a6daafc58b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:01:24.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6070" for this suite.

• [SLOW TEST:34.408 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":356,"completed":141,"skipped":2690,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:01:24.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9592
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:01:25.117: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"677aee38-f3f1-4ad1-8dd1-69e96863c232", Controller:(*bool)(0xc003ad8666), BlockOwnerDeletion:(*bool)(0xc003ad8667)}}
Jan 28 01:01:25.220: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1cc1f62d-2c8b-4fec-a861-efef2348bff6", Controller:(*bool)(0xc003ad88ea), BlockOwnerDeletion:(*bool)(0xc003ad88eb)}}
Jan 28 01:01:25.276: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9897b801-b758-4a5f-8270-97d51772d677", Controller:(*bool)(0xc002d25d1e), BlockOwnerDeletion:(*bool)(0xc002d25d1f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:01:30.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9592" for this suite.

• [SLOW TEST:5.625 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":356,"completed":142,"skipped":2704,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:01:30.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9105
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:01:30.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:01:33.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9105" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":356,"completed":143,"skipped":2708,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:01:34.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1460
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:01:34.223: INFO: Creating ReplicaSet my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9
Jan 28 01:01:34.261: INFO: Pod name my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9: Found 0 pods out of 1
Jan 28 01:01:39.290: INFO: Pod name my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9: Found 1 pods out of 1
Jan 28 01:01:39.290: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9" is running
Jan 28 01:01:39.307: INFO: Pod "my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9-zpmw5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:01:34 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:01:36 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:01:36 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:01:34 +0000 UTC Reason: Message:}])
Jan 28 01:01:39.307: INFO: Trying to dial the pod
Jan 28 01:01:44.435: INFO: Controller my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9: Got expected result from replica 1 [my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9-zpmw5]: "my-hostname-basic-e2651cda-d6d1-40ef-9de2-c0c729e5a8a9-zpmw5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 01:01:44.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1460" for this suite.

• [SLOW TEST:10.473 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":144,"skipped":2713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:01:44.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6170
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 01:01:58.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6170" for this suite.

• [SLOW TEST:13.579 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":356,"completed":145,"skipped":2776,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:01:58.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6343
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:01:58.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea" in namespace "projected-6343" to be "Succeeded or Failed"
Jan 28 01:01:58.336: INFO: Pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea": Phase="Pending", Reason="", readiness=false. Elapsed: 17.561953ms
Jan 28 01:02:00.362: INFO: Pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042977361s
Jan 28 01:02:02.422: INFO: Pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.103132045s
Jan 28 01:02:04.451: INFO: Pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.132644001s
STEP: Saw pod success
Jan 28 01:02:04.451: INFO: Pod "downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea" satisfied condition "Succeeded or Failed"
Jan 28 01:02:04.468: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea container client-container: <nil>
STEP: delete the pod
Jan 28 01:02:04.644: INFO: Waiting for pod downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea to disappear
Jan 28 01:02:04.662: INFO: Pod downwardapi-volume-38512ed4-0c03-453b-b651-0c7edc85afea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:02:04.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6343" for this suite.

• [SLOW TEST:6.633 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":146,"skipped":2784,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:04.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3287
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:02:04.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 create -f -'
Jan 28 01:02:05.900: INFO: stderr: ""
Jan 28 01:02:05.900: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 28 01:02:05.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 create -f -'
Jan 28 01:02:06.889: INFO: stderr: ""
Jan 28 01:02:06.889: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 28 01:02:07.920: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:02:07.920: INFO: Found 0 / 1
Jan 28 01:02:08.913: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:02:08.913: INFO: Found 1 / 1
Jan 28 01:02:08.913: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 28 01:02:08.930: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:02:08.930: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 01:02:08.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 describe pod agnhost-primary-zwk29'
Jan 28 01:02:09.068: INFO: stderr: ""
Jan 28 01:02:09.068: INFO: stdout: "Name:         agnhost-primary-zwk29\nNamespace:    kubectl-3287\nPriority:     0\nNode:         10.187.128.43/10.187.128.43\nStart Time:   Sat, 28 Jan 2023 01:02:05 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 7f858f252eb855cf1835ef1425e90a0e55e5e4ccc3ac6b8514ff16671f7bce33\n              cni.projectcalico.org/podIP: 172.30.90.84/32\n              cni.projectcalico.org/podIPs: 172.30.90.84/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.30.90.84\nIPs:\n  IP:           172.30.90.84\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://5664012544c14e96c71d52452cf2fb6aecb6fc4863f69207a3b1c65895420d63\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 28 Jan 2023 01:02:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hjzgf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-hjzgf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 600s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 600s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-3287/agnhost-primary-zwk29 to 10.187.128.43\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 28 01:02:09.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 describe rc agnhost-primary'
Jan 28 01:02:09.201: INFO: stderr: ""
Jan 28 01:02:09.201: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3287\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-zwk29\n"
Jan 28 01:02:09.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 describe service agnhost-primary'
Jan 28 01:02:09.324: INFO: stderr: ""
Jan 28 01:02:09.324: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3287\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.131.59\nIPs:               172.21.131.59\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.90.84:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 28 01:02:09.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 describe node 10.187.128.27'
Jan 28 01:02:09.663: INFO: stderr: ""
Jan 28 01:02:09.663: INFO: stdout: "Name:               10.187.128.27\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal13\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.62.189.99\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.187.128.27\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cfa4d0ad0pkltdfhcsmg-kubee2epvgz-default-0000019f\n                    ibm-cloud.kubernetes.io/worker-pool-id=cfa4d0ad0pkltdfhcsmg-c3a2726\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=1.24.9_1550\n                    ibm-cloud.kubernetes.io/zone=dal13\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.187.128.27\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=3095546\n                    publicVLAN=3094912\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal13\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.187.128.27/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.102.64\nCreationTimestamp:  Fri, 27 Jan 2023 22:02:34 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.187.128.27\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 28 Jan 2023 01:02:08 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 27 Jan 2023 22:02:56 +0000   Fri, 27 Jan 2023 22:02:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 28 Jan 2023 01:01:31 +0000   Fri, 27 Jan 2023 22:02:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 28 Jan 2023 01:01:31 +0000   Fri, 27 Jan 2023 22:02:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 28 Jan 2023 01:01:31 +0000   Fri, 27 Jan 2023 22:02:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 28 Jan 2023 01:01:31 +0000   Fri, 27 Jan 2023 22:02:55 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.187.128.27\n  ExternalIP:  169.62.189.99\n  Hostname:    10.187.128.27\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102624184Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16212380Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93927226085\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13440412Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 98f27d02d8d84470b24ca5c5efbc7f86\n  System UUID:                9841F74C-5757-37E4-9FAA-DAF4AB5A5746\n  Boot ID:                    04f8a5de-96c0-4b99-9be9-644a0647cf66\n  Kernel Version:             4.15.0-202-generic\n  OS Image:                   Ubuntu 18.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.24.9+IKS\n  Kube-Proxy Version:         v1.24.9+IKS\nProviderID:                   ibm://fee034388aa6435883a1f720010ab3a2///cfa4d0ad0pkltdfhcsmg/kube-cfa4d0ad0pkltdfhcsmg-kubee2epvgz-default-0000019f\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  ibm-system                  ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         138m\n  kube-system                 calico-node-9md4k                                          250m (6%)     0 (0%)      90Mi (0%)        0 (0%)         179m\n  kube-system                 calico-typha-987c59c59-r2mxh                               250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h4m\n  kube-system                 coredns-649f45bb5-4l67p                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     144m\n  kube-system                 dashboard-metrics-scraper-f74668d5f-bscb6                  1m (0%)       0 (0%)      10Mi (0%)        0 (0%)         26m\n  kube-system                 ibm-keepalived-watcher-qxwck                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         179m\n  kube-system                 ibm-master-proxy-static-10.187.128.27                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      179m\n  kube-system                 ibmcloud-block-storage-driver-mfqhd                        50m (1%)      300m (7%)   100Mi (0%)       300Mi (2%)     179m\n  kube-system                 ingress-cluster-healthcheck-56756684f7-qbpsl               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         138m\n  kube-system                 konnectivity-agent-8ws48                                   10m (0%)      0 (0%)      10Mi (0%)        500Mi (3%)     144m\n  kube-system                 metrics-server-666774474b-hfjbk                            126m (3%)     266m (6%)   191Mi (1%)       536Mi (4%)     117m\n  kube-system                 public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8        20m (0%)      0 (0%)      115Mi (0%)       0 (0%)         137m\n  sonobuoy                    sonobuoy-e2e-job-68fa4b4566bd42ca                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                852m (21%)     866m (22%)\n  memory             784914Ki (5%)  2277664Ki (16%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
Jan 28 01:02:09.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3287 describe namespace kubectl-3287'
Jan 28 01:02:09.798: INFO: stderr: ""
Jan 28 01:02:09.798: INFO: stdout: "Name:         kubectl-3287\nLabels:       e2e-framework=kubectl\n              e2e-run=5914f3c0-36a5-47cc-acf4-af23a2cbf1e8\n              kubernetes.io/metadata.name=kubectl-3287\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:02:09.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3287" for this suite.

• [SLOW TEST:5.135 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1110
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":356,"completed":147,"skipped":2803,"failed":0}
[sig-node] Lease 
  lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:09.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-5770
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:188
Jan 28 01:02:10.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5770" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":356,"completed":148,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:10.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6105
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of events
Jan 28 01:02:10.656: INFO: created test-event-1
Jan 28 01:02:10.676: INFO: created test-event-2
Jan 28 01:02:10.699: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jan 28 01:02:10.717: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jan 28 01:02:10.853: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:188
Jan 28 01:02:10.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6105" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":356,"completed":149,"skipped":2820,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:10.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8075
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0128 01:02:17.326069      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:02:17.326: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:02:17.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8075" for this suite.

• [SLOW TEST:6.476 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":356,"completed":150,"skipped":2838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:17.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2674
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-2674
STEP: creating service affinity-clusterip in namespace services-2674
STEP: creating replication controller affinity-clusterip in namespace services-2674
I0128 01:02:17.672991      26 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2674, replica count: 3
I0128 01:02:20.724312      26 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 01:02:23.724499      26 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0128 01:02:26.725548      26 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:02:26.772: INFO: Creating new exec pod
Jan 28 01:02:31.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2674 exec execpod-affinitycxhc8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 28 01:02:32.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 28 01:02:32.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:02:32.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2674 exec execpod-affinitycxhc8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.244.52 80'
Jan 28 01:02:32.470: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.244.52 80\nConnection to 172.21.244.52 80 port [tcp/http] succeeded!\n"
Jan 28 01:02:32.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:02:32.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2674 exec execpod-affinitycxhc8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.244.52:80/ ; done'
Jan 28 01:02:32.856: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.244.52:80/\n"
Jan 28 01:02:32.856: INFO: stdout: "\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc\naffinity-clusterip-hvgxc"
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Received response from host: affinity-clusterip-hvgxc
Jan 28 01:02:32.856: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2674, will wait for the garbage collector to delete the pods
Jan 28 01:02:33.010: INFO: Deleting ReplicationController affinity-clusterip took: 26.263138ms
Jan 28 01:02:33.111: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.138849ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:02:35.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2674" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:18.658 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":151,"skipped":2862,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:36.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6524
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:02:36.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:02:37.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6524" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":356,"completed":152,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:02:37.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4859
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-8edc092f-1057-43b9-98fb-4db8f626e0eb in namespace container-probe-4859
Jan 28 01:02:39.506: INFO: Started pod busybox-8edc092f-1057-43b9-98fb-4db8f626e0eb in namespace container-probe-4859
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 01:02:39.551: INFO: Initial restart count of pod busybox-8edc092f-1057-43b9-98fb-4db8f626e0eb is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:06:41.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4859" for this suite.

• [SLOW TEST:244.094 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":153,"skipped":2882,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:06:41.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4295
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 28 01:07:13.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4295" for this suite.

• [SLOW TEST:32.769 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":356,"completed":154,"skipped":2895,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:07:13.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1574
[It] should update a single-container pod's image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 28 01:07:14.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3593 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 28 01:07:14.273: INFO: stderr: ""
Jan 28 01:07:14.273: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jan 28 01:07:19.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3593 get pod e2e-test-httpd-pod -o json'
Jan 28 01:07:19.396: INFO: stderr: ""
Jan 28 01:07:19.396: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"3e31927d01db58f89f72914063d7141b31389159832ed1eaafc3346447d45289\",\n            \"cni.projectcalico.org/podIP\": \"172.30.90.66/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.90.66/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2023-01-28T01:07:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3593\",\n        \"resourceVersion\": \"36491\",\n        \"uid\": \"5bc7a89d-2251-4d08-b432-47c027041ee4\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-7x6vn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.187.128.43\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 600\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-7x6vn\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T01:07:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T01:07:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T01:07:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-28T01:07:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://16b8617a7055dd33d4ef8465c66f4127bd39b3b1e1152765faadcd7dcfc231c3\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-28T01:07:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.187.128.43\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.90.66\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.90.66\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-28T01:07:14Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 28 01:07:19.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3593 replace -f -'
Jan 28 01:07:19.630: INFO: stderr: ""
Jan 28 01:07:19.631: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1578
Jan 28 01:07:19.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-3593 delete pods e2e-test-httpd-pod'
Jan 28 01:07:21.752: INFO: stderr: ""
Jan 28 01:07:21.752: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:07:21.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3593" for this suite.

• [SLOW TEST:7.857 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1571
    should update a single-container pod's image  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":356,"completed":155,"skipped":2896,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:07:21.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1779
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 01:07:38.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1779" for this suite.

• [SLOW TEST:16.745 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":356,"completed":156,"skipped":2932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:07:38.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7476
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 28 01:07:38.828: INFO: Waiting up to 5m0s for pod "downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51" in namespace "downward-api-7476" to be "Succeeded or Failed"
Jan 28 01:07:38.847: INFO: Pod "downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51": Phase="Pending", Reason="", readiness=false. Elapsed: 18.759365ms
Jan 28 01:07:40.872: INFO: Pod "downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043368106s
Jan 28 01:07:42.899: INFO: Pod "downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070409689s
STEP: Saw pod success
Jan 28 01:07:42.899: INFO: Pod "downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51" satisfied condition "Succeeded or Failed"
Jan 28 01:07:42.923: INFO: Trying to get logs from node 10.187.128.43 pod downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51 container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:07:43.098: INFO: Waiting for pod downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51 to disappear
Jan 28 01:07:43.117: INFO: Pod downward-api-1abed19c-2894-4668-b9ab-4ca4f162db51 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 28 01:07:43.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7476" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":356,"completed":157,"skipped":2955,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:07:43.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1389
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ccde3aa1-548d-4b70-9363-8d47e1527a33
STEP: Creating the pod
Jan 28 01:07:43.478: INFO: The status of Pod pod-projected-configmaps-d2665d81-30e1-49a2-a43a-82769759a641 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:07:45.511: INFO: The status of Pod pod-projected-configmaps-d2665d81-30e1-49a2-a43a-82769759a641 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:07:47.510: INFO: The status of Pod pod-projected-configmaps-d2665d81-30e1-49a2-a43a-82769759a641 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-ccde3aa1-548d-4b70-9363-8d47e1527a33
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 01:08:56.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1389" for this suite.

• [SLOW TEST:73.699 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":158,"skipped":2966,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:08:56.875: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7229
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:08:57.518: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:08:59.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 8, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 8, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 8, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:09:02.716: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:09:15.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7229" for this suite.
STEP: Destroying namespace "webhook-7229-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:18.627 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":356,"completed":159,"skipped":2978,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:15.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4208
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:09:15.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:09:22.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4208" for this suite.

• [SLOW TEST:7.161 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":356,"completed":160,"skipped":3023,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:22.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7345
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:09:23.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7345" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":356,"completed":161,"skipped":3026,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:23.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9703
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-map-ed95daea-092a-4577-b274-b17d67806902
STEP: Creating a pod to test consume configMaps
Jan 28 01:09:23.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241" in namespace "projected-9703" to be "Succeeded or Failed"
Jan 28 01:09:23.372: INFO: Pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241": Phase="Pending", Reason="", readiness=false. Elapsed: 26.400248ms
Jan 28 01:09:25.398: INFO: Pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051955337s
Jan 28 01:09:27.425: INFO: Pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079421031s
Jan 28 01:09:29.458: INFO: Pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112181032s
STEP: Saw pod success
Jan 28 01:09:29.458: INFO: Pod "pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241" satisfied condition "Succeeded or Failed"
Jan 28 01:09:29.475: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:09:29.556: INFO: Waiting for pod pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241 to disappear
Jan 28 01:09:29.575: INFO: Pod pod-projected-configmaps-2e8875ae-f7e9-4a22-9b61-ce6c4cf84241 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 01:09:29.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9703" for this suite.

• [SLOW TEST:6.567 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":162,"skipped":3039,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:29.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4500
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating secret secrets-4500/secret-test-97b5a23f-a1a4-4570-934f-04f637ce88a4
STEP: Creating a pod to test consume secrets
Jan 28 01:09:29.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3" in namespace "secrets-4500" to be "Succeeded or Failed"
Jan 28 01:09:29.901: INFO: Pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 17.28312ms
Jan 28 01:09:31.932: INFO: Pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047861512s
Jan 28 01:09:33.956: INFO: Pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071992966s
Jan 28 01:09:35.983: INFO: Pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.099031904s
STEP: Saw pod success
Jan 28 01:09:35.983: INFO: Pod "pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3" satisfied condition "Succeeded or Failed"
Jan 28 01:09:36.000: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3 container env-test: <nil>
STEP: delete the pod
Jan 28 01:09:36.098: INFO: Waiting for pod pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3 to disappear
Jan 28 01:09:36.115: INFO: Pod pod-configmaps-c22700b7-ccf4-4dc2-abe3-82e65ba0f8e3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:09:36.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4500" for this suite.

• [SLOW TEST:6.537 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":163,"skipped":3058,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:36.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9624
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 28 01:09:36.447: INFO: Waiting up to 5m0s for pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f" in namespace "emptydir-9624" to be "Succeeded or Failed"
Jan 28 01:09:36.464: INFO: Pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.053276ms
Jan 28 01:09:38.494: INFO: Pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047289691s
Jan 28 01:09:40.521: INFO: Pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073978767s
Jan 28 01:09:42.552: INFO: Pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.104883545s
STEP: Saw pod success
Jan 28 01:09:42.552: INFO: Pod "pod-9b3feeda-be89-4503-87b0-34046862cc8f" satisfied condition "Succeeded or Failed"
Jan 28 01:09:42.569: INFO: Trying to get logs from node 10.187.128.43 pod pod-9b3feeda-be89-4503-87b0-34046862cc8f container test-container: <nil>
STEP: delete the pod
Jan 28 01:09:42.658: INFO: Waiting for pod pod-9b3feeda-be89-4503-87b0-34046862cc8f to disappear
Jan 28 01:09:42.675: INFO: Pod pod-9b3feeda-be89-4503-87b0-34046862cc8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:09:42.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9624" for this suite.

• [SLOW TEST:6.560 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":164,"skipped":3067,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:42.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9172
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:09:43.681: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:09:46.799: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:09:46.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7830-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:09:50.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9172" for this suite.
STEP: Destroying namespace "webhook-9172-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:7.683 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":356,"completed":165,"skipped":3083,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:09:50.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2973
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:09:51.032: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:09:53.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 9, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 9, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 9, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 9, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:09:56.181: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:09:56.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:09:59.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2973" for this suite.
STEP: Destroying namespace "webhook-2973-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.638 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":356,"completed":166,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:10:00.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7340
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:48
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:10:00.285: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff" in namespace "security-context-test-7340" to be "Succeeded or Failed"
Jan 28 01:10:00.302: INFO: Pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff": Phase="Pending", Reason="", readiness=false. Elapsed: 17.286081ms
Jan 28 01:10:02.330: INFO: Pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044760155s
Jan 28 01:10:04.364: INFO: Pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079165352s
Jan 28 01:10:04.364: INFO: Pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff" satisfied condition "Succeeded or Failed"
Jan 28 01:10:04.396: INFO: Got logs for pod "busybox-privileged-false-2fefd9c6-abe7-4845-934e-e8e6c5362dff": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 01:10:04.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7340" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":167,"skipped":3120,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:10:04.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-308
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-308, will wait for the garbage collector to delete the pods
Jan 28 01:10:08.789: INFO: Deleting Job.batch foo took: 26.356378ms
Jan 28 01:10:08.889: INFO: Terminating Job.batch foo pods took: 100.334686ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 28 01:10:40.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-308" for this suite.

• [SLOW TEST:36.322 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":356,"completed":168,"skipped":3128,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:10:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3607
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service nodeport-test with type=NodePort in namespace services-3607
STEP: creating replication controller nodeport-test in namespace services-3607
I0128 01:10:41.080770      26 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-3607, replica count: 2
I0128 01:10:44.133388      26 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:10:44.133: INFO: Creating new exec pod
Jan 28 01:10:49.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 28 01:10:49.560: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 28 01:10:49.560: INFO: stdout: ""
Jan 28 01:10:50.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 28 01:10:50.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 28 01:10:50.891: INFO: stdout: "nodeport-test-wbnwp"
Jan 28 01:10:50.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.141.201 80'
Jan 28 01:10:51.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.141.201 80\nConnection to 172.21.141.201 80 port [tcp/http] succeeded!\n"
Jan 28 01:10:51.176: INFO: stdout: "nodeport-test-q8xfc"
Jan 28 01:10:51.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 31934'
Jan 28 01:10:51.451: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 31934\nConnection to 10.187.128.27 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:51.451: INFO: stdout: ""
Jan 28 01:10:52.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 31934'
Jan 28 01:10:52.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 31934\nConnection to 10.187.128.27 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:52.735: INFO: stdout: ""
Jan 28 01:10:53.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 31934'
Jan 28 01:10:53.749: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 31934\nConnection to 10.187.128.27 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:53.749: INFO: stdout: ""
Jan 28 01:10:54.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 31934'
Jan 28 01:10:54.761: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 31934\nConnection to 10.187.128.27 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:54.761: INFO: stdout: "nodeport-test-q8xfc"
Jan 28 01:10:54.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.30 31934'
Jan 28 01:10:55.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.30 31934\nConnection to 10.187.128.30 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:55.025: INFO: stdout: ""
Jan 28 01:10:56.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3607 exec execpod6cp7p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.30 31934'
Jan 28 01:10:56.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.30 31934\nConnection to 10.187.128.30 31934 port [tcp/*] succeeded!\n"
Jan 28 01:10:56.272: INFO: stdout: "nodeport-test-wbnwp"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:10:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3607" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:15.565 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":356,"completed":169,"skipped":3152,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:10:56.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7648
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:10:56.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1" in namespace "projected-7648" to be "Succeeded or Failed"
Jan 28 01:10:56.620: INFO: Pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.149351ms
Jan 28 01:10:58.642: INFO: Pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1": Phase="Running", Reason="", readiness=true. Elapsed: 2.038951734s
Jan 28 01:11:00.671: INFO: Pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1": Phase="Running", Reason="", readiness=false. Elapsed: 4.068330556s
Jan 28 01:11:02.702: INFO: Pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098642074s
STEP: Saw pod success
Jan 28 01:11:02.702: INFO: Pod "downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1" satisfied condition "Succeeded or Failed"
Jan 28 01:11:02.729: INFO: Trying to get logs from node 10.187.128.27 pod downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1 container client-container: <nil>
STEP: delete the pod
Jan 28 01:11:02.919: INFO: Waiting for pod downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1 to disappear
Jan 28 01:11:02.936: INFO: Pod downwardapi-volume-d30d1654-f727-466f-b2d0-d71713808fb1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:11:02.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7648" for this suite.

• [SLOW TEST:6.646 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":170,"skipped":3193,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:11:02.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4852
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4852 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4852;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4852 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4852;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4852.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4852.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4852.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4852.svc;check="$$(dig +notcp +noall +answer +search 155.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4852 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4852;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4852 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4852;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4852.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4852.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4852.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4852.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4852.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4852.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4852.svc;check="$$(dig +notcp +noall +answer +search 155.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.242.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.242.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:11:07.452: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.474: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.521: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.545: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.570: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.594: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.618: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.734: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.760: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.783: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.806: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.828: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.850: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.873: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:07.987: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc wheezy_udp@_http._tcp.dns-test-service.dns-4852.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc jessie_udp@_http._tcp.dns-test-service.dns-4852.svc jessie_tcp@_http._tcp.dns-test-service.dns-4852.svc]

Jan 28 01:11:13.012: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.039: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.062: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.108: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.289: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.313: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.336: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.367: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.391: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.416: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:13.553: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc]

Jan 28 01:11:18.012: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.036: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.060: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.084: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.107: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.131: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.298: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.320: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.341: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.365: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.389: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.413: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:18.564: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc]

Jan 28 01:11:23.025: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.050: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.072: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.098: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.122: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.150: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.314: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.334: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.354: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.380: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.403: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.426: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:23.562: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc]

Jan 28 01:11:28.012: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.035: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.059: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.085: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.106: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.130: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.304: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.327: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.350: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.373: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.395: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.417: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:28.559: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc]

Jan 28 01:11:33.011: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.031: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.056: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.078: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.101: INFO: Unable to read wheezy_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.124: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.304: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.329: INFO: Unable to read jessie_udp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.351: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852 from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.378: INFO: Unable to read jessie_udp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.401: INFO: Unable to read jessie_tcp@dns-test-service.dns-4852.svc from pod dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818: the server could not find the requested resource (get pods dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818)
Jan 28 01:11:33.546: INFO: Lookups using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4852 wheezy_tcp@dns-test-service.dns-4852 wheezy_udp@dns-test-service.dns-4852.svc wheezy_tcp@dns-test-service.dns-4852.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4852 jessie_tcp@dns-test-service.dns-4852 jessie_udp@dns-test-service.dns-4852.svc jessie_tcp@dns-test-service.dns-4852.svc]

Jan 28 01:11:38.584: INFO: DNS probes using dns-4852/dns-test-be8555a6-fddd-40e0-971f-3c8b0f500818 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:11:38.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4852" for this suite.

• [SLOW TEST:35.852 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":356,"completed":171,"skipped":3278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:11:38.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9962
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:11:39.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 28 01:11:41.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 11, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 11, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 11, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:11:44.652: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:11:44.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4760-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:11:48.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9962" for this suite.
STEP: Destroying namespace "webhook-9962-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:9.658 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":356,"completed":172,"skipped":3303,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:11:48.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4586
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jan 28 01:11:49.520: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0128 01:11:49.520685      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:11:49.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4586" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":356,"completed":173,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:11:49.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1022
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override all
Jan 28 01:11:49.795: INFO: Waiting up to 5m0s for pod "client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035" in namespace "containers-1022" to be "Succeeded or Failed"
Jan 28 01:11:49.812: INFO: Pod "client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035": Phase="Pending", Reason="", readiness=false. Elapsed: 17.028979ms
Jan 28 01:11:51.834: INFO: Pod "client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038602572s
Jan 28 01:11:53.857: INFO: Pod "client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062243454s
STEP: Saw pod success
Jan 28 01:11:53.857: INFO: Pod "client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035" satisfied condition "Succeeded or Failed"
Jan 28 01:11:53.873: INFO: Trying to get logs from node 10.187.128.43 pod client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:11:54.021: INFO: Waiting for pod client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035 to disappear
Jan 28 01:11:54.043: INFO: Pod client-containers-d470fe0e-f6b6-4766-ab39-8552a22f7035 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 28 01:11:54.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1022" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":356,"completed":174,"skipped":3341,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:11:54.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3694
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jan 28 01:12:04.496: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0128 01:12:04.496318      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:12:04.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3694" for this suite.

• [SLOW TEST:10.460 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":356,"completed":175,"skipped":3344,"failed":0}
SSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:04.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-7459
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:04.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption-2
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2-8253
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-7459
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:188
Jan 28 01:12:09.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-8253" for this suite.
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 28 01:12:09.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7459" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":356,"completed":176,"skipped":3349,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:09.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2205
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 28 01:12:09.566: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37955 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:09.566: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37955 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 28 01:12:09.609: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37956 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:09.609: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37956 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 28 01:12:09.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37957 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:09.640: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37957 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 28 01:12:09.666: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37959 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:09.666: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2205  415db0c1-7896-41be-9772-2500eeeeda1c 37959 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 28 01:12:09.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2205  ff2aef75-1a66-42e5-86c5-33486539e7d0 37960 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:09.686: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2205  ff2aef75-1a66-42e5-86c5-33486539e7d0 37960 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 28 01:12:19.722: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2205  ff2aef75-1a66-42e5-86c5-33486539e7d0 38005 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:12:19.722: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2205  ff2aef75-1a66-42e5-86c5-33486539e7d0 38005 0 2023-01-28 01:12:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2023-01-28 01:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 28 01:12:29.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2205" for this suite.

• [SLOW TEST:20.460 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":356,"completed":177,"skipped":3360,"failed":0}
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:29.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8778
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jan 28 01:12:35.204: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:188
Jan 28 01:12:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8778" for this suite.

• [SLOW TEST:5.519 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":356,"completed":178,"skipped":3363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:35.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9224
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a collection of services
Jan 28 01:12:35.504: INFO: Creating e2e-svc-a-pq4rq
Jan 28 01:12:35.543: INFO: Creating e2e-svc-b-w7kpj
Jan 28 01:12:35.598: INFO: Creating e2e-svc-c-gnnqm
STEP: deleting service collection
Jan 28 01:12:35.784: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:12:35.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9224" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":356,"completed":179,"skipped":3390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:12:35.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8151
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8151
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a new StatefulSet
Jan 28 01:12:36.093: INFO: Found 0 stateful pods, waiting for 3
Jan 28 01:12:46.125: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:12:46.125: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:12:46.125: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jan 28 01:12:46.229: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 28 01:12:56.360: INFO: Updating stateful set ss2
Jan 28 01:12:56.392: INFO: Waiting for Pod statefulset-8151/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jan 28 01:13:06.602: INFO: Found 2 stateful pods, waiting for 3
Jan 28 01:13:16.633: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:13:16.633: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:13:16.633: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 28 01:13:16.720: INFO: Updating stateful set ss2
Jan 28 01:13:16.763: INFO: Waiting for Pod statefulset-8151/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jan 28 01:13:26.857: INFO: Updating stateful set ss2
Jan 28 01:13:26.890: INFO: Waiting for StatefulSet statefulset-8151/ss2 to complete update
Jan 28 01:13:26.890: INFO: Waiting for Pod statefulset-8151/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:13:36.922: INFO: Deleting all statefulset in ns statefulset-8151
Jan 28 01:13:36.935: INFO: Scaling statefulset ss2 to 0
Jan 28 01:13:47.007: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:13:47.021: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 01:13:47.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8151" for this suite.

• [SLOW TEST:71.288 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":356,"completed":180,"skipped":3421,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:13:47.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6306
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jan 28 01:13:47.386: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:13:49.412: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:13:51.418: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 28 01:13:52.555: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 01:13:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6306" for this suite.

• [SLOW TEST:5.536 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":356,"completed":181,"skipped":3434,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:13:52.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4453
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 28 01:13:52.976: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:13:54.997: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 28 01:13:55.066: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:13:57.086: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 28 01:13:57.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 28 01:13:57.202: INFO: Pod pod-with-prestop-http-hook still exists
Jan 28 01:13:59.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 28 01:13:59.232: INFO: Pod pod-with-prestop-http-hook still exists
Jan 28 01:14:01.202: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 28 01:14:01.235: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 28 01:14:01.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4453" for this suite.

• [SLOW TEST:8.704 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":356,"completed":182,"skipped":3448,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:14:01.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4061
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:14:01.643: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56" in namespace "projected-4061" to be "Succeeded or Failed"
Jan 28 01:14:01.659: INFO: Pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56": Phase="Pending", Reason="", readiness=false. Elapsed: 15.527702ms
Jan 28 01:14:03.680: INFO: Pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036295413s
Jan 28 01:14:05.709: INFO: Pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066034194s
Jan 28 01:14:07.733: INFO: Pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089835515s
STEP: Saw pod success
Jan 28 01:14:07.733: INFO: Pod "downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56" satisfied condition "Succeeded or Failed"
Jan 28 01:14:07.751: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56 container client-container: <nil>
STEP: delete the pod
Jan 28 01:14:07.911: INFO: Waiting for pod downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56 to disappear
Jan 28 01:14:07.928: INFO: Pod downwardapi-volume-fef3b74b-6e8e-46b6-a351-c79d008baa56 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:14:07.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4061" for this suite.

• [SLOW TEST:6.598 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":356,"completed":183,"skipped":3499,"failed":0}
S
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:14:07.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test env composition
Jan 28 01:14:08.211: INFO: Waiting up to 5m0s for pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365" in namespace "var-expansion-2593" to be "Succeeded or Failed"
Jan 28 01:14:08.229: INFO: Pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365": Phase="Pending", Reason="", readiness=false. Elapsed: 17.674608ms
Jan 28 01:14:10.254: INFO: Pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042494621s
Jan 28 01:14:12.288: INFO: Pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076314496s
Jan 28 01:14:14.329: INFO: Pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.117187346s
STEP: Saw pod success
Jan 28 01:14:14.329: INFO: Pod "var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365" satisfied condition "Succeeded or Failed"
Jan 28 01:14:14.347: INFO: Trying to get logs from node 10.187.128.43 pod var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365 container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:14:14.439: INFO: Waiting for pod var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365 to disappear
Jan 28 01:14:14.457: INFO: Pod var-expansion-cf30d0ce-faf0-4d01-99a3-5d5777762365 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:14:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2593" for this suite.

• [SLOW TEST:6.522 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":356,"completed":184,"skipped":3500,"failed":0}
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:14:14.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7418
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7418
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7418
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7418
Jan 28 01:14:14.781: INFO: Found 0 stateful pods, waiting for 1
Jan 28 01:14:24.802: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 28 01:14:24.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:14:25.101: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:14:25.101: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:14:25.101: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:14:25.125: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 28 01:14:35.153: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:14:35.153: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:14:35.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998638s
Jan 28 01:14:36.255: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.978652018s
Jan 28 01:14:37.274: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.947853897s
Jan 28 01:14:38.299: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.928303122s
Jan 28 01:14:39.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.903180108s
Jan 28 01:14:40.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.882083305s
Jan 28 01:14:41.377: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.854790108s
Jan 28 01:14:42.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.8255969s
Jan 28 01:14:43.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.801188594s
Jan 28 01:14:44.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 777.766297ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7418
Jan 28 01:14:45.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:14:45.746: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:14:45.746: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:14:45.746: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:14:45.765: INFO: Found 1 stateful pods, waiting for 3
Jan 28 01:14:55.793: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:14:55.793: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 28 01:14:55.793: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 28 01:14:55.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:14:56.100: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:14:56.100: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:14:56.100: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:14:56.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:14:56.415: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:14:56.415: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:14:56.415: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:14:56.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 28 01:14:56.731: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 28 01:14:56.731: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 28 01:14:56.731: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 28 01:14:56.731: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:14:56.750: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jan 28 01:15:06.806: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:15:06.806: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:15:06.806: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 28 01:15:06.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999872s
Jan 28 01:15:07.879: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980797203s
Jan 28 01:15:08.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.957495844s
Jan 28 01:15:09.929: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.935076161s
Jan 28 01:15:10.952: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.908347501s
Jan 28 01:15:11.978: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.884919106s
Jan 28 01:15:13.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.858772519s
Jan 28 01:15:14.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.82950783s
Jan 28 01:15:15.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.801899989s
Jan 28 01:15:16.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 772.62097ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7418
Jan 28 01:15:17.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:15:17.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:15:17.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:15:17.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:15:17.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:15:17.665: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:15:17.665: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:15:17.665: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:15:17.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=statefulset-7418 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 28 01:15:17.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 28 01:15:17.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 28 01:15:17.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 28 01:15:17.916: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:15:28.002: INFO: Deleting all statefulset in ns statefulset-7418
Jan 28 01:15:28.017: INFO: Scaling statefulset ss to 0
Jan 28 01:15:28.066: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:15:28.079: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 01:15:28.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7418" for this suite.

• [SLOW TEST:73.675 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":356,"completed":185,"skipped":3500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:15:28.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3996
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Given a Pod with a 'name' label pod-adoption is created
Jan 28 01:15:28.429: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:15:30.467: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:15:32.459: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 28 01:15:33.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3996" for this suite.

• [SLOW TEST:5.416 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":356,"completed":186,"skipped":3524,"failed":0}
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:15:33.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-827
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 28 01:15:33.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:15:33.958: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:15:35.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:15:35.001: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:15:36.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:15:36.002: INFO: Node 10.187.128.30 is running 0 daemon pod, expected 1
Jan 28 01:15:37.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:15:37.013: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
Jan 28 01:15:37.040: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jan 28 01:15:37.067: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jan 28 01:15:37.075: INFO: Observed &DaemonSet event: ADDED
Jan 28 01:15:37.075: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.075: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.075: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.075: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.075: INFO: Found daemon set daemon-set in namespace daemonsets-827 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 01:15:37.075: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jan 28 01:15:37.101: INFO: Observed &DaemonSet event: ADDED
Jan 28 01:15:37.101: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.102: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.102: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.102: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.102: INFO: Observed daemon set daemon-set in namespace daemonsets-827 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 28 01:15:37.102: INFO: Observed &DaemonSet event: MODIFIED
Jan 28 01:15:37.102: INFO: Found daemon set daemon-set in namespace daemonsets-827 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 28 01:15:37.102: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-827, will wait for the garbage collector to delete the pods
Jan 28 01:15:37.203: INFO: Deleting DaemonSet.extensions daemon-set took: 24.192417ms
Jan 28 01:15:37.303: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.173724ms
Jan 28 01:15:40.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:15:40.135: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:15:40.162: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39170"},"items":null}

Jan 28 01:15:40.179: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39170"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:15:40.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-827" for this suite.

• [SLOW TEST:6.696 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":356,"completed":187,"skipped":3530,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:15:40.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8342
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-upd-1920df1e-069c-4001-92b5-f0f0a37d7e59
STEP: Creating the pod
Jan 28 01:15:40.586: INFO: The status of Pod pod-configmaps-6829ebda-738c-4f28-8239-1e37f27cf5c1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:15:42.611: INFO: The status of Pod pod-configmaps-6829ebda-738c-4f28-8239-1e37f27cf5c1 is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-1920df1e-069c-4001-92b5-f0f0a37d7e59
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:15:44.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8342" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":188,"skipped":3548,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:15:44.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-4773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 28 01:15:45.046: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:16:45.186: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create pods that use 4/5 of node resources.
Jan 28 01:16:45.291: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 28 01:16:45.314: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 28 01:16:45.368: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 28 01:16:45.390: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 28 01:16:45.452: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 28 01:16:45.471: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:17:01.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4773" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80

• [SLOW TEST:77.389 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":356,"completed":189,"skipped":3552,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5261
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service endpoint-test2 in namespace services-5261
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5261 to expose endpoints map[]
Jan 28 01:17:02.548: INFO: successfully validated that service endpoint-test2 in namespace services-5261 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5261
Jan 28 01:17:02.642: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:04.675: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5261 to expose endpoints map[pod1:[80]]
Jan 28 01:17:04.741: INFO: successfully validated that service endpoint-test2 in namespace services-5261 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jan 28 01:17:04.741: INFO: Creating new exec pod
Jan 28 01:17:09.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:17:10.101: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:10.101: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:17:10.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.222.211 80'
Jan 28 01:17:10.391: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.222.211 80\nConnection to 172.21.222.211 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:10.391: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5261
Jan 28 01:17:10.436: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:12.471: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:14.467: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5261 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 28 01:17:14.560: INFO: successfully validated that service endpoint-test2 in namespace services-5261 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jan 28 01:17:15.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:17:15.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:15.851: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:17:15.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.222.211 80'
Jan 28 01:17:16.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.222.211 80\nConnection to 172.21.222.211 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:16.144: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5261
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5261 to expose endpoints map[pod2:[80]]
Jan 28 01:17:16.256: INFO: successfully validated that service endpoint-test2 in namespace services-5261 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jan 28 01:17:17.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 28 01:17:17.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:17.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:17:17.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5261 exec execpodqxf7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.222.211 80'
Jan 28 01:17:17.841: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.222.211 80\nConnection to 172.21.222.211 80 port [tcp/http] succeeded!\n"
Jan 28 01:17:17.841: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5261
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5261 to expose endpoints map[]
Jan 28 01:17:17.932: INFO: successfully validated that service endpoint-test2 in namespace services-5261 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:17:18.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5261" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:15.885 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":356,"completed":190,"skipped":3581,"failed":0}
[sig-api-machinery] server version 
  should find the server version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:18.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-5157
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  test/e2e/framework/framework.go:652
STEP: Request ServerVersion
STEP: Confirm major version
Jan 28 01:17:18.276: INFO: Major version: 1
STEP: Confirm minor version
Jan 28 01:17:18.276: INFO: cleanMinorVersion: 24
Jan 28 01:17:18.276: INFO: Minor version: 24
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:188
Jan 28 01:17:18.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-5157" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":356,"completed":191,"skipped":3581,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:18.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3668
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:17:18.584: INFO: Got root ca configmap in namespace "svcaccounts-3668"
Jan 28 01:17:18.614: INFO: Deleted root ca configmap in namespace "svcaccounts-3668"
STEP: waiting for a new root ca configmap created
Jan 28 01:17:19.139: INFO: Recreated root ca configmap in namespace "svcaccounts-3668"
Jan 28 01:17:19.168: INFO: Updated root ca configmap in namespace "svcaccounts-3668"
STEP: waiting for the root ca configmap reconciled
Jan 28 01:17:19.688: INFO: Reconciled root ca configmap in namespace "svcaccounts-3668"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 01:17:19.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3668" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":356,"completed":192,"skipped":3608,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:19.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-8636
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jan 28 01:17:20.045: INFO: pods: 0 < 3
Jan 28 01:17:22.069: INFO: running pods: 1 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 28 01:17:24.500: INFO: running pods: 2 < 3
Jan 28 01:17:26.519: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 28 01:17:28.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8636" for this suite.

• [SLOW TEST:9.016 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":356,"completed":193,"skipped":3618,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:28.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5867
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:17:28.993: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 28 01:17:29.044: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 01:17:34.087: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 28 01:17:34.087: INFO: Creating deployment "test-rolling-update-deployment"
Jan 28 01:17:34.121: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 28 01:17:34.166: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 28 01:17:36.238: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 28 01:17:36.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 34, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 17, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 17, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67c8f74c6c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:17:38.285: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:17:38.338: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5867  e74c9111-bf0f-4d4d-8ffd-3bbc3bcdbfc8 39886 1 2023-01-28 01:17:34 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2023-01-28 01:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444e508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:17:34 +0000 UTC,LastTransitionTime:2023-01-28 01:17:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-67c8f74c6c" has successfully progressed.,LastUpdateTime:2023-01-28 01:17:36 +0000 UTC,LastTransitionTime:2023-01-28 01:17:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:17:38.357: INFO: New ReplicaSet "test-rolling-update-deployment-67c8f74c6c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c  deployment-5867  9668e82e-7d03-4aef-abbc-78ab1a151b8e 39876 1 2023-01-28 01:17:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e74c9111-bf0f-4d4d-8ffd-3bbc3bcdbfc8 0xc00444ea07 0xc00444ea08}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e74c9111-bf0f-4d4d-8ffd-3bbc3bcdbfc8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67c8f74c6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00444eab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:17:38.357: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 28 01:17:38.357: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5867  c2d70fe0-9e94-43ef-a87f-7707dbee58b7 39884 2 2023-01-28 01:17:28 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e74c9111-bf0f-4d4d-8ffd-3bbc3bcdbfc8 0xc00444e8d7 0xc00444e8d8}] []  [{e2e.test Update apps/v1 2023-01-28 01:17:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e74c9111-bf0f-4d4d-8ffd-3bbc3bcdbfc8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:17:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00444e998 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:17:38.386: INFO: Pod "test-rolling-update-deployment-67c8f74c6c-jknsv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-67c8f74c6c-jknsv test-rolling-update-deployment-67c8f74c6c- deployment-5867  6216e34d-473f-4272-8a96-5880cbaaddc3 39875 0 2023-01-28 01:17:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:67c8f74c6c] map[cni.projectcalico.org/containerID:96c3c94dbc0ca2ac7c9eb0931ee3226e93dd49531f940c1eef644a32baabc686 cni.projectcalico.org/podIP:172.30.90.109/32 cni.projectcalico.org/podIPs:172.30.90.109/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-67c8f74c6c 9668e82e-7d03-4aef-abbc-78ab1a151b8e 0xc00444ef37 0xc00444ef38}] []  [{kube-controller-manager Update v1 2023-01-28 01:17:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9668e82e-7d03-4aef-abbc-78ab1a151b8e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:17:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:17:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mzbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mzbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:17:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.109,StartTime:2023-01-28 01:17:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:17:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://bf90becbdf0d7488c00295bb0afd311d762b76f16fa21d42b165d931d26f809e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:17:38.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5867" for this suite.

• [SLOW TEST:9.696 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":194,"skipped":3642,"failed":0}
SSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:38.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2842
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 28 01:17:38.781: INFO: The status of Pod pod-update-2ef2accb-ff08-4c33-ad24-9ddb5541caae is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:17:40.812: INFO: The status of Pod pod-update-2ef2accb-ff08-4c33-ad24-9ddb5541caae is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 28 01:17:41.408: INFO: Successfully updated pod "pod-update-2ef2accb-ff08-4c33-ad24-9ddb5541caae"
STEP: verifying the updated pod is in kubernetes
Jan 28 01:17:41.444: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:17:41.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2842" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":356,"completed":195,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:41.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7521
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:17:41.818: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 28 01:17:46.836: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jan 28 01:17:46.871: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jan 28 01:17:47.003: INFO: observed ReplicaSet test-rs in namespace replicaset-7521 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 01:17:47.006: INFO: observed ReplicaSet test-rs in namespace replicaset-7521 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 01:17:47.007: INFO: observed ReplicaSet test-rs in namespace replicaset-7521 with ReadyReplicas 1, AvailableReplicas 1
Jan 28 01:17:49.373: INFO: observed ReplicaSet test-rs in namespace replicaset-7521 with ReadyReplicas 2, AvailableReplicas 2
Jan 28 01:17:49.548: INFO: observed Replicaset test-rs in namespace replicaset-7521 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 01:17:49.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7521" for this suite.

• [SLOW TEST:8.124 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":356,"completed":196,"skipped":3669,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:17:49.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3952
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 01:18:01.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3952" for this suite.

• [SLOW TEST:11.521 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":356,"completed":197,"skipped":3696,"failed":0}
SSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:18:01.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-7329
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 28 01:18:01.503: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 28 01:18:01.583: INFO: starting watch
STEP: patching
STEP: updating
Jan 28 01:18:01.697: INFO: waiting for watch events with expected annotations
Jan 28 01:18:01.697: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:188
Jan 28 01:18:01.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-7329" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":356,"completed":198,"skipped":3700,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests 
  should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:18:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename conformance-tests
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in conformance-tests-4340
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should have at least two untainted nodes [Conformance]
  test/e2e/framework/framework.go:652
STEP: Getting node addresses
Jan 28 01:18:02.275: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:188
Jan 28 01:18:02.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-4340" for this suite.
•{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","total":356,"completed":199,"skipped":3714,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:18:02.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-7191
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 28 01:20:00.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7191" for this suite.

• [SLOW TEST:118.408 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":356,"completed":200,"skipped":3723,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:20:00.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1492
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-map-61843291-511c-4361-ad5d-4249c6c7b666
STEP: Creating a pod to test consume secrets
Jan 28 01:20:01.096: INFO: Waiting up to 5m0s for pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5" in namespace "secrets-1492" to be "Succeeded or Failed"
Jan 28 01:20:01.115: INFO: Pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.657882ms
Jan 28 01:20:03.159: INFO: Pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062135772s
Jan 28 01:20:05.187: INFO: Pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091043204s
Jan 28 01:20:07.220: INFO: Pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.124026296s
STEP: Saw pod success
Jan 28 01:20:07.221: INFO: Pod "pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5" satisfied condition "Succeeded or Failed"
Jan 28 01:20:07.239: INFO: Trying to get logs from node 10.187.128.30 pod pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5 container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:20:07.424: INFO: Waiting for pod pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5 to disappear
Jan 28 01:20:07.441: INFO: Pod pod-secrets-b5db1861-cc16-42d8-a28b-206b74fff5c5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:20:07.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1492" for this suite.

• [SLOW TEST:6.704 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":201,"skipped":3751,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:20:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3332
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-projected-q2c8
STEP: Creating a pod to test atomic-volume-subpath
Jan 28 01:20:07.798: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q2c8" in namespace "subpath-3332" to be "Succeeded or Failed"
Jan 28 01:20:07.814: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.968434ms
Jan 28 01:20:09.844: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045834337s
Jan 28 01:20:11.871: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 4.072936023s
Jan 28 01:20:13.903: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 6.104583726s
Jan 28 01:20:15.933: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 8.134744348s
Jan 28 01:20:17.970: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 10.171771896s
Jan 28 01:20:20.004: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 12.205610083s
Jan 28 01:20:22.033: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 14.234657094s
Jan 28 01:20:24.062: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 16.263931326s
Jan 28 01:20:26.093: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 18.29483276s
Jan 28 01:20:28.123: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 20.324932738s
Jan 28 01:20:30.155: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=true. Elapsed: 22.356392051s
Jan 28 01:20:32.188: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Running", Reason="", readiness=false. Elapsed: 24.390073579s
Jan 28 01:20:34.220: INFO: Pod "pod-subpath-test-projected-q2c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.422003523s
STEP: Saw pod success
Jan 28 01:20:34.220: INFO: Pod "pod-subpath-test-projected-q2c8" satisfied condition "Succeeded or Failed"
Jan 28 01:20:34.238: INFO: Trying to get logs from node 10.187.128.30 pod pod-subpath-test-projected-q2c8 container test-container-subpath-projected-q2c8: <nil>
STEP: delete the pod
Jan 28 01:20:34.324: INFO: Waiting for pod pod-subpath-test-projected-q2c8 to disappear
Jan 28 01:20:34.341: INFO: Pod pod-subpath-test-projected-q2c8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-q2c8
Jan 28 01:20:34.341: INFO: Deleting pod "pod-subpath-test-projected-q2c8" in namespace "subpath-3332"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 28 01:20:34.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3332" for this suite.

• [SLOW TEST:26.923 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","total":356,"completed":202,"skipped":3753,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:20:34.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5794
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-79714d8b-b783-4660-961a-b2fb3f42f248
STEP: Creating a pod to test consume secrets
Jan 28 01:20:34.708: INFO: Waiting up to 5m0s for pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781" in namespace "secrets-5794" to be "Succeeded or Failed"
Jan 28 01:20:34.726: INFO: Pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781": Phase="Pending", Reason="", readiness=false. Elapsed: 17.497402ms
Jan 28 01:20:36.756: INFO: Pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047395664s
Jan 28 01:20:38.780: INFO: Pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071154203s
Jan 28 01:20:40.812: INFO: Pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.103678122s
STEP: Saw pod success
Jan 28 01:20:40.812: INFO: Pod "pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781" satisfied condition "Succeeded or Failed"
Jan 28 01:20:40.827: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781 container secret-env-test: <nil>
STEP: delete the pod
Jan 28 01:20:41.039: INFO: Waiting for pod pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781 to disappear
Jan 28 01:20:41.057: INFO: Pod pod-secrets-1337ac5c-8264-49ac-80e1-488da11f3781 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:20:41.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5794" for this suite.

• [SLOW TEST:6.690 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":356,"completed":203,"skipped":3766,"failed":0}
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:20:41.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7535
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 28 01:20:41.418: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:20:43.452: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 28 01:20:43.515: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:20:45.549: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jan 28 01:20:45.614: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 01:20:45.632: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 28 01:20:47.632: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 01:20:47.694: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 28 01:20:49.633: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 28 01:20:49.659: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 28 01:20:49.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7535" for this suite.

• [SLOW TEST:8.644 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":356,"completed":204,"skipped":3771,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:20:49.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7504
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request.
Jan 28 01:20:50.082: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:20:52.113: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:20:54.120: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the pod with lifecycle hook
Jan 28 01:20:54.181: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:20:56.208: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 28 01:20:56.298: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 01:20:56.317: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 28 01:20:58.317: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 01:20:58.349: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 28 01:21:00.318: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 28 01:21:00.350: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:188
Jan 28 01:21:00.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7504" for this suite.

• [SLOW TEST:10.660 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":356,"completed":205,"skipped":3803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-879
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service multi-endpoint-test in namespace services-879
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-879 to expose endpoints map[]
Jan 28 01:21:00.729: INFO: successfully validated that service multi-endpoint-test in namespace services-879 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-879
Jan 28 01:21:00.786: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:21:02.815: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:21:04.814: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-879 to expose endpoints map[pod1:[100]]
Jan 28 01:21:04.884: INFO: successfully validated that service multi-endpoint-test in namespace services-879 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-879
Jan 28 01:21:04.938: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:21:06.970: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-879 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 28 01:21:07.066: INFO: successfully validated that service multi-endpoint-test in namespace services-879 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jan 28 01:21:07.066: INFO: Creating new exec pod
Jan 28 01:21:10.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-879 exec execpod45qh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 28 01:21:10.466: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 28 01:21:10.466: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:21:10.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-879 exec execpod45qh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.64.213 80'
Jan 28 01:21:10.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.64.213 80\nConnection to 172.21.64.213 80 port [tcp/http] succeeded!\n"
Jan 28 01:21:10.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:21:10.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-879 exec execpod45qh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 28 01:21:11.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 28 01:21:11.004: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:21:11.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-879 exec execpod45qh8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.64.213 81'
Jan 28 01:21:11.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.64.213 81\nConnection to 172.21.64.213 81 port [tcp/*] succeeded!\n"
Jan 28 01:21:11.323: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-879
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-879 to expose endpoints map[pod2:[101]]
Jan 28 01:21:11.443: INFO: successfully validated that service multi-endpoint-test in namespace services-879 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-879
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-879 to expose endpoints map[]
Jan 28 01:21:11.537: INFO: successfully validated that service multi-endpoint-test in namespace services-879 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:21:11.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-879" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:11.257 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":356,"completed":206,"skipped":3841,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2188
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-c0be1cfa-e5f3-463a-9dff-39d79f3c398c
STEP: Creating a pod to test consume configMaps
Jan 28 01:21:11.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184" in namespace "configmap-2188" to be "Succeeded or Failed"
Jan 28 01:21:11.959: INFO: Pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184": Phase="Pending", Reason="", readiness=false. Elapsed: 16.38158ms
Jan 28 01:21:13.990: INFO: Pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048181342s
Jan 28 01:21:16.022: INFO: Pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079780234s
Jan 28 01:21:18.053: INFO: Pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.110877838s
STEP: Saw pod success
Jan 28 01:21:18.053: INFO: Pod "pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184" satisfied condition "Succeeded or Failed"
Jan 28 01:21:18.075: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 28 01:21:18.167: INFO: Waiting for pod pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184 to disappear
Jan 28 01:21:18.184: INFO: Pod pod-configmaps-b9061ccf-0aa5-429c-83c6-e1f1703e8184 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:21:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2188" for this suite.

• [SLOW TEST:6.577 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":356,"completed":207,"skipped":3865,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:18.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2306
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2306
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2306
I0128 01:21:18.587257      26 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2306, replica count: 2
I0128 01:21:21.638863      26 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:21:21.639: INFO: Creating new exec pod
Jan 28 01:21:26.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2306 exec execpodzdh9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 28 01:21:27.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 28 01:21:27.048: INFO: stdout: "externalname-service-9s44g"
Jan 28 01:21:27.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2306 exec execpodzdh9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.133.232 80'
Jan 28 01:21:27.292: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.133.232 80\nConnection to 172.21.133.232 80 port [tcp/http] succeeded!\n"
Jan 28 01:21:27.292: INFO: stdout: ""
Jan 28 01:21:28.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-2306 exec execpodzdh9m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.133.232 80'
Jan 28 01:21:28.534: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.133.232 80\nConnection to 172.21.133.232 80 port [tcp/http] succeeded!\n"
Jan 28 01:21:28.534: INFO: stdout: "externalname-service-9s44g"
Jan 28 01:21:28.534: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:21:28.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2306" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:10.448 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":356,"completed":208,"skipped":3871,"failed":0}
SSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslicemirroring-2060
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/framework/framework.go:652
STEP: mirroring a new custom Endpoint
Jan 28 01:21:29.027: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
Jan 28 01:21:31.138: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:188
Jan 28 01:21:33.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2060" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":356,"completed":209,"skipped":3876,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:33.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sysctl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sysctl-4503
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 01:21:33.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4503" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":356,"completed":210,"skipped":3898,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:33.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5028
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-931c98d9-1b99-4293-b9d6-6a7f72896241
STEP: Creating a pod to test consume configMaps
Jan 28 01:21:33.820: INFO: Waiting up to 5m0s for pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97" in namespace "configmap-5028" to be "Succeeded or Failed"
Jan 28 01:21:33.836: INFO: Pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97": Phase="Pending", Reason="", readiness=false. Elapsed: 16.195595ms
Jan 28 01:21:35.875: INFO: Pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055286022s
Jan 28 01:21:37.904: INFO: Pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084355196s
Jan 28 01:21:39.941: INFO: Pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.121785112s
STEP: Saw pod success
Jan 28 01:21:39.941: INFO: Pod "pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97" satisfied condition "Succeeded or Failed"
Jan 28 01:21:39.983: INFO: Trying to get logs from node 10.187.128.27 pod pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:21:40.166: INFO: Waiting for pod pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97 to disappear
Jan 28 01:21:40.199: INFO: Pod pod-configmaps-c416ef94-5f90-4e1e-9a60-041f16031f97 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:21:40.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5028" for this suite.

• [SLOW TEST:6.718 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":211,"skipped":3914,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:40.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5586
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 28 01:21:43.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5586" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":356,"completed":212,"skipped":3978,"failed":0}
SS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:43.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2461
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
Jan 28 01:21:43.522: INFO: Creating simple deployment test-deployment-jz5ct
Jan 28 01:21:43.581: INFO: deployment "test-deployment-jz5ct" doesn't have the required revision set
Jan 28 01:21:45.656: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 21, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 21, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 21, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 21, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-jz5ct-688c4d6789\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status
Jan 28 01:21:47.748: INFO: Deployment test-deployment-jz5ct has Conditions: [{Available True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jz5ct-688c4d6789" has successfully progressed.}]
STEP: updating Deployment Status
Jan 28 01:21:47.805: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 21, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 21, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 21, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 21, 43, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-jz5ct-688c4d6789\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jan 28 01:21:47.814: INFO: Observed &Deployment event: ADDED
Jan 28 01:21:47.814: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jz5ct-688c4d6789"}
Jan 28 01:21:47.814: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.814: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jz5ct-688c4d6789"}
Jan 28 01:21:47.814: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:21:47.814: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.815: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:21:47.815: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jz5ct-688c4d6789" is progressing.}
Jan 28 01:21:47.815: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.815: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:21:47.815: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jz5ct-688c4d6789" has successfully progressed.}
Jan 28 01:21:47.816: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.816: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:21:47.816: INFO: Observed Deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jz5ct-688c4d6789" has successfully progressed.}
Jan 28 01:21:47.816: INFO: Found Deployment test-deployment-jz5ct in namespace deployment-2461 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 01:21:47.816: INFO: Deployment test-deployment-jz5ct has an updated status
STEP: patching the Statefulset Status
Jan 28 01:21:47.816: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 28 01:21:47.840: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jan 28 01:21:47.850: INFO: Observed &Deployment event: ADDED
Jan 28 01:21:47.850: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jz5ct-688c4d6789"}
Jan 28 01:21:47.850: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.850: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-jz5ct-688c4d6789"}
Jan 28 01:21:47.850: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:21:47.851: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.851: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 28 01:21:47.851: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:43 +0000 UTC 2023-01-28 01:21:43 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-jz5ct-688c4d6789" is progressing.}
Jan 28 01:21:47.852: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.852: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:21:47.852: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jz5ct-688c4d6789" has successfully progressed.}
Jan 28 01:21:47.852: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.852: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 28 01:21:47.852: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-28 01:21:46 +0000 UTC 2023-01-28 01:21:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-jz5ct-688c4d6789" has successfully progressed.}
Jan 28 01:21:47.853: INFO: Observed deployment test-deployment-jz5ct in namespace deployment-2461 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 28 01:21:47.853: INFO: Observed &Deployment event: MODIFIED
Jan 28 01:21:47.853: INFO: Found deployment test-deployment-jz5ct in namespace deployment-2461 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 28 01:21:47.853: INFO: Deployment test-deployment-jz5ct has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:21:47.870: INFO: Deployment "test-deployment-jz5ct":
&Deployment{ObjectMeta:{test-deployment-jz5ct  deployment-2461  181724fc-ff43-4439-b616-73c570f18a4e 41135 1 2023-01-28 01:21:43 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-28 01:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-28 01:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-28 01:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00399c4b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-jz5ct-688c4d6789",LastUpdateTime:2023-01-28 01:21:47 +0000 UTC,LastTransitionTime:2023-01-28 01:21:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:21:47.891: INFO: New ReplicaSet "test-deployment-jz5ct-688c4d6789" of Deployment "test-deployment-jz5ct":
&ReplicaSet{ObjectMeta:{test-deployment-jz5ct-688c4d6789  deployment-2461  2b3d3484-177e-42fd-94bf-a265cdb7d1e6 41129 1 2023-01-28 01:21:43 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-jz5ct 181724fc-ff43-4439-b616-73c570f18a4e 0xc00399c8a0 0xc00399c8a1}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"181724fc-ff43-4439-b616-73c570f18a4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:21:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 688c4d6789,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00399c948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:21:47.907: INFO: Pod "test-deployment-jz5ct-688c4d6789-78xvn" is available:
&Pod{ObjectMeta:{test-deployment-jz5ct-688c4d6789-78xvn test-deployment-jz5ct-688c4d6789- deployment-2461  ee5e744c-1e6b-4ae5-a898-b7f6c07ea9ac 41128 0 2023-01-28 01:21:43 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:688c4d6789] map[cni.projectcalico.org/containerID:60146a97bcec2ea2866ca5d9e1545dd2916505cde29ba78e53ed63ad91fd7f43 cni.projectcalico.org/podIP:172.30.90.99/32 cni.projectcalico.org/podIPs:172.30.90.99/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-jz5ct-688c4d6789 2b3d3484-177e-42fd-94bf-a265cdb7d1e6 0xc0036b6797 0xc0036b6798}] []  [{kube-controller-manager Update v1 2023-01-28 01:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2b3d3484-177e-42fd-94bf-a265cdb7d1e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:21:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:21:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.99\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8mpwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8mpwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:21:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.99,StartTime:2023-01-28 01:21:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:21:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0b0cb6667b81a2266f2025c19c75352f3e4bc373fdc68bbecad9ef34fe9a6f0d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.99,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:21:47.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2461" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":356,"completed":213,"skipped":3980,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:21:47.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2729
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:21:48.265: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:21:50.291: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:21:52.297: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:21:54.299: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:21:56.295: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:21:58.322: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:00.288: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:02.315: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:04.297: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:06.297: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:08.322: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = false)
Jan 28 01:22:10.300: INFO: The status of Pod test-webserver-d44c5f7e-6755-4bc2-842a-ff39aa1ea01c is Running (Ready = true)
Jan 28 01:22:10.317: INFO: Container started at 2023-01-28 01:21:49 +0000 UTC, pod became ready at 2023-01-28 01:22:08 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:22:10.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2729" for this suite.

• [SLOW TEST:22.413 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":356,"completed":214,"skipped":4026,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:22:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3851
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-820488e7-1d7b-4e46-a4c7-a3b0d354f6fd
STEP: Creating a pod to test consume configMaps
Jan 28 01:22:10.664: INFO: Waiting up to 5m0s for pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02" in namespace "configmap-3851" to be "Succeeded or Failed"
Jan 28 01:22:10.681: INFO: Pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02": Phase="Pending", Reason="", readiness=false. Elapsed: 17.66543ms
Jan 28 01:22:12.712: INFO: Pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047920066s
Jan 28 01:22:14.741: INFO: Pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077532286s
Jan 28 01:22:16.771: INFO: Pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107236962s
STEP: Saw pod success
Jan 28 01:22:16.771: INFO: Pod "pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02" satisfied condition "Succeeded or Failed"
Jan 28 01:22:16.803: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:22:16.888: INFO: Waiting for pod pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02 to disappear
Jan 28 01:22:16.905: INFO: Pod pod-configmaps-64604765-5503-4b42-b6ab-9be613004b02 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:22:16.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3851" for this suite.

• [SLOW TEST:6.578 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":215,"skipped":4039,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:22:16.963: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4046
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 28 01:22:17.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:22:38.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4046" for this suite.

• [SLOW TEST:21.735 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":356,"completed":216,"skipped":4047,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:22:38.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8752
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:22:39.333: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:22:41.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 22, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 22, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 22, 39, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 22, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:22:44.467: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jan 28 01:22:44.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:22:44.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8752" for this suite.
STEP: Destroying namespace "webhook-8752-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.181 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":356,"completed":217,"skipped":4050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:22:44.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2210
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2210.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2210.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2210.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2210.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:22:49.356: INFO: DNS probes using dns-2210/dns-test-f79b0c0c-a46c-4ec9-90ac-d272a0b3e9f6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:22:49.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2210" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","total":356,"completed":218,"skipped":4079,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:22:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-782
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 28 01:23:01.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-782" for this suite.

• [SLOW TEST:12.309 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":356,"completed":219,"skipped":4089,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:23:01.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9227
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating pod
Jan 28 01:23:02.101: INFO: The status of Pod pod-hostip-8f79513f-5dd1-40e0-b8b2-8ae6f92662fc is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:23:04.142: INFO: The status of Pod pod-hostip-8f79513f-5dd1-40e0-b8b2-8ae6f92662fc is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:23:06.131: INFO: The status of Pod pod-hostip-8f79513f-5dd1-40e0-b8b2-8ae6f92662fc is Running (Ready = true)
Jan 28 01:23:06.169: INFO: Pod pod-hostip-8f79513f-5dd1-40e0-b8b2-8ae6f92662fc has hostIP: 10.187.128.43
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:23:06.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9227" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":356,"completed":220,"skipped":4107,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:23:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3084
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-22b4fa36-0abf-4cde-ad14-f94408b46a8f
STEP: Creating secret with name s-test-opt-upd-e70c1562-340e-4732-ac30-8e4a0527eb1a
STEP: Creating the pod
Jan 28 01:23:06.560: INFO: The status of Pod pod-secrets-52eb9c18-a59d-4b57-93ce-9f1bf9c8c68c is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:23:08.584: INFO: The status of Pod pod-secrets-52eb9c18-a59d-4b57-93ce-9f1bf9c8c68c is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-22b4fa36-0abf-4cde-ad14-f94408b46a8f
STEP: Updating secret s-test-opt-upd-e70c1562-340e-4732-ac30-8e4a0527eb1a
STEP: Creating secret with name s-test-opt-create-578b1a8e-a095-44ef-95ca-40a93c0d81f5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:23:10.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3084" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":221,"skipped":4110,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:23:11.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3492
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:23:11.395: INFO: The status of Pod busybox-scheduling-67a2efaa-d3aa-4f74-a0ff-3d87a595ec70 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:23:13.421: INFO: The status of Pod busybox-scheduling-67a2efaa-d3aa-4f74-a0ff-3d87a595ec70 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:23:15.425: INFO: The status of Pod busybox-scheduling-67a2efaa-d3aa-4f74-a0ff-3d87a595ec70 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 28 01:23:15.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3492" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":356,"completed":222,"skipped":4118,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:23:15.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-980
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:24:15.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-980" for this suite.

• [SLOW TEST:60.379 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":356,"completed":223,"skipped":4128,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:24:15.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9054
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 28 01:24:16.243: INFO: Waiting up to 5m0s for pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61" in namespace "emptydir-9054" to be "Succeeded or Failed"
Jan 28 01:24:16.262: INFO: Pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61": Phase="Pending", Reason="", readiness=false. Elapsed: 19.06993ms
Jan 28 01:24:18.284: INFO: Pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61": Phase="Running", Reason="", readiness=true. Elapsed: 2.041143009s
Jan 28 01:24:20.307: INFO: Pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61": Phase="Running", Reason="", readiness=false. Elapsed: 4.063575112s
Jan 28 01:24:22.328: INFO: Pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085435181s
STEP: Saw pod success
Jan 28 01:24:22.329: INFO: Pod "pod-cf283fec-ebe0-4980-8267-deb1f7792c61" satisfied condition "Succeeded or Failed"
Jan 28 01:24:22.344: INFO: Trying to get logs from node 10.187.128.43 pod pod-cf283fec-ebe0-4980-8267-deb1f7792c61 container test-container: <nil>
STEP: delete the pod
Jan 28 01:24:22.437: INFO: Waiting for pod pod-cf283fec-ebe0-4980-8267-deb1f7792c61 to disappear
Jan 28 01:24:22.453: INFO: Pod pod-cf283fec-ebe0-4980-8267-deb1f7792c61 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:24:22.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9054" for this suite.

• [SLOW TEST:6.541 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":224,"skipped":4134,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:24:22.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1280
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0128 01:24:23.627210      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:24:23.627: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:24:23.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1280" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":356,"completed":225,"skipped":4134,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:24:23.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-413
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jan 28 01:24:23.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:24:28.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:24:42.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-413" for this suite.

• [SLOW TEST:18.987 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":356,"completed":226,"skipped":4156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:24:42.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7086
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 01:25:00.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7086" for this suite.

• [SLOW TEST:17.473 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":356,"completed":227,"skipped":4179,"failed":0}
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:25:00.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-3175
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 28 01:25:00.341: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 28 01:26:00.450: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:26:00.465: INFO: Starting informer...
STEP: Starting pods...
Jan 28 01:26:00.746: INFO: Pod1 is running on 10.187.128.43. Tainting Node
Jan 28 01:26:03.041: INFO: Pod2 is running on 10.187.128.43. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jan 28 01:26:09.022: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 28 01:26:29.087: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:26:29.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-3175" for this suite.

• [SLOW TEST:89.067 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":356,"completed":228,"skipped":4184,"failed":0}
SSS
------------------------------
[sig-node] Containers 
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9582
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test override arguments
Jan 28 01:26:29.438: INFO: Waiting up to 5m0s for pod "client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a" in namespace "containers-9582" to be "Succeeded or Failed"
Jan 28 01:26:29.458: INFO: Pod "client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.506396ms
Jan 28 01:26:31.488: INFO: Pod "client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049850426s
Jan 28 01:26:33.526: INFO: Pod "client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087964552s
STEP: Saw pod success
Jan 28 01:26:33.527: INFO: Pod "client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a" satisfied condition "Succeeded or Failed"
Jan 28 01:26:33.545: INFO: Trying to get logs from node 10.187.128.43 pod client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:26:33.710: INFO: Waiting for pod client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a to disappear
Jan 28 01:26:33.727: INFO: Pod client-containers-f15edc24-dbf9-43e4-867e-d04d43056b2a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:188
Jan 28 01:26:33.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9582" for this suite.
•{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","total":356,"completed":229,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:33.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3111
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jan 28 01:26:34.049: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.049: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.062: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.062: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.095: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.095: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.145: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:34.145: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 28 01:26:35.884: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 28 01:26:35.884: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 28 01:26:36.105: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jan 28 01:26:36.147: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.154: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 0
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.155: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.171: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.171: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.200: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.200: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:36.224: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:36.224: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:36.242: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:36.242: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:38.136: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:38.136: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:38.174: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
STEP: listing Deployments
Jan 28 01:26:38.203: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jan 28 01:26:38.250: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jan 28 01:26:38.283: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:38.283: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:38.296: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:38.325: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:38.345: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:40.133: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:40.161: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:40.183: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:40.218: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 28 01:26:41.913: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jan 28 01:26:42.015: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:42.016: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:42.016: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:42.016: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:42.016: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 1
Jan 28 01:26:42.017: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:42.017: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:42.017: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:42.017: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 2
Jan 28 01:26:42.017: INFO: observed Deployment test-deployment in namespace deployment-3111 with ReadyReplicas 3
STEP: deleting the Deployment
Jan 28 01:26:42.065: INFO: observed event type MODIFIED
Jan 28 01:26:42.065: INFO: observed event type MODIFIED
Jan 28 01:26:42.065: INFO: observed event type MODIFIED
Jan 28 01:26:42.065: INFO: observed event type MODIFIED
Jan 28 01:26:42.065: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
Jan 28 01:26:42.066: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:26:42.084: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 28 01:26:42.107: INFO: ReplicaSet "test-deployment-84b949bdfc":
&ReplicaSet{ObjectMeta:{test-deployment-84b949bdfc  deployment-3111  2bca215c-a5a8-48d0-89e8-96423413a7d8 42567 4 2023-01-28 01:26:36 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d7d7162c-6050-4e97-bfd7-c01d32933c60 0xc003b71c77 0xc003b71c78}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:26:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d7d7162c-6050-4e97-bfd7-c01d32933c60\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:26:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 84b949bdfc,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:84b949bdfc test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.7 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b71ec0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 28 01:26:42.131: INFO: pod: "test-deployment-84b949bdfc-cklxd":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-cklxd test-deployment-84b949bdfc- deployment-3111  7f3e0930-f1dc-4e91-bd88-6df754e664ee 42570 0 2023-01-28 01:26:38 +0000 UTC 2023-01-28 01:26:41 +0000 UTC 0xc0012d9718 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:097a8db80ba34be43d4395b79f0cb5e8d2f7ec5d2c1c364d32f96030fff54b2e cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 2bca215c-a5a8-48d0-89e8-96423413a7d8 0xc0012d9747 0xc0012d9748}] []  [{kube-controller-manager Update v1 2023-01-28 01:26:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bca215c-a5a8-48d0-89e8-96423413a7d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:26:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:26:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.253.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2prsh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2prsh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:172.30.253.250,StartTime:2023-01-28 01:26:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:26:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://5015fa4c1b333d78546a0b620c2ca987b2431cf81c6f7dccc0d660e1cdab7c42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.253.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 28 01:26:42.131: INFO: pod: "test-deployment-84b949bdfc-p4fhw":
&Pod{ObjectMeta:{test-deployment-84b949bdfc-p4fhw test-deployment-84b949bdfc- deployment-3111  18933419-593b-423a-acd4-060f372963f0 42564 0 2023-01-28 01:26:36 +0000 UTC 2023-01-28 01:26:42 +0000 UTC 0xc0012d9930 map[pod-template-hash:84b949bdfc test-deployment-static:true] map[cni.projectcalico.org/containerID:601ec661d52d05e3315fd37f4354874b4c144c686459e5d883a787bcbb939222 cni.projectcalico.org/podIP:172.30.90.118/32 cni.projectcalico.org/podIPs:172.30.90.118/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-84b949bdfc 2bca215c-a5a8-48d0-89e8-96423413a7d8 0xc0012d9987 0xc0012d9988}] []  [{kube-controller-manager Update v1 2023-01-28 01:26:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2bca215c-a5a8-48d0-89e8-96423413a7d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:26:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:26:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2srnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.7,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2srnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:26:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.118,StartTime:2023-01-28 01:26:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:26:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.7,ImageID:k8s.gcr.io/pause@sha256:bb6ed397957e9ca7c65ada0db5c5d1c707c9c8afc80a94acbe69f3ae76988f0c,ContainerID:containerd://36976a7a57fff7d45be6179b795c114a35b594f6c8b85ca602b2570ab0727d90,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.118,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:26:42.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3111" for this suite.

• [SLOW TEST:8.409 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":356,"completed":230,"skipped":4206,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:42.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9733
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jan 28 01:26:42.424: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9733  4757c997-37d6-4ebf-92ef-8e969d2942b0 42591 0 2023-01-28 01:26:42 +0000 UTC <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2023-01-28 01:26:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt46d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt46d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:26:42.446: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:26:44.474: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jan 28 01:26:44.474: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9733 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:26:44.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:26:44.474: INFO: ExecWithOptions: Clientset creation
Jan 28 01:26:44.474: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9733/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod...
Jan 28 01:26:44.695: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9733 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:26:44.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:26:44.695: INFO: ExecWithOptions: Clientset creation
Jan 28 01:26:44.696: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-9733/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 28 01:26:44.918: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:26:44.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9733" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":356,"completed":231,"skipped":4248,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:45.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-447
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-8f01dc14-aaef-4122-a714-42608159c1d6
STEP: Creating a pod to test consume secrets
Jan 28 01:26:45.298: INFO: Waiting up to 5m0s for pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e" in namespace "secrets-447" to be "Succeeded or Failed"
Jan 28 01:26:45.321: INFO: Pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e": Phase="Pending", Reason="", readiness=false. Elapsed: 23.038274ms
Jan 28 01:26:47.355: INFO: Pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e": Phase="Running", Reason="", readiness=true. Elapsed: 2.057341013s
Jan 28 01:26:49.377: INFO: Pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e": Phase="Running", Reason="", readiness=false. Elapsed: 4.079518217s
Jan 28 01:26:51.403: INFO: Pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.105375025s
STEP: Saw pod success
Jan 28 01:26:51.403: INFO: Pod "pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e" satisfied condition "Succeeded or Failed"
Jan 28 01:26:51.421: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:26:51.505: INFO: Waiting for pod pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e to disappear
Jan 28 01:26:51.524: INFO: Pod pod-secrets-a244b1fb-4bb1-4a7f-9a6a-bac3ee72b47e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:26:51.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-447" for this suite.

• [SLOW TEST:6.545 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":232,"skipped":4264,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:51.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4207
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:26:52.369: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:26:55.497: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:26:55.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:26:59.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4207" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.772 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":356,"completed":233,"skipped":4278,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:26:59.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5143
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating replication controller my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b
Jan 28 01:26:59.626: INFO: Pod name my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b: Found 0 pods out of 1
Jan 28 01:27:04.657: INFO: Pod name my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b: Found 1 pods out of 1
Jan 28 01:27:04.657: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b" are running
Jan 28 01:27:04.675: INFO: Pod "my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b-slwkc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:26:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:27:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:27:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-28 01:26:59 +0000 UTC Reason: Message:}])
Jan 28 01:27:04.675: INFO: Trying to dial the pod
Jan 28 01:27:09.782: INFO: Controller my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b: Got expected result from replica 1 [my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b-slwkc]: "my-hostname-basic-dc788315-0f12-48c5-86d7-0ebf3a14c51b-slwkc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 28 01:27:09.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5143" for this suite.

• [SLOW TEST:10.486 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":356,"completed":234,"skipped":4282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:27:09.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1428
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-map-3b0aca3d-6035-40f7-8d5c-415595c2b44c
STEP: Creating a pod to test consume configMaps
Jan 28 01:27:10.112: INFO: Waiting up to 5m0s for pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b" in namespace "configmap-1428" to be "Succeeded or Failed"
Jan 28 01:27:10.136: INFO: Pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b": Phase="Pending", Reason="", readiness=false. Elapsed: 23.840231ms
Jan 28 01:27:12.164: INFO: Pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052128498s
Jan 28 01:27:14.193: INFO: Pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080608768s
Jan 28 01:27:16.225: INFO: Pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.112918098s
STEP: Saw pod success
Jan 28 01:27:16.225: INFO: Pod "pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b" satisfied condition "Succeeded or Failed"
Jan 28 01:27:16.241: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:27:16.337: INFO: Waiting for pod pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b to disappear
Jan 28 01:27:16.356: INFO: Pod pod-configmaps-799ccf80-ee6a-4935-992e-816abc7ee58b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:27:16.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1428" for this suite.

• [SLOW TEST:6.569 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":356,"completed":235,"skipped":4333,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:27:16.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9932
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jan 28 01:27:16.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jan 28 01:27:33.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:27:36.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:27:49.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9932" for this suite.

• [SLOW TEST:33.571 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":356,"completed":236,"skipped":4354,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:27:49.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5320
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/framework/framework.go:652
STEP: create deployment with httpd image
Jan 28 01:27:50.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5320 create -f -'
Jan 28 01:27:50.905: INFO: stderr: ""
Jan 28 01:27:50.905: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jan 28 01:27:50.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5320 diff -f -'
Jan 28 01:27:51.147: INFO: rc: 1
Jan 28 01:27:51.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5320 delete -f -'
Jan 28 01:27:51.267: INFO: stderr: ""
Jan 28 01:27:51.268: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:27:51.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5320" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":356,"completed":237,"skipped":4358,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:27:51.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2434
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2434.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2434.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2434.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2434.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:27:55.787: INFO: DNS probes using dns-2434/dns-test-33e221ae-1355-4b3a-b0bb-df3642766571 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:27:55.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2434" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","total":356,"completed":238,"skipped":4361,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:27:55.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5349
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:27:56.149: INFO: Creating deployment "webserver-deployment"
Jan 28 01:27:56.172: INFO: Waiting for observed generation 1
Jan 28 01:27:58.218: INFO: Waiting for all required pods to come up
Jan 28 01:27:58.240: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 28 01:28:00.298: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 28 01:28:00.332: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 28 01:28:00.387: INFO: Updating deployment webserver-deployment
Jan 28 01:28:00.387: INFO: Waiting for observed generation 2
Jan 28 01:28:02.431: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 28 01:28:02.461: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 28 01:28:02.496: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 28 01:28:02.632: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 28 01:28:02.632: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 28 01:28:02.651: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 28 01:28:02.712: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 28 01:28:02.712: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 28 01:28:02.759: INFO: Updating deployment webserver-deployment
Jan 28 01:28:02.759: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 28 01:28:02.837: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 28 01:28:02.886: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:28:02.940: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5349  d90495a5-3e13-47ed-b622-e0833a2b8aba 43408 3 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f2c0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:28:02 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-57ccb67bb8" is progressing.,LastUpdateTime:2023-01-28 01:28:02 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 28 01:28:02.999: INFO: New ReplicaSet "webserver-deployment-57ccb67bb8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-57ccb67bb8  deployment-5349  a2db7f4c-e30e-411b-9c1b-ab05ee125668 43400 3 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d90495a5-3e13-47ed-b622-e0833a2b8aba 0xc004752b07 0xc004752b08}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d90495a5-3e13-47ed-b622-e0833a2b8aba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 57ccb67bb8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004752bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:28:02.999: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 28 01:28:02.999: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-55df494869  deployment-5349  12abf361-6b01-49c7-993e-f68886597476 43398 3 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d90495a5-3e13-47ed-b622-e0833a2b8aba 0xc0047529e7 0xc0047529e8}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d90495a5-3e13-47ed-b622-e0833a2b8aba\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004752aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:28:03.048: INFO: Pod "webserver-deployment-55df494869-2g5wh" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-2g5wh webserver-deployment-55df494869- deployment-5349  37dee0f9-5e04-4663-85a1-de03bed97771 43416 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2c477 0xc003f2c478}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5p42s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5p42s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.049: INFO: Pod "webserver-deployment-55df494869-4rt5t" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-4rt5t webserver-deployment-55df494869- deployment-5349  8dabf678-bd0b-400e-80bf-8dc4d3f8e9c2 43225 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:f5bcb9e5e36b9f1ec2dcac0e9383abc80bc26899cf4e4bc289fac5570d0deffa cni.projectcalico.org/podIP:172.30.253.211/32 cni.projectcalico.org/podIPs:172.30.253.211/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2c6a7 0xc003f2c6a8}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.253.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5zsw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5zsw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:172.30.253.211,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://da052c1628b6976882d7c706d0d5633817da23c8a7c395d07a8eb78f4f7a57cd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.253.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.049: INFO: Pod "webserver-deployment-55df494869-67tq8" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-67tq8 webserver-deployment-55df494869- deployment-5349  14a07d96-29aa-463e-9199-53b332e8f45c 43387 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2c937 0xc003f2c938}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zpk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zpk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.050: INFO: Pod "webserver-deployment-55df494869-8p652" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-8p652 webserver-deployment-55df494869- deployment-5349  3cb73d49-8d99-49a5-8475-16955016877e 43216 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:55b9b1d3ced4aec928cb96adcc044c6f54b251761af1ae67adee7d1d0e59d831 cni.projectcalico.org/podIP:172.30.90.71/32 cni.projectcalico.org/podIPs:172.30.90.71/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2cb90 0xc003f2cb91}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q8xd6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q8xd6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.71,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efdee66133d55df0349abd385b991d43348d5390af154bc36fe03bbfb345dc6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.050: INFO: Pod "webserver-deployment-55df494869-9snkm" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-9snkm webserver-deployment-55df494869- deployment-5349  4ca7a607-9f90-4f85-8dcf-97d3617380f7 43404 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2ce77 0xc003f2ce78}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kxzlq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kxzlq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.050: INFO: Pod "webserver-deployment-55df494869-9v6rg" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-9v6rg webserver-deployment-55df494869- deployment-5349  9b673fab-c999-4a89-977f-19b584b1621e 43232 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:babc25ce2755eafd1e6e2d0135e27c40b1e743808abea2598b3a56bae4e07324 cni.projectcalico.org/podIP:172.30.253.210/32 cni.projectcalico.org/podIPs:172.30.253.210/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2d0e7 0xc003f2d0e8}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.253.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2pt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2pt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:172.30.253.210,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8d1362ce44e40f4bae04bf6261927f41934101bd114738123f13976621516aa2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.253.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.051: INFO: Pod "webserver-deployment-55df494869-bwnm6" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-bwnm6 webserver-deployment-55df494869- deployment-5349  96e48fc8-546b-463a-855f-cd04ee7f696d 43201 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:9b0560932957ab50f8c61a0165ba5fbbc601b0e75b56117c6cd5e05d22a0a00b cni.projectcalico.org/podIP:172.30.102.102/32 cni.projectcalico.org/podIPs:172.30.102.102/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2d367 0xc003f2d368}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2btw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2btw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:172.30.102.102,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a3d5ab6cf26571ff22d7fa46980a33e8f841cfb1f1fadd5ab281856326ef11ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.051: INFO: Pod "webserver-deployment-55df494869-f98rr" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-f98rr webserver-deployment-55df494869- deployment-5349  85e5a779-e2f5-4d0c-90aa-db99815dde04 43384 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2d677 0xc003f2d678}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p6wm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p6wm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.051: INFO: Pod "webserver-deployment-55df494869-gkws5" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-gkws5 webserver-deployment-55df494869- deployment-5349  c3d33261-2cf9-48dc-9ec7-ee4919677cf5 43196 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ad5f8344a8928899ffe1e546f8abb8d33a6bdb489a447968b3622f079f561c25 cni.projectcalico.org/podIP:172.30.102.93/32 cni.projectcalico.org/podIPs:172.30.102.93/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2d850 0xc003f2d851}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56kv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56kv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:172.30.102.93,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a48e01e518737eefcf14bcff997e3f9dad9a1ccac2981c1c365bd4ac4a17b8df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.051: INFO: Pod "webserver-deployment-55df494869-hwrwc" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-hwrwc webserver-deployment-55df494869- deployment-5349  85c95198-5207-4a26-86b6-4fd68ae971a1 43383 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2da77 0xc003f2da78}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pbxdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pbxdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.052: INFO: Pod "webserver-deployment-55df494869-nj9tn" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-nj9tn webserver-deployment-55df494869- deployment-5349  984976ac-2b74-4b43-ad50-05500a8a374d 43393 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2dc90 0xc003f2dc91}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxv8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxv8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.052: INFO: Pod "webserver-deployment-55df494869-p7x9p" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-p7x9p webserver-deployment-55df494869- deployment-5349  d56a0961-6f2d-4d90-b4c3-2ed6b9b31807 43380 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc003f2df67 0xc003f2df68}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtxhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtxhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.052: INFO: Pod "webserver-deployment-55df494869-qdxmb" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-qdxmb webserver-deployment-55df494869- deployment-5349  6971dd57-a3f0-439d-9c41-ffa0982a5063 43237 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:92be05635eb9fb3c2a1e68125711b9d07369ae6ddfc39eeb00f85c81e5eac64e cni.projectcalico.org/podIP:172.30.102.97/32 cni.projectcalico.org/podIPs:172.30.102.97/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8c140 0xc002c8c141}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.102.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7s45z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7s45z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:172.30.102.97,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ad507fe512c457974302d14f7b1b65d4d32709476edb822f972fe2196498b0b8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.102.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.053: INFO: Pod "webserver-deployment-55df494869-qw6wn" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-qw6wn webserver-deployment-55df494869- deployment-5349  699e79bf-ccba-49a5-ba4a-5a5d3f7d2c68 43406 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8c3e7 0xc002c8c3e8}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2m2tc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2m2tc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.053: INFO: Pod "webserver-deployment-55df494869-rk962" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-rk962 webserver-deployment-55df494869- deployment-5349  223e9ff8-dac7-4e23-9bf2-22fad314d883 43213 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:ae6746d68e82e71226370f6098dbcbde538fdf0d0b21f4cc4a670d77c9176540 cni.projectcalico.org/podIP:172.30.90.126/32 cni.projectcalico.org/podIPs:172.30.90.126/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8c637 0xc002c8c638}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlzjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlzjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.126,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0e840efbd213e15d29157cf244b691a08ffb5e82d10cc64d98901c2993c09471,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.053: INFO: Pod "webserver-deployment-55df494869-s7p5r" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-s7p5r webserver-deployment-55df494869- deployment-5349  1fa68dce-c2fb-457e-b974-dc82cf346958 43390 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8c977 0xc002c8c978}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4tfm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4tfm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.054: INFO: Pod "webserver-deployment-55df494869-sc6jh" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-sc6jh webserver-deployment-55df494869- deployment-5349  0c1fc90a-92aa-432b-b527-ec6a0cc30c98 43403 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8cb20 0xc002c8cb21}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4wjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4wjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.054: INFO: Pod "webserver-deployment-55df494869-ttwnw" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-ttwnw webserver-deployment-55df494869- deployment-5349  c031763d-f4d6-42bd-af8d-6165746f196f 43379 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8cdc7 0xc002c8cdc8}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bcqj6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bcqj6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.054: INFO: Pod "webserver-deployment-55df494869-vvf8j" is not available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-vvf8j webserver-deployment-55df494869- deployment-5349  8e6d1ad5-25a8-4ed2-8340-74ccde28c4e6 43411 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8cfc0 0xc002c8cfc1}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k7z2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k7z2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.055: INFO: Pod "webserver-deployment-55df494869-x7bbc" is available:
&Pod{ObjectMeta:{webserver-deployment-55df494869-x7bbc webserver-deployment-55df494869- deployment-5349  03629c94-ed53-45d1-9a75-8e980002874e 43229 0 2023-01-28 01:27:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:8c8ba11893075d42fc21d8c063cbe9b7397826d9f605f29d4c90e68ed7671b76 cni.projectcalico.org/podIP:172.30.253.207/32 cni.projectcalico.org/podIPs:172.30.253.207/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-55df494869 12abf361-6b01-49c7-993e-f68886597476 0xc002c8d227 0xc002c8d228}] []  [{kube-controller-manager Update v1 2023-01-28 01:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12abf361-6b01-49c7-993e-f68886597476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:27:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.253.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7tb4v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7tb4v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:27:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:172.30.253.207,StartTime:2023-01-28 01:27:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:27:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://68360b25519c073c1ce2bc5d06dff00f75e59dd6eb747df2acaf3380b87475ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.253.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.055: INFO: Pod "webserver-deployment-57ccb67bb8-82b4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-82b4x webserver-deployment-57ccb67bb8- deployment-5349  91ee44e8-7c3c-4a56-b242-663c705dd1ea 43314 0 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:1cbd21d0a95d9d349662682208a5162b6cc678c89db4fb7f6d77ec5715acd467 cni.projectcalico.org/podIP:172.30.102.87/32 cni.projectcalico.org/podIPs:172.30.102.87/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8d477 0xc002c8d478}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 01:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5jnh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5jnh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:,StartTime:2023-01-28 01:28:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.055: INFO: Pod "webserver-deployment-57ccb67bb8-8tsmk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-8tsmk webserver-deployment-57ccb67bb8- deployment-5349  49a5117f-8cf9-487c-9812-0783102f709e 43389 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8d6c7 0xc002c8d6c8}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-86vgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-86vgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.055: INFO: Pod "webserver-deployment-57ccb67bb8-dczbg" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-dczbg webserver-deployment-57ccb67bb8- deployment-5349  68eea53f-00c9-4611-9266-4e96c5f39d71 43388 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8d860 0xc002c8d861}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bc9br,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bc9br,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.056: INFO: Pod "webserver-deployment-57ccb67bb8-f288g" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-f288g webserver-deployment-57ccb67bb8- deployment-5349  ad9a95d0-d8f1-4841-8ffc-917e15c362b9 43315 0 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:e3571e9b223cd017a71ea936c765802abf4d3b4dd4830e7a47c33a1cf2948533 cni.projectcalico.org/podIP:172.30.90.72/32 cni.projectcalico.org/podIPs:172.30.90.72/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8d9f0 0xc002c8d9f1}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 01:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xqp9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xqp9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.056: INFO: Pod "webserver-deployment-57ccb67bb8-f8wmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-f8wmh webserver-deployment-57ccb67bb8- deployment-5349  9e5460b3-ca6c-41af-b699-8bb7f68d21b4 43327 0 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:87213e858eab83c1d617a9d63390dc870f3d33e59a11637dd626907420fc9b39 cni.projectcalico.org/podIP:172.30.102.96/32 cni.projectcalico.org/podIPs:172.30.102.96/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8dc17 0xc002c8dc18}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 01:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hrft,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hrft,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:,StartTime:2023-01-28 01:28:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.057: INFO: Pod "webserver-deployment-57ccb67bb8-g9gzk" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-g9gzk webserver-deployment-57ccb67bb8- deployment-5349  98040f81-9d6e-4dfa-97a2-0bc43217f2d2 43428 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc002c8de67 0xc002c8de68}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4hggt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4hggt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.057: INFO: Pod "webserver-deployment-57ccb67bb8-j65r4" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-j65r4 webserver-deployment-57ccb67bb8- deployment-5349  e323d129-995e-4e45-b872-d8b58828cb07 43323 0 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:7d032f757174ec4d249a978c20b0138e87a08c2a64f40445c590cf58959a76e4 cni.projectcalico.org/podIP:172.30.253.212/32 cni.projectcalico.org/podIPs:172.30.253.212/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc0004886d7 0xc0004886d8}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 01:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lc2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lc2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:,StartTime:2023-01-28 01:28:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.057: INFO: Pod "webserver-deployment-57ccb67bb8-l7t2l" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-l7t2l webserver-deployment-57ccb67bb8- deployment-5349  61d57be5-d292-4dbc-9e1e-8dd41064d25a 43418 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc001610567 0xc001610568}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8cws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8cws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.057: INFO: Pod "webserver-deployment-57ccb67bb8-mpx4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-mpx4z webserver-deployment-57ccb67bb8- deployment-5349  8d130190-f196-4a74-afc7-258a9ebcc75e 43325 0 2023-01-28 01:28:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[cni.projectcalico.org/containerID:0bbc4c56ff123c09a7390cc3dc475fef81849c1f306597d67190c1254bdb141f cni.projectcalico.org/podIP:172.30.90.77/32 cni.projectcalico.org/podIPs:172.30.90.77/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc001610847 0xc001610848}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-28 01:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2xnhz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2xnhz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.058: INFO: Pod "webserver-deployment-57ccb67bb8-qj7rb" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-qj7rb webserver-deployment-57ccb67bb8- deployment-5349  0f5dea73-2871-4c43-be83-1920e9b52bf8 43381 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc001611047 0xc001611048}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7x4tz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7x4tz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.058: INFO: Pod "webserver-deployment-57ccb67bb8-r9gmx" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-r9gmx webserver-deployment-57ccb67bb8- deployment-5349  c0d0579e-a877-4992-90f4-60a84e4bc013 43424 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc001611260 0xc001611261}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gwzqt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gwzqt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.058: INFO: Pod "webserver-deployment-57ccb67bb8-wzw2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-wzw2r webserver-deployment-57ccb67bb8- deployment-5349  7ef5707b-5c51-4c50-85ce-e9a164c24804 43396 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc001611517 0xc001611518}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql2zl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql2zl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:28:03.059: INFO: Pod "webserver-deployment-57ccb67bb8-zttjp" is not available:
&Pod{ObjectMeta:{webserver-deployment-57ccb67bb8-zttjp webserver-deployment-57ccb67bb8- deployment-5349  33139de6-2b29-4bce-9f9e-04c65e7a4eb2 43421 0 2023-01-28 01:28:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:57ccb67bb8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-57ccb67bb8 a2db7f4c-e30e-411b-9c1b-ab05ee125668 0xc0016116b0 0xc0016116b1}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2db7f4c-e30e-411b-9c1b-ab05ee125668\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n9p8r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n9p8r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.30,PodIP:,StartTime:2023-01-28 01:28:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:28:03.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5349" for this suite.

• [SLOW TEST:7.182 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":356,"completed":239,"skipped":4378,"failed":0}
SSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:03.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-3910
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jan 28 01:28:23.864: INFO: EndpointSlice for Service endpointslice-3910/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 28 01:28:33.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-3910" for this suite.

• [SLOW TEST:30.847 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":356,"completed":240,"skipped":4386,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:33.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2123
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/framework/framework.go:652
STEP: validating cluster-info
Jan 28 01:28:34.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-2123 cluster-info'
Jan 28 01:28:34.319: INFO: stderr: ""
Jan 28 01:28:34.319: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:28:34.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2123" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":356,"completed":241,"skipped":4388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:34.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6539
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:28:34.610: INFO: The status of Pod busybox-host-aliases5d1c3889-2920-4e88-82d9-b33a14ff6633 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:28:36.626: INFO: The status of Pod busybox-host-aliases5d1c3889-2920-4e88-82d9-b33a14ff6633 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 28 01:28:36.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6539" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":242,"skipped":4412,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-8373
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating Indexed job
STEP: Ensuring job reaches completions
STEP: Ensuring pods with index for job exist
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 28 01:28:47.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8373" for this suite.

• [SLOW TEST:10.294 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","total":356,"completed":243,"skipped":4441,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:47.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1152
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:28:47.258: INFO: Creating deployment "test-recreate-deployment"
Jan 28 01:28:47.285: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 28 01:28:47.310: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 28 01:28:49.342: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 28 01:28:49.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 28, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 28, 47, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 28, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 28, 47, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-845d658455\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 28 01:28:51.377: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 28 01:28:51.450: INFO: Updating deployment test-recreate-deployment
Jan 28 01:28:51.450: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:28:51.624: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1152  13c3a305-23dc-445e-aec5-f36d1780dd66 44080 2 2023-01-28 01:28:47 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f5498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:28:51 +0000 UTC,LastTransitionTime:2023-01-28 01:28:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cd8586fc7" is progressing.,LastUpdateTime:2023-01-28 01:28:51 +0000 UTC,LastTransitionTime:2023-01-28 01:28:47 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 28 01:28:51.643: INFO: New ReplicaSet "test-recreate-deployment-cd8586fc7" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cd8586fc7  deployment-1152  3caef80c-1c9f-491c-9cf2-a936eb4648c7 44079 1 2023-01-28 01:28:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 13c3a305-23dc-445e-aec5-f36d1780dd66 0xc0039f59b0 0xc0039f59b1}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13c3a305-23dc-445e-aec5-f36d1780dd66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cd8586fc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f5ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:28:51.644: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 28 01:28:51.644: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-845d658455  deployment-1152  4b113d41-f6e4-4f18-86f9-5b37da662d1c 44067 2 2023-01-28 01:28:47 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 13c3a305-23dc-445e-aec5-f36d1780dd66 0xc0039f5867 0xc0039f5868}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:28:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"13c3a305-23dc-445e-aec5-f36d1780dd66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 845d658455,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:845d658455] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f5918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:28:51.655: INFO: Pod "test-recreate-deployment-cd8586fc7-7l4n8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cd8586fc7-7l4n8 test-recreate-deployment-cd8586fc7- deployment-1152  2a7af9bd-7d55-4a58-a7d4-de67cd3eeeb6 44078 0 2023-01-28 01:28:51 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cd8586fc7] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-cd8586fc7 3caef80c-1c9f-491c-9cf2-a936eb4648c7 0xc000a48220 0xc000a48221}] []  [{kube-controller-manager Update v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3caef80c-1c9f-491c-9cf2-a936eb4648c7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:28:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6ft9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6ft9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:28:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:28:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:28:51.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1152" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":356,"completed":244,"skipped":4473,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:51.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename endpointslice
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in endpointslice-9210
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 28 01:28:51.986: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 28 01:28:52.011: INFO: starting watch
STEP: patching
STEP: updating
Jan 28 01:28:52.066: INFO: waiting for watch events with expected annotations
Jan 28 01:28:52.066: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:188
Jan 28 01:28:52.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9210" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":356,"completed":245,"skipped":4487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:28:52.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-9800
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jan 28 01:28:57.015: INFO: Successfully updated pod "adopt-release-bdj4m"
STEP: Checking that the Job readopts the Pod
Jan 28 01:28:57.015: INFO: Waiting up to 15m0s for pod "adopt-release-bdj4m" in namespace "job-9800" to be "adopted"
Jan 28 01:28:57.026: INFO: Pod "adopt-release-bdj4m": Phase="Running", Reason="", readiness=true. Elapsed: 10.893579ms
Jan 28 01:28:59.043: INFO: Pod "adopt-release-bdj4m": Phase="Running", Reason="", readiness=true. Elapsed: 2.028388803s
Jan 28 01:28:59.043: INFO: Pod "adopt-release-bdj4m" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jan 28 01:28:59.577: INFO: Successfully updated pod "adopt-release-bdj4m"
STEP: Checking that the Job releases the Pod
Jan 28 01:28:59.577: INFO: Waiting up to 15m0s for pod "adopt-release-bdj4m" in namespace "job-9800" to be "released"
Jan 28 01:28:59.589: INFO: Pod "adopt-release-bdj4m": Phase="Running", Reason="", readiness=true. Elapsed: 11.992299ms
Jan 28 01:29:01.606: INFO: Pod "adopt-release-bdj4m": Phase="Running", Reason="", readiness=true. Elapsed: 2.028813402s
Jan 28 01:29:01.606: INFO: Pod "adopt-release-bdj4m" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:188
Jan 28 01:29:01.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9800" for this suite.

• [SLOW TEST:9.409 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":356,"completed":246,"skipped":4521,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:01.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3645
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-b84d7d49-6654-411d-a698-f2f4a3e15cb1
STEP: Creating a pod to test consume secrets
Jan 28 01:29:01.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0" in namespace "projected-3645" to be "Succeeded or Failed"
Jan 28 01:29:01.959: INFO: Pod "pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.40276ms
Jan 28 01:29:03.975: INFO: Pod "pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028911732s
Jan 28 01:29:05.989: INFO: Pod "pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043530293s
STEP: Saw pod success
Jan 28 01:29:05.989: INFO: Pod "pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0" satisfied condition "Succeeded or Failed"
Jan 28 01:29:06.001: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:29:06.068: INFO: Waiting for pod pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0 to disappear
Jan 28 01:29:06.079: INFO: Pod pod-projected-secrets-02f5394f-83c0-4b5a-8edb-b92a880554a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 01:29:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3645" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":247,"skipped":4527,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:06.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7925
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-7925
Jan 28 01:29:06.400: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:29:08.417: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 28 01:29:08.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 28 01:29:08.721: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 28 01:29:08.721: INFO: stdout: "iptables"
Jan 28 01:29:08.721: INFO: proxyMode: iptables
Jan 28 01:29:08.761: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 28 01:29:08.776: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7925
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7925
I0128 01:29:08.846248      26 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7925, replica count: 3
I0128 01:29:11.897317      26 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:29:11.954: INFO: Creating new exec pod
Jan 28 01:29:15.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 28 01:29:15.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 28 01:29:15.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:15.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.209 80'
Jan 28 01:29:15.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.209 80\nConnection to 172.21.13.209 80 port [tcp/http] succeeded!\n"
Jan 28 01:29:15.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:15.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 31872'
Jan 28 01:29:15.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 31872\nConnection to 10.187.128.27 31872 port [tcp/*] succeeded!\n"
Jan 28 01:29:15.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:15.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.30 31872'
Jan 28 01:29:16.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.30 31872\nConnection to 10.187.128.30 31872 port [tcp/*] succeeded!\n"
Jan 28 01:29:16.203: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:29:16.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.187.128.27:31872/ ; done'
Jan 28 01:29:16.534: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n"
Jan 28 01:29:16.534: INFO: stdout: "\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b\naffinity-nodeport-timeout-6rr8b"
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Received response from host: affinity-nodeport-timeout-6rr8b
Jan 28 01:29:16.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.187.128.27:31872/'
Jan 28 01:29:16.794: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n"
Jan 28 01:29:16.794: INFO: stdout: "affinity-nodeport-timeout-6rr8b"
Jan 28 01:29:36.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-7925 exec execpod-affinitysmkbf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.187.128.27:31872/'
Jan 28 01:29:37.087: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.187.128.27:31872/\n"
Jan 28 01:29:37.087: INFO: stdout: "affinity-nodeport-timeout-jcq56"
Jan 28 01:29:37.087: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7925, will wait for the garbage collector to delete the pods
Jan 28 01:29:37.235: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 26.642292ms
Jan 28 01:29:37.336: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.031426ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:29:40.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7925" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:33.932 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":248,"skipped":4543,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:40.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-4597
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 28 01:29:40.309: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jan 28 01:29:40.330: INFO: starting watch
STEP: patching
STEP: updating
Jan 28 01:29:40.419: INFO: waiting for watch events with expected annotations
Jan 28 01:29:40.420: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 28 01:29:40.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4597" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":356,"completed":249,"skipped":4548,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:40.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2182
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:29:40.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 28 01:29:43.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-2182 --namespace=crd-publish-openapi-2182 create -f -'
Jan 28 01:29:44.546: INFO: stderr: ""
Jan 28 01:29:44.546: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 28 01:29:44.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-2182 --namespace=crd-publish-openapi-2182 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
Jan 28 01:29:44.735: INFO: stderr: ""
Jan 28 01:29:44.735: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 28 01:29:44.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-2182 --namespace=crd-publish-openapi-2182 apply -f -'
Jan 28 01:29:44.945: INFO: stderr: ""
Jan 28 01:29:44.945: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 28 01:29:44.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-2182 --namespace=crd-publish-openapi-2182 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
Jan 28 01:29:45.036: INFO: stderr: ""
Jan 28 01:29:45.036: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jan 28 01:29:45.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-2182 explain e2e-test-crd-publish-openapi-2939-crds'
Jan 28 01:29:45.237: INFO: stderr: ""
Jan 28 01:29:45.237: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2939-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:29:48.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2182" for this suite.

• [SLOW TEST:7.859 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":356,"completed":250,"skipped":4548,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:48.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-913
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create services for rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 28 01:29:48.756: INFO: namespace kubectl-913
Jan 28 01:29:48.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-913 create -f -'
Jan 28 01:29:49.682: INFO: stderr: ""
Jan 28 01:29:49.682: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 28 01:29:50.695: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:29:50.695: INFO: Found 0 / 1
Jan 28 01:29:51.696: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:29:51.696: INFO: Found 0 / 1
Jan 28 01:29:52.697: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:29:52.697: INFO: Found 1 / 1
Jan 28 01:29:52.697: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 28 01:29:52.708: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:29:52.708: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 01:29:52.708: INFO: wait on agnhost-primary startup in kubectl-913 
Jan 28 01:29:52.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-913 logs agnhost-primary-s4blk agnhost-primary'
Jan 28 01:29:52.884: INFO: stderr: ""
Jan 28 01:29:52.884: INFO: stdout: "Paused\n"
STEP: exposing RC
Jan 28 01:29:52.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-913 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 28 01:29:53.001: INFO: stderr: ""
Jan 28 01:29:53.001: INFO: stdout: "service/rm2 exposed\n"
Jan 28 01:29:53.017: INFO: Service rm2 in namespace kubectl-913 found.
STEP: exposing service
Jan 28 01:29:55.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-913 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 28 01:29:55.189: INFO: stderr: ""
Jan 28 01:29:55.189: INFO: stdout: "service/rm3 exposed\n"
Jan 28 01:29:55.206: INFO: Service rm3 in namespace kubectl-913 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:29:57.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-913" for this suite.

• [SLOW TEST:8.724 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1249
    should create services for rc  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":356,"completed":251,"skipped":4574,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:29:57.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7125
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-4gn2
STEP: Creating a pod to test atomic-volume-subpath
Jan 28 01:29:57.531: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-4gn2" in namespace "subpath-7125" to be "Succeeded or Failed"
Jan 28 01:29:57.543: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.338503ms
Jan 28 01:29:59.560: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029653158s
Jan 28 01:30:01.582: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 4.051729955s
Jan 28 01:30:03.600: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 6.069168672s
Jan 28 01:30:05.615: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 8.084664195s
Jan 28 01:30:07.633: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 10.102457113s
Jan 28 01:30:09.652: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 12.121192734s
Jan 28 01:30:11.667: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 14.136098679s
Jan 28 01:30:13.682: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 16.151368793s
Jan 28 01:30:15.699: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 18.167933649s
Jan 28 01:30:17.715: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 20.184316957s
Jan 28 01:30:19.735: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=true. Elapsed: 22.204062873s
Jan 28 01:30:21.753: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Running", Reason="", readiness=false. Elapsed: 24.222146489s
Jan 28 01:30:23.768: INFO: Pod "pod-subpath-test-configmap-4gn2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.237726336s
STEP: Saw pod success
Jan 28 01:30:23.769: INFO: Pod "pod-subpath-test-configmap-4gn2" satisfied condition "Succeeded or Failed"
Jan 28 01:30:23.780: INFO: Trying to get logs from node 10.187.128.43 pod pod-subpath-test-configmap-4gn2 container test-container-subpath-configmap-4gn2: <nil>
STEP: delete the pod
Jan 28 01:30:23.850: INFO: Waiting for pod pod-subpath-test-configmap-4gn2 to disappear
Jan 28 01:30:23.861: INFO: Pod pod-subpath-test-configmap-4gn2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-4gn2
Jan 28 01:30:23.861: INFO: Deleting pod "pod-subpath-test-configmap-4gn2" in namespace "subpath-7125"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 28 01:30:23.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7125" for this suite.

• [SLOW TEST:26.640 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","total":356,"completed":252,"skipped":4608,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:30:23.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-228
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 01:30:24.128: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 01:30:24.171: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 01:30:24.184: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.27 before test
Jan 28 01:30:24.212: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 01:30:24.212: INFO: calico-node-9md4k from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:30:24.212: INFO: calico-typha-987c59c59-r2mxh from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:30:24.212: INFO: coredns-649f45bb5-4l67p from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:30:24.212: INFO: coredns-649f45bb5-bkr22 from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:30:24.212: INFO: dashboard-metrics-scraper-f74668d5f-bscb6 from kube-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 01:30:24.212: INFO: ibm-keepalived-watcher-qxwck from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:30:24.212: INFO: ibm-master-proxy-static-10.187.128.27 from kube-system started at 2023-01-27 22:02:32 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:30:24.212: INFO: ibmcloud-block-storage-driver-mfqhd from kube-system started at 2023-01-27 22:02:42 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:30:24.212: INFO: ingress-cluster-healthcheck-56756684f7-qbpsl from kube-system started at 2023-01-27 22:43:38 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 01:30:24.212: INFO: konnectivity-agent-8ws48 from kube-system started at 2023-01-27 22:37:37 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:30:24.212: INFO: metrics-server-666774474b-hfjbk from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:30:24.212: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:30:24.212: INFO: sonobuoy-e2e-job-68fa4b4566bd42ca from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container e2e ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:30:24.212: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:30:24.212: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.30 before test
Jan 28 01:30:24.237: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-s6m9x from ibm-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 01:30:24.237: INFO: calico-kube-controllers-6c58444bc9-hfrrk from kube-system started at 2023-01-28 00:35:28 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 01:30:24.237: INFO: calico-node-lg9x8 from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:30:24.237: INFO: calico-typha-987c59c59-qs9ww from kube-system started at 2023-01-27 22:03:10 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:30:24.237: INFO: coredns-649f45bb5-9gxmz from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:30:24.237: INFO: coredns-autoscaler-64db77d767-q9gk5 from kube-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibm-file-plugin-6b4fdfc7d8-gh8bd from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibm-keepalived-watcher-p94jf from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibm-master-proxy-static-10.187.128.30 from kube-system started at 2023-01-27 22:02:39 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:30:24.237: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibm-storage-watcher-6cd9b56547-d7wvk from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibmcloud-block-storage-driver-mqmrs from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:30:24.237: INFO: ibmcloud-block-storage-plugin-5f9c77dc8c-phr4k from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 01:30:24.237: INFO: konnectivity-agent-nbg9q from kube-system started at 2023-01-27 22:37:30 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:30:24.237: INFO: kubernetes-dashboard-65d4f9ccbf-52vtb from kube-system started at 2023-01-28 00:35:28 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.237: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 01:30:24.237: INFO: metrics-server-666774474b-x2b75 from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 01:30:24.238: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:30:24.238: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:30:24.238: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:30:24.238: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-grkj5 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.238: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:30:24.238: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-2j8w6 from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:30:24.238: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:30:24.238: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:33:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.238: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 01:30:24.238: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.43 before test
Jan 28 01:30:24.260: INFO: calico-node-tfsm6 from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:30:24.260: INFO: calico-typha-987c59c59-zd57m from kube-system started at 2023-01-28 01:26:40 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:30:24.260: INFO: ibm-keepalived-watcher-9flqg from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:30:24.260: INFO: ibm-master-proxy-static-10.187.128.43 from kube-system started at 2023-01-27 22:02:29 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:30:24.260: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:30:24.260: INFO: ibmcloud-block-storage-driver-d4h67 from kube-system started at 2023-01-27 22:02:49 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:30:24.260: INFO: konnectivity-agent-zrrn4 from kube-system started at 2023-01-27 22:37:33 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:30:24.260: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:19 +0000 UTC (1 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 01:30:24.260: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-m6nkr from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:30:24.260: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:30:24.260: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6045fe7f-b476-4bde-9be0-34d6e292f5fb 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.187.128.43 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6045fe7f-b476-4bde-9be0-34d6e292f5fb off the node 10.187.128.43
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6045fe7f-b476-4bde-9be0-34d6e292f5fb
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:35:30.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-228" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:306.744 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":356,"completed":253,"skipped":4623,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:35:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1502
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9790
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6928
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:35:45.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1502" for this suite.
STEP: Destroying namespace "nsdeletetest-9790" for this suite.
Jan 28 01:35:45.433: INFO: Namespace nsdeletetest-9790 was already deleted
STEP: Destroying namespace "nsdeletetest-6928" for this suite.

• [SLOW TEST:14.787 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":356,"completed":254,"skipped":4693,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:35:45.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9164
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pods
Jan 28 01:35:45.661: INFO: created test-pod-1
Jan 28 01:35:45.676: INFO: created test-pod-2
Jan 28 01:35:45.693: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running
Jan 28 01:35:45.693: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9164' to be running and ready
Jan 28 01:35:45.745: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 01:35:45.745: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 01:35:45.745: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 01:35:45.745: INFO: 0 / 3 pods in namespace 'pods-9164' are running and ready (0 seconds elapsed)
Jan 28 01:35:45.745: INFO: expected 0 pod replicas in namespace 'pods-9164', 0 are Running and Ready.
Jan 28 01:35:45.745: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jan 28 01:35:45.745: INFO: test-pod-1  10.187.128.43  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  }]
Jan 28 01:35:45.745: INFO: test-pod-2  10.187.128.43  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  }]
Jan 28 01:35:45.745: INFO: test-pod-3  10.187.128.43  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  }]
Jan 28 01:35:45.745: INFO: 
Jan 28 01:35:47.789: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 28 01:35:47.789: INFO: 2 / 3 pods in namespace 'pods-9164' are running and ready (2 seconds elapsed)
Jan 28 01:35:47.789: INFO: expected 0 pod replicas in namespace 'pods-9164', 0 are Running and Ready.
Jan 28 01:35:47.789: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jan 28 01:35:47.789: INFO: test-pod-3  10.187.128.43  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-28 01:35:45 +0000 UTC  }]
Jan 28 01:35:47.790: INFO: 
Jan 28 01:35:49.795: INFO: 3 / 3 pods in namespace 'pods-9164' are running and ready (4 seconds elapsed)
Jan 28 01:35:49.795: INFO: expected 0 pod replicas in namespace 'pods-9164', 0 are Running and Ready.
STEP: waiting for all pods to be deleted
Jan 28 01:35:49.861: INFO: Pod quantity 3 is different from expected quantity 0
Jan 28 01:35:50.874: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:35:51.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9164" for this suite.

• [SLOW TEST:6.458 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":356,"completed":255,"skipped":4700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:35:51.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1899
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:35:52.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a" in namespace "projected-1899" to be "Succeeded or Failed"
Jan 28 01:35:52.151: INFO: Pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.36321ms
Jan 28 01:35:54.168: INFO: Pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a": Phase="Running", Reason="", readiness=true. Elapsed: 2.028015032s
Jan 28 01:35:56.187: INFO: Pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.046567111s
Jan 28 01:35:58.209: INFO: Pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068857934s
STEP: Saw pod success
Jan 28 01:35:58.209: INFO: Pod "downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a" satisfied condition "Succeeded or Failed"
Jan 28 01:35:58.220: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a container client-container: <nil>
STEP: delete the pod
Jan 28 01:35:58.344: INFO: Waiting for pod downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a to disappear
Jan 28 01:35:58.355: INFO: Pod downwardapi-volume-db8cd602-8389-47f7-8063-0d60a1781a8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:35:58.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1899" for this suite.

• [SLOW TEST:6.483 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":256,"skipped":4723,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:35:58.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8830
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-db70732a-dd39-4bfa-8670-30675d90e7de
STEP: Creating a pod to test consume configMaps
Jan 28 01:35:58.648: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427" in namespace "projected-8830" to be "Succeeded or Failed"
Jan 28 01:35:58.660: INFO: Pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427": Phase="Pending", Reason="", readiness=false. Elapsed: 11.838923ms
Jan 28 01:36:00.678: INFO: Pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030167114s
Jan 28 01:36:02.695: INFO: Pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046532174s
Jan 28 01:36:04.710: INFO: Pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061864511s
STEP: Saw pod success
Jan 28 01:36:04.710: INFO: Pod "pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427" satisfied condition "Succeeded or Failed"
Jan 28 01:36:04.722: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:36:04.789: INFO: Waiting for pod pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427 to disappear
Jan 28 01:36:04.800: INFO: Pod pod-projected-configmaps-7b259df5-b534-4805-9145-c3dc2521d427 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 01:36:04.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8830" for this suite.

• [SLOW TEST:6.444 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":257,"skipped":4731,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:04.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9090
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap configmap-9090/configmap-test-e1ce9fc8-061a-4f9c-8053-b29947547173
STEP: Creating a pod to test consume configMaps
Jan 28 01:36:05.123: INFO: Waiting up to 5m0s for pod "pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124" in namespace "configmap-9090" to be "Succeeded or Failed"
Jan 28 01:36:05.135: INFO: Pod "pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124": Phase="Pending", Reason="", readiness=false. Elapsed: 11.547004ms
Jan 28 01:36:07.154: INFO: Pod "pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030591785s
Jan 28 01:36:09.171: INFO: Pod "pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048197746s
STEP: Saw pod success
Jan 28 01:36:09.171: INFO: Pod "pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124" satisfied condition "Succeeded or Failed"
Jan 28 01:36:09.183: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124 container env-test: <nil>
STEP: delete the pod
Jan 28 01:36:09.244: INFO: Waiting for pod pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124 to disappear
Jan 28 01:36:09.255: INFO: Pod pod-configmaps-eff76054-93c6-4901-a38b-ece8e8d0f124 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:36:09.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9090" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":356,"completed":258,"skipped":4761,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:09.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3296
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:36:09.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3296" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":356,"completed":259,"skipped":4763,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:09.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5256
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:36:12.032: INFO: DNS probes using dns-5256/dns-test-535da3ec-9597-476e-8ba6-909a5326ddbf succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:36:12.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5256" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":356,"completed":260,"skipped":4782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:12.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8085
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 28 01:36:12.326: INFO: Waiting up to 5m0s for pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9" in namespace "downward-api-8085" to be "Succeeded or Failed"
Jan 28 01:36:12.337: INFO: Pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550095ms
Jan 28 01:36:14.352: INFO: Pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.025841791s
Jan 28 01:36:16.367: INFO: Pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9": Phase="Running", Reason="", readiness=false. Elapsed: 4.041156512s
Jan 28 01:36:18.384: INFO: Pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057803004s
STEP: Saw pod success
Jan 28 01:36:18.384: INFO: Pod "downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9" satisfied condition "Succeeded or Failed"
Jan 28 01:36:18.396: INFO: Trying to get logs from node 10.187.128.43 pod downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9 container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:36:18.461: INFO: Waiting for pod downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9 to disappear
Jan 28 01:36:18.472: INFO: Pod downward-api-8e59044e-3a5e-4cf1-84f1-6069afebcdc9 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 28 01:36:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8085" for this suite.

• [SLOW TEST:6.412 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":356,"completed":261,"skipped":4826,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:18.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3236
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod pod-subpath-test-configmap-xtl9
STEP: Creating a pod to test atomic-volume-subpath
Jan 28 01:36:18.772: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xtl9" in namespace "subpath-3236" to be "Succeeded or Failed"
Jan 28 01:36:18.784: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.674701ms
Jan 28 01:36:20.801: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 2.029057474s
Jan 28 01:36:22.818: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 4.046408541s
Jan 28 01:36:24.834: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 6.062258636s
Jan 28 01:36:26.851: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 8.079110333s
Jan 28 01:36:28.874: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 10.10222017s
Jan 28 01:36:30.890: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 12.117974173s
Jan 28 01:36:32.908: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 14.13608556s
Jan 28 01:36:34.927: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 16.155526899s
Jan 28 01:36:36.945: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 18.172658961s
Jan 28 01:36:38.963: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=true. Elapsed: 20.190864832s
Jan 28 01:36:40.979: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Running", Reason="", readiness=false. Elapsed: 22.20762367s
Jan 28 01:36:42.996: INFO: Pod "pod-subpath-test-configmap-xtl9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.224366737s
STEP: Saw pod success
Jan 28 01:36:42.997: INFO: Pod "pod-subpath-test-configmap-xtl9" satisfied condition "Succeeded or Failed"
Jan 28 01:36:43.007: INFO: Trying to get logs from node 10.187.128.43 pod pod-subpath-test-configmap-xtl9 container test-container-subpath-configmap-xtl9: <nil>
STEP: delete the pod
Jan 28 01:36:43.074: INFO: Waiting for pod pod-subpath-test-configmap-xtl9 to disappear
Jan 28 01:36:43.085: INFO: Pod pod-subpath-test-configmap-xtl9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xtl9
Jan 28 01:36:43.085: INFO: Deleting pod "pod-subpath-test-configmap-xtl9" in namespace "subpath-3236"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:188
Jan 28 01:36:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3236" for this suite.

• [SLOW TEST:24.618 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","total":356,"completed":262,"skipped":4827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:43.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9656
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/framework/framework.go:652
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 28 01:36:43.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9656 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 28 01:36:43.434: INFO: stderr: ""
Jan 28 01:36:43.434: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jan 28 01:36:43.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9656 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 28 01:36:43.670: INFO: stderr: ""
Jan 28 01:36:43.670: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jan 28 01:36:43.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9656 delete pods e2e-test-httpd-pod'
Jan 28 01:36:46.877: INFO: stderr: ""
Jan 28 01:36:46.877: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:36:46.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9656" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":356,"completed":263,"skipped":4851,"failed":0}

------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:46.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-123
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 28 01:36:47.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-123" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","total":356,"completed":264,"skipped":4851,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:47.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3769
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:36:47.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties
Jan 28 01:36:52.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 create -f -'
Jan 28 01:36:53.214: INFO: stderr: ""
Jan 28 01:36:53.214: INFO: stdout: "e2e-test-crd-publish-openapi-928-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 28 01:36:53.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 delete e2e-test-crd-publish-openapi-928-crds test-foo'
Jan 28 01:36:53.398: INFO: stderr: ""
Jan 28 01:36:53.398: INFO: stdout: "e2e-test-crd-publish-openapi-928-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 28 01:36:53.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 apply -f -'
Jan 28 01:36:54.041: INFO: stderr: ""
Jan 28 01:36:54.041: INFO: stdout: "e2e-test-crd-publish-openapi-928-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 28 01:36:54.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 delete e2e-test-crd-publish-openapi-928-crds test-foo'
Jan 28 01:36:54.162: INFO: stderr: ""
Jan 28 01:36:54.162: INFO: stdout: "e2e-test-crd-publish-openapi-928-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values
Jan 28 01:36:54.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 create -f -'
Jan 28 01:36:54.358: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jan 28 01:36:54.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 create -f -'
Jan 28 01:36:54.538: INFO: rc: 1
Jan 28 01:36:54.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 apply -f -'
Jan 28 01:36:54.766: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties
Jan 28 01:36:54.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 create -f -'
Jan 28 01:36:54.978: INFO: rc: 1
Jan 28 01:36:54.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 --namespace=crd-publish-openapi-3769 apply -f -'
Jan 28 01:36:55.182: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jan 28 01:36:55.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 explain e2e-test-crd-publish-openapi-928-crds'
Jan 28 01:36:55.382: INFO: stderr: ""
Jan 28 01:36:55.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jan 28 01:36:55.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 explain e2e-test-crd-publish-openapi-928-crds.metadata'
Jan 28 01:36:55.564: INFO: stderr: ""
Jan 28 01:36:55.564: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     Deprecated: ClusterName is a legacy field that was always cleared by the\n     system and never used; it will be removed completely in 1.25.\n\n     The name in the go struct is changed to help clients detect accidental use.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 28 01:36:55.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 explain e2e-test-crd-publish-openapi-928-crds.spec'
Jan 28 01:36:56.236: INFO: stderr: ""
Jan 28 01:36:56.236: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 28 01:36:56.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 explain e2e-test-crd-publish-openapi-928-crds.spec.bars'
Jan 28 01:36:56.428: INFO: stderr: ""
Jan 28 01:36:56.428: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-928-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jan 28 01:36:56.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3769 explain e2e-test-crd-publish-openapi-928-crds.spec.bars2'
Jan 28 01:36:56.608: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:36:59.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3769" for this suite.

• [SLOW TEST:12.512 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":356,"completed":265,"skipped":4883,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:36:59.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9212
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 28 01:37:00.018: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 01:37:03.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9212" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":356,"completed":266,"skipped":4893,"failed":0}
SSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:37:03.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7051
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod with failed condition
STEP: updating the pod
Jan 28 01:39:04.827: INFO: Successfully updated pod "var-expansion-a7c7ee43-a59c-4d95-9a0f-187e5fa367a2"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jan 28 01:39:06.881: INFO: Deleting pod "var-expansion-a7c7ee43-a59c-4d95-9a0f-187e5fa367a2" in namespace "var-expansion-7051"
Jan 28 01:39:06.920: INFO: Wait up to 5m0s for pod "var-expansion-a7c7ee43-a59c-4d95-9a0f-187e5fa367a2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:39:38.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7051" for this suite.

• [SLOW TEST:155.050 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":356,"completed":267,"skipped":4896,"failed":0}
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:39:39.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9892
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 28 01:39:39.232: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 01:39:45.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9892" for this suite.

• [SLOW TEST:6.357 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":356,"completed":268,"skipped":4903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:39:45.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-263
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jan 28 01:39:45.667: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jan 28 01:39:45.767: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 28 01:39:45.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-263" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":356,"completed":269,"skipped":4926,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:39:45.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1792
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:39:46.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80" in namespace "downward-api-1792" to be "Succeeded or Failed"
Jan 28 01:39:46.227: INFO: Pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80": Phase="Pending", Reason="", readiness=false. Elapsed: 18.066363ms
Jan 28 01:39:48.251: INFO: Pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041214635s
Jan 28 01:39:50.282: INFO: Pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073114524s
Jan 28 01:39:52.301: INFO: Pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091371819s
STEP: Saw pod success
Jan 28 01:39:52.301: INFO: Pod "downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80" satisfied condition "Succeeded or Failed"
Jan 28 01:39:52.319: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80 container client-container: <nil>
STEP: delete the pod
Jan 28 01:39:52.470: INFO: Waiting for pod downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80 to disappear
Jan 28 01:39:52.488: INFO: Pod downwardapi-volume-2b7ca88a-06a7-47c2-8245-e6b37db1ce80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 01:39:52.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1792" for this suite.

• [SLOW TEST:6.591 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":356,"completed":270,"skipped":4951,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:39:52.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9485
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:39:52.824: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 28 01:39:52.861: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:52.861: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jan 28 01:39:52.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:52.941: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:39:53.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:53.982: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:39:54.968: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:54.969: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:39:55.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:39:55.967: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 28 01:39:56.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:39:56.051: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 28 01:39:57.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:57.082: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 28 01:39:57.124: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:57.124: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:39:58.154: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:58.154: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:39:59.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:39:59.156: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:40:00.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:40:00.156: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:40:01.152: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:40:01.153: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9485, will wait for the garbage collector to delete the pods
Jan 28 01:40:01.276: INFO: Deleting DaemonSet.extensions daemon-set took: 27.332651ms
Jan 28 01:40:01.377: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.34455ms
Jan 28 01:40:03.303: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:40:03.303: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:40:03.316: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"46064"},"items":null}

Jan 28 01:40:03.334: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"46064"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:40:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9485" for this suite.

• [SLOW TEST:10.930 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":356,"completed":271,"skipped":4952,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:03.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4706
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:40
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:84
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:188
Jan 28 01:40:07.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4706" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":356,"completed":272,"skipped":4968,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:07.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3169
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:188
Jan 28 01:40:24.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3169" for this suite.

• [SLOW TEST:16.792 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":356,"completed":273,"skipped":4986,"failed":0}
S
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:24.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8688
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 28 01:40:24.861: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:40:29.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8688" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":356,"completed":274,"skipped":4987,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:29.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7297
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name secret-emptykey-test-65ae0e34-39d8-4477-8beb-6a63cbb437a1
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:40:29.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7297" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":356,"completed":275,"skipped":5012,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:29.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2313
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:40:30.049: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jan 28 01:40:31.186: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:188
Jan 28 01:40:31.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2313" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":356,"completed":276,"skipped":5016,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:31.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8682
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating projection with secret that has name projected-secret-test-map-4e58c4de-605c-47f1-84a2-0f8f7387ba7f
STEP: Creating a pod to test consume secrets
Jan 28 01:40:31.512: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b" in namespace "projected-8682" to be "Succeeded or Failed"
Jan 28 01:40:31.530: INFO: Pod "pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.266978ms
Jan 28 01:40:33.554: INFO: Pod "pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041791651s
Jan 28 01:40:35.583: INFO: Pod "pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07064356s
STEP: Saw pod success
Jan 28 01:40:35.583: INFO: Pod "pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b" satisfied condition "Succeeded or Failed"
Jan 28 01:40:35.604: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:40:35.710: INFO: Waiting for pod pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b to disappear
Jan 28 01:40:35.728: INFO: Pod pod-projected-secrets-126837ad-84c5-4d6a-a62c-612663e07b2b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 01:40:35.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8682" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":277,"skipped":5020,"failed":0}

------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:35.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2574
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:40:36.050: INFO: The status of Pod server-envvars-06a8ad32-9765-4946-8347-2da03ea44d0d is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:40:38.069: INFO: The status of Pod server-envvars-06a8ad32-9765-4946-8347-2da03ea44d0d is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:40:40.068: INFO: The status of Pod server-envvars-06a8ad32-9765-4946-8347-2da03ea44d0d is Running (Ready = true)
Jan 28 01:40:40.156: INFO: Waiting up to 5m0s for pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32" in namespace "pods-2574" to be "Succeeded or Failed"
Jan 28 01:40:40.173: INFO: Pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32": Phase="Pending", Reason="", readiness=false. Elapsed: 16.984607ms
Jan 28 01:40:42.204: INFO: Pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048064773s
Jan 28 01:40:44.233: INFO: Pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076839043s
Jan 28 01:40:46.270: INFO: Pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.114202774s
STEP: Saw pod success
Jan 28 01:40:46.270: INFO: Pod "client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32" satisfied condition "Succeeded or Failed"
Jan 28 01:40:46.288: INFO: Trying to get logs from node 10.187.128.43 pod client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32 container env3cont: <nil>
STEP: delete the pod
Jan 28 01:40:46.384: INFO: Waiting for pod client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32 to disappear
Jan 28 01:40:46.402: INFO: Pod client-envvars-2ab23599-3d41-4063-b1af-9dabe3a7df32 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:40:46.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2574" for this suite.

• [SLOW TEST:10.670 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":356,"completed":278,"skipped":5020,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:40:46.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5870
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod busybox-464838d2-de81-4a17-b01d-427365b7adbe in namespace container-probe-5870
Jan 28 01:40:48.739: INFO: Started pod busybox-464838d2-de81-4a17-b01d-427365b7adbe in namespace container-probe-5870
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 01:40:48.756: INFO: Initial restart count of pod busybox-464838d2-de81-4a17-b01d-427365b7adbe is 0
Jan 28 01:41:39.498: INFO: Restart count of pod container-probe-5870/busybox-464838d2-de81-4a17-b01d-427365b7adbe is now 1 (50.741474264s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:41:39.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5870" for this suite.

• [SLOW TEST:53.151 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":356,"completed":279,"skipped":5046,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:41:39.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3093
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3093
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3093
STEP: creating replication controller externalsvc in namespace services-3093
I0128 01:41:39.961589      26 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3093, replica count: 2
I0128 01:41:43.012390      26 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jan 28 01:41:43.103: INFO: Creating new exec pod
Jan 28 01:41:45.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-3093 exec execpodxfhjd -- /bin/sh -x -c nslookup clusterip-service.services-3093.svc.cluster.local'
Jan 28 01:41:45.560: INFO: stderr: "+ nslookup clusterip-service.services-3093.svc.cluster.local\n"
Jan 28 01:41:45.560: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-3093.svc.cluster.local\tcanonical name = externalsvc.services-3093.svc.cluster.local.\nName:\texternalsvc.services-3093.svc.cluster.local\nAddress: 172.21.152.77\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3093, will wait for the garbage collector to delete the pods
Jan 28 01:41:45.651: INFO: Deleting ReplicationController externalsvc took: 22.810072ms
Jan 28 01:41:45.753: INFO: Terminating ReplicationController externalsvc pods took: 101.220877ms
Jan 28 01:41:47.948: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:41:47.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3093" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:8.445 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":356,"completed":280,"skipped":5063,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:41:48.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5928
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:41:48.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 28 01:41:48.304: INFO: The status of Pod pod-exec-websocket-e6fdc4f3-543e-41d4-b702-86f2838678f6 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:41:50.333: INFO: The status of Pod pod-exec-websocket-e6fdc4f3-543e-41d4-b702-86f2838678f6 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:41:52.338: INFO: The status of Pod pod-exec-websocket-e6fdc4f3-543e-41d4-b702-86f2838678f6 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:41:52.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5928" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":356,"completed":281,"skipped":5081,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:41:52.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3293
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jan 28 01:42:04.819: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0128 01:42:04.819140      26 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 28 01:42:04.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-28xpq" in namespace "gc-3293"
Jan 28 01:42:04.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-29xxw" in namespace "gc-3293"
Jan 28 01:42:04.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-2rl4q" in namespace "gc-3293"
Jan 28 01:42:05.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vrnh" in namespace "gc-3293"
Jan 28 01:42:05.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-498ff" in namespace "gc-3293"
Jan 28 01:42:05.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c7hb" in namespace "gc-3293"
Jan 28 01:42:05.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wt5t" in namespace "gc-3293"
Jan 28 01:42:05.292: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jb25" in namespace "gc-3293"
Jan 28 01:42:05.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-66f9t" in namespace "gc-3293"
Jan 28 01:42:05.440: INFO: Deleting pod "simpletest-rc-to-be-deleted-689p6" in namespace "gc-3293"
Jan 28 01:42:05.488: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k8x6" in namespace "gc-3293"
Jan 28 01:42:05.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mtpc" in namespace "gc-3293"
Jan 28 01:42:05.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-762fl" in namespace "gc-3293"
Jan 28 01:42:05.653: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n877" in namespace "gc-3293"
Jan 28 01:42:05.713: INFO: Deleting pod "simpletest-rc-to-be-deleted-85nqd" in namespace "gc-3293"
Jan 28 01:42:05.775: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cb9s" in namespace "gc-3293"
Jan 28 01:42:05.839: INFO: Deleting pod "simpletest-rc-to-be-deleted-8lx65" in namespace "gc-3293"
Jan 28 01:42:05.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-96m8c" in namespace "gc-3293"
Jan 28 01:42:05.967: INFO: Deleting pod "simpletest-rc-to-be-deleted-9gxfc" in namespace "gc-3293"
Jan 28 01:42:06.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jfsg" in namespace "gc-3293"
Jan 28 01:42:06.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kmck" in namespace "gc-3293"
Jan 28 01:42:06.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkt85" in namespace "gc-3293"
Jan 28 01:42:06.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnpn2" in namespace "gc-3293"
Jan 28 01:42:06.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9plm" in namespace "gc-3293"
Jan 28 01:42:06.406: INFO: Deleting pod "simpletest-rc-to-be-deleted-clhqb" in namespace "gc-3293"
Jan 28 01:42:06.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmwcf" in namespace "gc-3293"
Jan 28 01:42:06.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqhlq" in namespace "gc-3293"
Jan 28 01:42:06.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcckr" in namespace "gc-3293"
Jan 28 01:42:06.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlt9r" in namespace "gc-3293"
Jan 28 01:42:06.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpm29" in namespace "gc-3293"
Jan 28 01:42:06.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsn68" in namespace "gc-3293"
Jan 28 01:42:06.892: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqn79" in namespace "gc-3293"
Jan 28 01:42:06.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsx72" in namespace "gc-3293"
Jan 28 01:42:07.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxxx5" in namespace "gc-3293"
Jan 28 01:42:07.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8vpr" in namespace "gc-3293"
Jan 28 01:42:07.177: INFO: Deleting pod "simpletest-rc-to-be-deleted-gxdr9" in namespace "gc-3293"
Jan 28 01:42:07.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgw6j" in namespace "gc-3293"
Jan 28 01:42:07.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrqfs" in namespace "gc-3293"
Jan 28 01:42:07.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-j98ds" in namespace "gc-3293"
Jan 28 01:42:07.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-jg42m" in namespace "gc-3293"
Jan 28 01:42:07.485: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5qb5" in namespace "gc-3293"
Jan 28 01:42:07.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-k75pc" in namespace "gc-3293"
Jan 28 01:42:07.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-kgb79" in namespace "gc-3293"
Jan 28 01:42:07.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-ktvvq" in namespace "gc-3293"
Jan 28 01:42:07.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-kxmcg" in namespace "gc-3293"
Jan 28 01:42:07.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-l4v6m" in namespace "gc-3293"
Jan 28 01:42:07.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-l6pgx" in namespace "gc-3293"
Jan 28 01:42:08.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-l792r" in namespace "gc-3293"
Jan 28 01:42:08.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-lkz9x" in namespace "gc-3293"
Jan 28 01:42:08.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-lp5cg" in namespace "gc-3293"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:188
Jan 28 01:42:08.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3293" for this suite.

• [SLOW TEST:15.720 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":356,"completed":282,"skipped":5115,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2812
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:42:08.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2812" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":356,"completed":283,"skipped":5125,"failed":0}

------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:08.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2503
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 28 01:42:08.966: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48324 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:42:08.967: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48325 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:42:08.967: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48328 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 28 01:42:19.142: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48603 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:42:19.142: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48606 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:42:19.142: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2503  e0926ca5-819b-4c1a-9f97-c2acff2a99e3 48608 0 2023-01-28 01:42:08 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2023-01-28 01:42:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 28 01:42:19.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2503" for this suite.

• [SLOW TEST:10.574 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":356,"completed":284,"skipped":5125,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:19.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-817
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-projected-all-test-volume-f0c9c4e3-bd23-4dda-900a-3538d987db05
STEP: Creating secret with name secret-projected-all-test-volume-3c1e9133-259c-4823-871e-c8c1d891e0db
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 28 01:42:19.481: INFO: Waiting up to 5m0s for pod "projected-volume-9b80054b-4f42-4153-bccb-648735da3afa" in namespace "projected-817" to be "Succeeded or Failed"
Jan 28 01:42:19.501: INFO: Pod "projected-volume-9b80054b-4f42-4153-bccb-648735da3afa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.072299ms
Jan 28 01:42:21.529: INFO: Pod "projected-volume-9b80054b-4f42-4153-bccb-648735da3afa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048162562s
Jan 28 01:42:23.553: INFO: Pod "projected-volume-9b80054b-4f42-4153-bccb-648735da3afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072612686s
STEP: Saw pod success
Jan 28 01:42:23.553: INFO: Pod "projected-volume-9b80054b-4f42-4153-bccb-648735da3afa" satisfied condition "Succeeded or Failed"
Jan 28 01:42:23.581: INFO: Trying to get logs from node 10.187.128.43 pod projected-volume-9b80054b-4f42-4153-bccb-648735da3afa container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 28 01:42:23.720: INFO: Waiting for pod projected-volume-9b80054b-4f42-4153-bccb-648735da3afa to disappear
Jan 28 01:42:23.738: INFO: Pod projected-volume-9b80054b-4f42-4153-bccb-648735da3afa no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:188
Jan 28 01:42:23.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-817" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":356,"completed":285,"skipped":5132,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2197
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in volume subpath
Jan 28 01:42:24.037: INFO: Waiting up to 5m0s for pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd" in namespace "var-expansion-2197" to be "Succeeded or Failed"
Jan 28 01:42:24.054: INFO: Pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.624883ms
Jan 28 01:42:26.075: INFO: Pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037667817s
Jan 28 01:42:28.094: INFO: Pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0563583s
Jan 28 01:42:30.122: INFO: Pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.08514794s
STEP: Saw pod success
Jan 28 01:42:30.122: INFO: Pod "var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd" satisfied condition "Succeeded or Failed"
Jan 28 01:42:30.140: INFO: Trying to get logs from node 10.187.128.43 pod var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:42:30.223: INFO: Waiting for pod var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd to disappear
Jan 28 01:42:30.240: INFO: Pod var-expansion-78ec4276-6fb0-4c6b-a044-8f40453a9dcd no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:42:30.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2197" for this suite.

• [SLOW TEST:6.507 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":356,"completed":286,"skipped":5144,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-1581
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:188
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/framework/framework.go:652
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jan 28 01:42:30.699: INFO: starting watch
STEP: patching
STEP: updating
Jan 28 01:42:30.750: INFO: waiting for watch events with expected annotations
Jan 28 01:42:30.750: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:188
Jan 28 01:42:30.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1581" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":356,"completed":287,"skipped":5163,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:42:30.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename cronjob
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in cronjob-9711
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:188
Jan 28 01:44:01.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9711" for this suite.

• [SLOW TEST:90.394 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":356,"completed":288,"skipped":5181,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:01.334: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5022
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1412
STEP: creating an pod
Jan 28 01:44:01.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 28 01:44:01.661: INFO: stderr: ""
Jan 28 01:44:01.661: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for log generator to start.
Jan 28 01:44:01.661: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 28 01:44:01.661: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5022" to be "running and ready, or succeeded"
Jan 28 01:44:01.670: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.494606ms
Jan 28 01:44:03.685: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024474288s
Jan 28 01:44:03.685: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 28 01:44:03.685: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jan 28 01:44:03.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator'
Jan 28 01:44:03.862: INFO: stderr: ""
Jan 28 01:44:03.862: INFO: stdout: "I0128 01:44:03.103540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/gmx 461\nI0128 01:44:03.303661       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/8mg 367\nI0128 01:44:03.504207       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/4q7 250\nI0128 01:44:03.703599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/klcr 504\n"
STEP: limiting log lines
Jan 28 01:44:03.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator --tail=1'
Jan 28 01:44:03.964: INFO: stderr: ""
Jan 28 01:44:03.964: INFO: stdout: "I0128 01:44:03.903996       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/nth8 519\n"
Jan 28 01:44:03.964: INFO: got output "I0128 01:44:03.903996       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/nth8 519\n"
STEP: limiting log bytes
Jan 28 01:44:03.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator --limit-bytes=1'
Jan 28 01:44:04.082: INFO: stderr: ""
Jan 28 01:44:04.082: INFO: stdout: "I"
Jan 28 01:44:04.082: INFO: got output "I"
STEP: exposing timestamps
Jan 28 01:44:04.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 28 01:44:04.184: INFO: stderr: ""
Jan 28 01:44:04.184: INFO: stdout: "2023-01-28T01:44:04.104564695Z I0128 01:44:04.104407       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/hgr4 345\n"
Jan 28 01:44:04.184: INFO: got output "2023-01-28T01:44:04.104564695Z I0128 01:44:04.104407       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/hgr4 345\n"
STEP: restricting to a time range
Jan 28 01:44:06.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator --since=1s'
Jan 28 01:44:06.785: INFO: stderr: ""
Jan 28 01:44:06.785: INFO: stdout: "I0128 01:44:05.904380       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/kx8 494\nI0128 01:44:06.103647       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/j9g 451\nI0128 01:44:06.304011       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/wvmj 210\nI0128 01:44:06.504416       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lfgp 219\nI0128 01:44:06.703713       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/dk2j 381\n"
Jan 28 01:44:06.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 logs logs-generator logs-generator --since=24h'
Jan 28 01:44:06.930: INFO: stderr: ""
Jan 28 01:44:06.930: INFO: stdout: "I0128 01:44:03.103540       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/gmx 461\nI0128 01:44:03.303661       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/8mg 367\nI0128 01:44:03.504207       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/4q7 250\nI0128 01:44:03.703599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/klcr 504\nI0128 01:44:03.903996       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/nth8 519\nI0128 01:44:04.104407       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/hgr4 345\nI0128 01:44:04.303623       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/kx5 338\nI0128 01:44:04.504007       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/95wv 502\nI0128 01:44:04.704373       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/zsx 393\nI0128 01:44:04.903679       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/95qx 570\nI0128 01:44:05.104036       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/st2 541\nI0128 01:44:05.304430       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/9hq8 472\nI0128 01:44:05.503648       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/lr2 549\nI0128 01:44:05.704018       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/jpz 561\nI0128 01:44:05.904380       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/kx8 494\nI0128 01:44:06.103647       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/j9g 451\nI0128 01:44:06.304011       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/wvmj 210\nI0128 01:44:06.504416       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/lfgp 219\nI0128 01:44:06.703713       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/dk2j 381\nI0128 01:44:06.904124       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/lggr 598\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1417
Jan 28 01:44:06.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-5022 delete pod logs-generator'
Jan 28 01:44:08.578: INFO: stderr: ""
Jan 28 01:44:08.578: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:44:08.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5022" for this suite.

• [SLOW TEST:7.286 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1409
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":356,"completed":289,"skipped":5184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9145
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name configmap-test-volume-5d03f0fa-1c68-4da0-95a5-6bc58d748efc
STEP: Creating a pod to test consume configMaps
Jan 28 01:44:08.838: INFO: Waiting up to 5m0s for pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8" in namespace "configmap-9145" to be "Succeeded or Failed"
Jan 28 01:44:08.850: INFO: Pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.130101ms
Jan 28 01:44:10.866: INFO: Pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028541691s
Jan 28 01:44:12.882: INFO: Pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044375609s
Jan 28 01:44:14.897: INFO: Pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058908963s
STEP: Saw pod success
Jan 28 01:44:14.897: INFO: Pod "pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8" satisfied condition "Succeeded or Failed"
Jan 28 01:44:14.906: INFO: Trying to get logs from node 10.187.128.43 pod pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8 container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:44:14.961: INFO: Waiting for pod pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8 to disappear
Jan 28 01:44:14.970: INFO: Pod pod-configmaps-31d424de-6cb1-457e-9685-4e8585d470e8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:44:14.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9145" for this suite.

• [SLOW TEST:6.385 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":356,"completed":290,"skipped":5210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:15.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-2715
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jan 28 01:44:17.298: INFO: running pods: 0 < 3
Jan 28 01:44:19.315: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 28 01:44:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2715" for this suite.

• [SLOW TEST:6.352 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":356,"completed":291,"skipped":5247,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:21.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9374
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:44:21.568: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea" in namespace "downward-api-9374" to be "Succeeded or Failed"
Jan 28 01:44:21.579: INFO: Pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.96791ms
Jan 28 01:44:23.594: INFO: Pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026113547s
Jan 28 01:44:25.610: INFO: Pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041578531s
Jan 28 01:44:27.625: INFO: Pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057018535s
STEP: Saw pod success
Jan 28 01:44:27.625: INFO: Pod "downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea" satisfied condition "Succeeded or Failed"
Jan 28 01:44:27.634: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea container client-container: <nil>
STEP: delete the pod
Jan 28 01:44:27.694: INFO: Waiting for pod downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea to disappear
Jan 28 01:44:27.704: INFO: Pod downwardapi-volume-3272e8b0-0acf-4e36-ae05-b4419a1767ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 01:44:27.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9374" for this suite.

• [SLOW TEST:6.377 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":356,"completed":292,"skipped":5254,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:27.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2932
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating the pod
Jan 28 01:44:27.950: INFO: The status of Pod labelsupdatec8cfc5c6-7e6a-48b3-8e08-4e2ebd576f6b is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:44:29.965: INFO: The status of Pod labelsupdatec8cfc5c6-7e6a-48b3-8e08-4e2ebd576f6b is Running (Ready = true)
Jan 28 01:44:30.571: INFO: Successfully updated pod "labelsupdatec8cfc5c6-7e6a-48b3-8e08-4e2ebd576f6b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:188
Jan 28 01:44:32.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2932" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":356,"completed":293,"skipped":5293,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:32.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9082
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:297
[It] should create and stop a replication controller  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a replication controller
Jan 28 01:44:32.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 create -f -'
Jan 28 01:44:33.177: INFO: stderr: ""
Jan 28 01:44:33.177: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 28 01:44:33.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:44:33.273: INFO: stderr: ""
Jan 28 01:44:33.273: INFO: stdout: "update-demo-nautilus-mhn6d update-demo-nautilus-mpvbl "
Jan 28 01:44:33.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods update-demo-nautilus-mhn6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:44:33.357: INFO: stderr: ""
Jan 28 01:44:33.357: INFO: stdout: ""
Jan 28 01:44:33.357: INFO: update-demo-nautilus-mhn6d is created but not running
Jan 28 01:44:38.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 28 01:44:38.455: INFO: stderr: ""
Jan 28 01:44:38.455: INFO: stdout: "update-demo-nautilus-mhn6d update-demo-nautilus-mpvbl "
Jan 28 01:44:38.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods update-demo-nautilus-mhn6d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:44:38.550: INFO: stderr: ""
Jan 28 01:44:38.550: INFO: stdout: "true"
Jan 28 01:44:38.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods update-demo-nautilus-mhn6d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:44:38.628: INFO: stderr: ""
Jan 28 01:44:38.629: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 01:44:38.629: INFO: validating pod update-demo-nautilus-mhn6d
Jan 28 01:44:38.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:44:38.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:44:38.678: INFO: update-demo-nautilus-mhn6d is verified up and running
Jan 28 01:44:38.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods update-demo-nautilus-mpvbl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 28 01:44:38.768: INFO: stderr: ""
Jan 28 01:44:38.768: INFO: stdout: "true"
Jan 28 01:44:38.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods update-demo-nautilus-mpvbl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 28 01:44:38.857: INFO: stderr: ""
Jan 28 01:44:38.857: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jan 28 01:44:38.857: INFO: validating pod update-demo-nautilus-mpvbl
Jan 28 01:44:38.900: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 28 01:44:38.900: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 28 01:44:38.900: INFO: update-demo-nautilus-mpvbl is verified up and running
STEP: using delete to clean up resources
Jan 28 01:44:38.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 delete --grace-period=0 --force -f -'
Jan 28 01:44:39.027: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:44:39.027: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 28 01:44:39.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get rc,svc -l name=update-demo --no-headers'
Jan 28 01:44:39.138: INFO: stderr: "No resources found in kubectl-9082 namespace.\n"
Jan 28 01:44:39.138: INFO: stdout: ""
Jan 28 01:44:39.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-9082 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 28 01:44:39.219: INFO: stderr: ""
Jan 28 01:44:39.219: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:44:39.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9082" for this suite.

• [SLOW TEST:6.539 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:295
    should create and stop a replication controller  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":356,"completed":294,"skipped":5309,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:39.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-8531
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 28 01:44:39.464: INFO: Waiting up to 5m0s for pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0" in namespace "security-context-8531" to be "Succeeded or Failed"
Jan 28 01:44:39.476: INFO: Pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.203393ms
Jan 28 01:44:41.493: INFO: Pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028494585s
Jan 28 01:44:43.510: INFO: Pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045737121s
Jan 28 01:44:45.525: INFO: Pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060605104s
STEP: Saw pod success
Jan 28 01:44:45.525: INFO: Pod "security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0" satisfied condition "Succeeded or Failed"
Jan 28 01:44:45.535: INFO: Trying to get logs from node 10.187.128.43 pod security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0 container test-container: <nil>
STEP: delete the pod
Jan 28 01:44:45.596: INFO: Waiting for pod security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0 to disappear
Jan 28 01:44:45.606: INFO: Pod security-context-d62c4e1a-ace1-48d5-8218-0fffca77cfb0 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 01:44:45.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-8531" for this suite.

• [SLOW TEST:6.386 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":295,"skipped":5334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:44:45.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5717
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-5717
Jan 28 01:44:45.865: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:44:47.880: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 28 01:44:47.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 28 01:44:48.186: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 28 01:44:48.186: INFO: stdout: "iptables"
Jan 28 01:44:48.186: INFO: proxyMode: iptables
Jan 28 01:44:48.219: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 28 01:44:48.228: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-5717
STEP: creating replication controller affinity-clusterip-timeout in namespace services-5717
I0128 01:44:48.284375      26 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-5717, replica count: 3
I0128 01:44:51.335849      26 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:44:51.370: INFO: Creating new exec pod
Jan 28 01:44:56.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 28 01:44:56.692: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 28 01:44:56.692: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:44:56.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.20.105 80'
Jan 28 01:44:56.952: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.20.105 80\nConnection to 172.21.20.105 80 port [tcp/http] succeeded!\n"
Jan 28 01:44:56.952: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:44:56.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.20.105:80/ ; done'
Jan 28 01:44:57.284: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:44:57.284: INFO: stdout: "\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr\naffinity-clusterip-timeout-4brvr"
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.284: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Received response from host: affinity-clusterip-timeout-4brvr
Jan 28 01:44:57.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.20.105:80/'
Jan 28 01:44:57.574: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:44:57.574: INFO: stdout: "affinity-clusterip-timeout-4brvr"
Jan 28 01:45:17.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.20.105:80/'
Jan 28 01:45:17.865: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:45:17.865: INFO: stdout: "affinity-clusterip-timeout-4brvr"
Jan 28 01:45:37.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.20.105:80/'
Jan 28 01:45:38.168: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:45:38.168: INFO: stdout: "affinity-clusterip-timeout-4brvr"
Jan 28 01:45:58.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.20.105:80/'
Jan 28 01:45:58.459: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:45:58.459: INFO: stdout: "affinity-clusterip-timeout-4brvr"
Jan 28 01:46:18.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-5717 exec execpod-affinitymkb7x -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.20.105:80/'
Jan 28 01:46:18.764: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.20.105:80/\n"
Jan 28 01:46:18.764: INFO: stdout: "affinity-clusterip-timeout-qngjl"
Jan 28 01:46:18.764: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-5717, will wait for the garbage collector to delete the pods
Jan 28 01:46:18.892: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 26.658343ms
Jan 28 01:46:18.992: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.335522ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:46:21.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5717" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:96.257 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":356,"completed":296,"skipped":5359,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:21.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8421
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should create and stop a working application  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating all guestbook components
Jan 28 01:46:22.086: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 28 01:46:22.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:22.376: INFO: stderr: ""
Jan 28 01:46:22.376: INFO: stdout: "service/agnhost-replica created\n"
Jan 28 01:46:22.376: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 28 01:46:22.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:22.681: INFO: stderr: ""
Jan 28 01:46:22.681: INFO: stdout: "service/agnhost-primary created\n"
Jan 28 01:46:22.681: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 28 01:46:22.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:22.912: INFO: stderr: ""
Jan 28 01:46:22.912: INFO: stdout: "service/frontend created\n"
Jan 28 01:46:22.912: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 28 01:46:22.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:23.153: INFO: stderr: ""
Jan 28 01:46:23.153: INFO: stdout: "deployment.apps/frontend created\n"
Jan 28 01:46:23.153: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 28 01:46:23.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:23.400: INFO: stderr: ""
Jan 28 01:46:23.400: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 28 01:46:23.400: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 28 01:46:23.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 create -f -'
Jan 28 01:46:23.633: INFO: stderr: ""
Jan 28 01:46:23.633: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jan 28 01:46:23.634: INFO: Waiting for all frontend pods to be Running.
Jan 28 01:46:28.687: INFO: Waiting for frontend to serve content.
Jan 28 01:46:29.761: INFO: Trying to add a new entry to the guestbook.
Jan 28 01:46:29.858: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 28 01:46:29.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.054: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.054: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jan 28 01:46:30.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.206: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.206: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 28 01:46:30.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.391: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.391: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 28 01:46:30.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.543: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.543: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 28 01:46:30.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.674: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.674: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jan 28 01:46:30.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8421 delete --grace-period=0 --force -f -'
Jan 28 01:46:30.792: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 28 01:46:30.792: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:46:30.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8421" for this suite.

• [SLOW TEST:8.932 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:340
    should create and stop a working application  [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":356,"completed":297,"skipped":5395,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:30.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-431
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:46:31.731: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:46:33.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 46, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 46, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 46, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 46, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:46:36.879: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:46:36.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-431" for this suite.
STEP: Destroying namespace "webhook-431-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.308 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":356,"completed":298,"skipped":5422,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:37.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5856
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:46:37.818: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 28 01:46:39.883: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 28, 1, 46, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 46, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 28, 1, 46, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 28, 1, 46, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-68c7bd4684\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:46:42.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:46:43.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5856" for this suite.
STEP: Destroying namespace "webhook-5856-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104

• [SLOW TEST:6.691 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":356,"completed":299,"skipped":5428,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:43.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3189
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:46:44.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:46:47.572: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:46:47.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3189" for this suite.
STEP: Destroying namespace "webhook-3189-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":356,"completed":300,"skipped":5438,"failed":0}

------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:48.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7175
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:46:50.312: INFO: Deleting pod "var-expansion-7b189893-b7d0-44b3-8efc-f9220d121fdc" in namespace "var-expansion-7175"
Jan 28 01:46:50.346: INFO: Wait up to 5m0s for pod "var-expansion-7b189893-b7d0-44b3-8efc-f9220d121fdc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:46:54.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7175" for this suite.

• [SLOW TEST:6.413 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":356,"completed":301,"skipped":5438,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:46:54.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5923
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 28 01:46:54.792: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:46:54.792: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:46:55.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:46:55.867: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:46:56.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 28 01:46:56.847: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:46:57.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:46:57.837: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 28 01:46:57.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:46:57.943: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:46:58.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:46:58.987: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:46:59.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:46:59.993: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:47:01.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 28 01:47:01.013: INFO: Node 10.187.128.27 is running 0 daemon pod, expected 1
Jan 28 01:47:01.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan 28 01:47:01.994: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5923, will wait for the garbage collector to delete the pods
Jan 28 01:47:02.157: INFO: Deleting DaemonSet.extensions daemon-set took: 81.815591ms
Jan 28 01:47:02.258: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.031695ms
Jan 28 01:47:05.085: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 28 01:47:05.085: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 28 01:47:05.103: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"50437"},"items":null}

Jan 28 01:47:05.120: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"50437"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:47:05.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5923" for this suite.

• [SLOW TEST:10.790 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":356,"completed":302,"skipped":5461,"failed":0}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:05.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8020
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward api env vars
Jan 28 01:47:05.480: INFO: Waiting up to 5m0s for pod "downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba" in namespace "downward-api-8020" to be "Succeeded or Failed"
Jan 28 01:47:05.497: INFO: Pod "downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.681079ms
Jan 28 01:47:07.527: INFO: Pod "downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04670607s
Jan 28 01:47:09.560: INFO: Pod "downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079185397s
STEP: Saw pod success
Jan 28 01:47:09.560: INFO: Pod "downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba" satisfied condition "Succeeded or Failed"
Jan 28 01:47:09.577: INFO: Trying to get logs from node 10.187.128.43 pod downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:47:09.758: INFO: Waiting for pod downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba to disappear
Jan 28 01:47:09.776: INFO: Pod downward-api-98f9805d-9dd2-4245-8c37-b8c7351157ba no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:188
Jan 28 01:47:09.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8020" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":356,"completed":303,"skipped":5468,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:09.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-1555
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
STEP: reading a file in the container
Jan 28 01:47:14.160: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1555 pod-service-account-e9a48111-53d0-4fda-a66c-2b8cc6eb5575 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 28 01:47:14.459: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1555 pod-service-account-e9a48111-53d0-4fda-a66c-2b8cc6eb5575 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 28 01:47:14.763: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1555 pod-service-account-e9a48111-53d0-4fda-a66c-2b8cc6eb5575 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 28 01:47:15.031: INFO: Got root ca configmap in namespace "svcaccounts-1555"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 01:47:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1555" for this suite.

• [SLOW TEST:5.283 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":356,"completed":304,"skipped":5480,"failed":0}
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:15.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1506
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:47:19.537: INFO: DNS probes using dns-test-ff2b78c7-d80c-4c96-9a22-aa4d93f2f2eb succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:47:23.826: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:23.850: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:23.850: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:28.899: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:28.939: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:28.939: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:33.876: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:33.902: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:33.902: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:38.878: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:38.901: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:38.901: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:43.873: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:43.895: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:43.895: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:48.875: INFO: File wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:48.900: INFO: File jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local from pod  dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 28 01:47:48.900: INFO: Lookups using dns-1506/dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e failed for: [wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local]

Jan 28 01:47:53.898: INFO: DNS probes using dns-test-89199c21-4af0-4f17-9418-1c9e20edfd1e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1506.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1506.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:47:58.242: INFO: DNS probes using dns-test-9fc69567-522c-4c61-8ee2-2550f4e141c9 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:47:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1506" for this suite.

• [SLOW TEST:43.326 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":356,"completed":305,"skipped":5480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:58.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3639
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:188
Jan 28 01:47:58.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3639" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":356,"completed":306,"skipped":5517,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:58.853: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8859
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 28 01:47:59.170: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8859  87b9447d-5ba4-400a-9f5e-87e90d792c01 50710 0 2023-01-28 01:47:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-28 01:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 28 01:47:59.170: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8859  87b9447d-5ba4-400a-9f5e-87e90d792c01 50711 0 2023-01-28 01:47:59 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2023-01-28 01:47:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:188
Jan 28 01:47:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8859" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":356,"completed":307,"skipped":5568,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:47:59.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3784
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
Jan 28 01:47:59.417: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:188
Jan 28 01:48:04.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3784" for this suite.

• [SLOW TEST:5.205 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":356,"completed":308,"skipped":5628,"failed":0}
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:04.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6813
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test substitution in container's args
Jan 28 01:48:04.658: INFO: Waiting up to 5m0s for pod "var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20" in namespace "var-expansion-6813" to be "Succeeded or Failed"
Jan 28 01:48:04.677: INFO: Pod "var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20": Phase="Pending", Reason="", readiness=false. Elapsed: 18.800243ms
Jan 28 01:48:06.712: INFO: Pod "var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053725569s
Jan 28 01:48:08.737: INFO: Pod "var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078684074s
STEP: Saw pod success
Jan 28 01:48:08.737: INFO: Pod "var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20" satisfied condition "Succeeded or Failed"
Jan 28 01:48:08.758: INFO: Trying to get logs from node 10.187.128.43 pod var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20 container dapi-container: <nil>
STEP: delete the pod
Jan 28 01:48:08.854: INFO: Waiting for pod var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20 to disappear
Jan 28 01:48:08.870: INFO: Pod var-expansion-a5416e2b-8ab4-41aa-b816-f563d44f0d20 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:48:08.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6813" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":356,"completed":309,"skipped":5628,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1349
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-e425cea3-1069-4a24-8875-bfe79aa75182-5839
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:48:09.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1349" for this suite.
STEP: Destroying namespace "nspatchtest-e425cea3-1069-4a24-8875-bfe79aa75182-5839" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":356,"completed":310,"skipped":5630,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:09.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2090
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:48:09.635: INFO: Creating simple deployment test-new-deployment
Jan 28 01:48:09.694: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:48:11.948: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2090  bf2eef08-41b7-435c-b5b3-e2ab4899d2a8 50908 3 2023-01-28 01:48:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-28 01:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c0e3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-55df494869" has successfully progressed.,LastUpdateTime:2023-01-28 01:48:11 +0000 UTC,LastTransitionTime:2023-01-28 01:48:09 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-28 01:48:11 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:48:11.969: INFO: New ReplicaSet "test-new-deployment-55df494869" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-55df494869  deployment-2090  233abbdb-a30d-4365-997a-b0b74e0bb14f 50917 3 2023-01-28 01:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment bf2eef08-41b7-435c-b5b3-e2ab4899d2a8 0xc0042cf357 0xc0042cf358}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf2eef08-41b7-435c-b5b3-e2ab4899d2a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 55df494869,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042cf3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:4,FullyLabeledReplicas:4,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:48:11.990: INFO: Pod "test-new-deployment-55df494869-8ts8f" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-8ts8f test-new-deployment-55df494869- deployment-2090  e290fc05-f927-439d-aa67-7de255b13f03 50903 0 2023-01-28 01:48:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 233abbdb-a30d-4365-997a-b0b74e0bb14f 0xc0042cf7e7 0xc0042cf7e8}] []  [{kube-controller-manager Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233abbdb-a30d-4365-997a-b0b74e0bb14f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pljdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pljdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.27,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.27,PodIP:,StartTime:2023-01-28 01:48:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:48:11.990: INFO: Pod "test-new-deployment-55df494869-bxnws" is available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-bxnws test-new-deployment-55df494869- deployment-2090  9879a4f8-7c2d-42bc-9e01-67e6500ad909 50889 0 2023-01-28 01:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[cni.projectcalico.org/containerID:4eee9816daf4d4f20d9c535f3ce52fa3dd707f3f84c8d27355dccb6fff6b009f cni.projectcalico.org/podIP:172.30.90.122/32 cni.projectcalico.org/podIPs:172.30.90.122/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 233abbdb-a30d-4365-997a-b0b74e0bb14f 0xc0042cf9e7 0xc0042cf9e8}] []  [{kube-controller-manager Update v1 2023-01-28 01:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233abbdb-a30d-4365-997a-b0b74e0bb14f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2t4p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2t4p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.122,StartTime:2023-01-28 01:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:48:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://376ad1921f3f1528e6886f95590449802460927ae4c9349904120cb0e12f2a01,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.122,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:48:11.991: INFO: Pod "test-new-deployment-55df494869-pjccn" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-pjccn test-new-deployment-55df494869- deployment-2090  f514b3d8-c3dc-4c15-92c5-cda616157472 50920 0 2023-01-28 01:48:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 233abbdb-a30d-4365-997a-b0b74e0bb14f 0xc0042cfbf7 0xc0042cfbf8}] []  [{kube-controller-manager Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233abbdb-a30d-4365-997a-b0b74e0bb14f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7gqgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7gqgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:,StartTime:2023-01-28 01:48:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 28 01:48:11.991: INFO: Pod "test-new-deployment-55df494869-sk66p" is not available:
&Pod{ObjectMeta:{test-new-deployment-55df494869-sk66p test-new-deployment-55df494869- deployment-2090  d7beb9e2-d698-433e-baa3-5749a636981f 50909 0 2023-01-28 01:48:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:55df494869] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-new-deployment-55df494869 233abbdb-a30d-4365-997a-b0b74e0bb14f 0xc0042cfdc7 0xc0042cfdc8}] []  [{kube-controller-manager Update v1 2023-01-28 01:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"233abbdb-a30d-4365-997a-b0b74e0bb14f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55qlj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55qlj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.30,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:48:11.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2090" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":356,"completed":311,"skipped":5650,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:12.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6705
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 28 01:48:12.304: INFO: Waiting up to 5m0s for pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04" in namespace "emptydir-6705" to be "Succeeded or Failed"
Jan 28 01:48:12.321: INFO: Pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04": Phase="Pending", Reason="", readiness=false. Elapsed: 17.079388ms
Jan 28 01:48:14.345: INFO: Pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041701232s
Jan 28 01:48:16.374: INFO: Pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069982306s
Jan 28 01:48:18.400: INFO: Pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095920199s
STEP: Saw pod success
Jan 28 01:48:18.400: INFO: Pod "pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04" satisfied condition "Succeeded or Failed"
Jan 28 01:48:18.417: INFO: Trying to get logs from node 10.187.128.43 pod pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04 container test-container: <nil>
STEP: delete the pod
Jan 28 01:48:18.512: INFO: Waiting for pod pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04 to disappear
Jan 28 01:48:18.530: INFO: Pod pod-59b0cd45-0d7a-48ba-a52e-612c70a73f04 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:48:18.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6705" for this suite.

• [SLOW TEST:6.541 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":312,"skipped":5665,"failed":0}
S
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:18.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5080
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jan 28 01:48:18.829: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 28 01:48:23.857: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jan 28 01:48:23.880: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:188
Jan 28 01:48:23.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5080" for this suite.

• [SLOW TEST:5.481 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":356,"completed":313,"skipped":5666,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:24.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7950
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
STEP: set up a multi version CRD
Jan 28 01:48:24.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:48:43.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7950" for this suite.

• [SLOW TEST:19.690 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":356,"completed":314,"skipped":5672,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:48:43.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-610
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-610
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/framework/framework.go:652
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-610
STEP: Waiting until pod test-pod will start running in namespace statefulset-610
STEP: Creating statefulset with conflicting port in namespace statefulset-610
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-610
Jan 28 01:48:48.216: INFO: Observed stateful pod in namespace: statefulset-610, name: ss-0, uid: ecb91f9b-818c-432b-9bc5-88c6d39c9ce0, status phase: Pending. Waiting for statefulset controller to delete.
Jan 28 01:48:48.268: INFO: Observed stateful pod in namespace: statefulset-610, name: ss-0, uid: ecb91f9b-818c-432b-9bc5-88c6d39c9ce0, status phase: Failed. Waiting for statefulset controller to delete.
Jan 28 01:48:48.285: INFO: Observed stateful pod in namespace: statefulset-610, name: ss-0, uid: ecb91f9b-818c-432b-9bc5-88c6d39c9ce0, status phase: Failed. Waiting for statefulset controller to delete.
Jan 28 01:48:48.295: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-610
STEP: Removing pod with conflicting port in namespace statefulset-610
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-610 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 28 01:48:52.466: INFO: Deleting all statefulset in ns statefulset-610
Jan 28 01:48:52.478: INFO: Scaling statefulset ss to 0
Jan 28 01:49:02.587: INFO: Waiting for statefulset status.replicas updated to 0
Jan 28 01:49:02.601: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:188
Jan 28 01:49:02.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-610" for this suite.

• [SLOW TEST:18.963 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":356,"completed":315,"skipped":5673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:02.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5979
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:49:03.042: INFO: The status of Pod pod-secrets-fd8b3ac3-201b-4438-8828-6dbb7b9a6f09 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:49:05.094: INFO: The status of Pod pod-secrets-fd8b3ac3-201b-4438-8828-6dbb7b9a6f09 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:49:07.072: INFO: The status of Pod pod-secrets-fd8b3ac3-201b-4438-8828-6dbb7b9a6f09 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:188
Jan 28 01:49:07.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5979" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":356,"completed":316,"skipped":5700,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:07.244: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-114
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 28 01:49:07.487: INFO: Waiting up to 5m0s for pod "pod-5274232b-2aa5-4952-a513-291cd34fa463" in namespace "emptydir-114" to be "Succeeded or Failed"
Jan 28 01:49:07.505: INFO: Pod "pod-5274232b-2aa5-4952-a513-291cd34fa463": Phase="Pending", Reason="", readiness=false. Elapsed: 18.526837ms
Jan 28 01:49:09.536: INFO: Pod "pod-5274232b-2aa5-4952-a513-291cd34fa463": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049214881s
Jan 28 01:49:11.587: INFO: Pod "pod-5274232b-2aa5-4952-a513-291cd34fa463": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100053628s
STEP: Saw pod success
Jan 28 01:49:11.587: INFO: Pod "pod-5274232b-2aa5-4952-a513-291cd34fa463" satisfied condition "Succeeded or Failed"
Jan 28 01:49:11.602: INFO: Trying to get logs from node 10.187.128.43 pod pod-5274232b-2aa5-4952-a513-291cd34fa463 container test-container: <nil>
STEP: delete the pod
Jan 28 01:49:11.690: INFO: Waiting for pod pod-5274232b-2aa5-4952-a513-291cd34fa463 to disappear
Jan 28 01:49:11.706: INFO: Pod pod-5274232b-2aa5-4952-a513-291cd34fa463 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:49:11.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-114" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":317,"skipped":5733,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9607
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 28 01:49:12.022: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 28 01:49:12.084: INFO: Waiting for terminating namespaces to be deleted...
Jan 28 01:49:12.099: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.27 before test
Jan 28 01:49:12.146: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-dd2tx from ibm-system started at 2023-01-27 22:43:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 01:49:12.146: INFO: calico-node-9md4k from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:49:12.146: INFO: calico-typha-987c59c59-r2mxh from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:49:12.146: INFO: coredns-649f45bb5-4l67p from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:49:12.146: INFO: coredns-649f45bb5-bkr22 from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:49:12.146: INFO: dashboard-metrics-scraper-f74668d5f-bscb6 from kube-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
Jan 28 01:49:12.146: INFO: ibm-keepalived-watcher-qxwck from kube-system started at 2023-01-27 22:02:35 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:49:12.146: INFO: ibm-master-proxy-static-10.187.128.27 from kube-system started at 2023-01-27 22:02:32 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:49:12.146: INFO: ibmcloud-block-storage-driver-mfqhd from kube-system started at 2023-01-27 22:02:42 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:49:12.146: INFO: ingress-cluster-healthcheck-56756684f7-qbpsl from kube-system started at 2023-01-27 22:43:38 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container ingress-cluster-healthcheck ready: true, restart count 0
Jan 28 01:49:12.146: INFO: konnectivity-agent-8ws48 from kube-system started at 2023-01-27 22:37:37 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:49:12.146: INFO: metrics-server-666774474b-hfjbk from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:49:12.146: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-s29r8 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:49:12.146: INFO: sonobuoy-e2e-job-68fa4b4566bd42ca from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container e2e ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:49:12.146: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-p8m8d from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:49:12.146: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.30 before test
Jan 28 01:49:12.195: INFO: ibm-cloud-provider-ip-169-60-157-27-5bb6ccdf95-s6m9x from ibm-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.195: INFO: 	Container ibm-cloud-provider-ip-169-60-157-27 ready: true, restart count 0
Jan 28 01:49:12.195: INFO: calico-kube-controllers-6c58444bc9-hfrrk from kube-system started at 2023-01-28 00:35:28 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.195: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 28 01:49:12.195: INFO: calico-node-lg9x8 from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.195: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:49:12.195: INFO: calico-typha-987c59c59-qs9ww from kube-system started at 2023-01-27 22:03:10 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.195: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:49:12.195: INFO: coredns-649f45bb5-9gxmz from kube-system started at 2023-01-27 22:38:01 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container coredns ready: true, restart count 0
Jan 28 01:49:12.196: INFO: coredns-autoscaler-64db77d767-q9gk5 from kube-system started at 2023-01-28 00:35:29 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container autoscaler ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibm-file-plugin-6b4fdfc7d8-gh8bd from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibm-keepalived-watcher-p94jf from kube-system started at 2023-01-27 22:02:52 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibm-master-proxy-static-10.187.128.30 from kube-system started at 2023-01-27 22:02:39 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:49:12.196: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibm-storage-watcher-6cd9b56547-d7wvk from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibmcloud-block-storage-driver-mqmrs from kube-system started at 2023-01-27 22:03:00 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:49:12.196: INFO: ibmcloud-block-storage-plugin-5f9c77dc8c-phr4k from kube-system started at 2023-01-28 01:26:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jan 28 01:49:12.196: INFO: konnectivity-agent-nbg9q from kube-system started at 2023-01-27 22:37:30 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:49:12.196: INFO: kubernetes-dashboard-65d4f9ccbf-52vtb from kube-system started at 2023-01-28 00:35:28 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 28 01:49:12.196: INFO: metrics-server-666774474b-x2b75 from kube-system started at 2023-01-27 23:04:40 +0000 UTC (3 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container config-watcher ready: true, restart count 0
Jan 28 01:49:12.196: INFO: 	Container metrics-server ready: true, restart count 0
Jan 28 01:49:12.196: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan 28 01:49:12.196: INFO: public-crcfa4d0ad0pkltdfhcsmg-alb1-5cd49d8cb4-grkj5 from kube-system started at 2023-01-27 22:44:32 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.196: INFO: 	Container nginx-ingress ready: true, restart count 0
Jan 28 01:49:12.197: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-2j8w6 from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.197: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:49:12.197: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 28 01:49:12.197: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-01-27 22:33:03 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.197: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jan 28 01:49:12.197: INFO: 
Logging pods the apiserver thinks is on node 10.187.128.43 before test
Jan 28 01:49:12.236: INFO: calico-node-tfsm6 from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container calico-node ready: true, restart count 0
Jan 28 01:49:12.236: INFO: calico-typha-987c59c59-zd57m from kube-system started at 2023-01-28 01:26:40 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container calico-typha ready: true, restart count 0
Jan 28 01:49:12.236: INFO: ibm-keepalived-watcher-9flqg from kube-system started at 2023-01-27 22:02:41 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jan 28 01:49:12.236: INFO: ibm-master-proxy-static-10.187.128.43 from kube-system started at 2023-01-27 22:02:29 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jan 28 01:49:12.236: INFO: 	Container pause ready: true, restart count 0
Jan 28 01:49:12.236: INFO: ibmcloud-block-storage-driver-d4h67 from kube-system started at 2023-01-27 22:02:49 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jan 28 01:49:12.236: INFO: konnectivity-agent-zrrn4 from kube-system started at 2023-01-27 22:37:33 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 28 01:49:12.236: INFO: sonobuoy from sonobuoy started at 2023-01-28 00:13:19 +0000 UTC (1 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 28 01:49:12.236: INFO: sonobuoy-systemd-logs-daemon-set-85a480d64ae24f9e-m6nkr from sonobuoy started at 2023-01-28 00:13:23 +0000 UTC (2 container statuses recorded)
Jan 28 01:49:12.236: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 28 01:49:12.236: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2c28649c-1feb-4bcb-9593-d5ff3cbe9cdc 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-2c28649c-1feb-4bcb-9593-d5ff3cbe9cdc off the node 10.187.128.43
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2c28649c-1feb-4bcb-9593-d5ff3cbe9cdc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:188
Jan 28 01:49:20.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9607" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.880 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":356,"completed":318,"skipped":5737,"failed":0}
SSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:20.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2817
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:188
Jan 28 01:49:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2817" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":356,"completed":319,"skipped":5740,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4179
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Starting the proxy
Jan 28 01:49:21.156: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-4179 proxy --unix-socket=/tmp/kubectl-proxy-unix1355212112/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:49:21.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4179" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":356,"completed":320,"skipped":5760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-708
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:49:21.511: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 28 01:49:25.558: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 28 01:49:29.711: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-708  ddab1b00-21e3-4094-8704-bc2cc67ee915 51529 1 2023-01-28 01:49:25 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2023-01-28 01:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:49:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fc4e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-28 01:49:25 +0000 UTC,LastTransitionTime:2023-01-28 01:49:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-6755c7b765" has successfully progressed.,LastUpdateTime:2023-01-28 01:49:27 +0000 UTC,LastTransitionTime:2023-01-28 01:49:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 28 01:49:29.729: INFO: New ReplicaSet "test-cleanup-deployment-6755c7b765" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-6755c7b765  deployment-708  5068e83f-7fe0-4ce1-bef7-0446cff585fe 51518 1 2023-01-28 01:49:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment ddab1b00-21e3-4094-8704-bc2cc67ee915 0xc004f29c57 0xc004f29c58}] []  [{kube-controller-manager Update apps/v1 2023-01-28 01:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddab1b00-21e3-4094-8704-bc2cc67ee915\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-28 01:49:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6755c7b765,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f29d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 28 01:49:29.747: INFO: Pod "test-cleanup-deployment-6755c7b765-d2h88" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-6755c7b765-d2h88 test-cleanup-deployment-6755c7b765- deployment-708  e0ee7f90-5f02-4122-8cda-930cc1c5dcaa 51516 0 2023-01-28 01:49:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:6755c7b765] map[cni.projectcalico.org/containerID:b38caa25a213eea5abacbf711ddad37787deea14f20100722f73401716ce8a7b cni.projectcalico.org/podIP:172.30.90.81/32 cni.projectcalico.org/podIPs:172.30.90.81/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-6755c7b765 5068e83f-7fe0-4ce1-bef7-0446cff585fe 0xc004fc52c7 0xc004fc52c8}] []  [{kube-controller-manager Update v1 2023-01-28 01:49:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5068e83f-7fe0-4ce1-bef7-0446cff585fe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-28 01:49:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-28 01:49:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.90.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w2kr2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2kr2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.187.128.43,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*600,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:49:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:49:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-28 01:49:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.187.128.43,PodIP:172.30.90.81,StartTime:2023-01-28 01:49:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-28 01:49:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:containerd://ca9cfcadc4b7c9d374bbb28eadca76be2b09ec154eaeff3699a0ce72cf855338,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.90.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:188
Jan 28 01:49:29.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-708" for this suite.

• [SLOW TEST:8.557 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":356,"completed":321,"skipped":5788,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:29.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1423
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:49:30.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774" in namespace "projected-1423" to be "Succeeded or Failed"
Jan 28 01:49:30.086: INFO: Pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774": Phase="Pending", Reason="", readiness=false. Elapsed: 16.489804ms
Jan 28 01:49:32.120: INFO: Pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050553814s
Jan 28 01:49:34.141: INFO: Pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071684546s
Jan 28 01:49:36.165: INFO: Pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095279867s
STEP: Saw pod success
Jan 28 01:49:36.165: INFO: Pod "downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774" satisfied condition "Succeeded or Failed"
Jan 28 01:49:36.181: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774 container client-container: <nil>
STEP: delete the pod
Jan 28 01:49:36.276: INFO: Waiting for pod downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774 to disappear
Jan 28 01:49:36.293: INFO: Pod downwardapi-volume-fe5249d7-8968-4f05-babb-eae0fcfa0774 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:49:36.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1423" for this suite.

• [SLOW TEST:6.544 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":356,"completed":322,"skipped":5815,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:36.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6683
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-e2e10640-23d3-4c32-8108-6d6040ada883 in namespace container-probe-6683
Jan 28 01:49:40.620: INFO: Started pod liveness-e2e10640-23d3-4c32-8108-6d6040ada883 in namespace container-probe-6683
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 01:49:40.639: INFO: Initial restart count of pod liveness-e2e10640-23d3-4c32-8108-6d6040ada883 is 0
Jan 28 01:49:58.947: INFO: Restart count of pod container-probe-6683/liveness-e2e10640-23d3-4c32-8108-6d6040ada883 is now 1 (18.308047373s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:49:58.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6683" for this suite.

• [SLOW TEST:22.698 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":356,"completed":323,"skipped":5830,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:49:59.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-442
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 28 01:49:59.347: INFO: Waiting up to 5m0s for pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e" in namespace "emptydir-442" to be "Succeeded or Failed"
Jan 28 01:49:59.364: INFO: Pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.504099ms
Jan 28 01:50:01.395: INFO: Pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048153725s
Jan 28 01:50:03.428: INFO: Pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080709004s
Jan 28 01:50:05.455: INFO: Pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107946264s
STEP: Saw pod success
Jan 28 01:50:05.455: INFO: Pod "pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e" satisfied condition "Succeeded or Failed"
Jan 28 01:50:05.472: INFO: Trying to get logs from node 10.187.128.43 pod pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e container test-container: <nil>
STEP: delete the pod
Jan 28 01:50:05.595: INFO: Waiting for pod pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e to disappear
Jan 28 01:50:05.611: INFO: Pod pod-c83c81ee-3d89-409f-bb37-ff0a09cc896e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:50:05.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-442" for this suite.

• [SLOW TEST:6.616 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":324,"skipped":5844,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:05.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9439
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test downward API volume plugin
Jan 28 01:50:05.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec" in namespace "projected-9439" to be "Succeeded or Failed"
Jan 28 01:50:05.921: INFO: Pod "downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.171573ms
Jan 28 01:50:07.953: INFO: Pod "downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049698468s
Jan 28 01:50:09.984: INFO: Pod "downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081188289s
STEP: Saw pod success
Jan 28 01:50:09.984: INFO: Pod "downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec" satisfied condition "Succeeded or Failed"
Jan 28 01:50:10.002: INFO: Trying to get logs from node 10.187.128.43 pod downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec container client-container: <nil>
STEP: delete the pod
Jan 28 01:50:10.111: INFO: Waiting for pod downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec to disappear
Jan 28 01:50:10.130: INFO: Pod downwardapi-volume-f16423b3-014b-4fe7-ba4f-4c0398c95bec no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:188
Jan 28 01:50:10.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9439" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":356,"completed":325,"skipped":5852,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:10.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8797
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:245
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating Agnhost RC
Jan 28 01:50:10.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8797 create -f -'
Jan 28 01:50:11.013: INFO: stderr: ""
Jan 28 01:50:11.013: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jan 28 01:50:12.040: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:50:12.040: INFO: Found 0 / 1
Jan 28 01:50:13.041: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:50:13.041: INFO: Found 1 / 1
Jan 28 01:50:13.041: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 28 01:50:13.068: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:50:13.068: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 28 01:50:13.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=kubectl-8797 patch pod agnhost-primary-4sk6z -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 28 01:50:13.193: INFO: stderr: ""
Jan 28 01:50:13.193: INFO: stdout: "pod/agnhost-primary-4sk6z patched\n"
STEP: checking annotations
Jan 28 01:50:13.214: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 28 01:50:13.214: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:188
Jan 28 01:50:13.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8797" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":356,"completed":326,"skipped":5860,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:13.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename security-context
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-1323
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jan 28 01:50:13.503: INFO: Waiting up to 5m0s for pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b" in namespace "security-context-1323" to be "Succeeded or Failed"
Jan 28 01:50:13.520: INFO: Pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.076513ms
Jan 28 01:50:15.549: INFO: Pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045939174s
Jan 28 01:50:17.581: INFO: Pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0783716s
Jan 28 01:50:19.610: INFO: Pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.107195571s
STEP: Saw pod success
Jan 28 01:50:19.610: INFO: Pod "security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b" satisfied condition "Succeeded or Failed"
Jan 28 01:50:19.625: INFO: Trying to get logs from node 10.187.128.43 pod security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b container test-container: <nil>
STEP: delete the pod
Jan 28 01:50:19.707: INFO: Waiting for pod security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b to disappear
Jan 28 01:50:19.726: INFO: Pod security-context-348251a4-7216-4a4a-8917-0a2dbd7d1f0b no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:188
Jan 28 01:50:19.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1323" for this suite.

• [SLOW TEST:6.509 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":356,"completed":327,"skipped":5930,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:19.771: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4827
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jan 28 01:50:22.105: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4827 PodName:var-expansion-39568ce6-302a-4a74-b130-483eb2186140 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:50:22.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:50:22.106: INFO: ExecWithOptions: Clientset creation
Jan 28 01:50:22.106: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-4827/pods/var-expansion-39568ce6-302a-4a74-b130-483eb2186140/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path
Jan 28 01:50:22.357: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4827 PodName:var-expansion-39568ce6-302a-4a74-b130-483eb2186140 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 28 01:50:22.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
Jan 28 01:50:22.358: INFO: ExecWithOptions: Clientset creation
Jan 28 01:50:22.358: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-4827/pods/var-expansion-39568ce6-302a-4a74-b130-483eb2186140/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value
Jan 28 01:50:23.120: INFO: Successfully updated pod "var-expansion-39568ce6-302a-4a74-b130-483eb2186140"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jan 28 01:50:23.144: INFO: Deleting pod "var-expansion-39568ce6-302a-4a74-b130-483eb2186140" in namespace "var-expansion-4827"
Jan 28 01:50:23.178: INFO: Wait up to 5m0s for pod "var-expansion-39568ce6-302a-4a74-b130-483eb2186140" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:188
Jan 28 01:50:57.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4827" for this suite.

• [SLOW TEST:37.509 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":356,"completed":328,"skipped":5951,"failed":0}
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:57.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7290
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  test/e2e/framework/framework.go:652
STEP: Create set of pod templates
Jan 28 01:50:57.522: INFO: created test-podtemplate-1
Jan 28 01:50:57.541: INFO: created test-podtemplate-2
Jan 28 01:50:57.559: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jan 28 01:50:57.577: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jan 28 01:50:57.651: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:188
Jan 28 01:50:57.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7290" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":356,"completed":329,"skipped":5951,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:50:57.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9073
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 28 01:51:00.116: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.139: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.163: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.187: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.212: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.236: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.259: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.284: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:00.284: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:05.358: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:05.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:05.448: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:05.473: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:05.473: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:10.357: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:10.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:10.448: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:10.472: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:10.472: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:15.358: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:15.381: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:15.476: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:15.499: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:15.500: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:20.361: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:20.388: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:20.459: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:20.487: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:20.487: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:25.355: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:25.379: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:25.447: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:25.473: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local from pod dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e: the server could not find the requested resource (get pods dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e)
Jan 28 01:51:25.473: INFO: Lookups using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e failed for: [wheezy_udp@dns-test-service-2.dns-9073.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9073.svc.cluster.local jessie_udp@dns-test-service-2.dns-9073.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9073.svc.cluster.local]

Jan 28 01:51:30.475: INFO: DNS probes using dns-9073/dns-test-675c82e4-ff1e-4ea9-8212-d00a2833264e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:188
Jan 28 01:51:30.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9073" for this suite.

• [SLOW TEST:32.961 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":356,"completed":330,"skipped":5962,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:51:30.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4654
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:51:31.405: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:51:34.511: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:51:34.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:51:38.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4654" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139

• [SLOW TEST:7.591 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":356,"completed":331,"skipped":5968,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:51:38.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6168
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating service in namespace services-6168
STEP: creating service affinity-nodeport-transition in namespace services-6168
STEP: creating replication controller affinity-nodeport-transition in namespace services-6168
I0128 01:51:38.581384      26 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6168, replica count: 3
I0128 01:51:41.632501      26 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:51:41.698: INFO: Creating new exec pod
Jan 28 01:51:46.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 28 01:51:47.126: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 28 01:51:47.126: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:51:47.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.195.222 80'
Jan 28 01:51:47.410: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.195.222 80\nConnection to 172.21.195.222 80 port [tcp/http] succeeded!\n"
Jan 28 01:51:47.410: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:51:47.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.30 31873'
Jan 28 01:51:47.733: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.30 31873\nConnection to 10.187.128.30 31873 port [tcp/*] succeeded!\n"
Jan 28 01:51:47.733: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:51:47.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.43 31873'
Jan 28 01:51:48.032: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.43 31873\nConnection to 10.187.128.43 31873 port [tcp/*] succeeded!\n"
Jan 28 01:51:48.032: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 28 01:51:48.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.187.128.27:31873/ ; done'
Jan 28 01:51:48.460: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n"
Jan 28 01:51:48.460: INFO: stdout: "\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl"
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:51:48.460: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.187.128.27:31873/ ; done'
Jan 28 01:52:18.830: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n"
Jan 28 01:52:18.830: INFO: stdout: "\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-bplnl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-mkbpm\naffinity-nodeport-transition-bplnl"
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-mkbpm
Jan 28 01:52:18.830: INFO: Received response from host: affinity-nodeport-transition-bplnl
Jan 28 01:52:18.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6168 exec execpod-affinitydqrxl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.187.128.27:31873/ ; done'
Jan 28 01:52:19.291: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.187.128.27:31873/\n"
Jan 28 01:52:19.291: INFO: stdout: "\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl\naffinity-nodeport-transition-56nwl"
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Received response from host: affinity-nodeport-transition-56nwl
Jan 28 01:52:19.291: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6168, will wait for the garbage collector to delete the pods
Jan 28 01:52:19.428: INFO: Deleting ReplicationController affinity-nodeport-transition took: 23.977275ms
Jan 28 01:52:19.529: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.984433ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:52:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6168" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:44.126 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":356,"completed":332,"skipped":5979,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:52:22.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2718
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating server pod server in namespace prestop-2718
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2718
STEP: Deleting pre-stop pod
Jan 28 01:52:35.949: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:188
Jan 28 01:52:36.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2718" for this suite.

• [SLOW TEST:13.678 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":356,"completed":333,"skipped":5999,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:52:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2751
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-3301
STEP: Creating secret with name secret-test-d9d33fda-3297-4cc1-b07f-5882a8423e22
STEP: Creating a pod to test consume secrets
Jan 28 01:52:36.541: INFO: Waiting up to 5m0s for pod "pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec" in namespace "secrets-2751" to be "Succeeded or Failed"
Jan 28 01:52:36.560: INFO: Pod "pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 19.065029ms
Jan 28 01:52:38.584: INFO: Pod "pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042094513s
Jan 28 01:52:40.615: INFO: Pod "pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073097176s
STEP: Saw pod success
Jan 28 01:52:40.615: INFO: Pod "pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec" satisfied condition "Succeeded or Failed"
Jan 28 01:52:40.633: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:52:40.787: INFO: Waiting for pod pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec to disappear
Jan 28 01:52:40.805: INFO: Pod pod-secrets-97ce9dec-0c0d-40ff-9826-37222a82f0ec no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:52:40.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2751" for this suite.
STEP: Destroying namespace "secret-namespace-3301" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":356,"completed":334,"skipped":6047,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] version v1
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:52:40.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4927
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:52:41.087: INFO: Creating pod...
Jan 28 01:52:45.170: INFO: Creating service...
Jan 28 01:52:45.228: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=DELETE
Jan 28 01:52:45.289: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:52:45.289: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=OPTIONS
Jan 28 01:52:45.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:52:45.341: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=PATCH
Jan 28 01:52:45.364: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:52:45.365: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=POST
Jan 28 01:52:45.387: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:52:45.387: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=PUT
Jan 28 01:52:45.412: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 01:52:45.412: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 28 01:52:45.442: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 28 01:52:45.442: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 28 01:52:45.474: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 28 01:52:45.474: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 28 01:52:45.505: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 28 01:52:45.505: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=POST
Jan 28 01:52:45.535: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 28 01:52:45.535: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=PUT
Jan 28 01:52:45.566: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 28 01:52:45.566: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=GET
Jan 28 01:52:45.582: INFO: http.Client request:GET StatusCode:301
Jan 28 01:52:45.582: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=GET
Jan 28 01:52:45.606: INFO: http.Client request:GET StatusCode:301
Jan 28 01:52:45.606: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/pods/agnhost/proxy?method=HEAD
Jan 28 01:52:45.623: INFO: http.Client request:HEAD StatusCode:301
Jan 28 01:52:45.623: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-4927/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 28 01:52:45.646: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:188
Jan 28 01:52:45.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4927" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","total":356,"completed":335,"skipped":6096,"failed":0}

------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:52:45.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5539
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:61
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating pod liveness-e29b1530-574c-4574-9eed-98f566e85125 in namespace container-probe-5539
Jan 28 01:52:49.998: INFO: Started pod liveness-e29b1530-574c-4574-9eed-98f566e85125 in namespace container-probe-5539
STEP: checking the pod's current state and verifying that restartCount is present
Jan 28 01:52:50.021: INFO: Initial restart count of pod liveness-e29b1530-574c-4574-9eed-98f566e85125 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:188
Jan 28 01:56:51.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5539" for this suite.

• [SLOW TEST:246.253 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":356,"completed":336,"skipped":6096,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:56:51.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6959
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6959
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6959
I0128 01:56:52.295514      26 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6959, replica count: 2
Jan 28 01:56:55.346: INFO: Creating new exec pod
I0128 01:56:55.346614      26 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 28 01:56:58.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6959 exec execpodc9ttn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 28 01:56:58.757: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 28 01:56:58.757: INFO: stdout: "externalname-service-xnx8w"
Jan 28 01:56:58.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6959 exec execpodc9ttn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.145.106 80'
Jan 28 01:56:59.017: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.145.106 80\nConnection to 172.21.145.106 80 port [tcp/http] succeeded!\n"
Jan 28 01:56:59.017: INFO: stdout: "externalname-service-xnx8w"
Jan 28 01:56:59.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6959 exec execpodc9ttn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.27 32091'
Jan 28 01:56:59.275: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.27 32091\nConnection to 10.187.128.27 32091 port [tcp/*] succeeded!\n"
Jan 28 01:56:59.275: INFO: stdout: "externalname-service-xnx8w"
Jan 28 01:56:59.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=services-6959 exec execpodc9ttn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.187.128.43 32091'
Jan 28 01:56:59.580: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.187.128.43 32091\nConnection to 10.187.128.43 32091 port [tcp/*] succeeded!\n"
Jan 28 01:56:59.581: INFO: stdout: "externalname-service-xnx8w"
Jan 28 01:56:59.581: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:188
Jan 28 01:56:59.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6959" for this suite.
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762

• [SLOW TEST:7.806 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":356,"completed":337,"skipped":6131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:56:59.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9378
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name secret-test-2722f8b4-86cf-4404-b3f0-d9fb7233587e
STEP: Creating a pod to test consume secrets
Jan 28 01:57:00.018: INFO: Waiting up to 5m0s for pod "pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6" in namespace "secrets-9378" to be "Succeeded or Failed"
Jan 28 01:57:00.037: INFO: Pod "pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.460145ms
Jan 28 01:57:02.080: INFO: Pod "pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061996642s
Jan 28 01:57:04.115: INFO: Pod "pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097049551s
STEP: Saw pod success
Jan 28 01:57:04.115: INFO: Pod "pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6" satisfied condition "Succeeded or Failed"
Jan 28 01:57:04.133: INFO: Trying to get logs from node 10.187.128.43 pod pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6 container secret-volume-test: <nil>
STEP: delete the pod
Jan 28 01:57:04.281: INFO: Waiting for pod pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6 to disappear
Jan 28 01:57:04.300: INFO: Pod pod-secrets-c6353508-8e9e-412a-a3b6-dacb71ebd4d6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:57:04.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9378" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":356,"completed":338,"skipped":6243,"failed":0}
SSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:04.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6717
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:191
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jan 28 01:57:04.609: INFO: The status of Pod pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:57:06.638: INFO: The status of Pod pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76 is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:57:08.639: INFO: The status of Pod pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 28 01:57:09.232: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76"
Jan 28 01:57:09.233: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76" in namespace "pods-6717" to be "terminated due to deadline exceeded"
Jan 28 01:57:09.251: INFO: Pod "pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76": Phase="Running", Reason="", readiness=true. Elapsed: 18.060667ms
Jan 28 01:57:11.280: INFO: Pod "pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.047606358s
Jan 28 01:57:11.281: INFO: Pod "pod-update-activedeadlineseconds-e86ff19a-774b-4985-afb6-6a8fe0785d76" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:188
Jan 28 01:57:11.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6717" for this suite.

• [SLOW TEST:6.986 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":356,"completed":339,"skipped":6250,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:11.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9385
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:188
Jan 28 01:57:11.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9385" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":356,"completed":340,"skipped":6252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3690
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:57:12.157: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties
Jan 28 01:57:15.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3690 --namespace=crd-publish-openapi-3690 create -f -'
Jan 28 01:57:16.373: INFO: stderr: ""
Jan 28 01:57:16.373: INFO: stdout: "e2e-test-crd-publish-openapi-3136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 28 01:57:16.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3690 --namespace=crd-publish-openapi-3690 delete e2e-test-crd-publish-openapi-3136-crds test-cr'
Jan 28 01:57:16.479: INFO: stderr: ""
Jan 28 01:57:16.479: INFO: stdout: "e2e-test-crd-publish-openapi-3136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 28 01:57:16.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3690 --namespace=crd-publish-openapi-3690 apply -f -'
Jan 28 01:57:17.134: INFO: stderr: ""
Jan 28 01:57:17.134: INFO: stdout: "e2e-test-crd-publish-openapi-3136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 28 01:57:17.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3690 --namespace=crd-publish-openapi-3690 delete e2e-test-crd-publish-openapi-3136-crds test-cr'
Jan 28 01:57:17.242: INFO: stderr: ""
Jan 28 01:57:17.242: INFO: stdout: "e2e-test-crd-publish-openapi-3136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jan 28 01:57:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2429286396 --namespace=crd-publish-openapi-3690 explain e2e-test-crd-publish-openapi-3136-crds'
Jan 28 01:57:17.448: INFO: stderr: ""
Jan 28 01:57:17.448: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3136-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:57:20.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3690" for this suite.

• [SLOW TEST:8.693 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":356,"completed":341,"skipped":6288,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:20.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-2225
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:188
Jan 28 01:57:20.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":356,"completed":342,"skipped":6306,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:20.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4160
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:57:21.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4160" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":356,"completed":343,"skipped":6363,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:57:21.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-132
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating secret with name s-test-opt-del-1e5ec0ad-502c-494b-a09d-25dd1dd56f24
STEP: Creating secret with name s-test-opt-upd-d51526a4-6e50-4794-9741-97825853ce46
STEP: Creating the pod
Jan 28 01:57:21.734: INFO: The status of Pod pod-projected-secrets-9d2a1860-5939-4e5c-90fc-da4c715e714f is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:57:23.761: INFO: The status of Pod pod-projected-secrets-9d2a1860-5939-4e5c-90fc-da4c715e714f is Pending, waiting for it to be Running (with Ready = true)
Jan 28 01:57:25.771: INFO: The status of Pod pod-projected-secrets-9d2a1860-5939-4e5c-90fc-da4c715e714f is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-1e5ec0ad-502c-494b-a09d-25dd1dd56f24
STEP: Updating secret s-test-opt-upd-d51526a4-6e50-4794-9741-97825853ce46
STEP: Creating secret with name s-test-opt-create-51e9085e-026a-4c4d-b022-4f7691417ea3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:188
Jan 28 01:58:37.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-132" for this suite.

• [SLOW TEST:76.418 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":356,"completed":344,"skipped":6379,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:37.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7013
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap that has name configmap-test-emptyKey-4d310c22-9dbe-4308-b708-1f26b1c6a8d5
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:188
Jan 28 01:58:38.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7013" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":356,"completed":345,"skipped":6380,"failed":0}
SSS
------------------------------
[sig-node] RuntimeClass 
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:38.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-5735
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:188
Jan 28 01:58:38.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5735" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","total":356,"completed":346,"skipped":6383,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:38.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7703
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 28 01:58:38.741: INFO: Waiting up to 5m0s for pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b" in namespace "emptydir-7703" to be "Succeeded or Failed"
Jan 28 01:58:38.759: INFO: Pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.529267ms
Jan 28 01:58:40.791: INFO: Pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049647597s
Jan 28 01:58:42.811: INFO: Pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069570142s
Jan 28 01:58:44.840: INFO: Pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098286037s
STEP: Saw pod success
Jan 28 01:58:44.840: INFO: Pod "pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b" satisfied condition "Succeeded or Failed"
Jan 28 01:58:44.857: INFO: Trying to get logs from node 10.187.128.43 pod pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b container test-container: <nil>
STEP: delete the pod
Jan 28 01:58:44.952: INFO: Waiting for pod pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b to disappear
Jan 28 01:58:44.970: INFO: Pod pod-76e7d8d4-b238-4f48-bd9c-7d8ebec2a85b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:58:44.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7703" for this suite.

• [SLOW TEST:6.550 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":347,"skipped":6452,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:45.029: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7964
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 28 01:58:45.307: INFO: Waiting up to 5m0s for pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933" in namespace "emptydir-7964" to be "Succeeded or Failed"
Jan 28 01:58:45.324: INFO: Pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933": Phase="Pending", Reason="", readiness=false. Elapsed: 16.768781ms
Jan 28 01:58:47.350: INFO: Pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933": Phase="Running", Reason="", readiness=true. Elapsed: 2.042675035s
Jan 28 01:58:49.376: INFO: Pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933": Phase="Running", Reason="", readiness=false. Elapsed: 4.068444302s
Jan 28 01:58:51.397: INFO: Pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090032304s
STEP: Saw pod success
Jan 28 01:58:51.398: INFO: Pod "pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933" satisfied condition "Succeeded or Failed"
Jan 28 01:58:51.414: INFO: Trying to get logs from node 10.187.128.43 pod pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933 container test-container: <nil>
STEP: delete the pod
Jan 28 01:58:51.496: INFO: Waiting for pod pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933 to disappear
Jan 28 01:58:51.512: INFO: Pod pod-30ee12a3-b09d-45cf-b95d-238cf1ebc933 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:58:51.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7964" for this suite.

• [SLOW TEST:6.540 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":348,"skipped":6458,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:51.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4057
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 28 01:58:51.888: INFO: Waiting up to 5m0s for pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd" in namespace "emptydir-4057" to be "Succeeded or Failed"
Jan 28 01:58:51.909: INFO: Pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.917737ms
Jan 28 01:58:53.935: INFO: Pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047544155s
Jan 28 01:58:55.964: INFO: Pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076068751s
Jan 28 01:58:57.983: INFO: Pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.095083301s
STEP: Saw pod success
Jan 28 01:58:57.983: INFO: Pod "pod-4b21343d-b20b-43b0-9d02-40cbf96487fd" satisfied condition "Succeeded or Failed"
Jan 28 01:58:58.000: INFO: Trying to get logs from node 10.187.128.43 pod pod-4b21343d-b20b-43b0-9d02-40cbf96487fd container test-container: <nil>
STEP: delete the pod
Jan 28 01:58:58.103: INFO: Waiting for pod pod-4b21343d-b20b-43b0-9d02-40cbf96487fd to disappear
Jan 28 01:58:58.120: INFO: Pod pod-4b21343d-b20b-43b0-9d02-40cbf96487fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:58:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4057" for this suite.

• [SLOW TEST:6.610 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":349,"skipped":6472,"failed":0}
[sig-node] Secrets 
  should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2584
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:188
Jan 28 01:58:58.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2584" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":356,"completed":350,"skipped":6472,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:58:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7752
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jan 28 01:58:59.544: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jan 28 01:59:02.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/framework/framework.go:652
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:188
Jan 28 01:59:03.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7752" for this suite.
STEP: Destroying namespace "webhook-7752-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":356,"completed":351,"skipped":6486,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:59:03.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6982
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 28 01:59:03.694: INFO: Waiting up to 5m0s for pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7" in namespace "emptydir-6982" to be "Succeeded or Failed"
Jan 28 01:59:03.716: INFO: Pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.228645ms
Jan 28 01:59:05.748: INFO: Pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.054269713s
Jan 28 01:59:07.776: INFO: Pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7": Phase="Running", Reason="", readiness=false. Elapsed: 4.082371559s
Jan 28 01:59:09.809: INFO: Pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.115805329s
STEP: Saw pod success
Jan 28 01:59:09.810: INFO: Pod "pod-7655949e-ddb0-46de-bf06-e28aadec86d7" satisfied condition "Succeeded or Failed"
Jan 28 01:59:09.826: INFO: Trying to get logs from node 10.187.128.43 pod pod-7655949e-ddb0-46de-bf06-e28aadec86d7 container test-container: <nil>
STEP: delete the pod
Jan 28 01:59:09.941: INFO: Waiting for pod pod-7655949e-ddb0-46de-bf06-e28aadec86d7 to disappear
Jan 28 01:59:09.957: INFO: Pod pod-7655949e-ddb0-46de-bf06-e28aadec86d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:59:09.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6982" for this suite.

• [SLOW TEST:6.586 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":352,"skipped":6490,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:59:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1493
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 28 01:59:10.281: INFO: Waiting up to 5m0s for pod "pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed" in namespace "emptydir-1493" to be "Succeeded or Failed"
Jan 28 01:59:10.298: INFO: Pod "pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed": Phase="Pending", Reason="", readiness=false. Elapsed: 16.676817ms
Jan 28 01:59:12.330: INFO: Pod "pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048473648s
Jan 28 01:59:14.357: INFO: Pod "pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075771653s
STEP: Saw pod success
Jan 28 01:59:14.357: INFO: Pod "pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed" satisfied condition "Succeeded or Failed"
Jan 28 01:59:14.374: INFO: Trying to get logs from node 10.187.128.43 pod pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed container test-container: <nil>
STEP: delete the pod
Jan 28 01:59:14.502: INFO: Waiting for pod pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed to disappear
Jan 28 01:59:14.520: INFO: Pod pod-4c2cf44b-a7bc-4937-a6a7-d465001d78ed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:188
Jan 28 01:59:14.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1493" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":356,"completed":353,"skipped":6520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:59:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename disruption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in disruption-1795
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/framework/framework.go:652
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:188
Jan 28 01:59:15.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1795" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":356,"completed":354,"skipped":6545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:59:15.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4660
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/framework/framework.go:652
STEP: Creating configMap with name projected-configmap-test-volume-49cc1f56-22ae-49f4-8700-dffbefbebfae
STEP: Creating a pod to test consume configMaps
Jan 28 01:59:15.385: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e" in namespace "projected-4660" to be "Succeeded or Failed"
Jan 28 01:59:15.403: INFO: Pod "pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.535335ms
Jan 28 01:59:17.435: INFO: Pod "pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04925381s
Jan 28 01:59:19.462: INFO: Pod "pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076851062s
STEP: Saw pod success
Jan 28 01:59:19.462: INFO: Pod "pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e" satisfied condition "Succeeded or Failed"
Jan 28 01:59:19.478: INFO: Trying to get logs from node 10.187.128.43 pod pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e container agnhost-container: <nil>
STEP: delete the pod
Jan 28 01:59:19.557: INFO: Waiting for pod pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e to disappear
Jan 28 01:59:19.575: INFO: Pod pod-projected-configmaps-85703c50-0542-47a0-a8d8-a52b84e8858e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:188
Jan 28 01:59:19.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4660" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":356,"completed":355,"skipped":6571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
STEP: Creating a kubernetes client
Jan 28 01:59:19.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2429286396
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-5200
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/framework/framework.go:652
Jan 28 01:59:20.248: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 28 01:59:20.255: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 28 01:59:20.255: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 28 01:59:20.255: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 28 01:59:20.256: INFO: Checking APIGroup: apps
Jan 28 01:59:20.262: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 28 01:59:20.262: INFO: Versions found [{apps/v1 v1}]
Jan 28 01:59:20.262: INFO: apps/v1 matches apps/v1
Jan 28 01:59:20.262: INFO: Checking APIGroup: events.k8s.io
Jan 28 01:59:20.271: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 28 01:59:20.271: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.271: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 28 01:59:20.271: INFO: Checking APIGroup: authentication.k8s.io
Jan 28 01:59:20.282: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 28 01:59:20.282: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 28 01:59:20.282: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 28 01:59:20.282: INFO: Checking APIGroup: authorization.k8s.io
Jan 28 01:59:20.289: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 28 01:59:20.289: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 28 01:59:20.289: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 28 01:59:20.289: INFO: Checking APIGroup: autoscaling
Jan 28 01:59:20.295: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 28 01:59:20.295: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jan 28 01:59:20.295: INFO: autoscaling/v2 matches autoscaling/v2
Jan 28 01:59:20.295: INFO: Checking APIGroup: batch
Jan 28 01:59:20.302: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 28 01:59:20.302: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jan 28 01:59:20.302: INFO: batch/v1 matches batch/v1
Jan 28 01:59:20.302: INFO: Checking APIGroup: certificates.k8s.io
Jan 28 01:59:20.309: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 28 01:59:20.309: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 28 01:59:20.309: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 28 01:59:20.309: INFO: Checking APIGroup: networking.k8s.io
Jan 28 01:59:20.317: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 28 01:59:20.317: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 28 01:59:20.317: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 28 01:59:20.317: INFO: Checking APIGroup: policy
Jan 28 01:59:20.323: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 28 01:59:20.323: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jan 28 01:59:20.323: INFO: policy/v1 matches policy/v1
Jan 28 01:59:20.324: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 28 01:59:20.330: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 28 01:59:20.330: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 28 01:59:20.330: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 28 01:59:20.330: INFO: Checking APIGroup: storage.k8s.io
Jan 28 01:59:20.369: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 28 01:59:20.369: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.369: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 28 01:59:20.369: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 28 01:59:20.376: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 28 01:59:20.376: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 28 01:59:20.376: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 28 01:59:20.377: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 28 01:59:20.383: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 28 01:59:20.383: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 28 01:59:20.383: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 28 01:59:20.383: INFO: Checking APIGroup: scheduling.k8s.io
Jan 28 01:59:20.390: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 28 01:59:20.390: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 28 01:59:20.390: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 28 01:59:20.390: INFO: Checking APIGroup: coordination.k8s.io
Jan 28 01:59:20.397: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 28 01:59:20.397: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 28 01:59:20.397: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 28 01:59:20.397: INFO: Checking APIGroup: node.k8s.io
Jan 28 01:59:20.404: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 28 01:59:20.404: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.404: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 28 01:59:20.404: INFO: Checking APIGroup: discovery.k8s.io
Jan 28 01:59:20.413: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 28 01:59:20.413: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.413: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 28 01:59:20.413: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 28 01:59:20.419: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 28 01:59:20.419: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.419: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 28 01:59:20.419: INFO: Checking APIGroup: crd.projectcalico.org
Jan 28 01:59:20.426: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 28 01:59:20.426: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 28 01:59:20.426: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 28 01:59:20.426: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 28 01:59:20.433: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 28 01:59:20.433: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.433: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 28 01:59:20.433: INFO: Checking APIGroup: ibm.com
Jan 28 01:59:20.440: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jan 28 01:59:20.440: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jan 28 01:59:20.440: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jan 28 01:59:20.440: INFO: Checking APIGroup: metrics.k8s.io
Jan 28 01:59:20.448: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 28 01:59:20.448: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 28 01:59:20.449: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:188
Jan 28 01:59:20.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5200" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":356,"completed":356,"skipped":6599,"failed":0}
SSSSSSSSSSSSSSSSSSJan 28 01:59:20.519: INFO: Running AfterSuite actions on all nodes
Jan 28 01:59:20.519: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func19.2
Jan 28 01:59:20.519: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 28 01:59:20.519: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jan 28 01:59:20.519: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 28 01:59:20.519: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 28 01:59:20.520: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 28 01:59:20.520: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jan 28 01:59:20.520: INFO: Running AfterSuite actions on node 1
Jan 28 01:59:20.520: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":356,"completed":356,"skipped":6617,"failed":0}

Ran 356 of 6973 Specs in 6347.739 seconds
SUCCESS! -- 356 Passed | 0 Failed | 0 Pending | 6617 Skipped
PASS

Ginkgo ran 1 suite in 1h45m50.427074579s
Test Suite Passed
