Conformance test: not doing test setup.
Feb 20 18:10:50.043: INFO: Overriding default scale value of zero to 1
Feb 20 18:10:50.043: INFO: Overriding default milliseconds value of zero to 5000
I0220 18:10:50.468704   29647 e2e.go:304] Starting e2e run "da625892-353a-11e9-9b2a-cafe91372a39" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550686249 - Will randomize all specs
Will run 188 of 2011 specs

Feb 20 18:10:50.630: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:10:50.633: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 20 18:10:50.753: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 20 18:10:50.884: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 20 18:10:50.884: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 20 18:10:50.884: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 20 18:10:50.913: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 20 18:10:50.913: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 20 18:10:50.913: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 20 18:10:50.913: INFO: e2e test version: v1.12.5
Feb 20 18:10:50.934: INFO: kube-apiserver version: v1.12.5
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:10:50.935: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
Feb 20 18:10:51.884: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 20 18:10:51.956: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-4xj9f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-q5pwr in namespace e2e-tests-proxy-4xj9f
I0220 18:10:52.202380   29647 runners.go:180] Created replication controller with name: proxy-service-q5pwr, namespace: e2e-tests-proxy-4xj9f, replica count: 1
I0220 18:10:53.252704   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 18:10:54.252973   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 18:10:55.253327   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 18:10:56.253819   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:10:57.254251   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:10:58.254533   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:10:59.254812   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:11:00.255124   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0220 18:11:01.255463   29647 runners.go:180] proxy-service-q5pwr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 18:11:01.278: INFO: setup took 9.129932914s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 20 18:11:01.311: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 32.311107ms)
Feb 20 18:11:01.311: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 32.119553ms)
Feb 20 18:11:01.311: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 32.247553ms)
Feb 20 18:11:01.311: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 32.191214ms)
Feb 20 18:11:01.315: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 36.908342ms)
Feb 20 18:11:01.315: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 37.104313ms)
Feb 20 18:11:01.315: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 37.010179ms)
Feb 20 18:11:01.316: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 36.985234ms)
Feb 20 18:11:01.316: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 37.145382ms)
Feb 20 18:11:01.319: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 41.035787ms)
Feb 20 18:11:01.323: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 44.219656ms)
Feb 20 18:11:01.324: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 45.684318ms)
Feb 20 18:11:01.325: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 46.441782ms)
Feb 20 18:11:01.325: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 46.443451ms)
Feb 20 18:11:01.325: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 46.950729ms)
Feb 20 18:11:01.326: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 48.217107ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.042098ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.197465ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.067547ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.276813ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.168989ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.208106ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.483793ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.267797ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 26.354459ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.311958ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.305604ms)
Feb 20 18:11:01.353: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 26.53777ms)
Feb 20 18:11:01.355: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 27.965362ms)
Feb 20 18:11:01.355: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 28.751438ms)
Feb 20 18:11:01.355: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 28.712663ms)
Feb 20 18:11:01.356: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 28.764507ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.224806ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.173573ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.198818ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.260587ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.262435ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.70163ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.631296ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.730967ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.868798ms)
Feb 20 18:11:01.382: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.811213ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 36.119416ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 36.147467ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 36.14211ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 36.187176ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 36.190022ms)
Feb 20 18:11:01.392: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 36.238271ms)
Feb 20 18:11:01.418: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 25.721118ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.117828ms)
Feb 20 18:11:01.418: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 25.722077ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 25.879329ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 25.859109ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 26.904ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.333249ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.814829ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.55298ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.202435ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.975578ms)
Feb 20 18:11:01.419: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 26.715593ms)
Feb 20 18:11:01.420: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 27.696065ms)
Feb 20 18:11:01.420: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 27.917ms)
Feb 20 18:11:01.421: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 28.944674ms)
Feb 20 18:11:01.421: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 28.423432ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 25.850974ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.389673ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.447727ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.874665ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.00509ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.240998ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.004586ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.38972ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.502358ms)
Feb 20 18:11:01.448: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.40731ms)
Feb 20 18:11:01.449: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.067227ms)
Feb 20 18:11:01.449: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.381878ms)
Feb 20 18:11:01.449: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 27.385268ms)
Feb 20 18:11:01.450: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 28.609931ms)
Feb 20 18:11:01.450: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 28.400954ms)
Feb 20 18:11:01.451: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 28.913641ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.53992ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.65498ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.577453ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.746883ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.747754ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 27.783267ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.586401ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.762211ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.974161ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.796609ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.747166ms)
Feb 20 18:11:01.479: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.95519ms)
Feb 20 18:11:01.522: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 71.012657ms)
Feb 20 18:11:01.522: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 71.200179ms)
Feb 20 18:11:01.522: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 71.149069ms)
Feb 20 18:11:01.522: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 71.433548ms)
Feb 20 18:11:01.549: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.678786ms)
Feb 20 18:11:01.549: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.781879ms)
Feb 20 18:11:01.549: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.785973ms)
Feb 20 18:11:01.549: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.938379ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.170533ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.298459ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.267323ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.384233ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.70474ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.625178ms)
Feb 20 18:11:01.550: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.554097ms)
Feb 20 18:11:01.593: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.454215ms)
Feb 20 18:11:01.593: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 70.485353ms)
Feb 20 18:11:01.593: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 70.437329ms)
Feb 20 18:11:01.593: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.65381ms)
Feb 20 18:11:01.594: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 71.11357ms)
Feb 20 18:11:01.620: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.249218ms)
Feb 20 18:11:01.620: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.251025ms)
Feb 20 18:11:01.620: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.817901ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.265447ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.162992ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.28308ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.158138ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 27.16681ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.322699ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.183706ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.255024ms)
Feb 20 18:11:01.621: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.415136ms)
Feb 20 18:11:01.622: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 28.141372ms)
Feb 20 18:11:01.663: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 69.678822ms)
Feb 20 18:11:01.663: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 69.800484ms)
Feb 20 18:11:01.663: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 69.7264ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.790785ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.574545ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.602075ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.651671ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.777701ms)
Feb 20 18:11:01.690: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.917262ms)
Feb 20 18:11:01.691: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.235258ms)
Feb 20 18:11:01.691: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.090096ms)
Feb 20 18:11:01.691: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.194211ms)
Feb 20 18:11:01.691: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.569903ms)
Feb 20 18:11:01.692: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 28.455634ms)
Feb 20 18:11:01.693: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 29.262049ms)
Feb 20 18:11:01.693: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 29.354415ms)
Feb 20 18:11:01.693: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 29.118809ms)
Feb 20 18:11:01.693: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 29.342454ms)
Feb 20 18:11:01.694: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 29.946112ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.465011ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.496648ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.531984ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.547918ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.678331ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.797527ms)
Feb 20 18:11:01.720: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.876072ms)
Feb 20 18:11:01.721: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.883491ms)
Feb 20 18:11:01.721: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.001105ms)
Feb 20 18:11:01.721: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.020743ms)
Feb 20 18:11:01.721: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.541554ms)
Feb 20 18:11:01.721: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.407253ms)
Feb 20 18:11:01.722: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 27.829582ms)
Feb 20 18:11:01.722: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 28.335933ms)
Feb 20 18:11:01.722: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 28.315675ms)
Feb 20 18:11:01.723: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 29.258933ms)
Feb 20 18:11:01.750: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 27.289778ms)
Feb 20 18:11:01.750: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.213205ms)
Feb 20 18:11:01.750: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.649959ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 27.467123ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.84959ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 26.940273ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.602599ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 28.012042ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.251138ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.419669ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.105012ms)
Feb 20 18:11:01.751: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.364295ms)
Feb 20 18:11:01.793: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 69.985923ms)
Feb 20 18:11:01.793: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 69.537981ms)
Feb 20 18:11:01.793: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 70.316031ms)
Feb 20 18:11:01.794: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 69.832004ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 27.417313ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.288637ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.271178ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.204493ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.392479ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.323951ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.547246ms)
Feb 20 18:11:01.821: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.570559ms)
Feb 20 18:11:01.822: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 28.117668ms)
Feb 20 18:11:01.822: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.939964ms)
Feb 20 18:11:01.822: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.911455ms)
Feb 20 18:11:01.822: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 28.004887ms)
Feb 20 18:11:01.841: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 46.982302ms)
Feb 20 18:11:01.841: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 46.877432ms)
Feb 20 18:11:01.841: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 47.042952ms)
Feb 20 18:11:01.841: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 46.973389ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.752723ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.869195ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.869297ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.939578ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.863857ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.920969ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.157822ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.340852ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.248994ms)
Feb 20 18:11:01.868: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.296017ms)
Feb 20 18:11:01.869: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.882107ms)
Feb 20 18:11:01.869: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.903441ms)
Feb 20 18:11:01.912: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 70.575299ms)
Feb 20 18:11:01.912: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 70.638983ms)
Feb 20 18:11:01.912: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.604321ms)
Feb 20 18:11:01.912: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.546457ms)
Feb 20 18:11:01.938: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.588666ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.647625ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.753373ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.696738ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.729571ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.740248ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.797777ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.924917ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 26.900107ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.729644ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.80434ms)
Feb 20 18:11:01.939: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 26.849896ms)
Feb 20 18:11:01.983: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.708877ms)
Feb 20 18:11:01.983: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.774567ms)
Feb 20 18:11:01.983: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 70.765157ms)
Feb 20 18:11:01.983: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 70.94343ms)
Feb 20 18:11:02.009: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.566994ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.618342ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.734027ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.753956ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.812555ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.703259ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 26.689765ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.872456ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.737413ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.847473ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 26.74582ms)
Feb 20 18:11:02.010: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.787083ms)
Feb 20 18:11:02.053: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 70.536731ms)
Feb 20 18:11:02.053: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.549295ms)
Feb 20 18:11:02.053: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.596493ms)
Feb 20 18:11:02.054: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 70.611205ms)
Feb 20 18:11:02.083: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 29.244865ms)
Feb 20 18:11:02.083: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 29.336024ms)
Feb 20 18:11:02.083: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 29.515642ms)
Feb 20 18:11:02.083: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 29.516807ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 29.946258ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 30.193255ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 30.133546ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 30.162617ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 30.062607ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 30.055322ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 30.250532ms)
Feb 20 18:11:02.084: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 30.154826ms)
Feb 20 18:11:02.085: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 31.333748ms)
Feb 20 18:11:02.086: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 31.99028ms)
Feb 20 18:11:02.087: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 32.771931ms)
Feb 20 18:11:02.087: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 32.780337ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.923751ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.00872ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.962374ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 27.084773ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.150384ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.233875ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.59927ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.833138ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.847273ms)
Feb 20 18:11:02.114: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 27.752986ms)
Feb 20 18:11:02.115: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.838116ms)
Feb 20 18:11:02.115: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 27.835398ms)
Feb 20 18:11:02.116: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 29.53342ms)
Feb 20 18:11:02.118: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 30.980468ms)
Feb 20 18:11:02.118: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 30.904789ms)
Feb 20 18:11:02.118: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 30.951714ms)
Feb 20 18:11:02.144: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.031229ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.845626ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.90274ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.973799ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 26.921265ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.948589ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 27.321883ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 27.50935ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.39437ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.387917ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.485954ms)
Feb 20 18:11:02.145: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.591134ms)
Feb 20 18:11:02.188: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.642924ms)
Feb 20 18:11:02.188: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 70.751771ms)
Feb 20 18:11:02.188: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.638773ms)
Feb 20 18:11:02.188: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 70.619648ms)
Feb 20 18:11:02.215: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.526304ms)
Feb 20 18:11:02.215: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.502934ms)
Feb 20 18:11:02.215: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.575641ms)
Feb 20 18:11:02.215: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.488472ms)
Feb 20 18:11:02.215: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.5737ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 27.1711ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.131351ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 27.358769ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.338185ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.196687ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 27.878312ms)
Feb 20 18:11:02.216: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 27.820542ms)
Feb 20 18:11:02.217: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 28.410608ms)
Feb 20 18:11:02.259: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 70.471596ms)
Feb 20 18:11:02.259: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.574142ms)
Feb 20 18:11:02.259: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 70.509019ms)
Feb 20 18:11:02.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.090454ms)
Feb 20 18:11:02.286: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:1080/proxy/rewri... (200; 26.247469ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 26.266128ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:162/proxy/: bar (200; 26.810844ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz:160/proxy/: foo (200; 27.149172ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/http:proxy-service-q5pwr-pcfbz:1080/proxy/... (200; 26.965974ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/proxy-service-q5pwr-pcfbz/proxy/rewriteme"... (200; 26.524509ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:460/proxy/: tls baz (200; 26.744883ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:443/proxy/... (200; 27.182371ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/pods/https:proxy-service-q5pwr-pcfbz:462/proxy/: tls qux (200; 26.384544ms)
Feb 20 18:11:02.287: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname2/proxy/: tls qux (200; 26.748974ms)
Feb 20 18:11:02.288: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/https:proxy-service-q5pwr:tlsportname1/proxy/: tls baz (200; 28.128006ms)
Feb 20 18:11:02.329: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname1/proxy/: foo (200; 69.586144ms)
Feb 20 18:11:02.329: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname2/proxy/: bar (200; 70.19788ms)
Feb 20 18:11:02.329: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/http:proxy-service-q5pwr:portname1/proxy/: foo (200; 68.939263ms)
Feb 20 18:11:02.329: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-4xj9f/services/proxy-service-q5pwr:portname2/proxy/: bar (200; 69.809119ms)
STEP: deleting { ReplicationController} proxy-service-q5pwr in namespace e2e-tests-proxy-4xj9f, will wait for the garbage collector to delete the pods
Feb 20 18:11:02.429: INFO: Deleting { ReplicationController} proxy-service-q5pwr took: 25.855087ms
Feb 20 18:11:02.532: INFO: Terminating { ReplicationController} proxy-service-q5pwr pods took: 102.564638ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:11:13.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-4xj9f" for this suite.
Feb 20 18:11:19.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:11:19.567: INFO: namespace: e2e-tests-proxy-4xj9f, resource: bindings, ignored listing per whitelist
Feb 20 18:11:20.130: INFO: namespace e2e-tests-proxy-4xj9f deletion completed in 6.974226044s

 [SLOW TEST:29.195 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:11:20.130: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-7pf47
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ed03e235-353a-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:11:21.284: INFO: Waiting up to 5m0s for pod "pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-7pf47" to be "success or failure"
Feb 20 18:11:21.307: INFO: Pod "pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.002579ms
Feb 20 18:11:23.332: INFO: Pod "pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047824025s
Feb 20 18:11:25.422: INFO: Pod "pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.138198109s
STEP: Saw pod success
Feb 20 18:11:25.422: INFO: Pod "pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:11:25.523: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:11:25.841: INFO: Waiting for pod pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:11:25.864: INFO: Pod pod-secrets-ed07774d-353a-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:11:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-7pf47" for this suite.
Feb 20 18:11:31.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:11:32.676: INFO: namespace: e2e-tests-secrets-7pf47, resource: bindings, ignored listing per whitelist
Feb 20 18:11:32.884: INFO: namespace e2e-tests-secrets-7pf47 deletion completed in 6.963193456s

 [SLOW TEST:12.754 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:11:32.885: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-lb47q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 20 18:11:40.199: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-f4a4800b-353a-11e9-9b2a-cafe91372a39", GenerateName:"", Namespace:"e2e-tests-pods-lb47q", SelfLink:"/api/v1/namespaces/e2e-tests-pods-lb47q/pods/pod-submit-remove-f4a4800b-353a-11e9-9b2a-cafe91372a39", UID:"f4acd8e5-353a-11e9-a0a3-2a5d9248fe67", ResourceVersion:"2957", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686283094, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"30752264"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.5/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-f99t8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001bce400), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-f99t8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0016a82a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000cfb0e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016a82e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0016a8300)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0016a8308), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283093, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283099, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283099, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283094, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.3", PodIP:"100.96.1.5", StartTime:(*v1.Time)(0xc001f1afa0), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001f1afc0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://f1cad79951a77498dba69899581530cc58536f17c06760919f66413a535aa75c"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 20 18:11:45.259: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:11:45.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-lb47q" for this suite.
Feb 20 18:11:51.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:11:52.063: INFO: namespace: e2e-tests-pods-lb47q, resource: bindings, ignored listing per whitelist
Feb 20 18:11:52.294: INFO: namespace e2e-tests-pods-lb47q deletion completed in 6.98814124s

 [SLOW TEST:19.410 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:11:52.294: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-2hcgb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:11:53.455: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-2hcgb" to be "success or failure"
Feb 20 18:11:53.478: INFO: Pod "downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.57812ms
Feb 20 18:11:55.501: INFO: Pod "downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046460591s
Feb 20 18:11:57.544: INFO: Pod "downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089096628s
STEP: Saw pod success
Feb 20 18:11:57.544: INFO: Pod "downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:11:57.568: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:11:57.626: INFO: Waiting for pod downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:11:57.649: INFO: Pod downwardapi-volume-00347b12-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:11:57.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-2hcgb" for this suite.
Feb 20 18:12:03.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:03.906: INFO: namespace: e2e-tests-projected-2hcgb, resource: bindings, ignored listing per whitelist
Feb 20 18:12:04.649: INFO: namespace e2e-tests-projected-2hcgb deletion completed in 6.951681357s

 [SLOW TEST:12.355 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:04.650: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-6645n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:12:05.872: INFO: (0) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 119.347452ms)
Feb 20 18:12:05.898: INFO: (1) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.188774ms)
Feb 20 18:12:05.924: INFO: (2) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.196753ms)
Feb 20 18:12:05.950: INFO: (3) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.653848ms)
Feb 20 18:12:05.976: INFO: (4) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.278337ms)
Feb 20 18:12:06.003: INFO: (5) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.814166ms)
Feb 20 18:12:06.029: INFO: (6) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.141324ms)
Feb 20 18:12:06.056: INFO: (7) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.552397ms)
Feb 20 18:12:06.083: INFO: (8) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.723508ms)
Feb 20 18:12:06.109: INFO: (9) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.380096ms)
Feb 20 18:12:06.135: INFO: (10) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.886025ms)
Feb 20 18:12:06.162: INFO: (11) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.545657ms)
Feb 20 18:12:06.188: INFO: (12) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.976667ms)
Feb 20 18:12:06.213: INFO: (13) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.582585ms)
Feb 20 18:12:06.239: INFO: (14) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.040074ms)
Feb 20 18:12:06.265: INFO: (15) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.682098ms)
Feb 20 18:12:06.291: INFO: (16) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.246824ms)
Feb 20 18:12:06.317: INFO: (17) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.508212ms)
Feb 20 18:12:06.342: INFO: (18) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.072553ms)
Feb 20 18:12:06.368: INFO: (19) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.347999ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:12:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-6645n" for this suite.
Feb 20 18:12:12.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:13.133: INFO: namespace: e2e-tests-proxy-6645n, resource: bindings, ignored listing per whitelist
Feb 20 18:12:13.385: INFO: namespace e2e-tests-proxy-6645n deletion completed in 6.993314772s

 [SLOW TEST:8.736 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:13.386: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kn4l5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-0cc75ea6-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:12:14.575: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-kn4l5" to be "success or failure"
Feb 20 18:12:14.598: INFO: Pod "pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.609626ms
Feb 20 18:12:16.622: INFO: Pod "pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39": Phase="Running", Reason="", readiness=true. Elapsed: 2.0462492s
Feb 20 18:12:18.647: INFO: Pod "pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071283358s
STEP: Saw pod success
Feb 20 18:12:18.647: INFO: Pod "pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:12:18.670: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:12:18.730: INFO: Waiting for pod pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:12:18.753: INFO: Pod pod-projected-secrets-0ccb15ce-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:12:18.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kn4l5" for this suite.
Feb 20 18:12:24.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:25.451: INFO: namespace: e2e-tests-projected-kn4l5, resource: bindings, ignored listing per whitelist
Feb 20 18:12:25.732: INFO: namespace e2e-tests-projected-kn4l5 deletion completed in 6.955058935s

 [SLOW TEST:12.347 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:25.733: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xb6bg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:12:26.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-xb6bg" to be "success or failure"
Feb 20 18:12:26.946: INFO: Pod "downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.067614ms
Feb 20 18:12:28.969: INFO: Pod "downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046940761s
STEP: Saw pod success
Feb 20 18:12:28.970: INFO: Pod "downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:12:28.993: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:12:29.055: INFO: Waiting for pod downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:12:29.078: INFO: Pod downwardapi-volume-1426f416-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:12:29.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xb6bg" for this suite.
Feb 20 18:12:35.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:35.703: INFO: namespace: e2e-tests-projected-xb6bg, resource: bindings, ignored listing per whitelist
Feb 20 18:12:36.123: INFO: namespace e2e-tests-projected-xb6bg deletion completed in 7.020957676s

 [SLOW TEST:10.390 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:36.123: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-vgxvg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 20 18:12:37.233: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 18:12:37.280: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 18:12:37.303: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk before test
Feb 20 18:12:37.339: INFO: metrics-server-6c5b747679-8tgqt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 18:12:37.339: INFO: addons-kube-lego-648f8c9f5c-c9t8z from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 18:12:37.339: INFO: blackbox-exporter-64f6f7f998-h9cxk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 18:12:37.339: INFO: kube-proxy-2lxpj from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:12:37.339: INFO: vpn-shoot-56b45cd8c8-2p7dt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 18:12:37.339: INFO: coredns-5f4748c5f-5qhjk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container coredns ready: true, restart count 0
Feb 20 18:12:37.339: INFO: addons-kubernetes-dashboard-5f64f76bd-h2vfh from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 18:12:37.339: INFO: calico-node-mmtdl from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:12:37.339: INFO: node-exporter-c4nt2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:12:37.339: INFO: addons-nginx-ingress-controller-55d976867d-ttlp2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 18:12:37.339: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-sn6mb from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.339: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 18:12:37.340: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd before test
Feb 20 18:12:37.392: INFO: kube-proxy-j2qvq from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.392: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:12:37.392: INFO: calico-node-6skx5 from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.392: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:12:37.392: INFO: node-exporter-prqpl from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:12:37.392: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15852506b7643947], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:12:38.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-vgxvg" for this suite.
Feb 20 18:12:44.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:45.050: INFO: namespace: e2e-tests-sched-pred-vgxvg, resource: bindings, ignored listing per whitelist
Feb 20 18:12:45.572: INFO: namespace e2e-tests-sched-pred-vgxvg deletion completed in 7.032562506s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

 [SLOW TEST:9.449 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:45.573: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-h6f9n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-1fea8c32-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:12:46.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-h6f9n" to be "success or failure"
Feb 20 18:12:46.707: INFO: Pod "pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.055036ms
Feb 20 18:12:48.731: INFO: Pod "pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046997503s
STEP: Saw pod success
Feb 20 18:12:48.731: INFO: Pod "pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:12:48.755: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:12:48.817: INFO: Waiting for pod pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:12:48.840: INFO: Pod pod-configmaps-1fee2e3a-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:12:48.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-h6f9n" for this suite.
Feb 20 18:12:56.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:12:57.003: INFO: namespace: e2e-tests-configmap-h6f9n, resource: bindings, ignored listing per whitelist
Feb 20 18:12:57.909: INFO: namespace e2e-tests-configmap-h6f9n deletion completed in 9.045074123s

 [SLOW TEST:12.337 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:12:57.909: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-95hdd
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-275ddf4e-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:12:59.182: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-95hdd" to be "success or failure"
Feb 20 18:12:59.205: INFO: Pod "pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.910918ms
Feb 20 18:13:01.229: INFO: Pod "pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046690465s
Feb 20 18:13:03.253: INFO: Pod "pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071139935s
STEP: Saw pod success
Feb 20 18:13:03.253: INFO: Pod "pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:13:03.277: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:13:03.337: INFO: Waiting for pod pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:13:03.361: INFO: Pod pod-projected-secrets-27618262-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:13:03.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-95hdd" for this suite.
Feb 20 18:13:09.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:13:10.346: INFO: namespace: e2e-tests-projected-95hdd, resource: bindings, ignored listing per whitelist
Feb 20 18:13:10.346: INFO: namespace e2e-tests-projected-95hdd deletion completed in 6.962170226s

 [SLOW TEST:12.436 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:13:10.346: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-j4xwb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-j4xwb/configmap-test-2ed4848a-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:13:11.704: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-j4xwb" to be "success or failure"
Feb 20 18:13:11.727: INFO: Pod "pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.90411ms
Feb 20 18:13:13.751: INFO: Pod "pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046917612s
Feb 20 18:13:15.776: INFO: Pod "pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07103447s
STEP: Saw pod success
Feb 20 18:13:15.776: INFO: Pod "pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:13:15.799: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39 container env-test: <nil>
STEP: delete the pod
Feb 20 18:13:15.858: INFO: Waiting for pod pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:13:15.882: INFO: Pod pod-configmaps-2ed82c2d-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:13:15.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-j4xwb" for this suite.
Feb 20 18:13:21.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:13:22.545: INFO: namespace: e2e-tests-configmap-j4xwb, resource: bindings, ignored listing per whitelist
Feb 20 18:13:22.947: INFO: namespace e2e-tests-configmap-j4xwb deletion completed in 7.040754756s

 [SLOW TEST:12.600 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:13:22.947: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-9xm7m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 20 18:13:24.167: INFO: Waiting up to 5m0s for pod "var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-var-expansion-9xm7m" to be "success or failure"
Feb 20 18:13:24.190: INFO: Pod "var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.783605ms
Feb 20 18:13:26.221: INFO: Pod "var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05375736s
STEP: Saw pod success
Feb 20 18:13:26.221: INFO: Pod "var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:13:26.244: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:13:26.306: INFO: Waiting for pod var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:13:26.329: INFO: Pod var-expansion-3645de2c-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:13:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-9xm7m" for this suite.
Feb 20 18:13:32.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:13:33.075: INFO: namespace: e2e-tests-var-expansion-9xm7m, resource: bindings, ignored listing per whitelist
Feb 20 18:13:33.315: INFO: namespace e2e-tests-var-expansion-9xm7m deletion completed in 6.961611115s

 [SLOW TEST:10.368 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:13:33.315: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-dpg49
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-dpg49
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 18:13:34.432: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 18:13:54.901: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.16:8080/dial?request=hostName&protocol=http&host=100.96.1.15&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-dpg49 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:13:54.901: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:13:55.537: INFO: Waiting for endpoints: map[]
Feb 20 18:13:55.560: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.16:8080/dial?request=hostName&protocol=http&host=100.96.0.10&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-dpg49 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:13:55.560: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:13:56.049: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:13:56.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-dpg49" for this suite.
Feb 20 18:14:20.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:14:20.660: INFO: namespace: e2e-tests-pod-network-test-dpg49, resource: bindings, ignored listing per whitelist
Feb 20 18:14:21.166: INFO: namespace e2e-tests-pod-network-test-dpg49 deletion completed in 25.093490321s

 [SLOW TEST:47.851 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:14:21.166: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-fpkgx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 20 18:14:22.361: INFO: Waiting up to 5m0s for pod "var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-var-expansion-fpkgx" to be "success or failure"
Feb 20 18:14:22.384: INFO: Pod "var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.954514ms
Feb 20 18:14:24.408: INFO: Pod "var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046748891s
STEP: Saw pod success
Feb 20 18:14:24.408: INFO: Pod "var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:14:24.432: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:14:24.492: INFO: Waiting for pod var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:14:24.516: INFO: Pod var-expansion-58f59d1a-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:14:24.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-fpkgx" for this suite.
Feb 20 18:14:30.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:14:31.415: INFO: namespace: e2e-tests-var-expansion-fpkgx, resource: bindings, ignored listing per whitelist
Feb 20 18:14:31.506: INFO: namespace e2e-tests-var-expansion-fpkgx deletion completed in 6.966938922s

 [SLOW TEST:10.340 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:14:31.506: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-sxqw2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 18:14:36.872: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 18:14:36.895: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 18:14:38.896: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 18:14:38.920: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 18:14:40.896: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 18:14:40.920: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 18:14:42.896: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 18:14:42.919: INFO: Pod pod-with-poststart-http-hook still exists
Feb 20 18:14:44.896: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 20 18:14:44.920: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:14:44.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-sxqw2" for this suite.
Feb 20 18:15:09.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:09.408: INFO: namespace: e2e-tests-container-lifecycle-hook-sxqw2, resource: bindings, ignored listing per whitelist
Feb 20 18:15:09.897: INFO: namespace e2e-tests-container-lifecycle-hook-sxqw2 deletion completed in 24.953012705s

 [SLOW TEST:38.390 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:15:09.897: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-fl6mk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:15:11.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fl6mk" for this suite.
Feb 20 18:15:35.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:15:35.429: INFO: namespace: e2e-tests-pods-fl6mk, resource: bindings, ignored listing per whitelist
Feb 20 18:15:36.097: INFO: namespace e2e-tests-pods-fl6mk deletion completed in 24.993522783s

 [SLOW TEST:26.200 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:15:36.097: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-6pkql
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:15:37.236: INFO: Creating deployment "nginx-deployment"
Feb 20 18:15:37.261: INFO: Waiting for observed generation 1
Feb 20 18:15:37.284: INFO: Waiting for all required pods to come up
Feb 20 18:15:37.308: INFO: Pod name nginx: Found 1 pods out of 10
Feb 20 18:15:42.335: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 20 18:15:48.383: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 20 18:15:48.430: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 20 18:15:48.478: INFO: Updating deployment nginx-deployment
Feb 20 18:15:48.478: INFO: Waiting for observed generation 2
Feb 20 18:15:50.526: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 20 18:15:50.550: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 20 18:15:50.573: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 18:15:50.643: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 20 18:15:50.643: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 20 18:15:50.668: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 20 18:15:50.715: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 20 18:15:50.715: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 20 18:15:50.763: INFO: Updating deployment nginx-deployment
Feb 20 18:15:50.763: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 20 18:15:50.822: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 20 18:15:52.921: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 18:15:52.968: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-6pkql,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6pkql/deployments/nginx-deployment,UID:859b8fa3-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3946,Generation:3,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:9,UnavailableReplicas:24,Conditions:[{Available False 2019-02-20 18:15:50 +0000 UTC 2019-02-20 18:15:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 18:15:52 +0000 UTC 2019-02-20 18:15:37 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:9,CollisionCount:nil,},}

Feb 20 18:15:52.992: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-6pkql,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6pkql/replicasets/nginx-deployment-7dc8f79789,UID:8c4c7d50-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3911,Generation:3,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 859b8fa3-353b-11e9-a0a3-2a5d9248fe67 0xc0017aefb7 0xc0017aefb8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:15:52.992: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 20 18:15:52.992: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-6pkql,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-6pkql/replicasets/nginx-deployment-7f9675fb8b,UID:859ce749-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3945,Generation:3,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 859b8fa3-353b-11e9-a0a3-2a5d9248fe67 0xc0017af087 0xc0017af088}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:9,AvailableReplicas:9,Conditions:[],},}
Feb 20 18:15:53.024: INFO: Pod "nginx-deployment-7dc8f79789-4hqd5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-4hqd5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-4hqd5,UID:8c597749-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3850,Generation:0,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.16/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbce40 0xc001bbce41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbceb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbced0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-5kc8z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-5kc8z,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-5kc8z,UID:8db6a492-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3937,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbcfa0 0xc001bbcfa1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-bm5s5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-bm5s5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-bm5s5,UID:8db6a7ad-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3949,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd100 0xc001bbd101}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-bxhwl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-bxhwl,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-bxhwl,UID:8db6983a-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3950,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.23/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd260 0xc001bbd261}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-dp6d9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-dp6d9,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-dp6d9,UID:8c666d2d-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3848,Generation:0,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.29/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd3c0 0xc001bbd3c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-gbxrt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gbxrt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-gbxrt,UID:8dabdfac-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3934,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.20/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd520 0xc001bbd521}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd5b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-gdj6p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gdj6p,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-gdj6p,UID:8dab4501-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3928,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.17/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd680 0xc001bbd681}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-h4rqn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-h4rqn,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-h4rqn,UID:8dc03fa0-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3938,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd7e0 0xc001bbd7e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-htbjq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-htbjq,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-htbjq,UID:8c4de713-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3951,Generation:0,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.27/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbd950 0xc001bbd951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbd9c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbd9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.27,StartTime:2019-02-20 18:15:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.025: INFO: Pod "nginx-deployment-7dc8f79789-kpxgk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-kpxgk,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-kpxgk,UID:8db6d09b-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3947,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbdaf0 0xc001bbdaf1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbdb60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbdb80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7dc8f79789-tfpp9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-tfpp9,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-tfpp9,UID:8db5f4be-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3931,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.31/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbdc70 0xc001bbdc71}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbdd00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbdd20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7dc8f79789-v75s2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-v75s2,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-v75s2,UID:8c572bde-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3854,Generation:0,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.15/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc001bbde00 0xc001bbde01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001bbde70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001bbdea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.0.15,StartTime:2019-02-20 18:15:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7dc8f79789-zd47x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-zd47x,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7dc8f79789-zd47x,UID:8c5736d0-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3846,Generation:0,CreationTimestamp:2019-02-20 18:15:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.28/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 8c4c7d50-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6000 0xc0020a6001}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a60b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a60d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-5k6br" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-5k6br,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-5k6br,UID:8dab6f0d-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3939,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.21/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a61a0 0xc0020a61a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-6mcj6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-6mcj6,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-6mcj6,UID:85aa1b39-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3765,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.22/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6310 0xc0020a6311}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.22,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://00d15bc1d49606309d2ff02f193dfda65594c6b2b3d1ec087390f90a81894756}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-7qlrd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-7qlrd,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-7qlrd,UID:85b4b79d-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3790,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.13/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6510 0xc0020a6511}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a65e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.0.13,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f666521be9e214c631a83c224d8101ccf804253577b567cf26414a51a48b5b96}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-fl8mz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fl8mz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-fl8mz,UID:8db6140d-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3929,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a66d0 0xc0020a66d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-hbnf5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-hbnf5,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-hbnf5,UID:8db7188c-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3948,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.24/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6810 0xc0020a6811}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.026: INFO: Pod "nginx-deployment-7f9675fb8b-jkq9v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jkq9v,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-jkq9v,UID:859e6931-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3777,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.21/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6950 0xc0020a6951}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a69e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.21,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://be67b00b958b505f3460c6d2116751287acbd082e103d63c51792d5c6f0cae89}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-jsvcl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-jsvcl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-jsvcl,UID:8daacaa4-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3933,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.19/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6ba0 0xc0020a6ba1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-k7kzz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-k7kzz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-k7kzz,UID:8db6147b-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3936,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6ce0 0xc0020a6ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6d40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-llgm7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-llgm7,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-llgm7,UID:8db615b3-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3942,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.22/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6e20 0xc0020a6e21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-m6c7k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-m6c7k,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-m6c7k,UID:85aa228b-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3784,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.11/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a6f60 0xc0020a6f61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a6fc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a6fe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.0.11,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:41 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ebb6d7e3ec24a89ed4812b9f824656e8c127fcdfabf3cf150a81442165e42889}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-nf7fq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-nf7fq,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-nf7fq,UID:85aace18-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3771,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.25/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a70c0 0xc0020a70c1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.25,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://2934c1ccdb94cc34ef06d799d0fcb4d2e73a3a92b23eaea3e06c208b3343aaec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-pkp97" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-pkp97,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-pkp97,UID:85aacd7e-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3779,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.24/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7210 0xc0020a7211}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.24,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://b3ef9ab404b12e8c80bf556c70274eb935c942d11b4eba8fde6de649a12d0f97}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-pwzlt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-pwzlt,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-pwzlt,UID:8db72eb2-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3940,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7360 0xc0020a7361}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a73c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a73e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-qh74j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-qh74j,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-qh74j,UID:85b4b1bf-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3774,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a74a0 0xc0020a74a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.26,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://da4406efa042bd6a3050b16592ce1fc01ba7f22c2045951fbc5be47c9ac7007a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.027: INFO: Pod "nginx-deployment-7f9675fb8b-swvv2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-swvv2,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-swvv2,UID:8db73714-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3941,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a75f0 0xc0020a75f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.028: INFO: Pod "nginx-deployment-7f9675fb8b-t6nhz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-t6nhz,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-t6nhz,UID:8db73946-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3943,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7730 0xc0020a7731}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a77b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.028: INFO: Pod "nginx-deployment-7f9675fb8b-trn5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-trn5p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-trn5p,UID:8db73a2a-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3952,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.26/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7870 0xc0020a7871}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a78d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a78f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.028: INFO: Pod "nginx-deployment-7f9675fb8b-wfv5b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-wfv5b,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-wfv5b,UID:85aace9b-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3778,Generation:0,CreationTimestamp:2019-02-20 18:15:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.23/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a79b0 0xc0020a79b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:37 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.23,StartTime:2019-02-20 18:15:37 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:38 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://445d4c9acc716c2bbb36f9b202859df00068e0f516493b6072a963e4784e0ed0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.028: INFO: Pod "nginx-deployment-7f9675fb8b-xkxpk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-xkxpk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-xkxpk,UID:8db618c5-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3935,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7b00 0xc0020a7b01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 20 18:15:53.028: INFO: Pod "nginx-deployment-7f9675fb8b-zbcmn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-zbcmn,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-6pkql,SelfLink:/api/v1/namespaces/e2e-tests-deployment-6pkql/pods/nginx-deployment-7f9675fb8b-zbcmn,UID:8dab729b-353b-11e9-a0a3-2a5d9248fe67,ResourceVersion:3944,Generation:0,CreationTimestamp:2019-02-20 18:15:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.18/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 859ce749-353b-11e9-a0a3-2a5d9248fe67 0xc0020a7c40 0xc0020a7c41}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-p2zws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-p2zws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-p2zws true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020a7ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020a7cc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:15:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.2,PodIP:100.96.0.18,StartTime:2019-02-20 18:15:50 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-20 18:15:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ac122b62224bc2cf429cbcebdfa3b484cd07ff138dcf471d946f173ef6f5e470}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:15:53.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-6pkql" for this suite.
Feb 20 18:16:01.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:16:02.043: INFO: namespace: e2e-tests-deployment-6pkql, resource: bindings, ignored listing per whitelist
Feb 20 18:16:02.044: INFO: namespace e2e-tests-deployment-6pkql deletion completed in 8.991794579s

 [SLOW TEST:25.947 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:16:02.044: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-h6klx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-9509b7b6-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:16:03.180: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-h6klx" to be "success or failure"
Feb 20 18:16:03.203: INFO: Pod "pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.984515ms
Feb 20 18:16:05.227: INFO: Pod "pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047451452s
Feb 20 18:16:07.251: INFO: Pod "pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071615308s
STEP: Saw pod success
Feb 20 18:16:07.251: INFO: Pod "pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:16:07.275: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:16:07.334: INFO: Waiting for pod pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:16:07.356: INFO: Pod pod-projected-secrets-950d4ad2-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:16:07.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h6klx" for this suite.
Feb 20 18:16:13.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:16:14.272: INFO: namespace: e2e-tests-projected-h6klx, resource: bindings, ignored listing per whitelist
Feb 20 18:16:14.551: INFO: namespace e2e-tests-projected-h6klx deletion completed in 7.170648347s

 [SLOW TEST:12.507 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:16:14.551: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-5d5nx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 20 18:16:15.632: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 18:16:15.679: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 18:16:15.702: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk before test
Feb 20 18:16:15.734: INFO: coredns-5f4748c5f-5qhjk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container coredns ready: true, restart count 0
Feb 20 18:16:15.734: INFO: blackbox-exporter-64f6f7f998-h9cxk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 18:16:15.734: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-sn6mb from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 18:16:15.734: INFO: vpn-shoot-56b45cd8c8-2p7dt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 18:16:15.734: INFO: metrics-server-6c5b747679-8tgqt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 18:16:15.734: INFO: addons-kube-lego-648f8c9f5c-c9t8z from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 18:16:15.734: INFO: kube-proxy-2lxpj from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 18:16:15.734: INFO: addons-nginx-ingress-controller-55d976867d-ttlp2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 18:16:15.734: INFO: addons-kubernetes-dashboard-5f64f76bd-h2vfh from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.734: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 18:16:15.734: INFO: calico-node-mmtdl from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.735: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:16:15.735: INFO: node-exporter-c4nt2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.735: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:16:15.735: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd before test
Feb 20 18:16:15.806: INFO: calico-node-6skx5 from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.806: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 18:16:15.806: INFO: node-exporter-prqpl from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.806: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 18:16:15.806: INFO: kube-proxy-j2qvq from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 18:16:15.806: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
STEP: verifying the node has the label node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
Feb 20 18:16:15.956: INFO: Pod addons-kube-lego-648f8c9f5c-c9t8z requesting resource cpu=20m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod addons-kubernetes-dashboard-5f64f76bd-h2vfh requesting resource cpu=50m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod addons-nginx-ingress-controller-55d976867d-ttlp2 requesting resource cpu=100m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-sn6mb requesting resource cpu=0m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod blackbox-exporter-64f6f7f998-h9cxk requesting resource cpu=5m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod calico-node-6skx5 requesting resource cpu=100m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
Feb 20 18:16:15.956: INFO: Pod calico-node-mmtdl requesting resource cpu=100m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod coredns-5f4748c5f-5qhjk requesting resource cpu=50m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod kube-proxy-2lxpj requesting resource cpu=20m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod kube-proxy-j2qvq requesting resource cpu=20m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
Feb 20 18:16:15.956: INFO: Pod metrics-server-6c5b747679-8tgqt requesting resource cpu=20m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod node-exporter-c4nt2 requesting resource cpu=5m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
Feb 20 18:16:15.956: INFO: Pod node-exporter-prqpl requesting resource cpu=5m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
Feb 20 18:16:15.956: INFO: Pod vpn-shoot-56b45cd8c8-2p7dt requesting resource cpu=50m on Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39.1585253996aa0e8c], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-5d5nx/filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39 to shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39.15852539c5af94ac], Reason = [Pulling], Message = [pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39.15852539dcb30d2b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39.15852539e04f85ab], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9caeef08-353b-11e9-9b2a-cafe91372a39.15852539e969236a], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb30ec9-353b-11e9-9b2a-cafe91372a39.158525399818e974], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-5d5nx/filler-pod-9cb30ec9-353b-11e9-9b2a-cafe91372a39 to shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb30ec9-353b-11e9-9b2a-cafe91372a39.15852539c41b5eb2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb30ec9-353b-11e9-9b2a-cafe91372a39.15852539c791324c], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9cb30ec9-353b-11e9-9b2a-cafe91372a39.15852539d0d351be], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1585253a177f6ce1], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:16:19.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-5d5nx" for this suite.
Feb 20 18:16:25.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:16:25.719: INFO: namespace: e2e-tests-sched-pred-5d5nx, resource: bindings, ignored listing per whitelist
Feb 20 18:16:26.669: INFO: namespace e2e-tests-sched-pred-5d5nx deletion completed in 7.328025086s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

 [SLOW TEST:12.117 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:16:26.669: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-s5kpt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 18:16:28.045: INFO: Waiting up to 5m0s for pod "pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-s5kpt" to be "success or failure"
Feb 20 18:16:28.070: INFO: Pod "pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 24.669402ms
Feb 20 18:16:30.094: INFO: Pod "pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048680122s
Feb 20 18:16:32.118: INFO: Pod "pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072545067s
STEP: Saw pod success
Feb 20 18:16:32.118: INFO: Pod "pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:16:32.141: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:16:32.233: INFO: Waiting for pod pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:16:32.256: INFO: Pod pod-a3dfaf9c-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:16:32.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-s5kpt" for this suite.
Feb 20 18:16:38.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:16:38.538: INFO: namespace: e2e-tests-emptydir-s5kpt, resource: bindings, ignored listing per whitelist
Feb 20 18:16:39.278: INFO: namespace e2e-tests-emptydir-s5kpt deletion completed in 6.997482773s

 [SLOW TEST:12.609 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:16:39.278: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-crvfb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 18:16:48.569: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:48.592: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:16:50.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:50.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:16:52.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:52.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:16:54.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:54.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:16:56.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:56.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:16:58.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:16:58.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:00.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:00.618: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:02.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:02.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:04.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:04.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:06.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:06.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:08.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:08.617: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:10.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:10.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:12.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:12.616: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 20 18:17:14.593: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 20 18:17:14.616: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:17:14.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-crvfb" for this suite.
Feb 20 18:17:50.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:17:51.622: INFO: namespace: e2e-tests-container-lifecycle-hook-crvfb, resource: bindings, ignored listing per whitelist
Feb 20 18:17:51.715: INFO: namespace e2e-tests-container-lifecycle-hook-crvfb deletion completed in 37.042201631s

 [SLOW TEST:72.437 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:17:51.715: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jzsvb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-d66c4a58-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:17:52.877: INFO: Waiting up to 5m0s for pod "pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-jzsvb" to be "success or failure"
Feb 20 18:17:52.900: INFO: Pod "pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.765403ms
Feb 20 18:17:54.924: INFO: Pod "pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046879497s
STEP: Saw pod success
Feb 20 18:17:54.924: INFO: Pod "pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:17:54.948: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:17:55.008: INFO: Waiting for pod pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:17:55.031: INFO: Pod pod-configmaps-d66fdf91-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:17:55.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jzsvb" for this suite.
Feb 20 18:18:01.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:18:01.775: INFO: namespace: e2e-tests-configmap-jzsvb, resource: bindings, ignored listing per whitelist
Feb 20 18:18:02.013: INFO: namespace e2e-tests-configmap-jzsvb deletion completed in 6.95702892s

 [SLOW TEST:10.298 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:18:02.013: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6tfnv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-dc906190-353b-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:18:03.181: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-6tfnv" to be "success or failure"
Feb 20 18:18:03.203: INFO: Pod "pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.698215ms
Feb 20 18:18:05.228: INFO: Pod "pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046744057s
STEP: Saw pod success
Feb 20 18:18:05.228: INFO: Pod "pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:18:05.251: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:18:05.317: INFO: Waiting for pod pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:18:05.340: INFO: Pod pod-projected-configmaps-dc93fd90-353b-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:18:05.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6tfnv" for this suite.
Feb 20 18:18:11.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:18:11.994: INFO: namespace: e2e-tests-projected-6tfnv, resource: bindings, ignored listing per whitelist
Feb 20 18:18:12.322: INFO: namespace e2e-tests-projected-6tfnv deletion completed in 6.958298099s

 [SLOW TEST:10.309 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:18:12.322: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d8fnw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:18:13.531: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-d8fnw'
Feb 20 18:18:13.802: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 20 18:18:13.802: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Feb 20 18:18:13.825: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-d8fnw'
Feb 20 18:18:14.038: INFO: stderr: ""
Feb 20 18:18:14.038: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:18:14.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d8fnw" for this suite.
Feb 20 18:18:38.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:18:39.019: INFO: namespace: e2e-tests-kubectl-d8fnw, resource: bindings, ignored listing per whitelist
Feb 20 18:18:39.019: INFO: namespace e2e-tests-kubectl-d8fnw deletion completed in 24.956795198s

 [SLOW TEST:26.697 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:18:39.019: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-8nbqt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:18:40.326: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f2b5884a-353b-11e9-a0a3-2a5d9248fe67", Controller:(*bool)(0xc0010fcc92), BlockOwnerDeletion:(*bool)(0xc0010fcc93)}}
Feb 20 18:18:40.351: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f2ae363d-353b-11e9-a0a3-2a5d9248fe67", Controller:(*bool)(0xc0017ee28a), BlockOwnerDeletion:(*bool)(0xc0017ee28b)}}
Feb 20 18:18:40.376: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f2b1e792-353b-11e9-a0a3-2a5d9248fe67", Controller:(*bool)(0xc0008f9e6a), BlockOwnerDeletion:(*bool)(0xc0008f9e6b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:18:45.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8nbqt" for this suite.
Feb 20 18:18:51.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:18:51.632: INFO: namespace: e2e-tests-gc-8nbqt, resource: bindings, ignored listing per whitelist
Feb 20 18:18:52.440: INFO: namespace e2e-tests-gc-8nbqt deletion completed in 6.991195313s

 [SLOW TEST:13.421 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:18:52.441: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-r5pbz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:18:53.590: INFO: (0) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 27.625616ms)
Feb 20 18:18:53.633: INFO: (1) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.294903ms)
Feb 20 18:18:53.660: INFO: (2) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.27219ms)
Feb 20 18:18:53.686: INFO: (3) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.252105ms)
Feb 20 18:18:53.712: INFO: (4) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.640406ms)
Feb 20 18:18:53.738: INFO: (5) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.330789ms)
Feb 20 18:18:53.765: INFO: (6) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.358515ms)
Feb 20 18:18:53.791: INFO: (7) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.847388ms)
Feb 20 18:18:53.816: INFO: (8) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.681686ms)
Feb 20 18:18:53.842: INFO: (9) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.541535ms)
Feb 20 18:18:53.868: INFO: (10) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.840016ms)
Feb 20 18:18:53.895: INFO: (11) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.674211ms)
Feb 20 18:18:53.921: INFO: (12) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.117976ms)
Feb 20 18:18:53.947: INFO: (13) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.828759ms)
Feb 20 18:18:53.972: INFO: (14) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.257317ms)
Feb 20 18:18:54.056: INFO: (15) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 83.567827ms)
Feb 20 18:18:54.081: INFO: (16) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.636876ms)
Feb 20 18:18:54.108: INFO: (17) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 25.965593ms)
Feb 20 18:18:54.134: INFO: (18) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.274066ms)
Feb 20 18:18:54.160: INFO: (19) /api/v1/nodes/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 26.019265ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:18:54.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-r5pbz" for this suite.
Feb 20 18:19:00.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:19:00.882: INFO: namespace: e2e-tests-proxy-r5pbz, resource: bindings, ignored listing per whitelist
Feb 20 18:19:01.176: INFO: namespace e2e-tests-proxy-r5pbz deletion completed in 6.991863244s

 [SLOW TEST:8.735 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:19:01.176: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-242pq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0220 18:19:12.378410   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:19:12.378: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:19:12.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-242pq" for this suite.
Feb 20 18:19:18.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:19:18.952: INFO: namespace: e2e-tests-gc-242pq, resource: bindings, ignored listing per whitelist
Feb 20 18:19:19.373: INFO: namespace e2e-tests-gc-242pq deletion completed in 6.97055073s

 [SLOW TEST:18.197 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:19:19.373: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-q4wnh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:19:20.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-q4wnh" to be "success or failure"
Feb 20 18:19:20.579: INFO: Pod "downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.531096ms
Feb 20 18:19:22.621: INFO: Pod "downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.06499611s
STEP: Saw pod success
Feb 20 18:19:22.621: INFO: Pod "downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:19:22.644: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:19:22.734: INFO: Waiting for pod downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:19:22.757: INFO: Pod downwardapi-volume-0ab289b4-353c-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:19:22.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q4wnh" for this suite.
Feb 20 18:19:28.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:19:29.016: INFO: namespace: e2e-tests-downward-api-q4wnh, resource: bindings, ignored listing per whitelist
Feb 20 18:19:29.742: INFO: namespace e2e-tests-downward-api-q4wnh deletion completed in 6.95978572s

 [SLOW TEST:10.369 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:19:29.742: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-c4gms
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-10e85ffa-353c-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-10e85ffa-353c-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:19:35.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-c4gms" for this suite.
Feb 20 18:19:59.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:20:00.124: INFO: namespace: e2e-tests-projected-c4gms, resource: bindings, ignored listing per whitelist
Feb 20 18:20:00.215: INFO: namespace e2e-tests-projected-c4gms deletion completed in 25.003084372s

 [SLOW TEST:30.473 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:20:00.215: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-hgsrf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:20:01.382: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 18:20:03.430: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 20 18:20:05.454: INFO: Creating deployment "test-rollover-deployment"
Feb 20 18:20:05.501: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 20 18:20:07.547: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 20 18:20:07.595: INFO: Ensure that both replica sets have 1 created replica
Feb 20 18:20:07.642: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 20 18:20:07.691: INFO: Updating deployment test-rollover-deployment
Feb 20 18:20:07.691: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 20 18:20:09.737: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 20 18:20:09.785: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 20 18:20:09.832: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:09.832: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283607, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:11.880: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:11.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283609, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:13.880: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:13.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283609, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:15.881: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:15.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283609, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:17.881: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:17.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283609, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:19.880: INFO: all replica sets need to contain the pod-template-hash label
Feb 20 18:20:19.880: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283609, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283605, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 18:20:21.881: INFO: 
Feb 20 18:20:21.881: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 18:20:21.951: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-hgsrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hgsrf/deployments/test-rollover-deployment,UID:257a48b2-353c-11e9-a0a3-2a5d9248fe67,ResourceVersion:4940,Generation:2,CreationTimestamp:2019-02-20 18:20:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 18:20:05 +0000 UTC 2019-02-20 18:20:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 18:20:20 +0000 UTC 2019-02-20 18:20:05 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 18:20:21.975: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-hgsrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hgsrf/replicasets/test-rollover-deployment-5b76ff8c4,UID:26cca177-353c-11e9-a0a3-2a5d9248fe67,ResourceVersion:4933,Generation:2,CreationTimestamp:2019-02-20 18:20:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 257a48b2-353c-11e9-a0a3-2a5d9248fe67 0xc00100c9f0 0xc00100c9f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 18:20:21.975: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 20 18:20:21.975: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-hgsrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hgsrf/replicasets/test-rollover-controller,UID:230562c5-353c-11e9-a0a3-2a5d9248fe67,ResourceVersion:4939,Generation:2,CreationTimestamp:2019-02-20 18:20:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 257a48b2-353c-11e9-a0a3-2a5d9248fe67 0xc00100c8af 0xc00100c8c0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:20:21.975: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-hgsrf,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-hgsrf/replicasets/test-rollover-deployment-6975f4fb87,UID:257cdd7b-353c-11e9-a0a3-2a5d9248fe67,ResourceVersion:4898,Generation:2,CreationTimestamp:2019-02-20 18:20:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 257a48b2-353c-11e9-a0a3-2a5d9248fe67 0xc00100cdc7 0xc00100cdc8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 18:20:21.999: INFO: Pod "test-rollover-deployment-5b76ff8c4-qphtl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-qphtl,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-hgsrf,SelfLink:/api/v1/namespaces/e2e-tests-deployment-hgsrf/pods/test-rollover-deployment-5b76ff8c4-qphtl,UID:26d6027c-353c-11e9-a0a3-2a5d9248fe67,ResourceVersion:4910,Generation:0,CreationTimestamp:2019-02-20 18:20:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.55/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 26cca177-353c-11e9-a0a3-2a5d9248fe67 0xc001b50e50 0xc001b50e51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-642wt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-642wt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-642wt true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b50eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b50f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:20:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:20:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:20:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:20:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.55,StartTime:2019-02-20 18:20:07 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 18:20:09 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ebbcf09c5a26d955f9e4831b735ebceb82f21dcec774428e086c638843f7e941}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:20:21.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-hgsrf" for this suite.
Feb 20 18:20:28.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:20:28.164: INFO: namespace: e2e-tests-deployment-hgsrf, resource: bindings, ignored listing per whitelist
Feb 20 18:20:29.017: INFO: namespace e2e-tests-deployment-hgsrf deletion completed in 6.994009438s

 [SLOW TEST:28.802 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:20:29.018: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-nsgbv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-6n65
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:20:30.207: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6n65" in namespace "e2e-tests-subpath-nsgbv" to be "success or failure"
Feb 20 18:20:30.229: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Pending", Reason="", readiness=false. Elapsed: 22.501913ms
Feb 20 18:20:32.253: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046618269s
Feb 20 18:20:34.278: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 4.070883401s
Feb 20 18:20:36.301: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 6.0948006s
Feb 20 18:20:38.326: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 8.119219164s
Feb 20 18:20:40.350: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 10.143804698s
Feb 20 18:20:42.374: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 12.167775735s
Feb 20 18:20:44.399: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 14.192128004s
Feb 20 18:20:46.423: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 16.216696556s
Feb 20 18:20:48.448: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 18.240982888s
Feb 20 18:20:50.472: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 20.264981449s
Feb 20 18:20:52.497: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Running", Reason="", readiness=false. Elapsed: 22.289942042s
Feb 20 18:20:54.521: INFO: Pod "pod-subpath-test-projected-6n65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.313966026s
STEP: Saw pod success
Feb 20 18:20:54.521: INFO: Pod "pod-subpath-test-projected-6n65" satisfied condition "success or failure"
Feb 20 18:20:54.544: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-subpath-test-projected-6n65 container test-container-subpath-projected-6n65: <nil>
STEP: delete the pod
Feb 20 18:20:54.605: INFO: Waiting for pod pod-subpath-test-projected-6n65 to disappear
Feb 20 18:20:54.628: INFO: Pod pod-subpath-test-projected-6n65 no longer exists
STEP: Deleting pod pod-subpath-test-projected-6n65
Feb 20 18:20:54.628: INFO: Deleting pod "pod-subpath-test-projected-6n65" in namespace "e2e-tests-subpath-nsgbv"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:20:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-nsgbv" for this suite.
Feb 20 18:21:00.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:21:00.902: INFO: namespace: e2e-tests-subpath-nsgbv, resource: bindings, ignored listing per whitelist
Feb 20 18:21:01.720: INFO: namespace e2e-tests-subpath-nsgbv deletion completed in 7.045435106s

 [SLOW TEST:32.702 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:21:01.720: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-lndt8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:21:02.828: INFO: PodSpec: initContainers in spec.initContainers
Feb 20 18:21:45.046: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-47ac1e06-353c-11e9-9b2a-cafe91372a39", GenerateName:"", Namespace:"e2e-tests-init-container-lndt8", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-lndt8/pods/pod-init-47ac1e06-353c-11e9-9b2a-cafe91372a39", UID:"47ad716f-353c-11e9-a0a3-2a5d9248fe67", ResourceVersion:"5158", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63686283662, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"828293194"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.57/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-btfnw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0018282c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-btfnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-btfnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-btfnw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001e224c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002082fc0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e226f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001e22710)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001e22718), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283662, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283662, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283662, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686283662, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.3", PodIP:"100.96.1.57", StartTime:(*v1.Time)(0xc001f7d240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c7b5e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001c7b650)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://5797818fca5743f8aa13e718f39362a517e32bd159efd1ae6d85333df86ded14"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f7d280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001f7d260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:21:45.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-lndt8" for this suite.
Feb 20 18:22:09.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:09.395: INFO: namespace: e2e-tests-init-container-lndt8, resource: bindings, ignored listing per whitelist
Feb 20 18:22:10.101: INFO: namespace e2e-tests-init-container-lndt8 deletion completed in 25.031037821s

 [SLOW TEST:68.381 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:22:10.101: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-tpxjs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-tpxjs/secret-test-707143d6-353c-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:22:11.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-tpxjs" to be "success or failure"
Feb 20 18:22:11.305: INFO: Pod "pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.938558ms
Feb 20 18:22:13.331: INFO: Pod "pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04950128s
Feb 20 18:22:15.355: INFO: Pod "pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074098418s
STEP: Saw pod success
Feb 20 18:22:15.355: INFO: Pod "pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:22:15.379: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39 container env-test: <nil>
STEP: delete the pod
Feb 20 18:22:15.442: INFO: Waiting for pod pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:22:15.465: INFO: Pod pod-configmaps-7074f9d7-353c-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:22:15.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-tpxjs" for this suite.
Feb 20 18:22:21.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:22.238: INFO: namespace: e2e-tests-secrets-tpxjs, resource: bindings, ignored listing per whitelist
Feb 20 18:22:22.445: INFO: namespace e2e-tests-secrets-tpxjs deletion completed in 6.954603169s

 [SLOW TEST:12.345 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:22:22.446: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-b9kzp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-xdwc
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:22:23.610: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xdwc" in namespace "e2e-tests-subpath-b9kzp" to be "success or failure"
Feb 20 18:22:23.633: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.903562ms
Feb 20 18:22:25.657: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046909359s
Feb 20 18:22:27.681: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 4.071680105s
Feb 20 18:22:29.706: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 6.096389495s
Feb 20 18:22:31.730: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 8.120703269s
Feb 20 18:22:33.758: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 10.148016469s
Feb 20 18:22:35.782: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 12.171788179s
Feb 20 18:22:37.807: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 14.196891466s
Feb 20 18:22:39.831: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 16.221714665s
Feb 20 18:22:41.856: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 18.246141151s
Feb 20 18:22:43.882: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 20.271933476s
Feb 20 18:22:45.906: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Running", Reason="", readiness=false. Elapsed: 22.296301096s
Feb 20 18:22:47.930: INFO: Pod "pod-subpath-test-secret-xdwc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.32008057s
STEP: Saw pod success
Feb 20 18:22:47.930: INFO: Pod "pod-subpath-test-secret-xdwc" satisfied condition "success or failure"
Feb 20 18:22:47.953: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-subpath-test-secret-xdwc container test-container-subpath-secret-xdwc: <nil>
STEP: delete the pod
Feb 20 18:22:48.055: INFO: Waiting for pod pod-subpath-test-secret-xdwc to disappear
Feb 20 18:22:48.078: INFO: Pod pod-subpath-test-secret-xdwc no longer exists
STEP: Deleting pod pod-subpath-test-secret-xdwc
Feb 20 18:22:48.078: INFO: Deleting pod "pod-subpath-test-secret-xdwc" in namespace "e2e-tests-subpath-b9kzp"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:22:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-b9kzp" for this suite.
Feb 20 18:22:54.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:22:54.384: INFO: namespace: e2e-tests-subpath-b9kzp, resource: bindings, ignored listing per whitelist
Feb 20 18:22:55.082: INFO: namespace e2e-tests-subpath-b9kzp deletion completed in 6.954788061s

 [SLOW TEST:32.636 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:22:55.082: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-twbs5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-twbs5
Feb 20 18:22:58.210: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-twbs5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:22:58.234: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:26:59.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-twbs5" for this suite.
Feb 20 18:27:05.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:27:06.144: INFO: namespace: e2e-tests-container-probe-twbs5, resource: bindings, ignored listing per whitelist
Feb 20 18:27:06.305: INFO: namespace e2e-tests-container-probe-twbs5 deletion completed in 6.964946144s

 [SLOW TEST:251.223 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:27:06.305: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-pmhz8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-pmhz8.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-pmhz8.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-pmhz8.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-pmhz8.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-pmhz8.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-pmhz8.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 18:27:32.877: INFO: DNS probes using e2e-tests-dns-pmhz8/dns-test-20fdd11e-353d-11e9-9b2a-cafe91372a39 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:27:32.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-pmhz8" for this suite.
Feb 20 18:27:38.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:27:39.612: INFO: namespace: e2e-tests-dns-pmhz8, resource: bindings, ignored listing per whitelist
Feb 20 18:27:39.953: INFO: namespace e2e-tests-dns-pmhz8 deletion completed in 7.024342729s

 [SLOW TEST:33.648 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:27:39.953: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-v9fjz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 20 18:27:45.250: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:27:45.273: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:27:47.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:27:47.298: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:27:49.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:27:49.298: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:27:51.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:27:51.298: INFO: Pod pod-with-prestop-http-hook still exists
Feb 20 18:27:53.274: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 20 18:27:53.298: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:27:53.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-v9fjz" for this suite.
Feb 20 18:28:17.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:28:18.566: INFO: namespace: e2e-tests-container-lifecycle-hook-v9fjz, resource: bindings, ignored listing per whitelist
Feb 20 18:28:18.613: INFO: namespace e2e-tests-container-lifecycle-hook-v9fjz deletion completed in 24.992899041s

 [SLOW TEST:38.660 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:28:18.614: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5hk77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Feb 20 18:28:19.733: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-5hk77'
Feb 20 18:28:21.521: INFO: stderr: ""
Feb 20 18:28:21.521: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 20 18:28:22.544: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:28:22.544: INFO: Found 0 / 1
Feb 20 18:28:23.545: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:28:23.545: INFO: Found 1 / 1
Feb 20 18:28:23.545: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 18:28:23.569: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:28:23.569: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 20 18:28:23.569: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77'
Feb 20 18:28:23.759: INFO: stderr: ""
Feb 20 18:28:23.759: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 18:28:22.683 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 18:28:22.683 # Server started, Redis version 3.2.12\n1:M 20 Feb 18:28:22.683 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 18:28:22.683 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 20 18:28:23.759: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77 --tail=1'
Feb 20 18:28:23.977: INFO: stderr: ""
Feb 20 18:28:23.977: INFO: stdout: "1:M 20 Feb 18:28:22.683 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 20 18:28:23.978: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77 --limit-bytes=1'
Feb 20 18:28:24.195: INFO: stderr: ""
Feb 20 18:28:24.195: INFO: stdout: " "
STEP: exposing timestamps
Feb 20 18:28:24.195: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77 --tail=1 --timestamps'
Feb 20 18:28:24.387: INFO: stderr: ""
Feb 20 18:28:24.387: INFO: stdout: "2019-02-20T18:28:22.683669729Z 1:M 20 Feb 18:28:22.683 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 20 18:28:26.888: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77 --since=1s'
Feb 20 18:28:27.069: INFO: stderr: ""
Feb 20 18:28:27.069: INFO: stdout: ""
Feb 20 18:28:27.069: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml log redis-master-477sf redis-master --namespace=e2e-tests-kubectl-5hk77 --since=24h'
Feb 20 18:28:27.290: INFO: stderr: ""
Feb 20 18:28:27.290: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 18:28:22.683 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 18:28:22.683 # Server started, Redis version 3.2.12\n1:M 20 Feb 18:28:22.683 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 18:28:22.683 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Feb 20 18:28:27.290: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-5hk77'
Feb 20 18:28:27.488: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:28:27.488: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 20 18:28:27.488: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-5hk77'
Feb 20 18:28:27.693: INFO: stderr: "No resources found.\n"
Feb 20 18:28:27.693: INFO: stdout: ""
Feb 20 18:28:27.693: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-5hk77 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 18:28:27.867: INFO: stderr: ""
Feb 20 18:28:27.867: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:28:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5hk77" for this suite.
Feb 20 18:28:33.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:28:34.443: INFO: namespace: e2e-tests-kubectl-5hk77, resource: bindings, ignored listing per whitelist
Feb 20 18:28:34.871: INFO: namespace e2e-tests-kubectl-5hk77 deletion completed in 6.980352886s

 [SLOW TEST:16.258 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:28:34.872: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-9vn2k
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:28:36.030: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:28:39.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-9vn2k" for this suite.
Feb 20 18:28:47.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:28:48.166: INFO: namespace: e2e-tests-init-container-9vn2k, resource: bindings, ignored listing per whitelist
Feb 20 18:28:48.880: INFO: namespace e2e-tests-init-container-9vn2k deletion completed in 8.986624399s

 [SLOW TEST:14.008 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:28:48.880: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-qmdrb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 20 18:28:50.057: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-qmdrb" to be "success or failure"
Feb 20 18:28:50.080: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 22.565311ms
Feb 20 18:28:52.104: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046796125s
Feb 20 18:28:54.129: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071498454s
STEP: Saw pod success
Feb 20 18:28:54.129: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 20 18:28:54.152: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 20 18:28:54.218: INFO: Waiting for pod pod-host-path-test to disappear
Feb 20 18:28:54.242: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:28:54.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-qmdrb" for this suite.
Feb 20 18:29:00.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:29:00.916: INFO: namespace: e2e-tests-hostpath-qmdrb, resource: bindings, ignored listing per whitelist
Feb 20 18:29:01.264: INFO: namespace e2e-tests-hostpath-qmdrb deletion completed in 6.995450461s

 [SLOW TEST:12.384 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:29:01.265: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cwxzk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 20 18:29:02.429: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml api-versions'
Feb 20 18:29:02.632: INFO: stderr: ""
Feb 20 18:29:02.632: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:29:02.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cwxzk" for this suite.
Feb 20 18:29:08.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:29:09.578: INFO: namespace: e2e-tests-kubectl-cwxzk, resource: bindings, ignored listing per whitelist
Feb 20 18:29:09.647: INFO: namespace e2e-tests-kubectl-cwxzk deletion completed in 6.990687941s

 [SLOW TEST:8.382 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:29:09.647: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-p7xr6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0220 18:29:40.924436   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:29:40.924: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:29:40.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-p7xr6" for this suite.
Feb 20 18:29:47.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:29:47.866: INFO: namespace: e2e-tests-gc-p7xr6, resource: bindings, ignored listing per whitelist
Feb 20 18:29:47.981: INFO: namespace e2e-tests-gc-p7xr6 deletion completed in 7.033476921s

 [SLOW TEST:38.334 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:29:47.981: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-wvcdf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 20 18:29:49.132: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml --namespace=e2e-tests-kubectl-wvcdf run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 20 18:29:51.609: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 20 18:29:51.609: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:29:53.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-wvcdf" for this suite.
Feb 20 18:30:03.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:30:04.141: INFO: namespace: e2e-tests-kubectl-wvcdf, resource: bindings, ignored listing per whitelist
Feb 20 18:30:04.666: INFO: namespace e2e-tests-kubectl-wvcdf deletion completed in 10.986645404s

 [SLOW TEST:16.685 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:30:04.666: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-kq9vd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 18:30:16.191219   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:30:16.191: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:30:16.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-kq9vd" for this suite.
Feb 20 18:30:22.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:30:22.879: INFO: namespace: e2e-tests-gc-kq9vd, resource: bindings, ignored listing per whitelist
Feb 20 18:30:23.224: INFO: namespace e2e-tests-gc-kq9vd deletion completed in 7.010047405s

 [SLOW TEST:18.558 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:30:23.224: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-jc8ml
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-965d29c5-353d-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:30:26.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-jc8ml" for this suite.
Feb 20 18:30:48.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:30:49.120: INFO: namespace: e2e-tests-configmap-jc8ml, resource: bindings, ignored listing per whitelist
Feb 20 18:30:49.605: INFO: namespace e2e-tests-configmap-jc8ml deletion completed in 22.992080983s

 [SLOW TEST:26.381 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:30:49.605: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-cmt2j
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-t29d
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:30:50.802: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-t29d" in namespace "e2e-tests-subpath-cmt2j" to be "success or failure"
Feb 20 18:30:50.825: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.836101ms
Feb 20 18:30:52.849: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046938852s
Feb 20 18:30:54.873: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 4.070885289s
Feb 20 18:30:56.898: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 6.095320954s
Feb 20 18:30:58.922: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 8.119329827s
Feb 20 18:31:00.946: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 10.143880346s
Feb 20 18:31:02.970: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 12.167101171s
Feb 20 18:31:04.993: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 14.190994761s
Feb 20 18:31:07.017: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 16.214174105s
Feb 20 18:31:09.041: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 18.238146108s
Feb 20 18:31:11.064: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 20.261960763s
Feb 20 18:31:13.088: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Running", Reason="", readiness=false. Elapsed: 22.285940326s
Feb 20 18:31:15.112: INFO: Pod "pod-subpath-test-configmap-t29d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.3094482s
STEP: Saw pod success
Feb 20 18:31:15.112: INFO: Pod "pod-subpath-test-configmap-t29d" satisfied condition "success or failure"
Feb 20 18:31:15.135: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-subpath-test-configmap-t29d container test-container-subpath-configmap-t29d: <nil>
STEP: delete the pod
Feb 20 18:31:15.420: INFO: Waiting for pod pod-subpath-test-configmap-t29d to disappear
Feb 20 18:31:15.443: INFO: Pod pod-subpath-test-configmap-t29d no longer exists
STEP: Deleting pod pod-subpath-test-configmap-t29d
Feb 20 18:31:15.443: INFO: Deleting pod "pod-subpath-test-configmap-t29d" in namespace "e2e-tests-subpath-cmt2j"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:31:15.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-cmt2j" for this suite.
Feb 20 18:31:21.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:31:21.746: INFO: namespace: e2e-tests-subpath-cmt2j, resource: bindings, ignored listing per whitelist
Feb 20 18:31:22.581: INFO: namespace e2e-tests-subpath-cmt2j deletion completed in 7.091035655s

 [SLOW TEST:32.976 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:31:22.581: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bmxbm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-756w
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:31:23.807: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-756w" in namespace "e2e-tests-subpath-bmxbm" to be "success or failure"
Feb 20 18:31:23.830: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Pending", Reason="", readiness=false. Elapsed: 22.948496ms
Feb 20 18:31:25.854: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047281431s
Feb 20 18:31:28.023: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 4.216441967s
Feb 20 18:31:30.047: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 6.24065097s
Feb 20 18:31:32.072: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 8.264953924s
Feb 20 18:31:34.095: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 10.288686325s
Feb 20 18:31:36.120: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 12.312783563s
Feb 20 18:31:38.144: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 14.336847005s
Feb 20 18:31:40.168: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 16.360821811s
Feb 20 18:31:42.191: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 18.384439985s
Feb 20 18:31:44.215: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 20.408458989s
Feb 20 18:31:46.239: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Running", Reason="", readiness=false. Elapsed: 22.432582375s
Feb 20 18:31:48.263: INFO: Pod "pod-subpath-test-configmap-756w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.455817761s
STEP: Saw pod success
Feb 20 18:31:48.263: INFO: Pod "pod-subpath-test-configmap-756w" satisfied condition "success or failure"
Feb 20 18:31:48.285: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-subpath-test-configmap-756w container test-container-subpath-configmap-756w: <nil>
STEP: delete the pod
Feb 20 18:31:48.346: INFO: Waiting for pod pod-subpath-test-configmap-756w to disappear
Feb 20 18:31:48.369: INFO: Pod pod-subpath-test-configmap-756w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-756w
Feb 20 18:31:48.369: INFO: Deleting pod "pod-subpath-test-configmap-756w" in namespace "e2e-tests-subpath-bmxbm"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:31:48.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bmxbm" for this suite.
Feb 20 18:31:54.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:31:54.765: INFO: namespace: e2e-tests-subpath-bmxbm, resource: bindings, ignored listing per whitelist
Feb 20 18:31:55.366: INFO: namespace e2e-tests-subpath-bmxbm deletion completed in 6.948897837s

 [SLOW TEST:32.785 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:31:55.366: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-tzcld
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-tzcld
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 20 18:31:56.502: INFO: Found 1 stateful pods, waiting for 3
Feb 20 18:32:06.527: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:32:06.527: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:32:06.527: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:32:06.598: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-tzcld ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:32:07.248: INFO: stderr: ""
Feb 20 18:32:07.248: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:32:07.248: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 18:32:17.404: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 20 18:32:17.477: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-tzcld ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:32:18.221: INFO: stderr: ""
Feb 20 18:32:18.221: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:32:18.221: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:32:28.366: INFO: Waiting for StatefulSet e2e-tests-statefulset-tzcld/ss2 to complete update
Feb 20 18:32:28.366: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:32:28.366: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 18:32:38.414: INFO: Waiting for StatefulSet e2e-tests-statefulset-tzcld/ss2 to complete update
Feb 20 18:32:38.414: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 20 18:32:48.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-tzcld ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:32:49.214: INFO: stderr: ""
Feb 20 18:32:49.214: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:32:49.214: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:32:59.368: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 20 18:32:59.439: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-tzcld ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:33:00.117: INFO: stderr: ""
Feb 20 18:33:00.117: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:33:00.117: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:33:10.259: INFO: Waiting for StatefulSet e2e-tests-statefulset-tzcld/ss2 to complete update
Feb 20 18:33:10.259: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:33:10.259: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:33:10.259: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:33:20.307: INFO: Waiting for StatefulSet e2e-tests-statefulset-tzcld/ss2 to complete update
Feb 20 18:33:20.307: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:33:20.307: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 20 18:33:30.308: INFO: Waiting for StatefulSet e2e-tests-statefulset-tzcld/ss2 to complete update
Feb 20 18:33:30.308: INFO: Waiting for Pod e2e-tests-statefulset-tzcld/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 18:33:40.307: INFO: Deleting all statefulset in ns e2e-tests-statefulset-tzcld
Feb 20 18:33:40.330: INFO: Scaling statefulset ss2 to 0
Feb 20 18:34:10.426: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:34:10.449: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:34:10.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-tzcld" for this suite.
Feb 20 18:34:18.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:18.753: INFO: namespace: e2e-tests-statefulset-tzcld, resource: bindings, ignored listing per whitelist
Feb 20 18:34:19.496: INFO: namespace e2e-tests-statefulset-tzcld deletion completed in 8.95265139s

 [SLOW TEST:144.130 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:34:19.497: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-k6djf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-23331be5-353e-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:34:20.681: INFO: Waiting up to 5m0s for pod "pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-k6djf" to be "success or failure"
Feb 20 18:34:20.704: INFO: Pod "pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.742831ms
Feb 20 18:34:22.728: INFO: Pod "pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046691952s
STEP: Saw pod success
Feb 20 18:34:22.728: INFO: Pod "pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:34:22.751: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:34:22.821: INFO: Waiting for pod pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:34:22.843: INFO: Pod pod-secrets-2336b09f-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:34:22.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-k6djf" for this suite.
Feb 20 18:34:28.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:29.451: INFO: namespace: e2e-tests-secrets-k6djf, resource: bindings, ignored listing per whitelist
Feb 20 18:34:29.823: INFO: namespace e2e-tests-secrets-k6djf deletion completed in 6.955199017s

 [SLOW TEST:10.326 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:34:29.823: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-v5n9v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:34:30.957: INFO: Waiting up to 5m0s for pod "downward-api-2956d039-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-v5n9v" to be "success or failure"
Feb 20 18:34:30.980: INFO: Pod "downward-api-2956d039-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.619534ms
Feb 20 18:34:33.004: INFO: Pod "downward-api-2956d039-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046513412s
STEP: Saw pod success
Feb 20 18:34:33.004: INFO: Pod "downward-api-2956d039-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:34:33.027: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downward-api-2956d039-353e-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:34:33.088: INFO: Waiting for pod downward-api-2956d039-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:34:33.111: INFO: Pod downward-api-2956d039-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:34:33.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-v5n9v" for this suite.
Feb 20 18:34:39.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:34:39.480: INFO: namespace: e2e-tests-downward-api-v5n9v, resource: bindings, ignored listing per whitelist
Feb 20 18:34:40.124: INFO: namespace e2e-tests-downward-api-v5n9v deletion completed in 6.988893678s

 [SLOW TEST:10.301 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:34:40.124: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-nzggs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-nzggs
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 18:34:41.229: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 18:34:59.630: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.89:8080/dial?request=hostName&protocol=udp&host=100.96.0.38&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-nzggs PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:34:59.630: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:35:00.215: INFO: Waiting for endpoints: map[]
Feb 20 18:35:00.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.89:8080/dial?request=hostName&protocol=udp&host=100.96.1.88&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-nzggs PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 18:35:00.239: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 18:35:00.782: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:35:00.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-nzggs" for this suite.
Feb 20 18:35:24.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:25.315: INFO: namespace: e2e-tests-pod-network-test-nzggs, resource: bindings, ignored listing per whitelist
Feb 20 18:35:25.796: INFO: namespace e2e-tests-pod-network-test-nzggs deletion completed in 24.989250716s

 [SLOW TEST:45.672 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:35:25.796: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-r2khm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:35:26.928: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-r2khm'
Feb 20 18:35:27.134: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 20 18:35:27.135: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 20 18:35:27.181: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-wszzp]
Feb 20 18:35:27.181: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-wszzp" in namespace "e2e-tests-kubectl-r2khm" to be "running and ready"
Feb 20 18:35:27.204: INFO: Pod "e2e-test-nginx-rc-wszzp": Phase="Pending", Reason="", readiness=false. Elapsed: 23.372932ms
Feb 20 18:35:29.229: INFO: Pod "e2e-test-nginx-rc-wszzp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048297944s
Feb 20 18:35:31.253: INFO: Pod "e2e-test-nginx-rc-wszzp": Phase="Running", Reason="", readiness=true. Elapsed: 4.072138839s
Feb 20 18:35:31.253: INFO: Pod "e2e-test-nginx-rc-wszzp" satisfied condition "running and ready"
Feb 20 18:35:31.253: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-wszzp]
Feb 20 18:35:31.253: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r2khm'
Feb 20 18:35:31.598: INFO: stderr: ""
Feb 20 18:35:31.598: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Feb 20 18:35:31.598: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-r2khm'
Feb 20 18:35:31.766: INFO: stderr: ""
Feb 20 18:35:31.766: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:35:31.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-r2khm" for this suite.
Feb 20 18:35:55.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:35:55.943: INFO: namespace: e2e-tests-kubectl-r2khm, resource: bindings, ignored listing per whitelist
Feb 20 18:35:56.753: INFO: namespace e2e-tests-kubectl-r2khm deletion completed in 24.963370262s

 [SLOW TEST:30.957 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:35:56.754: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q727t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-5d233456-353e-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 18:35:57.884: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-q727t" to be "success or failure"
Feb 20 18:35:57.908: INFO: Pod "pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.341716ms
Feb 20 18:35:59.932: INFO: Pod "pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047046392s
STEP: Saw pod success
Feb 20 18:35:59.932: INFO: Pod "pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:35:59.955: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 18:36:00.019: INFO: Waiting for pod pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:36:00.043: INFO: Pod pod-projected-secrets-5d26c943-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:36:00.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q727t" for this suite.
Feb 20 18:36:06.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:06.892: INFO: namespace: e2e-tests-projected-q727t, resource: bindings, ignored listing per whitelist
Feb 20 18:36:07.050: INFO: namespace e2e-tests-projected-q727t deletion completed in 6.983352312s

 [SLOW TEST:10.297 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:36:07.051: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-knnqp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 18:36:08.129: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:08.548: INFO: stderr: ""
Feb 20 18:36:08.548: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:36:08.548: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:08.708: INFO: stderr: ""
Feb 20 18:36:08.708: INFO: stdout: "update-demo-nautilus-h62x7 update-demo-nautilus-pwskv "
Feb 20 18:36:08.708: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:08.858: INFO: stderr: ""
Feb 20 18:36:08.858: INFO: stdout: ""
Feb 20 18:36:08.858: INFO: update-demo-nautilus-h62x7 is created but not running
Feb 20 18:36:13.858: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:14.033: INFO: stderr: ""
Feb 20 18:36:14.033: INFO: stdout: "update-demo-nautilus-h62x7 update-demo-nautilus-pwskv "
Feb 20 18:36:14.033: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:14.193: INFO: stderr: ""
Feb 20 18:36:14.193: INFO: stdout: "true"
Feb 20 18:36:14.193: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:14.403: INFO: stderr: ""
Feb 20 18:36:14.404: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:36:14.404: INFO: validating pod update-demo-nautilus-h62x7
Feb 20 18:36:14.515: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:36:14.515: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:36:14.515: INFO: update-demo-nautilus-h62x7 is verified up and running
Feb 20 18:36:14.515: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-pwskv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:14.688: INFO: stderr: ""
Feb 20 18:36:14.688: INFO: stdout: "true"
Feb 20 18:36:14.688: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-pwskv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:14.871: INFO: stderr: ""
Feb 20 18:36:14.871: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:36:14.871: INFO: validating pod update-demo-nautilus-pwskv
Feb 20 18:36:14.981: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:36:14.982: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:36:14.982: INFO: update-demo-nautilus-pwskv is verified up and running
STEP: scaling down the replication controller
Feb 20 18:36:14.990: INFO: scanned /root for discovery docs: <nil>
Feb 20 18:36:14.990: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:15.256: INFO: stderr: ""
Feb 20 18:36:15.257: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:36:15.257: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:15.452: INFO: stderr: ""
Feb 20 18:36:15.452: INFO: stdout: "update-demo-nautilus-h62x7 update-demo-nautilus-pwskv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 18:36:20.452: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:20.632: INFO: stderr: ""
Feb 20 18:36:20.632: INFO: stdout: "update-demo-nautilus-h62x7 update-demo-nautilus-pwskv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 18:36:25.632: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:25.787: INFO: stderr: ""
Feb 20 18:36:25.787: INFO: stdout: "update-demo-nautilus-h62x7 update-demo-nautilus-pwskv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 20 18:36:30.787: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:30.975: INFO: stderr: ""
Feb 20 18:36:30.975: INFO: stdout: "update-demo-nautilus-h62x7 "
Feb 20 18:36:30.975: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:31.157: INFO: stderr: ""
Feb 20 18:36:31.157: INFO: stdout: "true"
Feb 20 18:36:31.157: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:31.328: INFO: stderr: ""
Feb 20 18:36:31.328: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:36:31.328: INFO: validating pod update-demo-nautilus-h62x7
Feb 20 18:36:31.355: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:36:31.355: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:36:31.355: INFO: update-demo-nautilus-h62x7 is verified up and running
STEP: scaling up the replication controller
Feb 20 18:36:31.360: INFO: scanned /root for discovery docs: <nil>
Feb 20 18:36:31.360: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:32.608: INFO: stderr: ""
Feb 20 18:36:32.608: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:36:32.609: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:32.833: INFO: stderr: ""
Feb 20 18:36:32.833: INFO: stdout: "update-demo-nautilus-5hfgj update-demo-nautilus-h62x7 "
Feb 20 18:36:32.833: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-5hfgj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:33.074: INFO: stderr: ""
Feb 20 18:36:33.074: INFO: stdout: "true"
Feb 20 18:36:33.074: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-5hfgj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:33.265: INFO: stderr: ""
Feb 20 18:36:33.265: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:36:33.265: INFO: validating pod update-demo-nautilus-5hfgj
Feb 20 18:36:33.375: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:36:33.375: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:36:33.375: INFO: update-demo-nautilus-5hfgj is verified up and running
Feb 20 18:36:33.376: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:33.572: INFO: stderr: ""
Feb 20 18:36:33.572: INFO: stdout: "true"
Feb 20 18:36:33.572: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-h62x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:33.748: INFO: stderr: ""
Feb 20 18:36:33.748: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:36:33.748: INFO: validating pod update-demo-nautilus-h62x7
Feb 20 18:36:33.776: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:36:33.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:36:33.776: INFO: update-demo-nautilus-h62x7 is verified up and running
STEP: using delete to clean up resources
Feb 20 18:36:33.776: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:33.948: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:36:33.948: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 18:36:33.948: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-knnqp'
Feb 20 18:36:34.117: INFO: stderr: "No resources found.\n"
Feb 20 18:36:34.117: INFO: stdout: ""
Feb 20 18:36:34.117: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-knnqp -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 18:36:34.301: INFO: stderr: ""
Feb 20 18:36:34.301: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:36:34.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-knnqp" for this suite.
Feb 20 18:36:58.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:36:58.932: INFO: namespace: e2e-tests-kubectl-knnqp, resource: bindings, ignored listing per whitelist
Feb 20 18:36:59.278: INFO: namespace e2e-tests-kubectl-knnqp deletion completed in 24.95347597s

 [SLOW TEST:52.227 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:36:59.278: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-9tj77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:37:00.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-9tj77" to be "success or failure"
Feb 20 18:37:00.382: INFO: Pod "downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 24.049456ms
Feb 20 18:37:02.406: INFO: Pod "downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048001883s
STEP: Saw pod success
Feb 20 18:37:02.406: INFO: Pod "downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:37:02.429: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:37:02.489: INFO: Waiting for pod downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:37:02.512: INFO: Pod downwardapi-volume-826382f6-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:37:02.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-9tj77" for this suite.
Feb 20 18:37:08.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:08.700: INFO: namespace: e2e-tests-projected-9tj77, resource: bindings, ignored listing per whitelist
Feb 20 18:37:09.481: INFO: namespace e2e-tests-projected-9tj77 deletion completed in 6.944788203s

 [SLOW TEST:10.202 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:37:09.481: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-s54wz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-8887a6a0-353e-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:37:10.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-s54wz" to be "success or failure"
Feb 20 18:37:10.707: INFO: Pod "pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.744009ms
Feb 20 18:37:12.731: INFO: Pod "pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046960497s
STEP: Saw pod success
Feb 20 18:37:12.731: INFO: Pod "pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:37:12.755: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:37:12.816: INFO: Waiting for pod pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:37:12.839: INFO: Pod pod-configmaps-888b3457-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:37:12.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-s54wz" for this suite.
Feb 20 18:37:18.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:19.761: INFO: namespace: e2e-tests-configmap-s54wz, resource: bindings, ignored listing per whitelist
Feb 20 18:37:19.807: INFO: namespace e2e-tests-configmap-s54wz deletion completed in 6.943643671s

 [SLOW TEST:10.326 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:37:19.808: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-bfxr2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:37:20.869: INFO: Waiting up to 5m0s for pod "downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-bfxr2" to be "success or failure"
Feb 20 18:37:20.892: INFO: Pod "downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.06863ms
Feb 20 18:37:22.916: INFO: Pod "downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046826856s
STEP: Saw pod success
Feb 20 18:37:22.916: INFO: Pod "downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:37:22.939: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:37:22.996: INFO: Waiting for pod downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:37:23.019: INFO: Pod downward-api-8e9d394a-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:37:23.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-bfxr2" for this suite.
Feb 20 18:37:29.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:29.593: INFO: namespace: e2e-tests-downward-api-bfxr2, resource: bindings, ignored listing per whitelist
Feb 20 18:37:30.030: INFO: namespace e2e-tests-downward-api-bfxr2 deletion completed in 6.987108993s

 [SLOW TEST:10.222 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:37:30.030: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rwm87
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:37:31.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-rwm87" to be "success or failure"
Feb 20 18:37:31.186: INFO: Pod "downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.923134ms
Feb 20 18:37:33.210: INFO: Pod "downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046714096s
STEP: Saw pod success
Feb 20 18:37:33.210: INFO: Pod "downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:37:33.233: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:37:33.292: INFO: Waiting for pod downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:37:33.316: INFO: Pod downwardapi-volume-94bff32b-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:37:33.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rwm87" for this suite.
Feb 20 18:37:39.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:40.330: INFO: namespace: e2e-tests-projected-rwm87, resource: bindings, ignored listing per whitelist
Feb 20 18:37:40.330: INFO: namespace e2e-tests-projected-rwm87 deletion completed in 6.986943893s

 [SLOW TEST:10.300 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:37:40.330: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-6lcwv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:37:41.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-6lcwv'
Feb 20 18:37:41.640: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 20 18:37:41.640: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Feb 20 18:37:43.686: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-6lcwv'
Feb 20 18:37:43.877: INFO: stderr: ""
Feb 20 18:37:43.877: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:37:43.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-6lcwv" for this suite.
Feb 20 18:37:49.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:37:50.286: INFO: namespace: e2e-tests-kubectl-6lcwv, resource: bindings, ignored listing per whitelist
Feb 20 18:37:50.920: INFO: namespace e2e-tests-kubectl-6lcwv deletion completed in 7.019673345s

 [SLOW TEST:10.590 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:37:50.921: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-v4brj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-v4brj;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-v4brj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v4brj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 200.120.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.120.200_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 200.120.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.120.200_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-v4brj;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-v4brj;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-v4brj.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-v4brj.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-v4brj.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-v4brj.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-v4brj.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 200.120.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.120.200_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 200.120.66.100.in-addr.arpa. PTR)" && echo OK > /results/100.66.120.200_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 20 18:38:06.296: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.338: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.364: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.390: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.416: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.442: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.509: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.553: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:06.985: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.011: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.037: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.064: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.090: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.116: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.141: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.168: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:07.577: INFO: Lookups using e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-v4brj wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj wheezy_udp@dns-test-service.e2e-tests-dns-v4brj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-v4brj jessie_tcp@dns-test-service.e2e-tests-dns-v4brj jessie_udp@dns-test-service.e2e-tests-dns-v4brj.svc jessie_tcp@dns-test-service.e2e-tests-dns-v4brj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc]

Feb 20 18:38:16.208: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.251: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.277: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.302: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.327: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.357: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.412: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.823: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.849: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.875: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.900: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-v4brj from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.926: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.952: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:16.977: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:17.003: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc from pod e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39: the server could not find the requested resource (get pods dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39)
Feb 20 18:38:17.369: INFO: Lookups using e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-v4brj wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj wheezy_udp@dns-test-service.e2e-tests-dns-v4brj.svc wheezy_tcp@dns-test-service.e2e-tests-dns-v4brj.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-v4brj jessie_tcp@dns-test-service.e2e-tests-dns-v4brj jessie_udp@dns-test-service.e2e-tests-dns-v4brj.svc jessie_tcp@dns-test-service.e2e-tests-dns-v4brj.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-v4brj.svc]

Feb 20 18:38:28.042: INFO: DNS probes using e2e-tests-dns-v4brj/dns-test-a13c2c74-353e-11e9-9b2a-cafe91372a39 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:38:28.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-v4brj" for this suite.
Feb 20 18:38:34.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:38:34.702: INFO: namespace: e2e-tests-dns-v4brj, resource: bindings, ignored listing per whitelist
Feb 20 18:38:35.086: INFO: namespace e2e-tests-dns-v4brj deletion completed in 6.933233599s

 [SLOW TEST:44.165 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:38:35.086: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-b7s4x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 20 18:38:36.329: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 20 18:38:36.329: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:37.924: INFO: stderr: ""
Feb 20 18:38:37.924: INFO: stdout: "service/redis-slave created\n"
Feb 20 18:38:37.924: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 20 18:38:37.924: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:38.246: INFO: stderr: ""
Feb 20 18:38:38.246: INFO: stdout: "service/redis-master created\n"
Feb 20 18:38:38.246: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 20 18:38:38.246: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:38.565: INFO: stderr: ""
Feb 20 18:38:38.565: INFO: stdout: "service/frontend created\n"
Feb 20 18:38:38.565: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 20 18:38:38.565: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:38.842: INFO: stderr: ""
Feb 20 18:38:38.842: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 20 18:38:38.843: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 20 18:38:38.843: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:39.154: INFO: stderr: ""
Feb 20 18:38:39.154: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 20 18:38:39.154: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 20 18:38:39.154: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:38:39.469: INFO: stderr: ""
Feb 20 18:38:39.469: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 20 18:38:39.469: INFO: Waiting for all frontend pods to be Running.
Feb 20 18:39:04.520: INFO: Waiting for frontend to serve content.
Feb 20 18:39:04.630: INFO: Trying to add a new entry to the guestbook.
Feb 20 18:39:04.729: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 20 18:39:04.763: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:04.996: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:04.996: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 18:39:04.997: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:05.216: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:05.216: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 18:39:05.216: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:05.391: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:05.391: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 18:39:05.391: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:05.552: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:05.552: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 18:39:05.553: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:05.720: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:05.720: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 20 18:39:05.720: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-b7s4x'
Feb 20 18:39:05.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:39:05.887: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:39:05.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-b7s4x" for this suite.
Feb 20 18:39:47.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:39:48.903: INFO: namespace: e2e-tests-kubectl-b7s4x, resource: bindings, ignored listing per whitelist
Feb 20 18:39:48.926: INFO: namespace e2e-tests-kubectl-b7s4x deletion completed in 43.015617735s

 [SLOW TEST:73.840 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:39:48.926: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-5cxtc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 18:39:50.079: INFO: Waiting up to 5m0s for pod "downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-5cxtc" to be "success or failure"
Feb 20 18:39:50.102: INFO: Pod "downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.951305ms
Feb 20 18:39:52.137: INFO: Pod "downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057817667s
Feb 20 18:39:54.161: INFO: Pod "downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082067041s
STEP: Saw pod success
Feb 20 18:39:54.161: INFO: Pod "downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:39:54.184: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 18:39:54.244: INFO: Waiting for pod downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:39:54.266: INFO: Pod downward-api-e78cf201-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:39:54.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-5cxtc" for this suite.
Feb 20 18:40:00.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:00.843: INFO: namespace: e2e-tests-downward-api-5cxtc, resource: bindings, ignored listing per whitelist
Feb 20 18:40:01.304: INFO: namespace e2e-tests-downward-api-5cxtc deletion completed in 7.014347783s

 [SLOW TEST:12.378 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:40:01.305: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-dx7r2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 20 18:40:02.458: INFO: Waiting up to 5m0s for pod "client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-containers-dx7r2" to be "success or failure"
Feb 20 18:40:02.482: INFO: Pod "client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.617913ms
Feb 20 18:40:04.506: INFO: Pod "client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04772354s
Feb 20 18:40:06.529: INFO: Pod "client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071289188s
STEP: Saw pod success
Feb 20 18:40:06.530: INFO: Pod "client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:40:06.552: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:40:06.610: INFO: Waiting for pod client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:40:06.633: INFO: Pod client-containers-eeeddaa7-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:40:06.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-dx7r2" for this suite.
Feb 20 18:40:14.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:14.842: INFO: namespace: e2e-tests-containers-dx7r2, resource: bindings, ignored listing per whitelist
Feb 20 18:40:15.601: INFO: namespace e2e-tests-containers-dx7r2 deletion completed in 8.944331922s

 [SLOW TEST:14.296 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:40:15.601: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rsnxb
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 20 18:40:16.762: INFO: Waiting up to 5m0s for pod "pod-f773fcfa-353e-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-rsnxb" to be "success or failure"
Feb 20 18:40:16.784: INFO: Pod "pod-f773fcfa-353e-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.865328ms
Feb 20 18:40:18.809: INFO: Pod "pod-f773fcfa-353e-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04695131s
STEP: Saw pod success
Feb 20 18:40:18.809: INFO: Pod "pod-f773fcfa-353e-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:40:18.833: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-f773fcfa-353e-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:40:18.891: INFO: Waiting for pod pod-f773fcfa-353e-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:40:18.914: INFO: Pod pod-f773fcfa-353e-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:40:18.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rsnxb" for this suite.
Feb 20 18:40:25.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:25.802: INFO: namespace: e2e-tests-emptydir-rsnxb, resource: bindings, ignored listing per whitelist
Feb 20 18:40:25.891: INFO: namespace e2e-tests-emptydir-rsnxb deletion completed in 6.953974052s

 [SLOW TEST:10.290 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:40:25.892: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-s7ksm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 20 18:40:27.649: INFO: created pod pod-service-account-defaultsa
Feb 20 18:40:27.649: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 20 18:40:27.673: INFO: created pod pod-service-account-mountsa
Feb 20 18:40:27.673: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 20 18:40:27.696: INFO: created pod pod-service-account-nomountsa
Feb 20 18:40:27.696: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 20 18:40:27.720: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 20 18:40:27.720: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 20 18:40:27.744: INFO: created pod pod-service-account-mountsa-mountspec
Feb 20 18:40:27.744: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 20 18:40:27.767: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 20 18:40:27.767: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 20 18:40:27.791: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 20 18:40:27.791: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 20 18:40:27.814: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 20 18:40:27.814: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 20 18:40:27.838: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 20 18:40:27.838: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:40:27.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-s7ksm" for this suite.
Feb 20 18:40:33.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:34.001: INFO: namespace: e2e-tests-svcaccounts-s7ksm, resource: bindings, ignored listing per whitelist
Feb 20 18:40:34.796: INFO: namespace e2e-tests-svcaccounts-s7ksm deletion completed in 6.935707415s

 [SLOW TEST:8.904 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:40:34.797: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-552vp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:40:35.829: INFO: Creating ReplicaSet my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39
Feb 20 18:40:35.875: INFO: Pod name my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39: Found 1 pods out of 1
Feb 20 18:40:35.875: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39" is running
Feb 20 18:40:39.922: INFO: Pod "my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39-zkjd8" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 18:40:35 +0000 UTC Reason: Message:}])
Feb 20 18:40:39.922: INFO: Trying to dial the pod
Feb 20 18:40:45.078: INFO: Controller my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39: Got expected result from replica 1 [my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39-zkjd8]: "my-hostname-basic-02d5f866-353f-11e9-9b2a-cafe91372a39-zkjd8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:40:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-552vp" for this suite.
Feb 20 18:40:51.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:40:51.483: INFO: namespace: e2e-tests-replicaset-552vp, resource: bindings, ignored listing per whitelist
Feb 20 18:40:52.150: INFO: namespace e2e-tests-replicaset-552vp deletion completed in 7.04820743s

 [SLOW TEST:17.353 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:40:52.150: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-dcjcn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 18:40:56.018: INFO: Successfully updated pod "labelsupdate0d4404ce-353f-11e9-9b2a-cafe91372a39"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:40:58.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-dcjcn" for this suite.
Feb 20 18:41:22.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:22.607: INFO: namespace: e2e-tests-projected-dcjcn, resource: bindings, ignored listing per whitelist
Feb 20 18:41:23.068: INFO: namespace e2e-tests-projected-dcjcn deletion completed in 24.948254763s

 [SLOW TEST:30.917 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:41:23.068: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-msjn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 20 18:41:24.144: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix361148855/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:41:24.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-msjn9" for this suite.
Feb 20 18:41:30.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:31.040: INFO: namespace: e2e-tests-kubectl-msjn9, resource: bindings, ignored listing per whitelist
Feb 20 18:41:31.210: INFO: namespace e2e-tests-kubectl-msjn9 deletion completed in 6.951978872s

 [SLOW TEST:8.142 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:41:31.210: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kxwmk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 18:41:32.358: INFO: Waiting up to 5m0s for pod "pod-24837f6f-353f-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-kxwmk" to be "success or failure"
Feb 20 18:41:32.381: INFO: Pod "pod-24837f6f-353f-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.984561ms
Feb 20 18:41:34.404: INFO: Pod "pod-24837f6f-353f-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045745626s
Feb 20 18:41:36.428: INFO: Pod "pod-24837f6f-353f-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069450848s
STEP: Saw pod success
Feb 20 18:41:36.428: INFO: Pod "pod-24837f6f-353f-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:41:36.451: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-24837f6f-353f-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:41:36.511: INFO: Waiting for pod pod-24837f6f-353f-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:41:36.534: INFO: Pod pod-24837f6f-353f-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:41:36.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kxwmk" for this suite.
Feb 20 18:41:42.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:41:43.008: INFO: namespace: e2e-tests-emptydir-kxwmk, resource: bindings, ignored listing per whitelist
Feb 20 18:41:43.685: INFO: namespace e2e-tests-emptydir-kxwmk deletion completed in 7.1277105s

 [SLOW TEST:12.476 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:41:43.686: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-rzbdb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-rzbdb
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-rzbdb
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-rzbdb
Feb 20 18:41:44.898: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 20 18:41:54.923: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 20 18:41:54.947: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:41:55.726: INFO: stderr: ""
Feb 20 18:41:55.726: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:41:55.726: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:41:55.749: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 18:42:05.774: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:42:05.774: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:42:05.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999566s
Feb 20 18:42:06.893: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976315917s
Feb 20 18:42:07.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.95163794s
Feb 20 18:42:08.943: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.926699744s
Feb 20 18:42:09.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.901917618s
Feb 20 18:42:10.992: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.87745422s
Feb 20 18:42:12.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.852440084s
Feb 20 18:42:13.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.828259046s
Feb 20 18:42:14.066: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.803521608s
Feb 20 18:42:15.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 778.438327ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-rzbdb
Feb 20 18:42:16.115: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:42:16.820: INFO: stderr: ""
Feb 20 18:42:16.820: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:42:16.820: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:42:16.820: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:42:17.516: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 18:42:17.517: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:42:17.517: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:42:17.517: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:42:18.232: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 20 18:42:18.232: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 18:42:18.232: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 18:42:18.256: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:42:18.256: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 18:42:18.256: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 20 18:42:18.279: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:42:18.962: INFO: stderr: ""
Feb 20 18:42:18.962: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:42:18.962: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:42:18.962: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:42:19.659: INFO: stderr: ""
Feb 20 18:42:19.659: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:42:19.659: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:42:19.659: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 18:42:20.424: INFO: stderr: ""
Feb 20 18:42:20.424: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 18:42:20.424: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 18:42:20.424: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:42:20.448: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 20 18:42:30.497: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:42:30.497: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:42:30.497: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 18:42:30.567: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:30.567: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:30.567: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:30.567: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:30.567: INFO: 
Feb 20 18:42:30.567: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:31.591: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:31.591: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:31.591: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:31.591: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:31.591: INFO: 
Feb 20 18:42:31.591: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:32.616: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:32.616: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:32.616: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:32.616: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:32.616: INFO: 
Feb 20 18:42:32.616: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:33.640: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:33.640: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:33.640: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:33.640: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:33.640: INFO: 
Feb 20 18:42:33.640: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:34.664: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:34.664: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:34.664: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:34.664: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:34.664: INFO: 
Feb 20 18:42:34.664: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:35.688: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:35.688: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:35.688: INFO: ss-1  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:35.688: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:35.688: INFO: 
Feb 20 18:42:35.688: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 20 18:42:36.712: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:36.712: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:36.712: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:36.712: INFO: 
Feb 20 18:42:36.712: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:42:37.736: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:37.736: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:37.736: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:37.736: INFO: 
Feb 20 18:42:37.736: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:42:38.760: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:38.760: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:38.761: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:38.761: INFO: 
Feb 20 18:42:38.761: INFO: StatefulSet ss has not reached scale 0, at 2
Feb 20 18:42:39.785: INFO: POD   NODE                                                     PHASE    GRACE  CONDITIONS
Feb 20 18:42:39.785: INFO: ss-0  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:41:44 +0000 UTC  }]
Feb 20 18:42:39.785: INFO: ss-2  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 18:42:05 +0000 UTC  }]
Feb 20 18:42:39.786: INFO: 
Feb 20 18:42:39.786: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-rzbdb
Feb 20 18:42:40.810: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:42:41.192: INFO: rc: 1
Feb 20 18:42:41.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0012c50b0 exit status 1 <nil> <nil> true [0xc001d12088 0xc001d120a0 0xc001d120b8] [0xc001d12088 0xc001d120a0 0xc001d120b8] [0xc001d12098 0xc001d120b0] [0x932420 0x932420] 0xc001958540 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 20 18:42:51.192: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:42:51.370: INFO: rc: 1
Feb 20 18:42:51.370: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012c5350 exit status 1 <nil> <nil> true [0xc001d120c0 0xc001d120d8 0xc001d120f0] [0xc001d120c0 0xc001d120d8 0xc001d120f0] [0xc001d120d0 0xc001d120e8] [0x932420 0x932420] 0xc001958840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:01.370: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:01.546: INFO: rc: 1
Feb 20 18:43:01.546: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b44660 exit status 1 <nil> <nil> true [0xc001708428 0xc001708440 0xc001708458] [0xc001708428 0xc001708440 0xc001708458] [0xc001708438 0xc001708450] [0x932420 0x932420] 0xc00179fc20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:11.547: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:11.739: INFO: rc: 1
Feb 20 18:43:11.740: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b44930 exit status 1 <nil> <nil> true [0xc001708460 0xc001708478 0xc001708490] [0xc001708460 0xc001708478 0xc001708490] [0xc001708470 0xc001708488] [0x932420 0x932420] 0xc00179ff20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:21.740: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:21.881: INFO: rc: 1
Feb 20 18:43:21.881: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00119eb70 exit status 1 <nil> <nil> true [0xc0015d6a08 0xc0015d6a48 0xc0015d6a70] [0xc0015d6a08 0xc0015d6a48 0xc0015d6a70] [0xc0015d6a28 0xc0015d6a68] [0x932420 0x932420] 0xc001963aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:31.881: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:32.029: INFO: rc: 1
Feb 20 18:43:32.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e63f0 exit status 1 <nil> <nil> true [0xc00000e068 0xc00000e120 0xc00000e140] [0xc00000e068 0xc00000e120 0xc00000e140] [0xc00000e110 0xc00000e138] [0x932420 0x932420] 0xc002682fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:42.030: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:42.232: INFO: rc: 1
Feb 20 18:43:42.232: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00157e2a0 exit status 1 <nil> <nil> true [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2258 0xc0000f2358] [0x932420 0x932420] 0xc001bc6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:43:52.233: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:43:52.401: INFO: rc: 1
Feb 20 18:43:52.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00107a270 exit status 1 <nil> <nil> true [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa180 0xc0006fa240] [0x932420 0x932420] 0xc0018e2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:02.402: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:02.624: INFO: rc: 1
Feb 20 18:44:02.624: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00157e540 exit status 1 <nil> <nil> true [0xc0000f2370 0xc0000f23e0 0xc0000f24c8] [0xc0000f2370 0xc0000f23e0 0xc0000f24c8] [0xc0000f2390 0xc0000f2460] [0x932420 0x932420] 0xc001bc6720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:12.624: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:12.833: INFO: rc: 1
Feb 20 18:44:12.834: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131a2a0 exit status 1 <nil> <nil> true [0xc0015d6000 0xc0015d6020 0xc0015d6070] [0xc0015d6000 0xc0015d6020 0xc0015d6070] [0xc0015d6010 0xc0015d6058] [0x932420 0x932420] 0xc000f10240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:22.834: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:23.050: INFO: rc: 1
Feb 20 18:44:23.050: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6750 exit status 1 <nil> <nil> true [0xc00000e150 0xc00000e1a0 0xc00000e210] [0xc00000e150 0xc00000e1a0 0xc00000e210] [0xc00000e170 0xc00000e1e8] [0x932420 0x932420] 0xc0026832c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:33.050: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:33.195: INFO: rc: 1
Feb 20 18:44:33.195: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00157e7b0 exit status 1 <nil> <nil> true [0xc0000f2528 0xc0000f2550 0xc0000f25f0] [0xc0000f2528 0xc0000f2550 0xc0000f25f0] [0xc0000f2538 0xc0000f25b0] [0x932420 0x932420] 0xc001bc6a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:43.195: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:43.415: INFO: rc: 1
Feb 20 18:44:43.416: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00107a510 exit status 1 <nil> <nil> true [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa288 0xc0006fa378] [0x932420 0x932420] 0xc0018e2540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:44:53.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:44:53.694: INFO: rc: 1
Feb 20 18:44:53.694: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131a690 exit status 1 <nil> <nil> true [0xc0015d6088 0xc0015d60b0 0xc0015d60f0] [0xc0015d6088 0xc0015d60b0 0xc0015d60f0] [0xc0015d6098 0xc0015d60e8] [0x932420 0x932420] 0xc000f10600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:03.694: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:03.859: INFO: rc: 1
Feb 20 18:45:03.859: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6c30 exit status 1 <nil> <nil> true [0xc00000e230 0xc00000e2e0 0xc00000e670] [0xc00000e230 0xc00000e2e0 0xc00000e670] [0xc00000e2a0 0xc00000e660] [0x932420 0x932420] 0xc0026835c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:13.859: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:14.026: INFO: rc: 1
Feb 20 18:45:14.026: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6ed0 exit status 1 <nil> <nil> true [0xc00000e690 0xc00000e6e8 0xc00000e780] [0xc00000e690 0xc00000e6e8 0xc00000e780] [0xc00000e6b8 0xc00000e778] [0x932420 0x932420] 0xc0026838c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:24.026: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:24.206: INFO: rc: 1
Feb 20 18:45:24.206: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00107a9f0 exit status 1 <nil> <nil> true [0xc0006fa3d8 0xc0006fa568 0xc0006fa670] [0xc0006fa3d8 0xc0006fa568 0xc0006fa670] [0xc0006fa500 0xc0006fa628] [0x932420 0x932420] 0xc0018e2840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:34.206: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:34.390: INFO: rc: 1
Feb 20 18:45:34.390: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00107a2a0 exit status 1 <nil> <nil> true [0xc0006fa148 0xc0006fa208 0xc0006fa260] [0xc0006fa148 0xc0006fa208 0xc0006fa260] [0xc0006fa1e8 0xc0006fa258] [0x932420 0x932420] 0xc0018e2240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:44.391: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:44.589: INFO: rc: 1
Feb 20 18:45:44.589: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00107a5a0 exit status 1 <nil> <nil> true [0xc0006fa280 0xc0006fa360 0xc0006fa3d8] [0xc0006fa280 0xc0006fa360 0xc0006fa3d8] [0xc0006fa310 0xc0006fa3b0] [0x932420 0x932420] 0xc0018e2540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:45:54.590: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:45:54.737: INFO: rc: 1
Feb 20 18:45:54.738: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00157e270 exit status 1 <nil> <nil> true [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2258 0xc0000f2358] [0x932420 0x932420] 0xc001bc6420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:04.738: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:04.878: INFO: rc: 1
Feb 20 18:46:04.878: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6420 exit status 1 <nil> <nil> true [0xc00000e058 0xc00000e110 0xc00000e138] [0xc00000e058 0xc00000e110 0xc00000e138] [0xc00000e0f0 0xc00000e130] [0x932420 0x932420] 0xc002682fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:14.879: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:15.067: INFO: rc: 1
Feb 20 18:46:15.067: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6780 exit status 1 <nil> <nil> true [0xc00000e140 0xc00000e170 0xc00000e1e8] [0xc00000e140 0xc00000e170 0xc00000e1e8] [0xc00000e158 0xc00000e1e0] [0x932420 0x932420] 0xc0026832c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:25.067: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:25.354: INFO: rc: 1
Feb 20 18:46:25.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131a2d0 exit status 1 <nil> <nil> true [0xc0015d6000 0xc0015d6020 0xc0015d6070] [0xc0015d6000 0xc0015d6020 0xc0015d6070] [0xc0015d6010 0xc0015d6058] [0x932420 0x932420] 0xc000f10240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:35.354: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:35.508: INFO: rc: 1
Feb 20 18:46:35.508: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6c90 exit status 1 <nil> <nil> true [0xc00000e210 0xc00000e2a0 0xc00000e660] [0xc00000e210 0xc00000e2a0 0xc00000e660] [0xc00000e270 0xc00000e640] [0x932420 0x932420] 0xc0026835c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:45.509: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:45.757: INFO: rc: 1
Feb 20 18:46:45.757: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131a6f0 exit status 1 <nil> <nil> true [0xc0015d6088 0xc0015d60b0 0xc0015d60f0] [0xc0015d6088 0xc0015d60b0 0xc0015d60f0] [0xc0015d6098 0xc0015d60e8] [0x932420 0x932420] 0xc000f10600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:46:55.757: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:46:55.976: INFO: rc: 1
Feb 20 18:46:55.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131ae10 exit status 1 <nil> <nil> true [0xc0015d60f8 0xc0015d6140 0xc0015d6180] [0xc0015d60f8 0xc0015d6140 0xc0015d6180] [0xc0015d6120 0xc0015d6178] [0x932420 0x932420] 0xc000f109c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:47:05.977: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:47:06.121: INFO: rc: 1
Feb 20 18:47:06.121: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0027e6f30 exit status 1 <nil> <nil> true [0xc00000e670 0xc00000e6b8 0xc00000e778] [0xc00000e670 0xc00000e6b8 0xc00000e778] [0xc00000e698 0xc00000e760] [0x932420 0x932420] 0xc0026838c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:47:16.121: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:47:21.320: INFO: rc: 1
Feb 20 18:47:21.320: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131b0b0 exit status 1 <nil> <nil> true [0xc0015d6188 0xc0015d61a0 0xc0015d61e0] [0xc0015d6188 0xc0015d61a0 0xc0015d61e0] [0xc0015d6198 0xc0015d61d0] [0x932420 0x932420] 0xc000f10cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:47:31.321: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:47:31.553: INFO: rc: 1
Feb 20 18:47:31.553: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00131b380 exit status 1 <nil> <nil> true [0xc0015d6208 0xc0015d6268 0xc0015d6280] [0xc0015d6208 0xc0015d6268 0xc0015d6280] [0xc0015d6248 0xc0015d6278] [0x932420 0x932420] 0xc000f10fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Feb 20 18:47:41.553: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-rzbdb ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 18:47:41.728: INFO: rc: 1
Feb 20 18:47:41.728: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Feb 20 18:47:41.728: INFO: Scaling statefulset ss to 0
Feb 20 18:47:41.799: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 18:47:41.823: INFO: Deleting all statefulset in ns e2e-tests-statefulset-rzbdb
Feb 20 18:47:41.846: INFO: Scaling statefulset ss to 0
Feb 20 18:47:41.917: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 18:47:41.941: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:47:42.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-rzbdb" for this suite.
Feb 20 18:47:50.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:47:50.445: INFO: namespace: e2e-tests-statefulset-rzbdb, resource: bindings, ignored listing per whitelist
Feb 20 18:47:51.061: INFO: namespace e2e-tests-statefulset-rzbdb deletion completed in 9.024356163s

 [SLOW TEST:367.375 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:47:51.061: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lwswf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 18:47:52.132: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-lwswf'
Feb 20 18:47:52.324: INFO: stderr: ""
Feb 20 18:47:52.324: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 20 18:47:57.374: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-lwswf -o json'
Feb 20 18:47:57.563: INFO: stderr: ""
Feb 20 18:47:57.563: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.121/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-20T18:47:52Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-lwswf\",\n        \"resourceVersion\": \"9621\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-lwswf/pods/e2e-test-nginx-pod\",\n        \"uid\": \"06fcfd72-3540-11e9-a0a3-2a5d9248fe67\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-m9fdt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-m9fdt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-m9fdt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T18:47:52Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T18:47:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T18:47:54Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-20T18:47:52Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://1ecb779edca08c291d2f11671606ce0d06471493be0fcaae021da1009a0e3824\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-20T18:47:53Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.121\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-20T18:47:52Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 20 18:47:57.564: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-lwswf'
Feb 20 18:47:58.026: INFO: stderr: ""
Feb 20 18:47:58.027: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Feb 20 18:47:58.050: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-lwswf'
Feb 20 18:48:03.000: INFO: stderr: ""
Feb 20 18:48:03.000: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:48:03.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lwswf" for this suite.
Feb 20 18:48:09.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:48:09.277: INFO: namespace: e2e-tests-kubectl-lwswf, resource: bindings, ignored listing per whitelist
Feb 20 18:48:10.009: INFO: namespace e2e-tests-kubectl-lwswf deletion completed in 6.984538279s

 [SLOW TEST:18.948 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:48:10.009: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-sxx99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-sxx99
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-sxx99
STEP: Deleting pre-stop pod
Feb 20 18:48:26.439: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:48:26.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-sxx99" for this suite.
Feb 20 18:49:06.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:49:07.086: INFO: namespace: e2e-tests-prestop-sxx99, resource: bindings, ignored listing per whitelist
Feb 20 18:49:07.621: INFO: namespace e2e-tests-prestop-sxx99 deletion completed in 41.134486366s

 [SLOW TEST:57.612 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:49:07.621: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-t58f8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 18:49:08.825: INFO: Number of nodes with available pods: 0
Feb 20 18:49:08.825: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:49:09.873: INFO: Number of nodes with available pods: 0
Feb 20 18:49:09.873: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:49:10.873: INFO: Number of nodes with available pods: 1
Feb 20 18:49:10.873: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:49:11.874: INFO: Number of nodes with available pods: 2
Feb 20 18:49:11.874: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 20 18:49:11.992: INFO: Number of nodes with available pods: 1
Feb 20 18:49:11.992: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:49:13.040: INFO: Number of nodes with available pods: 1
Feb 20 18:49:13.041: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:49:14.041: INFO: Number of nodes with available pods: 2
Feb 20 18:49:14.041: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-t58f8, will wait for the garbage collector to delete the pods
Feb 20 18:49:14.187: INFO: Deleting {extensions DaemonSet} daemon-set took: 25.589728ms
Feb 20 18:49:14.287: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.242317ms
Feb 20 18:49:56.410: INFO: Number of nodes with available pods: 0
Feb 20 18:49:56.411: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 18:49:56.435: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-t58f8/daemonsets","resourceVersion":"9941"},"items":null}

Feb 20 18:49:56.458: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-t58f8/pods","resourceVersion":"9941"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:49:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-t58f8" for this suite.
Feb 20 18:50:02.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:50:03.126: INFO: namespace: e2e-tests-daemonsets-t58f8, resource: bindings, ignored listing per whitelist
Feb 20 18:50:03.540: INFO: namespace e2e-tests-daemonsets-t58f8 deletion completed in 6.987409418s

 [SLOW TEST:55.919 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:50:03.541: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b2fkr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 20 18:50:04.657: INFO: Waiting up to 5m0s for pod "pod-55de1ad0-3540-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-b2fkr" to be "success or failure"
Feb 20 18:50:04.680: INFO: Pod "pod-55de1ad0-3540-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.428113ms
Feb 20 18:50:06.705: INFO: Pod "pod-55de1ad0-3540-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048033673s
Feb 20 18:50:08.728: INFO: Pod "pod-55de1ad0-3540-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071328282s
STEP: Saw pod success
Feb 20 18:50:08.728: INFO: Pod "pod-55de1ad0-3540-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:50:08.751: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-55de1ad0-3540-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:50:08.813: INFO: Waiting for pod pod-55de1ad0-3540-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:50:08.836: INFO: Pod pod-55de1ad0-3540-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:50:08.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b2fkr" for this suite.
Feb 20 18:50:14.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:50:15.206: INFO: namespace: e2e-tests-emptydir-b2fkr, resource: bindings, ignored listing per whitelist
Feb 20 18:50:15.809: INFO: namespace e2e-tests-emptydir-b2fkr deletion completed in 6.949533105s

 [SLOW TEST:12.268 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:50:15.809: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-wp6lb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0220 18:50:23.095917   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 18:50:23.095: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:50:23.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-wp6lb" for this suite.
Feb 20 18:50:29.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:50:29.578: INFO: namespace: e2e-tests-gc-wp6lb, resource: bindings, ignored listing per whitelist
Feb 20 18:50:30.109: INFO: namespace e2e-tests-gc-wp6lb deletion completed in 6.989856096s

 [SLOW TEST:14.301 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:50:30.109: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-vxzwg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-8494
STEP: Creating a pod to test atomic-volume-subpath
Feb 20 18:50:31.305: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8494" in namespace "e2e-tests-subpath-vxzwg" to be "success or failure"
Feb 20 18:50:31.327: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Pending", Reason="", readiness=false. Elapsed: 22.848427ms
Feb 20 18:50:33.354: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04901361s
Feb 20 18:50:35.377: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 4.072498415s
Feb 20 18:50:37.401: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 6.096759837s
Feb 20 18:50:39.425: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 8.120823711s
Feb 20 18:50:41.450: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 10.145242578s
Feb 20 18:50:43.474: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 12.169502948s
Feb 20 18:50:45.499: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 14.194091187s
Feb 20 18:50:47.523: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 16.218178141s
Feb 20 18:50:49.547: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 18.242334016s
Feb 20 18:50:51.571: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 20.266834618s
Feb 20 18:50:53.598: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Running", Reason="", readiness=false. Elapsed: 22.293034018s
Feb 20 18:50:55.622: INFO: Pod "pod-subpath-test-downwardapi-8494": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.317071445s
STEP: Saw pod success
Feb 20 18:50:55.622: INFO: Pod "pod-subpath-test-downwardapi-8494" satisfied condition "success or failure"
Feb 20 18:50:55.645: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-subpath-test-downwardapi-8494 container test-container-subpath-downwardapi-8494: <nil>
STEP: delete the pod
Feb 20 18:50:55.753: INFO: Waiting for pod pod-subpath-test-downwardapi-8494 to disappear
Feb 20 18:50:55.777: INFO: Pod pod-subpath-test-downwardapi-8494 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8494
Feb 20 18:50:55.777: INFO: Deleting pod "pod-subpath-test-downwardapi-8494" in namespace "e2e-tests-subpath-vxzwg"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:50:55.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-vxzwg" for this suite.
Feb 20 18:51:01.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:51:02.079: INFO: namespace: e2e-tests-subpath-vxzwg, resource: bindings, ignored listing per whitelist
Feb 20 18:51:02.776: INFO: namespace e2e-tests-subpath-vxzwg deletion completed in 6.951449826s

 [SLOW TEST:32.666 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:51:02.776: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-7tdfj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-7944efd8-3540-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:51:04.075: INFO: Waiting up to 5m0s for pod "pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-7tdfj" to be "success or failure"
Feb 20 18:51:04.097: INFO: Pod "pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.62212ms
Feb 20 18:51:06.121: INFO: Pod "pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046277984s
STEP: Saw pod success
Feb 20 18:51:06.121: INFO: Pod "pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:51:06.144: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:51:06.223: INFO: Waiting for pod pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:51:06.247: INFO: Pod pod-configmaps-79488c0b-3540-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:51:06.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-7tdfj" for this suite.
Feb 20 18:51:12.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:51:12.453: INFO: namespace: e2e-tests-configmap-7tdfj, resource: bindings, ignored listing per whitelist
Feb 20 18:51:13.369: INFO: namespace e2e-tests-configmap-7tdfj deletion completed in 7.097254431s

 [SLOW TEST:10.593 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:51:13.369: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-fh7gn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 18:51:14.431: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:51:18.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-fh7gn" for this suite.
Feb 20 18:51:40.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:51:41.005: INFO: namespace: e2e-tests-init-container-fh7gn, resource: bindings, ignored listing per whitelist
Feb 20 18:51:41.482: INFO: namespace e2e-tests-init-container-fh7gn deletion completed in 23.047602206s

 [SLOW TEST:28.113 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:51:41.482: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-k7kft
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:51:42.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-k7kft" to be "success or failure"
Feb 20 18:51:42.781: INFO: Pod "downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.089898ms
Feb 20 18:51:44.808: INFO: Pod "downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04953804s
STEP: Saw pod success
Feb 20 18:51:44.808: INFO: Pod "downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:51:44.831: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:51:44.891: INFO: Waiting for pod downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:51:44.915: INFO: Pod downwardapi-volume-90571792-3540-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:51:44.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-k7kft" for this suite.
Feb 20 18:51:51.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:51:51.357: INFO: namespace: e2e-tests-downward-api-k7kft, resource: bindings, ignored listing per whitelist
Feb 20 18:51:52.018: INFO: namespace e2e-tests-downward-api-k7kft deletion completed in 7.078802634s

 [SLOW TEST:10.536 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:51:52.018: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7nt2v
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:51:53.137: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml version --client'
Feb 20 18:51:53.359: INFO: stderr: ""
Feb 20 18:51:53.359: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T18:08:58Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 20 18:51:53.382: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-7nt2v'
Feb 20 18:51:56.979: INFO: stderr: ""
Feb 20 18:51:56.979: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 20 18:51:56.979: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-7nt2v'
Feb 20 18:51:57.442: INFO: stderr: ""
Feb 20 18:51:57.442: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 18:51:58.471: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:51:58.471: INFO: Found 0 / 1
Feb 20 18:51:59.466: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:51:59.466: INFO: Found 1 / 1
Feb 20 18:51:59.466: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 18:51:59.490: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 18:51:59.490: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 18:51:59.490: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe pod redis-master-8mtqn --namespace=e2e-tests-kubectl-7nt2v'
Feb 20 18:52:00.079: INFO: stderr: ""
Feb 20 18:52:00.079: INFO: stdout: "Name:               redis-master-8mtqn\nNamespace:          e2e-tests-kubectl-7nt2v\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd/10.250.0.3\nStart Time:         Wed, 20 Feb 2019 18:51:56 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.137/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.137\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://c54c7222bc4f79a4b78776eccf7eec84a4f9cbe8c7d13efc9687fbfadd9e3b33\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 20 Feb 2019 18:51:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-j7qw9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-j7qw9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-j7qw9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                              Message\n  ----    ------     ----  ----                                                              -------\n  Normal  Scheduled  4s    default-scheduler                                                 Successfully assigned e2e-tests-kubectl-7nt2v/redis-master-8mtqn to shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd\n  Normal  Pulled     3s    kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Created container\n  Normal  Started    2s    kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd  Started container\n"
Feb 20 18:52:00.080: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-7nt2v'
Feb 20 18:52:00.453: INFO: stderr: ""
Feb 20 18:52:00.453: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-7nt2v\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-8mtqn\n"
Feb 20 18:52:00.453: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-7nt2v'
Feb 20 18:52:00.855: INFO: stderr: ""
Feb 20 18:52:00.855: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-7nt2v\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.69.239.107\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.137:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 20 18:52:00.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk'
Feb 20 18:52:01.285: INFO: stderr: ""
Feb 20 18:52:01.285: INFO: stdout: "Name:               shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n1-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west1\n                    failure-domain.beta.kubernetes.io/zone=europe-west1-b\n                    kubernetes.io/hostname=shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.2/32\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 20 Feb 2019 17:53:46 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 20 Feb 2019 17:54:11 +0000   Wed, 20 Feb 2019 17:54:11 +0000   RouteCreated                 RouteController created a route\n  OutOfDisk            False   Wed, 20 Feb 2019 18:51:51 +0000   Wed, 20 Feb 2019 17:53:46 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure       False   Wed, 20 Feb 2019 18:51:51 +0000   Wed, 20 Feb 2019 17:53:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 20 Feb 2019 18:51:51 +0000   Wed, 20 Feb 2019 17:53:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 20 Feb 2019 18:51:51 +0000   Wed, 20 Feb 2019 17:53:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 20 Feb 2019 18:51:51 +0000   Wed, 20 Feb 2019 17:54:06 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.0.2\n  ExternalIP:   35.205.90.85\n  InternalDNS:  shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk.c.sap-se-gcp-scp-k8s-dev.internal\n  Hostname:     shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk.c.sap-se-gcp-scp-k8s-dev.internal\nCapacity:\n attachable-volumes-gce-pd:  64\n cpu:                        4\n ephemeral-storage:          17897500Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     15396260Ki\n pods:                       110\nAllocatable:\n attachable-volumes-gce-pd:  64\n cpu:                        3920m\n ephemeral-storage:          17410687987\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     13299108Ki\n pods:                       110\nSystem Info:\n Machine ID:                 d310a8dea388546e45a6beadc46e45e4\n System UUID:                D310A8DE-A388-546E-45A6-BEADC46E45E4\n Boot ID:                    7fc6c160-1886-49eb-bfc7-e78e1b40a4b8\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.5\n Kube-Proxy Version:         v1.12.5\nPodCIDR:                     100.96.0.0/24\nProviderID:                  gce://sap-se-gcp-scp-k8s-dev/europe-west1-b/shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube-lego-648f8c9f5c-c9t8z                                  20m (0%)      50m (1%)    8Mi (0%)         32Mi (0%)\n  kube-system                addons-kubernetes-dashboard-5f64f76bd-h2vfh                        50m (1%)      100m (2%)   50Mi (0%)        256Mi (1%)\n  kube-system                addons-nginx-ingress-controller-55d976867d-ttlp2                   100m (2%)     2 (51%)     100Mi (0%)       800Mi (6%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-sn6mb    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-64f6f7f998-h9cxk                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)\n  kube-system                calico-node-mmtdl                                                  100m (2%)     500m (12%)  100Mi (0%)       700Mi (5%)\n  kube-system                coredns-5f4748c5f-5qhjk                                            50m (1%)      100m (2%)   15Mi (0%)        100Mi (0%)\n  kube-system                kube-proxy-2lxpj                                                   20m (0%)      900m (22%)  64Mi (0%)        200Mi (1%)\n  kube-system                metrics-server-6c5b747679-8tgqt                                    20m (0%)      80m (2%)    100Mi (0%)       400Mi (3%)\n  kube-system                node-exporter-c4nt2                                                5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)\n  kube-system                vpn-shoot-56b45cd8c8-2p7dt                                         50m (1%)      100m (2%)   50Mi (0%)        100Mi (0%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        420m (10%)  3855m (98%)\n  memory                     502Mi (3%)  2673Mi (20%)\n  attachable-volumes-gce-pd  0           0\nEvents:\n  Type    Reason                   Age   From                                                                 Message\n  ----    ------                   ----  ----                                                                 -------\n  Normal  Starting                 58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Starting kubelet.\n  Normal  NodeHasSufficientDisk    58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk status is now: NodeHasSufficientDisk\n  Normal  NodeHasSufficientMemory  58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  58m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Updated Node Allocatable limit across pods\n  Normal  Starting                 58m   kube-proxy, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk  Starting kube-proxy.\n  Normal  NodeReady                57m   kubelet, shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk     Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk status is now: NodeReady\n"
Feb 20 18:52:01.285: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml describe namespace e2e-tests-kubectl-7nt2v'
Feb 20 18:52:01.759: INFO: stderr: ""
Feb 20 18:52:01.759: INFO: stdout: "Name:         e2e-tests-kubectl-7nt2v\nLabels:       e2e-framework=kubectl\n              e2e-run=da625892-353a-11e9-9b2a-cafe91372a39\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:52:01.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7nt2v" for this suite.
Feb 20 18:52:25.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:52:26.712: INFO: namespace: e2e-tests-kubectl-7nt2v, resource: bindings, ignored listing per whitelist
Feb 20 18:52:26.758: INFO: namespace e2e-tests-kubectl-7nt2v deletion completed in 24.975000476s

 [SLOW TEST:34.740 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:52:26.759: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gjrh4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 18:52:45.938: INFO: Container started at 2019-02-20 18:52:28 +0000 UTC, pod became ready at 2019-02-20 18:52:45 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:52:45.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gjrh4" for this suite.
Feb 20 18:53:10.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:10.474: INFO: namespace: e2e-tests-container-probe-gjrh4, resource: bindings, ignored listing per whitelist
Feb 20 18:53:10.956: INFO: namespace e2e-tests-container-probe-gjrh4 deletion completed in 24.994363212s

 [SLOW TEST:44.197 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:53:10.956: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-rqhdh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 18:53:12.058: INFO: Waiting up to 5m0s for pod "pod-c591095b-3540-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-rqhdh" to be "success or failure"
Feb 20 18:53:12.081: INFO: Pod "pod-c591095b-3540-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.831227ms
Feb 20 18:53:14.106: INFO: Pod "pod-c591095b-3540-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048524907s
STEP: Saw pod success
Feb 20 18:53:14.106: INFO: Pod "pod-c591095b-3540-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:53:14.135: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-c591095b-3540-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 18:53:14.222: INFO: Waiting for pod pod-c591095b-3540-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:53:14.245: INFO: Pod pod-c591095b-3540-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:53:14.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-rqhdh" for this suite.
Feb 20 18:53:20.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:20.732: INFO: namespace: e2e-tests-emptydir-rqhdh, resource: bindings, ignored listing per whitelist
Feb 20 18:53:21.306: INFO: namespace e2e-tests-emptydir-rqhdh deletion completed in 7.036148328s

 [SLOW TEST:10.349 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:53:21.306: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mg5fh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 20 18:53:22.433: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:53:22.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mg5fh" for this suite.
Feb 20 18:53:28.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:53:29.249: INFO: namespace: e2e-tests-kubectl-mg5fh, resource: bindings, ignored listing per whitelist
Feb 20 18:53:29.669: INFO: namespace e2e-tests-kubectl-mg5fh deletion completed in 7.011421262s

 [SLOW TEST:8.363 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:53:29.669: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-c9qgt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 18:53:31.024: INFO: Number of nodes with available pods: 0
Feb 20 18:53:31.024: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:32.071: INFO: Number of nodes with available pods: 0
Feb 20 18:53:32.071: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:33.071: INFO: Number of nodes with available pods: 2
Feb 20 18:53:33.071: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 20 18:53:33.190: INFO: Number of nodes with available pods: 1
Feb 20 18:53:33.190: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:34.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:34.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:35.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:35.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:36.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:36.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:37.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:37.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:38.240: INFO: Number of nodes with available pods: 1
Feb 20 18:53:38.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:39.240: INFO: Number of nodes with available pods: 1
Feb 20 18:53:39.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:40.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:40.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:41.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:41.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:42.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:42.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:43.240: INFO: Number of nodes with available pods: 1
Feb 20 18:53:43.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:44.240: INFO: Number of nodes with available pods: 1
Feb 20 18:53:44.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:45.245: INFO: Number of nodes with available pods: 1
Feb 20 18:53:45.245: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:46.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:46.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:47.239: INFO: Number of nodes with available pods: 1
Feb 20 18:53:47.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:48.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:48.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:49.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:49.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:50.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:50.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:51.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:51.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:52.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:52.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:53.239: INFO: Number of nodes with available pods: 1
Feb 20 18:53:53.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:54.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:54.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:55.240: INFO: Number of nodes with available pods: 1
Feb 20 18:53:55.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:56.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:56.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:57.238: INFO: Number of nodes with available pods: 1
Feb 20 18:53:57.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:58.239: INFO: Number of nodes with available pods: 1
Feb 20 18:53:58.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:53:59.237: INFO: Number of nodes with available pods: 1
Feb 20 18:53:59.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:00.238: INFO: Number of nodes with available pods: 1
Feb 20 18:54:00.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:01.238: INFO: Number of nodes with available pods: 1
Feb 20 18:54:01.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:02.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:02.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:03.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:03.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:04.240: INFO: Number of nodes with available pods: 1
Feb 20 18:54:04.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:05.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:05.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:06.237: INFO: Number of nodes with available pods: 1
Feb 20 18:54:06.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:07.238: INFO: Number of nodes with available pods: 1
Feb 20 18:54:07.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:08.238: INFO: Number of nodes with available pods: 1
Feb 20 18:54:08.238: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:09.240: INFO: Number of nodes with available pods: 1
Feb 20 18:54:09.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:10.237: INFO: Number of nodes with available pods: 1
Feb 20 18:54:10.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:11.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:11.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:12.241: INFO: Number of nodes with available pods: 1
Feb 20 18:54:12.241: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:13.240: INFO: Number of nodes with available pods: 1
Feb 20 18:54:13.240: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:14.237: INFO: Number of nodes with available pods: 1
Feb 20 18:54:14.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:15.237: INFO: Number of nodes with available pods: 1
Feb 20 18:54:15.237: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:16.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:16.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:17.239: INFO: Number of nodes with available pods: 1
Feb 20 18:54:17.239: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 18:54:18.237: INFO: Number of nodes with available pods: 2
Feb 20 18:54:18.237: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-c9qgt, will wait for the garbage collector to delete the pods
Feb 20 18:54:18.362: INFO: Deleting {extensions DaemonSet} daemon-set took: 26.907378ms
Feb 20 18:54:18.462: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.223506ms
Feb 20 18:54:56.386: INFO: Number of nodes with available pods: 0
Feb 20 18:54:56.386: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 18:54:56.409: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-c9qgt/daemonsets","resourceVersion":"10868"},"items":null}

Feb 20 18:54:56.433: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-c9qgt/pods","resourceVersion":"10868"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:54:56.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-c9qgt" for this suite.
Feb 20 18:55:04.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:55:05.101: INFO: namespace: e2e-tests-daemonsets-c9qgt, resource: bindings, ignored listing per whitelist
Feb 20 18:55:05.519: INFO: namespace e2e-tests-daemonsets-c9qgt deletion completed in 8.992839114s

 [SLOW TEST:95.850 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:55:05.519: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-52s57
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 18:55:09.334: INFO: Successfully updated pod "annotationupdate09e000dc-3541-11e9-9b2a-cafe91372a39"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:55:11.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-52s57" for this suite.
Feb 20 18:55:35.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:55:35.623: INFO: namespace: e2e-tests-projected-52s57, resource: bindings, ignored listing per whitelist
Feb 20 18:55:36.395: INFO: namespace e2e-tests-projected-52s57 deletion completed in 24.974629662s

 [SLOW TEST:30.876 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:55:36.395: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-h5l9c
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-1c3c1258-3541-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 18:55:37.485: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-h5l9c" to be "success or failure"
Feb 20 18:55:37.508: INFO: Pod "pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.491573ms
Feb 20 18:55:39.532: INFO: Pod "pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046210815s
STEP: Saw pod success
Feb 20 18:55:39.532: INFO: Pod "pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:55:39.555: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 18:55:39.612: INFO: Waiting for pod pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:55:39.635: INFO: Pod pod-projected-configmaps-1c3fa9ad-3541-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:55:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-h5l9c" for this suite.
Feb 20 18:55:45.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:55:46.562: INFO: namespace: e2e-tests-projected-h5l9c, resource: bindings, ignored listing per whitelist
Feb 20 18:55:46.679: INFO: namespace e2e-tests-projected-h5l9c deletion completed in 7.019922984s

 [SLOW TEST:10.283 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:55:46.679: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7zl7x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 20 18:55:47.732: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:48.136: INFO: stderr: ""
Feb 20 18:55:48.136: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:55:48.136: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:48.325: INFO: stderr: ""
Feb 20 18:55:48.325: INFO: stdout: "update-demo-nautilus-lc9l5 update-demo-nautilus-zxd9t "
Feb 20 18:55:48.326: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lc9l5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:48.487: INFO: stderr: ""
Feb 20 18:55:48.487: INFO: stdout: ""
Feb 20 18:55:48.487: INFO: update-demo-nautilus-lc9l5 is created but not running
Feb 20 18:55:53.487: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:53.692: INFO: stderr: ""
Feb 20 18:55:53.692: INFO: stdout: "update-demo-nautilus-lc9l5 update-demo-nautilus-zxd9t "
Feb 20 18:55:53.692: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lc9l5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:53.877: INFO: stderr: ""
Feb 20 18:55:53.877: INFO: stdout: "true"
Feb 20 18:55:53.877: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-lc9l5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:54.066: INFO: stderr: ""
Feb 20 18:55:54.066: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:55:54.066: INFO: validating pod update-demo-nautilus-lc9l5
Feb 20 18:55:54.146: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:55:54.146: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:55:54.146: INFO: update-demo-nautilus-lc9l5 is verified up and running
Feb 20 18:55:54.146: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-zxd9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:54.299: INFO: stderr: ""
Feb 20 18:55:54.299: INFO: stdout: "true"
Feb 20 18:55:54.299: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-zxd9t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:55:54.489: INFO: stderr: ""
Feb 20 18:55:54.489: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:55:54.489: INFO: validating pod update-demo-nautilus-zxd9t
Feb 20 18:55:54.599: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:55:54.599: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:55:54.599: INFO: update-demo-nautilus-zxd9t is verified up and running
STEP: rolling-update to new replication controller
Feb 20 18:55:54.603: INFO: scanned /root for discovery docs: <nil>
Feb 20 18:55:54.603: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:09.824: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 18:56:09.824: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:56:09.824: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:09.992: INFO: stderr: ""
Feb 20 18:56:09.992: INFO: stdout: "update-demo-kitten-2wf8s update-demo-kitten-cz5ps "
Feb 20 18:56:09.993: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-2wf8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:10.184: INFO: stderr: ""
Feb 20 18:56:10.184: INFO: stdout: "true"
Feb 20 18:56:10.184: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-2wf8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:10.379: INFO: stderr: ""
Feb 20 18:56:10.379: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 18:56:10.379: INFO: validating pod update-demo-kitten-2wf8s
Feb 20 18:56:10.488: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 18:56:10.488: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 18:56:10.488: INFO: update-demo-kitten-2wf8s is verified up and running
Feb 20 18:56:10.488: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-cz5ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:10.666: INFO: stderr: ""
Feb 20 18:56:10.666: INFO: stdout: "true"
Feb 20 18:56:10.666: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-kitten-cz5ps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7zl7x'
Feb 20 18:56:10.820: INFO: stderr: ""
Feb 20 18:56:10.820: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 20 18:56:10.820: INFO: validating pod update-demo-kitten-cz5ps
Feb 20 18:56:10.931: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 20 18:56:10.931: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 20 18:56:10.931: INFO: update-demo-kitten-cz5ps is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:56:10.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7zl7x" for this suite.
Feb 20 18:56:35.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:56:35.876: INFO: namespace: e2e-tests-kubectl-7zl7x, resource: bindings, ignored listing per whitelist
Feb 20 18:56:35.900: INFO: namespace e2e-tests-kubectl-7zl7x deletion completed in 24.944180784s

 [SLOW TEST:49.220 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:56:35.900: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-2f8lq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 20 18:56:37.032: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:37.368: INFO: stderr: ""
Feb 20 18:56:37.368: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 20 18:56:37.368: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:37.529: INFO: stderr: ""
Feb 20 18:56:37.529: INFO: stdout: "update-demo-nautilus-twljj update-demo-nautilus-x5j9s "
Feb 20 18:56:37.529: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-twljj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:37.708: INFO: stderr: ""
Feb 20 18:56:37.708: INFO: stdout: ""
Feb 20 18:56:37.708: INFO: update-demo-nautilus-twljj is created but not running
Feb 20 18:56:42.708: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:42.903: INFO: stderr: ""
Feb 20 18:56:42.903: INFO: stdout: "update-demo-nautilus-twljj update-demo-nautilus-x5j9s "
Feb 20 18:56:42.903: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-twljj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:43.103: INFO: stderr: ""
Feb 20 18:56:43.103: INFO: stdout: "true"
Feb 20 18:56:43.103: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-twljj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:43.288: INFO: stderr: ""
Feb 20 18:56:43.288: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:56:43.288: INFO: validating pod update-demo-nautilus-twljj
Feb 20 18:56:43.400: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:56:43.400: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:56:43.400: INFO: update-demo-nautilus-twljj is verified up and running
Feb 20 18:56:43.400: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-x5j9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:43.660: INFO: stderr: ""
Feb 20 18:56:43.660: INFO: stdout: "true"
Feb 20 18:56:43.660: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods update-demo-nautilus-x5j9s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:43.818: INFO: stderr: ""
Feb 20 18:56:43.818: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 20 18:56:43.818: INFO: validating pod update-demo-nautilus-x5j9s
Feb 20 18:56:43.926: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 20 18:56:43.926: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 20 18:56:43.926: INFO: update-demo-nautilus-x5j9s is verified up and running
STEP: using delete to clean up resources
Feb 20 18:56:43.927: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:44.136: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:56:44.136: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 20 18:56:44.136: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-2f8lq'
Feb 20 18:56:44.312: INFO: stderr: "No resources found.\n"
Feb 20 18:56:44.312: INFO: stdout: ""
Feb 20 18:56:44.312: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-2f8lq -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 18:56:44.473: INFO: stderr: ""
Feb 20 18:56:44.473: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:56:44.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-2f8lq" for this suite.
Feb 20 18:57:08.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:57:08.975: INFO: namespace: e2e-tests-kubectl-2f8lq, resource: bindings, ignored listing per whitelist
Feb 20 18:57:09.486: INFO: namespace e2e-tests-kubectl-2f8lq deletion completed in 24.989613391s

 [SLOW TEST:33.587 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:57:09.487: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-nfp5p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-nfp5p
Feb 20 18:57:12.807: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-nfp5p
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:57:12.831: INFO: Initial restart count of pod liveness-exec is 0
Feb 20 18:58:03.493: INFO: Restart count of pod e2e-tests-container-probe-nfp5p/liveness-exec is now 1 (50.661384065s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:58:03.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-nfp5p" for this suite.
Feb 20 18:58:09.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:10.293: INFO: namespace: e2e-tests-container-probe-nfp5p, resource: bindings, ignored listing per whitelist
Feb 20 18:58:10.501: INFO: namespace e2e-tests-container-probe-nfp5p deletion completed in 6.955607092s

 [SLOW TEST:61.014 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:58:10.501: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-lvg5h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 18:58:11.758: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-lvg5h" to be "success or failure"
Feb 20 18:58:11.781: INFO: Pod "downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.034119ms
Feb 20 18:58:13.806: INFO: Pod "downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048082872s
STEP: Saw pod success
Feb 20 18:58:13.807: INFO: Pod "downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 18:58:13.830: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 18:58:13.890: INFO: Waiting for pod downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39 to disappear
Feb 20 18:58:13.913: INFO: Pod downwardapi-volume-7833aa85-3541-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:58:13.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-lvg5h" for this suite.
Feb 20 18:58:20.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:20.519: INFO: namespace: e2e-tests-downward-api-lvg5h, resource: bindings, ignored listing per whitelist
Feb 20 18:58:20.979: INFO: namespace e2e-tests-downward-api-lvg5h deletion completed in 7.042155883s

 [SLOW TEST:10.478 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:58:20.979: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-dcjl5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Feb 20 18:58:22.133: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:22.472: INFO: stderr: ""
Feb 20 18:58:22.472: INFO: stdout: "pod/pause created\n"
Feb 20 18:58:22.472: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 20 18:58:22.473: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-dcjl5" to be "running and ready"
Feb 20 18:58:22.495: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.817217ms
Feb 20 18:58:24.520: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.046999392s
Feb 20 18:58:24.520: INFO: Pod "pause" satisfied condition "running and ready"
Feb 20 18:58:24.520: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 20 18:58:24.520: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:24.688: INFO: stderr: ""
Feb 20 18:58:24.688: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 20 18:58:24.688: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:24.853: INFO: stderr: ""
Feb 20 18:58:24.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 20 18:58:24.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:25.065: INFO: stderr: ""
Feb 20 18:58:25.065: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 20 18:58:25.065: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:25.248: INFO: stderr: ""
Feb 20 18:58:25.248: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Feb 20 18:58:25.248: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:25.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 20 18:58:25.467: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 20 18:58:25.468: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-dcjl5'
Feb 20 18:58:25.696: INFO: stderr: "No resources found.\n"
Feb 20 18:58:25.696: INFO: stdout: ""
Feb 20 18:58:25.697: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-dcjl5 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 20 18:58:25.865: INFO: stderr: ""
Feb 20 18:58:25.865: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 18:58:25.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-dcjl5" for this suite.
Feb 20 18:58:33.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 18:58:34.483: INFO: namespace: e2e-tests-kubectl-dcjl5, resource: bindings, ignored listing per whitelist
Feb 20 18:58:34.869: INFO: namespace e2e-tests-kubectl-dcjl5 deletion completed in 8.979553932s

 [SLOW TEST:13.889 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 18:58:34.869: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-n94bf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-n94bf
Feb 20 18:58:38.002: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-n94bf
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 18:58:38.025: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:02:39.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-n94bf" for this suite.
Feb 20 19:02:45.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:02:45.835: INFO: namespace: e2e-tests-container-probe-n94bf, resource: bindings, ignored listing per whitelist
Feb 20 19:02:46.100: INFO: namespace e2e-tests-container-probe-n94bf deletion completed in 7.049791728s

 [SLOW TEST:251.232 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:02:46.101: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-thsfg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:03:47.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-thsfg" for this suite.
Feb 20 19:04:11.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:04:12.215: INFO: namespace: e2e-tests-container-probe-thsfg, resource: bindings, ignored listing per whitelist
Feb 20 19:04:12.466: INFO: namespace e2e-tests-container-probe-thsfg deletion completed in 25.053419113s

 [SLOW TEST:86.365 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:04:12.466: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7jppt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:04:13.545: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml version'
Feb 20 19:04:14.100: INFO: stderr: ""
Feb 20 19:04:14.100: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-20T18:08:58Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"clean\", BuildDate:\"2019-01-16T18:14:49Z\", GoVersion:\"go1.10.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:04:14.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7jppt" for this suite.
Feb 20 19:04:20.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:04:20.761: INFO: namespace: e2e-tests-kubectl-7jppt, resource: bindings, ignored listing per whitelist
Feb 20 19:04:21.200: INFO: namespace e2e-tests-kubectl-7jppt deletion completed in 7.073430343s

 [SLOW TEST:8.733 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:04:21.200: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vz9sh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 19:04:27.004: INFO: Successfully updated pod "pod-update-5518e8b8-3542-11e9-9b2a-cafe91372a39"
STEP: verifying the updated pod is in kubernetes
Feb 20 19:04:27.050: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:04:27.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vz9sh" for this suite.
Feb 20 19:04:51.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:04:51.719: INFO: namespace: e2e-tests-pods-vz9sh, resource: bindings, ignored listing per whitelist
Feb 20 19:04:52.076: INFO: namespace e2e-tests-pods-vz9sh deletion completed in 25.00156714s

 [SLOW TEST:30.876 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:04:52.076: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-m52c6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 20 19:04:53.261: INFO: Waiting up to 5m0s for pod "client-containers-678439d0-3542-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-containers-m52c6" to be "success or failure"
Feb 20 19:04:53.284: INFO: Pod "client-containers-678439d0-3542-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.332613ms
Feb 20 19:04:55.324: INFO: Pod "client-containers-678439d0-3542-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.063181402s
STEP: Saw pod success
Feb 20 19:04:55.324: INFO: Pod "client-containers-678439d0-3542-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:04:55.355: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod client-containers-678439d0-3542-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:04:55.463: INFO: Waiting for pod client-containers-678439d0-3542-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:04:55.486: INFO: Pod client-containers-678439d0-3542-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:04:55.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-m52c6" for this suite.
Feb 20 19:05:01.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:05:02.417: INFO: namespace: e2e-tests-containers-m52c6, resource: bindings, ignored listing per whitelist
Feb 20 19:05:02.463: INFO: namespace e2e-tests-containers-m52c6 deletion completed in 6.95386552s

 [SLOW TEST:10.387 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:05:02.463: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-x5sxh
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-6dbaf9eb-3542-11e9-9b2a-cafe91372a39
STEP: Creating secret with name s-test-opt-upd-6dbafa35-3542-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6dbaf9eb-3542-11e9-9b2a-cafe91372a39
STEP: Updating secret s-test-opt-upd-6dbafa35-3542-11e9-9b2a-cafe91372a39
STEP: Creating secret with name s-test-opt-create-6dbafa52-3542-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:06:21.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x5sxh" for this suite.
Feb 20 19:06:45.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:06:46.242: INFO: namespace: e2e-tests-secrets-x5sxh, resource: bindings, ignored listing per whitelist
Feb 20 19:06:46.724: INFO: namespace e2e-tests-secrets-x5sxh deletion completed in 25.091424269s

 [SLOW TEST:104.261 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:06:46.724: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-2jr29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:06:47.831: INFO: Creating deployment "test-recreate-deployment"
Feb 20 19:06:47.856: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 20 19:06:47.902: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 20 19:06:47.925: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-79f694ff59\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:06:49.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286407, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:06:51.949: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 20 19:06:51.995: INFO: Updating deployment test-recreate-deployment
Feb 20 19:06:51.995: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:06:52.190: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-2jr29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-2jr29/deployments/test-recreate-deployment,UID:abd2ccb6-3542-11e9-a0a3-2a5d9248fe67,ResourceVersion:12644,Generation:2,CreationTimestamp:2019-02-20 19:06:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-20 19:06:52 +0000 UTC 2019-02-20 19:06:52 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-20 19:06:52 +0000 UTC 2019-02-20 19:06:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 20 19:06:52.215: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-2jr29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-2jr29/replicasets/test-recreate-deployment-7cf749666b,UID:ae548f14-3542-11e9-a0a3-2a5d9248fe67,ResourceVersion:12641,Generation:1,CreationTimestamp:2019-02-20 19:06:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment abd2ccb6-3542-11e9-a0a3-2a5d9248fe67 0xc001ad2867 0xc001ad2868}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:06:52.215: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 20 19:06:52.215: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-2jr29,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-2jr29/replicasets/test-recreate-deployment-79f694ff59,UID:abd40cf2-3542-11e9-a0a3-2a5d9248fe67,ResourceVersion:12634,Generation:2,CreationTimestamp:2019-02-20 19:06:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment abd2ccb6-3542-11e9-a0a3-2a5d9248fe67 0xc001ad27a7 0xc001ad27a8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:06:52.239: INFO: Pod "test-recreate-deployment-7cf749666b-xh422" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-xh422,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-2jr29,SelfLink:/api/v1/namespaces/e2e-tests-deployment-2jr29/pods/test-recreate-deployment-7cf749666b-xh422,UID:ae54feee-3542-11e9-a0a3-2a5d9248fe67,ResourceVersion:12640,Generation:0,CreationTimestamp:2019-02-20 19:06:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b ae548f14-3542-11e9-a0a3-2a5d9248fe67 0xc001ad3147 0xc001ad3148}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-5kldq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5kldq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-5kldq true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ad3a90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ad3ab0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:06:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:06:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:06:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:06:52 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:,StartTime:2019-02-20 19:06:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:06:52.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-2jr29" for this suite.
Feb 20 19:06:58.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:06:59.214: INFO: namespace: e2e-tests-deployment-2jr29, resource: bindings, ignored listing per whitelist
Feb 20 19:06:59.262: INFO: namespace e2e-tests-deployment-2jr29 deletion completed in 6.998661265s

 [SLOW TEST:12.538 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:06:59.262: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-vmzsv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 19:07:03.110: INFO: Successfully updated pod "labelsupdateb3552b7a-3542-11e9-9b2a-cafe91372a39"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:07:05.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-vmzsv" for this suite.
Feb 20 19:07:29.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:07:29.507: INFO: namespace: e2e-tests-downward-api-vmzsv, resource: bindings, ignored listing per whitelist
Feb 20 19:07:30.237: INFO: namespace e2e-tests-downward-api-vmzsv deletion completed in 25.039651012s

 [SLOW TEST:30.975 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:07:30.237: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-pwrkq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-c5cf8167-3542-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:07:31.485: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-pwrkq" to be "success or failure"
Feb 20 19:07:31.508: INFO: Pod "pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.652169ms
Feb 20 19:07:33.533: INFO: Pod "pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047079774s
Feb 20 19:07:35.566: INFO: Pod "pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080531492s
STEP: Saw pod success
Feb 20 19:07:35.566: INFO: Pod "pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:07:35.590: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:07:35.652: INFO: Waiting for pod pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:07:35.675: INFO: Pod pod-projected-configmaps-c5d344f2-3542-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:07:35.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-pwrkq" for this suite.
Feb 20 19:07:41.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:07:42.359: INFO: namespace: e2e-tests-projected-pwrkq, resource: bindings, ignored listing per whitelist
Feb 20 19:07:42.717: INFO: namespace e2e-tests-projected-pwrkq deletion completed in 7.018089575s

 [SLOW TEST:12.480 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:07:42.717: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-nlwtr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-nlwtr
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 20 19:07:43.917: INFO: Found 1 stateful pods, waiting for 3
Feb 20 19:07:53.942: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:07:53.942: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:07:53.942: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 20 19:07:54.069: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 20 19:07:54.193: INFO: Updating stateful set ss2
Feb 20 19:07:54.240: INFO: Waiting for Pod e2e-tests-statefulset-nlwtr/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 20 19:08:04.366: INFO: Found 2 stateful pods, waiting for 3
Feb 20 19:08:14.391: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:08:14.391: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:08:14.391: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 20 19:08:14.496: INFO: Updating stateful set ss2
Feb 20 19:08:14.542: INFO: Waiting for Pod e2e-tests-statefulset-nlwtr/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 19:08:24.592: INFO: Waiting for Pod e2e-tests-statefulset-nlwtr/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 19:08:34.648: INFO: Updating stateful set ss2
Feb 20 19:08:34.694: INFO: Waiting for StatefulSet e2e-tests-statefulset-nlwtr/ss2 to complete update
Feb 20 19:08:34.694: INFO: Waiting for Pod e2e-tests-statefulset-nlwtr/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 20 19:08:44.741: INFO: Waiting for StatefulSet e2e-tests-statefulset-nlwtr/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:08:54.751: INFO: Deleting all statefulset in ns e2e-tests-statefulset-nlwtr
Feb 20 19:08:54.774: INFO: Scaling statefulset ss2 to 0
Feb 20 19:09:14.870: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:09:14.892: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:09:14.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-nlwtr" for this suite.
Feb 20 19:09:23.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:09:23.622: INFO: namespace: e2e-tests-statefulset-nlwtr, resource: bindings, ignored listing per whitelist
Feb 20 19:09:24.035: INFO: namespace e2e-tests-statefulset-nlwtr deletion completed in 9.043927641s

 [SLOW TEST:101.318 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:09:24.035: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-9nnls
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 19:09:25.159: INFO: Waiting up to 5m0s for pod "pod-0994a705-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-9nnls" to be "success or failure"
Feb 20 19:09:25.181: INFO: Pod "pod-0994a705-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.561678ms
Feb 20 19:09:27.205: INFO: Pod "pod-0994a705-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046631617s
STEP: Saw pod success
Feb 20 19:09:27.205: INFO: Pod "pod-0994a705-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:09:27.229: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-0994a705-3543-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:09:27.289: INFO: Waiting for pod pod-0994a705-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:09:27.312: INFO: Pod pod-0994a705-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:09:27.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-9nnls" for this suite.
Feb 20 19:09:33.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:09:33.592: INFO: namespace: e2e-tests-emptydir-9nnls, resource: bindings, ignored listing per whitelist
Feb 20 19:09:34.290: INFO: namespace e2e-tests-emptydir-9nnls deletion completed in 6.953994459s

 [SLOW TEST:10.255 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:09:34.290: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-qsqb7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-qsqb7
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 19:09:35.532: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 19:09:53.963: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.1.166 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qsqb7 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:09:53.963: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:09:55.589: INFO: Found all expected endpoints: [netserver-0]
Feb 20 19:09:55.613: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.0.59 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-qsqb7 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:09:55.613: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:09:57.132: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:09:57.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-qsqb7" for this suite.
Feb 20 19:10:19.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:10:20.048: INFO: namespace: e2e-tests-pod-network-test-qsqb7, resource: bindings, ignored listing per whitelist
Feb 20 19:10:20.189: INFO: namespace e2e-tests-pod-network-test-qsqb7 deletion completed in 23.031824474s

 [SLOW TEST:45.899 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:10:20.189: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-vzxxj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:10:21.350: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 20 19:10:21.397: INFO: Number of nodes with available pods: 0
Feb 20 19:10:21.397: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 20 19:10:21.493: INFO: Number of nodes with available pods: 0
Feb 20 19:10:21.493: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:22.516: INFO: Number of nodes with available pods: 0
Feb 20 19:10:22.516: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:23.516: INFO: Number of nodes with available pods: 1
Feb 20 19:10:23.516: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 20 19:10:23.615: INFO: Number of nodes with available pods: 0
Feb 20 19:10:23.615: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 20 19:10:23.663: INFO: Number of nodes with available pods: 0
Feb 20 19:10:23.663: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:24.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:24.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:25.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:25.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:26.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:26.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:27.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:27.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:28.686: INFO: Number of nodes with available pods: 0
Feb 20 19:10:28.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:29.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:29.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:30.692: INFO: Number of nodes with available pods: 0
Feb 20 19:10:30.692: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:31.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:31.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:32.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:32.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:33.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:33.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:34.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:34.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:35.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:35.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:36.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:36.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:37.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:37.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:38.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:38.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:39.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:39.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:40.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:40.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:41.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:41.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:42.689: INFO: Number of nodes with available pods: 0
Feb 20 19:10:42.689: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:43.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:43.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:44.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:44.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:45.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:45.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:46.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:46.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:47.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:47.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:48.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:48.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:49.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:49.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:50.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:50.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:51.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:51.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:52.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:52.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:53.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:53.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:54.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:54.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:55.693: INFO: Number of nodes with available pods: 0
Feb 20 19:10:55.693: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:56.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:56.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:57.687: INFO: Number of nodes with available pods: 0
Feb 20 19:10:57.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:58.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:58.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:10:59.688: INFO: Number of nodes with available pods: 0
Feb 20 19:10:59.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:00.688: INFO: Number of nodes with available pods: 0
Feb 20 19:11:00.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:01.688: INFO: Number of nodes with available pods: 0
Feb 20 19:11:01.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:02.687: INFO: Number of nodes with available pods: 0
Feb 20 19:11:02.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:03.687: INFO: Number of nodes with available pods: 0
Feb 20 19:11:03.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:04.688: INFO: Number of nodes with available pods: 0
Feb 20 19:11:04.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:05.687: INFO: Number of nodes with available pods: 0
Feb 20 19:11:05.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:06.687: INFO: Number of nodes with available pods: 0
Feb 20 19:11:06.687: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:07.687: INFO: Number of nodes with available pods: 0
Feb 20 19:11:07.688: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:11:08.687: INFO: Number of nodes with available pods: 1
Feb 20 19:11:08.687: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-vzxxj, will wait for the garbage collector to delete the pods
Feb 20 19:11:08.833: INFO: Deleting {extensions DaemonSet} daemon-set took: 25.371476ms
Feb 20 19:11:08.933: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.267886ms
Feb 20 19:11:46.359: INFO: Number of nodes with available pods: 0
Feb 20 19:11:46.359: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 19:11:46.382: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-vzxxj/daemonsets","resourceVersion":"13537"},"items":null}

Feb 20 19:11:46.406: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-vzxxj/pods","resourceVersion":"13537"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:11:46.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-vzxxj" for this suite.
Feb 20 19:11:52.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:11:53.023: INFO: namespace: e2e-tests-daemonsets-vzxxj, resource: bindings, ignored listing per whitelist
Feb 20 19:11:53.510: INFO: namespace e2e-tests-daemonsets-vzxxj deletion completed in 6.971470884s

 [SLOW TEST:93.322 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:11:53.511: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zqk6m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-62b004b9-3543-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:11:54.679: INFO: Waiting up to 5m0s for pod "pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-zqk6m" to be "success or failure"
Feb 20 19:11:54.702: INFO: Pod "pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.843633ms
Feb 20 19:11:56.726: INFO: Pod "pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046745995s
STEP: Saw pod success
Feb 20 19:11:56.726: INFO: Pod "pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:11:56.749: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:11:56.822: INFO: Waiting for pod pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:11:56.846: INFO: Pod pod-configmaps-62b3ab36-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:11:56.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zqk6m" for this suite.
Feb 20 19:12:02.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:03.053: INFO: namespace: e2e-tests-configmap-zqk6m, resource: bindings, ignored listing per whitelist
Feb 20 19:12:03.868: INFO: namespace e2e-tests-configmap-zqk6m deletion completed in 6.998466013s

 [SLOW TEST:10.358 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:03.869: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-flnqg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:12:05.191: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 19:12:07.238: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:12:09.429: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-flnqg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-flnqg/deployments/test-cleanup-deployment,UID:6a3b6ea4-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13641,Generation:1,CreationTimestamp:2019-02-20 19:12:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 19:12:07 +0000 UTC 2019-02-20 19:12:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 19:12:08 +0000 UTC 2019-02-20 19:12:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-755f6b95cc" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 19:12:09.453: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-flnqg,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-flnqg/replicasets/test-cleanup-deployment-755f6b95cc,UID:6a3da3d7-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13634,Generation:1,CreationTimestamp:2019-02-20 19:12:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 6a3b6ea4-3543-11e9-a0a3-2a5d9248fe67 0xc001eb8397 0xc001eb8398}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 19:12:09.477: INFO: Pod "test-cleanup-deployment-755f6b95cc-hdbp5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-hdbp5,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-flnqg,SelfLink:/api/v1/namespaces/e2e-tests-deployment-flnqg/pods/test-cleanup-deployment-755f6b95cc-hdbp5,UID:6a3e39ff-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13633,Generation:0,CreationTimestamp:2019-02-20 19:12:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.170/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc 6a3da3d7-3543-11e9-a0a3-2a5d9248fe67 0xc001eb9607 0xc001eb9608}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-npdrl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-npdrl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-npdrl true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001eb9670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001eb9690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:07 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:07 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.170,StartTime:2019-02-20 19:12:07 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 19:12:08 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://04b55fa032ce28a7a3aceb3b3e74fcd5b6d5727d5271a0ea0e85f144e3ed897a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:12:09.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-flnqg" for this suite.
Feb 20 19:12:15.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:16.305: INFO: namespace: e2e-tests-deployment-flnqg, resource: bindings, ignored listing per whitelist
Feb 20 19:12:16.497: INFO: namespace e2e-tests-deployment-flnqg deletion completed in 6.995767398s

 [SLOW TEST:12.628 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:16.497: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-bzkqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 20 19:12:17.731: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-bzkqg,SelfLink:/api/v1/namespaces/e2e-tests-watch-bzkqg/configmaps/e2e-watch-test-watch-closed,UID:706aa40e-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13676,Generation:0,CreationTimestamp:2019-02-20 19:12:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:12:17.731: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-bzkqg,SelfLink:/api/v1/namespaces/e2e-tests-watch-bzkqg/configmaps/e2e-watch-test-watch-closed,UID:706aa40e-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13678,Generation:0,CreationTimestamp:2019-02-20 19:12:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 20 19:12:17.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-bzkqg,SelfLink:/api/v1/namespaces/e2e-tests-watch-bzkqg/configmaps/e2e-watch-test-watch-closed,UID:706aa40e-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13679,Generation:0,CreationTimestamp:2019-02-20 19:12:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:12:17.829: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-bzkqg,SelfLink:/api/v1/namespaces/e2e-tests-watch-bzkqg/configmaps/e2e-watch-test-watch-closed,UID:706aa40e-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13680,Generation:0,CreationTimestamp:2019-02-20 19:12:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:12:17.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-bzkqg" for this suite.
Feb 20 19:12:23.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:24.265: INFO: namespace: e2e-tests-watch-bzkqg, resource: bindings, ignored listing per whitelist
Feb 20 19:12:25.114: INFO: namespace e2e-tests-watch-bzkqg deletion completed in 7.260915095s

 [SLOW TEST:8.617 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:25.114: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-fhdv7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:12:26.335: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 20 19:12:26.381: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 20 19:12:28.429: INFO: Creating deployment "test-rolling-update-deployment"
Feb 20 19:12:28.453: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 20 19:12:28.498: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 20 19:12:28.521: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:0, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286748, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286748, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286748, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63686286748, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 20 19:12:30.545: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 20 19:12:30.616: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-fhdv7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fhdv7/deployments/test-rolling-update-deployment,UID:76d5e67e-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13745,Generation:1,CreationTimestamp:2019-02-20 19:12:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-20 19:12:28 +0000 UTC 2019-02-20 19:12:28 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-20 19:12:30 +0000 UTC 2019-02-20 19:12:28 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 20 19:12:30.640: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-fhdv7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fhdv7/replicasets/test-rolling-update-deployment-65b7695dcf,UID:76d887b1-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13738,Generation:1,CreationTimestamp:2019-02-20 19:12:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 76d5e67e-3543-11e9-a0a3-2a5d9248fe67 0xc00100fa57 0xc00100fa58}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 20 19:12:30.640: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 20 19:12:30.640: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-fhdv7,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fhdv7/replicasets/test-rolling-update-controller,UID:75966346-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13744,Generation:2,CreationTimestamp:2019-02-20 19:12:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 76d5e67e-3543-11e9-a0a3-2a5d9248fe67 0xc00100f997 0xc00100f998}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 20 19:12:30.666: INFO: Pod "test-rolling-update-deployment-65b7695dcf-nmgks" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-nmgks,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-fhdv7,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fhdv7/pods/test-rolling-update-deployment-65b7695dcf-nmgks,UID:76d90e61-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13737,Generation:0,CreationTimestamp:2019-02-20 19:12:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.172/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf 76d887b1-3543-11e9-a0a3-2a5d9248fe67 0xc001b14307 0xc001b14308}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-pcnrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pcnrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pcnrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b14370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b14390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:12:28 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.172,StartTime:2019-02-20 19:12:28 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-20 19:12:29 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://eb2bf0a23c5433c897ef2da46bd398968c14f0178f72941a9e70554b405dc2c3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:12:30.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-fhdv7" for this suite.
Feb 20 19:12:36.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:36.926: INFO: namespace: e2e-tests-deployment-fhdv7, resource: bindings, ignored listing per whitelist
Feb 20 19:12:37.688: INFO: namespace e2e-tests-deployment-fhdv7 deletion completed in 6.997445336s

 [SLOW TEST:12.574 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:37.689: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6zcnt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 19:12:38.975: INFO: Waiting up to 5m0s for pod "downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-6zcnt" to be "success or failure"
Feb 20 19:12:38.998: INFO: Pod "downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.984432ms
Feb 20 19:12:41.021: INFO: Pod "downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046387943s
STEP: Saw pod success
Feb 20 19:12:41.021: INFO: Pod "downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:12:41.044: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:12:41.104: INFO: Waiting for pod downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:12:41.129: INFO: Pod downward-api-7d1a8148-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:12:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6zcnt" for this suite.
Feb 20 19:12:47.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:48.105: INFO: namespace: e2e-tests-downward-api-6zcnt, resource: bindings, ignored listing per whitelist
Feb 20 19:12:48.152: INFO: namespace e2e-tests-downward-api-6zcnt deletion completed in 6.998129907s

 [SLOW TEST:10.463 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:48.152: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6m7kc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:12:49.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-6m7kc" to be "success or failure"
Feb 20 19:12:49.282: INFO: Pod "downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.743522ms
Feb 20 19:12:51.306: INFO: Pod "downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046512914s
STEP: Saw pod success
Feb 20 19:12:51.306: INFO: Pod "downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:12:51.330: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:12:51.408: INFO: Waiting for pod downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:12:51.431: INFO: Pod downwardapi-volume-833bdd55-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:12:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6m7kc" for this suite.
Feb 20 19:12:57.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:12:57.712: INFO: namespace: e2e-tests-downward-api-6m7kc, resource: bindings, ignored listing per whitelist
Feb 20 19:12:58.452: INFO: namespace e2e-tests-downward-api-6m7kc deletion completed in 6.995947933s

 [SLOW TEST:10.299 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:12:58.452: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-7tbk9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 20 19:12:59.557: INFO: Waiting up to 5m0s for pod "var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-var-expansion-7tbk9" to be "success or failure"
Feb 20 19:12:59.581: INFO: Pod "var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.17844ms
Feb 20 19:13:01.605: INFO: Pod "var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047690801s
Feb 20 19:13:03.630: INFO: Pod "var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072385848s
STEP: Saw pod success
Feb 20 19:13:03.630: INFO: Pod "var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:13:03.653: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:13:03.722: INFO: Waiting for pod var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:13:03.745: INFO: Pod var-expansion-895f3912-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:13:03.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-7tbk9" for this suite.
Feb 20 19:13:09.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:13:10.496: INFO: namespace: e2e-tests-var-expansion-7tbk9, resource: bindings, ignored listing per whitelist
Feb 20 19:13:10.750: INFO: namespace e2e-tests-var-expansion-7tbk9 deletion completed in 6.980759934s

 [SLOW TEST:12.298 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:13:10.750: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-bwkpc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 20 19:13:11.831: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13900,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:13:11.831: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13900,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 20 19:13:21.885: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13920,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 19:13:21.885: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13920,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 20 19:13:31.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13941,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:13:31.932: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13941,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 20 19:13:41.959: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13960,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:13:41.959: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-a,UID:90b0d4f2-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13960,Generation:0,CreationTimestamp:2019-02-20 19:13:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 20 19:13:51.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-b,UID:a89fc135-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13980,Generation:0,CreationTimestamp:2019-02-20 19:13:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:13:51.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-b,UID:a89fc135-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:13980,Generation:0,CreationTimestamp:2019-02-20 19:13:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 20 19:14:02.013: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-b,UID:a89fc135-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:14001,Generation:0,CreationTimestamp:2019-02-20 19:13:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:14:02.013: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-bwkpc,SelfLink:/api/v1/namespaces/e2e-tests-watch-bwkpc/configmaps/e2e-watch-test-configmap-b,UID:a89fc135-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:14001,Generation:0,CreationTimestamp:2019-02-20 19:13:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:14:12.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-bwkpc" for this suite.
Feb 20 19:14:18.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:14:18.720: INFO: namespace: e2e-tests-watch-bwkpc, resource: bindings, ignored listing per whitelist
Feb 20 19:14:19.094: INFO: namespace e2e-tests-watch-bwkpc deletion completed in 7.055928238s

 [SLOW TEST:68.344 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:14:19.094: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-d7qww
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 20 19:14:22.257: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-b96abea3-3543-11e9-9b2a-cafe91372a39,GenerateName:,Namespace:e2e-tests-events-d7qww,SelfLink:/api/v1/namespaces/e2e-tests-events-d7qww/pods/send-events-b96abea3-3543-11e9-9b2a-cafe91372a39,UID:b96b91c6-3543-11e9-a0a3-2a5d9248fe67,ResourceVersion:14053,Generation:0,CreationTimestamp:2019-02-20 19:14:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 137249420,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.176/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-nql6j {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nql6j,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-nql6j true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00187c920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00187c940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:14:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:14:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:14:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-20 19:14:20 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.3,PodIP:100.96.1.176,StartTime:2019-02-20 19:14:20 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-20 19:14:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://2dc9cee4e48288cab9b3b6e7ac96994476f257d3c6062a698317e7afe75523e1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 20 19:14:24.281: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 20 19:14:26.305: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:14:26.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-d7qww" for this suite.
Feb 20 19:15:06.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:15:06.957: INFO: namespace: e2e-tests-events-d7qww, resource: bindings, ignored listing per whitelist
Feb 20 19:15:07.540: INFO: namespace e2e-tests-events-d7qww deletion completed in 41.186440998s

 [SLOW TEST:48.446 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:15:07.540: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-h6sqg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-h6sqg
I0220 19:15:08.656524   29647 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-h6sqg, replica count: 1
I0220 19:15:09.707123   29647 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 19:15:10.707433   29647 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0220 19:15:11.707614   29647 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 20 19:15:11.836: INFO: Created: latency-svc-6zhrz
Feb 20 19:15:11.844: INFO: Got endpoints: latency-svc-6zhrz [36.572854ms]
Feb 20 19:15:11.900: INFO: Created: latency-svc-9wbc8
Feb 20 19:15:11.904: INFO: Got endpoints: latency-svc-9wbc8 [33.27356ms]
Feb 20 19:15:11.904: INFO: Created: latency-svc-6x2b9
Feb 20 19:15:11.908: INFO: Created: latency-svc-5h72h
Feb 20 19:15:11.908: INFO: Got endpoints: latency-svc-6x2b9 [37.465223ms]
Feb 20 19:15:11.914: INFO: Got endpoints: latency-svc-5h72h [42.968135ms]
Feb 20 19:15:11.916: INFO: Created: latency-svc-k6v7q
Feb 20 19:15:11.918: INFO: Got endpoints: latency-svc-k6v7q [47.559943ms]
Feb 20 19:15:11.925: INFO: Created: latency-svc-nncbg
Feb 20 19:15:11.926: INFO: Got endpoints: latency-svc-nncbg [54.89839ms]
Feb 20 19:15:11.926: INFO: Created: latency-svc-n9vjx
Feb 20 19:15:11.930: INFO: Got endpoints: latency-svc-n9vjx [59.163799ms]
Feb 20 19:15:11.930: INFO: Created: latency-svc-j8hk2
Feb 20 19:15:11.932: INFO: Got endpoints: latency-svc-j8hk2 [88.043635ms]
Feb 20 19:15:11.936: INFO: Created: latency-svc-xfm6d
Feb 20 19:15:11.940: INFO: Created: latency-svc-4rswl
Feb 20 19:15:11.944: INFO: Created: latency-svc-v67hq
Feb 20 19:15:11.948: INFO: Created: latency-svc-w6nql
Feb 20 19:15:12.003: INFO: Created: latency-svc-wbmdz
Feb 20 19:15:12.003: INFO: Got endpoints: latency-svc-v67hq [132.024387ms]
Feb 20 19:15:12.003: INFO: Got endpoints: latency-svc-xfm6d [132.308672ms]
Feb 20 19:15:12.003: INFO: Got endpoints: latency-svc-4rswl [132.527809ms]
Feb 20 19:15:12.003: INFO: Got endpoints: latency-svc-w6nql [132.605321ms]
Feb 20 19:15:12.006: INFO: Got endpoints: latency-svc-wbmdz [135.743705ms]
Feb 20 19:15:12.006: INFO: Created: latency-svc-9frg9
Feb 20 19:15:12.011: INFO: Created: latency-svc-wfv5k
Feb 20 19:15:12.011: INFO: Got endpoints: latency-svc-9frg9 [166.533333ms]
Feb 20 19:15:12.015: INFO: Got endpoints: latency-svc-wfv5k [170.898085ms]
Feb 20 19:15:12.016: INFO: Created: latency-svc-cx5xn
Feb 20 19:15:12.018: INFO: Got endpoints: latency-svc-cx5xn [173.141423ms]
Feb 20 19:15:12.020: INFO: Created: latency-svc-nbvt5
Feb 20 19:15:12.026: INFO: Created: latency-svc-zjxs7
Feb 20 19:15:12.031: INFO: Created: latency-svc-n9w4n
Feb 20 19:15:12.036: INFO: Created: latency-svc-dt7qg
Feb 20 19:15:12.040: INFO: Created: latency-svc-dzvwz
Feb 20 19:15:12.046: INFO: Created: latency-svc-zjwsb
Feb 20 19:15:12.050: INFO: Created: latency-svc-ssjhd
Feb 20 19:15:12.056: INFO: Created: latency-svc-wlh5d
Feb 20 19:15:12.057: INFO: Got endpoints: latency-svc-nbvt5 [153.235465ms]
Feb 20 19:15:12.057: INFO: Got endpoints: latency-svc-zjxs7 [149.113326ms]
Feb 20 19:15:12.057: INFO: Got endpoints: latency-svc-n9w4n [143.867129ms]
Feb 20 19:15:12.057: INFO: Got endpoints: latency-svc-dt7qg [139.084395ms]
Feb 20 19:15:12.058: INFO: Got endpoints: latency-svc-dzvwz [131.940757ms]
Feb 20 19:15:12.062: INFO: Created: latency-svc-2gqd4
Feb 20 19:15:12.062: INFO: Got endpoints: latency-svc-wlh5d [59.554207ms]
Feb 20 19:15:12.063: INFO: Got endpoints: latency-svc-ssjhd [130.207431ms]
Feb 20 19:15:12.063: INFO: Got endpoints: latency-svc-zjwsb [132.806019ms]
Feb 20 19:15:12.063: INFO: Got endpoints: latency-svc-2gqd4 [59.896945ms]
Feb 20 19:15:12.066: INFO: Created: latency-svc-wxwkn
Feb 20 19:15:12.116: INFO: Got endpoints: latency-svc-wxwkn [112.40252ms]
Feb 20 19:15:12.116: INFO: Created: latency-svc-kj5cr
Feb 20 19:15:12.119: INFO: Got endpoints: latency-svc-kj5cr [115.172348ms]
Feb 20 19:15:12.125: INFO: Created: latency-svc-mcv9l
Feb 20 19:15:12.130: INFO: Created: latency-svc-wvr6j
Feb 20 19:15:12.135: INFO: Created: latency-svc-85hhq
Feb 20 19:15:12.140: INFO: Created: latency-svc-52zx8
Feb 20 19:15:12.145: INFO: Created: latency-svc-cmrj6
Feb 20 19:15:12.151: INFO: Created: latency-svc-8z48r
Feb 20 19:15:12.156: INFO: Created: latency-svc-st796
Feb 20 19:15:12.157: INFO: Got endpoints: latency-svc-85hhq [141.635563ms]
Feb 20 19:15:12.157: INFO: Got endpoints: latency-svc-mcv9l [150.818975ms]
Feb 20 19:15:12.157: INFO: Got endpoints: latency-svc-wvr6j [146.441524ms]
Feb 20 19:15:12.158: INFO: Got endpoints: latency-svc-52zx8 [140.032092ms]
Feb 20 19:15:12.158: INFO: Got endpoints: latency-svc-cmrj6 [100.502351ms]
Feb 20 19:15:12.161: INFO: Got endpoints: latency-svc-st796 [103.25154ms]
Feb 20 19:15:12.161: INFO: Got endpoints: latency-svc-8z48r [103.911092ms]
Feb 20 19:15:12.163: INFO: Created: latency-svc-4ntdd
Feb 20 19:15:12.170: INFO: Got endpoints: latency-svc-4ntdd [112.797249ms]
Feb 20 19:15:12.203: INFO: Created: latency-svc-b69n9
Feb 20 19:15:12.205: INFO: Got endpoints: latency-svc-b69n9 [147.176107ms]
Feb 20 19:15:12.209: INFO: Created: latency-svc-hlx4z
Feb 20 19:15:12.221: INFO: Created: latency-svc-dcgkh
Feb 20 19:15:12.225: INFO: Created: latency-svc-gv7mz
Feb 20 19:15:12.231: INFO: Created: latency-svc-ddkfl
Feb 20 19:15:12.237: INFO: Created: latency-svc-7h864
Feb 20 19:15:12.242: INFO: Created: latency-svc-dkmsj
Feb 20 19:15:12.247: INFO: Created: latency-svc-gnr9p
Feb 20 19:15:12.252: INFO: Created: latency-svc-bwzc9
Feb 20 19:15:12.256: INFO: Created: latency-svc-hhl4d
Feb 20 19:15:12.256: INFO: Got endpoints: latency-svc-dcgkh [193.965107ms]
Feb 20 19:15:12.257: INFO: Got endpoints: latency-svc-hlx4z [194.29852ms]
Feb 20 19:15:12.260: INFO: Created: latency-svc-npsf5
Feb 20 19:15:12.265: INFO: Created: latency-svc-5g5zh
Feb 20 19:15:12.269: INFO: Created: latency-svc-m2smz
Feb 20 19:15:12.275: INFO: Created: latency-svc-7cpth
Feb 20 19:15:12.279: INFO: Created: latency-svc-5bbjv
Feb 20 19:15:12.301: INFO: Created: latency-svc-l2m9x
Feb 20 19:15:12.304: INFO: Got endpoints: latency-svc-gv7mz [241.373387ms]
Feb 20 19:15:12.304: INFO: Created: latency-svc-lf4kc
Feb 20 19:15:12.310: INFO: Created: latency-svc-dqp9l
Feb 20 19:15:12.332: INFO: Created: latency-svc-nw5jj
Feb 20 19:15:12.354: INFO: Got endpoints: latency-svc-ddkfl [290.576533ms]
Feb 20 19:15:12.381: INFO: Created: latency-svc-rk6jq
Feb 20 19:15:12.403: INFO: Got endpoints: latency-svc-7h864 [287.504504ms]
Feb 20 19:15:12.433: INFO: Created: latency-svc-v48fk
Feb 20 19:15:12.453: INFO: Got endpoints: latency-svc-dkmsj [334.45191ms]
Feb 20 19:15:12.483: INFO: Created: latency-svc-j2xst
Feb 20 19:15:12.503: INFO: Got endpoints: latency-svc-gnr9p [346.261027ms]
Feb 20 19:15:12.537: INFO: Created: latency-svc-djq5h
Feb 20 19:15:12.553: INFO: Got endpoints: latency-svc-bwzc9 [396.050795ms]
Feb 20 19:15:12.583: INFO: Created: latency-svc-w5mkg
Feb 20 19:15:12.603: INFO: Got endpoints: latency-svc-hhl4d [445.394032ms]
Feb 20 19:15:12.633: INFO: Created: latency-svc-bg24z
Feb 20 19:15:12.653: INFO: Got endpoints: latency-svc-npsf5 [495.226259ms]
Feb 20 19:15:12.683: INFO: Created: latency-svc-d8j9c
Feb 20 19:15:12.703: INFO: Got endpoints: latency-svc-5g5zh [545.024462ms]
Feb 20 19:15:12.734: INFO: Created: latency-svc-qm6n4
Feb 20 19:15:12.754: INFO: Got endpoints: latency-svc-m2smz [592.551289ms]
Feb 20 19:15:12.783: INFO: Created: latency-svc-cnxqf
Feb 20 19:15:12.804: INFO: Got endpoints: latency-svc-7cpth [643.39323ms]
Feb 20 19:15:12.833: INFO: Created: latency-svc-dc9vs
Feb 20 19:15:12.853: INFO: Got endpoints: latency-svc-5bbjv [682.986725ms]
Feb 20 19:15:12.882: INFO: Created: latency-svc-ljxxz
Feb 20 19:15:12.905: INFO: Got endpoints: latency-svc-l2m9x [700.688289ms]
Feb 20 19:15:12.942: INFO: Created: latency-svc-z2bgv
Feb 20 19:15:12.957: INFO: Got endpoints: latency-svc-lf4kc [700.569446ms]
Feb 20 19:15:12.987: INFO: Created: latency-svc-vkdnp
Feb 20 19:15:13.004: INFO: Got endpoints: latency-svc-dqp9l [746.649713ms]
Feb 20 19:15:13.034: INFO: Created: latency-svc-46hs9
Feb 20 19:15:13.053: INFO: Got endpoints: latency-svc-nw5jj [749.382ms]
Feb 20 19:15:13.082: INFO: Created: latency-svc-22j5v
Feb 20 19:15:13.103: INFO: Got endpoints: latency-svc-rk6jq [749.729235ms]
Feb 20 19:15:13.133: INFO: Created: latency-svc-f9p48
Feb 20 19:15:13.153: INFO: Got endpoints: latency-svc-v48fk [750.039154ms]
Feb 20 19:15:13.182: INFO: Created: latency-svc-tmwr5
Feb 20 19:15:13.203: INFO: Got endpoints: latency-svc-j2xst [750.284782ms]
Feb 20 19:15:13.233: INFO: Created: latency-svc-v9nnk
Feb 20 19:15:13.253: INFO: Got endpoints: latency-svc-djq5h [750.062562ms]
Feb 20 19:15:13.282: INFO: Created: latency-svc-cnx9z
Feb 20 19:15:13.303: INFO: Got endpoints: latency-svc-w5mkg [749.761236ms]
Feb 20 19:15:13.332: INFO: Created: latency-svc-45w29
Feb 20 19:15:13.353: INFO: Got endpoints: latency-svc-bg24z [750.172862ms]
Feb 20 19:15:13.382: INFO: Created: latency-svc-lskpm
Feb 20 19:15:13.403: INFO: Got endpoints: latency-svc-d8j9c [749.851626ms]
Feb 20 19:15:13.432: INFO: Created: latency-svc-zj956
Feb 20 19:15:13.453: INFO: Got endpoints: latency-svc-qm6n4 [750.310862ms]
Feb 20 19:15:13.482: INFO: Created: latency-svc-p5x57
Feb 20 19:15:13.503: INFO: Got endpoints: latency-svc-cnxqf [749.763047ms]
Feb 20 19:15:13.533: INFO: Created: latency-svc-qjwd5
Feb 20 19:15:13.553: INFO: Got endpoints: latency-svc-dc9vs [748.882638ms]
Feb 20 19:15:13.582: INFO: Created: latency-svc-shdzp
Feb 20 19:15:13.603: INFO: Got endpoints: latency-svc-ljxxz [749.948678ms]
Feb 20 19:15:13.633: INFO: Created: latency-svc-2h6xf
Feb 20 19:15:13.653: INFO: Got endpoints: latency-svc-z2bgv [747.736412ms]
Feb 20 19:15:13.686: INFO: Created: latency-svc-tcbkp
Feb 20 19:15:13.703: INFO: Got endpoints: latency-svc-vkdnp [746.457109ms]
Feb 20 19:15:13.734: INFO: Created: latency-svc-xjgpr
Feb 20 19:15:13.753: INFO: Got endpoints: latency-svc-46hs9 [749.541427ms]
Feb 20 19:15:13.782: INFO: Created: latency-svc-9jpxg
Feb 20 19:15:13.806: INFO: Got endpoints: latency-svc-22j5v [752.653899ms]
Feb 20 19:15:13.836: INFO: Created: latency-svc-pqbdn
Feb 20 19:15:13.854: INFO: Got endpoints: latency-svc-f9p48 [750.354845ms]
Feb 20 19:15:13.883: INFO: Created: latency-svc-vvg8j
Feb 20 19:15:13.903: INFO: Got endpoints: latency-svc-tmwr5 [749.900165ms]
Feb 20 19:15:13.933: INFO: Created: latency-svc-mxw96
Feb 20 19:15:13.953: INFO: Got endpoints: latency-svc-v9nnk [749.572329ms]
Feb 20 19:15:13.983: INFO: Created: latency-svc-w2tkj
Feb 20 19:15:14.003: INFO: Got endpoints: latency-svc-cnx9z [749.831174ms]
Feb 20 19:15:14.032: INFO: Created: latency-svc-2ndmk
Feb 20 19:15:14.054: INFO: Got endpoints: latency-svc-45w29 [750.632891ms]
Feb 20 19:15:14.083: INFO: Created: latency-svc-65vg8
Feb 20 19:15:14.103: INFO: Got endpoints: latency-svc-lskpm [750.268423ms]
Feb 20 19:15:14.134: INFO: Created: latency-svc-45shf
Feb 20 19:15:14.153: INFO: Got endpoints: latency-svc-zj956 [750.220266ms]
Feb 20 19:15:14.183: INFO: Created: latency-svc-b726z
Feb 20 19:15:14.206: INFO: Got endpoints: latency-svc-p5x57 [752.871242ms]
Feb 20 19:15:14.236: INFO: Created: latency-svc-zhks9
Feb 20 19:15:14.253: INFO: Got endpoints: latency-svc-qjwd5 [749.499915ms]
Feb 20 19:15:14.282: INFO: Created: latency-svc-wcms9
Feb 20 19:15:14.303: INFO: Got endpoints: latency-svc-shdzp [749.711377ms]
Feb 20 19:15:14.332: INFO: Created: latency-svc-pj2tr
Feb 20 19:15:14.353: INFO: Got endpoints: latency-svc-2h6xf [750.203912ms]
Feb 20 19:15:14.383: INFO: Created: latency-svc-sn5nv
Feb 20 19:15:14.404: INFO: Got endpoints: latency-svc-tcbkp [750.281567ms]
Feb 20 19:15:14.438: INFO: Created: latency-svc-4b86d
Feb 20 19:15:14.453: INFO: Got endpoints: latency-svc-xjgpr [749.491806ms]
Feb 20 19:15:14.485: INFO: Created: latency-svc-d5zzb
Feb 20 19:15:14.503: INFO: Got endpoints: latency-svc-9jpxg [750.232532ms]
Feb 20 19:15:14.535: INFO: Created: latency-svc-snlps
Feb 20 19:15:14.553: INFO: Got endpoints: latency-svc-pqbdn [746.757778ms]
Feb 20 19:15:14.583: INFO: Created: latency-svc-jl42b
Feb 20 19:15:14.604: INFO: Got endpoints: latency-svc-vvg8j [749.72891ms]
Feb 20 19:15:14.633: INFO: Created: latency-svc-b9k78
Feb 20 19:15:14.653: INFO: Got endpoints: latency-svc-mxw96 [750.022923ms]
Feb 20 19:15:14.684: INFO: Created: latency-svc-76fw4
Feb 20 19:15:14.703: INFO: Got endpoints: latency-svc-w2tkj [750.049576ms]
Feb 20 19:15:14.736: INFO: Created: latency-svc-5ckdm
Feb 20 19:15:14.756: INFO: Got endpoints: latency-svc-2ndmk [753.003977ms]
Feb 20 19:15:14.788: INFO: Created: latency-svc-qqqln
Feb 20 19:15:14.803: INFO: Got endpoints: latency-svc-65vg8 [749.32257ms]
Feb 20 19:15:14.836: INFO: Created: latency-svc-tgj2s
Feb 20 19:15:14.854: INFO: Got endpoints: latency-svc-45shf [749.984476ms]
Feb 20 19:15:14.884: INFO: Created: latency-svc-9rw2v
Feb 20 19:15:14.903: INFO: Got endpoints: latency-svc-b726z [749.617716ms]
Feb 20 19:15:14.938: INFO: Created: latency-svc-6jt8s
Feb 20 19:15:14.954: INFO: Got endpoints: latency-svc-zhks9 [747.057986ms]
Feb 20 19:15:14.984: INFO: Created: latency-svc-szlcg
Feb 20 19:15:15.003: INFO: Got endpoints: latency-svc-wcms9 [750.316327ms]
Feb 20 19:15:15.033: INFO: Created: latency-svc-k6z6h
Feb 20 19:15:15.053: INFO: Got endpoints: latency-svc-pj2tr [749.886808ms]
Feb 20 19:15:15.083: INFO: Created: latency-svc-5sxvf
Feb 20 19:15:15.103: INFO: Got endpoints: latency-svc-sn5nv [749.610056ms]
Feb 20 19:15:15.133: INFO: Created: latency-svc-f7xcn
Feb 20 19:15:15.153: INFO: Got endpoints: latency-svc-4b86d [749.785488ms]
Feb 20 19:15:15.183: INFO: Created: latency-svc-hk77h
Feb 20 19:15:15.204: INFO: Got endpoints: latency-svc-d5zzb [750.421162ms]
Feb 20 19:15:15.234: INFO: Created: latency-svc-kbpst
Feb 20 19:15:15.253: INFO: Got endpoints: latency-svc-snlps [749.804818ms]
Feb 20 19:15:15.283: INFO: Created: latency-svc-8lj9k
Feb 20 19:15:15.303: INFO: Got endpoints: latency-svc-jl42b [749.981025ms]
Feb 20 19:15:15.333: INFO: Created: latency-svc-l7vgc
Feb 20 19:15:15.354: INFO: Got endpoints: latency-svc-b9k78 [750.261074ms]
Feb 20 19:15:15.384: INFO: Created: latency-svc-kwlw5
Feb 20 19:15:15.403: INFO: Got endpoints: latency-svc-76fw4 [749.661874ms]
Feb 20 19:15:15.433: INFO: Created: latency-svc-vx8l5
Feb 20 19:15:15.455: INFO: Got endpoints: latency-svc-5ckdm [752.258219ms]
Feb 20 19:15:15.486: INFO: Created: latency-svc-bsrv5
Feb 20 19:15:15.503: INFO: Got endpoints: latency-svc-qqqln [746.808661ms]
Feb 20 19:15:15.534: INFO: Created: latency-svc-c7t8b
Feb 20 19:15:15.554: INFO: Got endpoints: latency-svc-9rw2v [699.899935ms]
Feb 20 19:15:15.583: INFO: Created: latency-svc-xrs5m
Feb 20 19:15:15.604: INFO: Got endpoints: latency-svc-tgj2s [800.469822ms]
Feb 20 19:15:15.635: INFO: Created: latency-svc-tr9tw
Feb 20 19:15:15.653: INFO: Got endpoints: latency-svc-6jt8s [750.008431ms]
Feb 20 19:15:15.683: INFO: Created: latency-svc-zz7xr
Feb 20 19:15:15.703: INFO: Got endpoints: latency-svc-szlcg [749.63602ms]
Feb 20 19:15:15.733: INFO: Created: latency-svc-5prp4
Feb 20 19:15:15.757: INFO: Got endpoints: latency-svc-k6z6h [753.665454ms]
Feb 20 19:15:15.787: INFO: Created: latency-svc-4wxf6
Feb 20 19:15:15.804: INFO: Got endpoints: latency-svc-5sxvf [750.696051ms]
Feb 20 19:15:15.834: INFO: Created: latency-svc-8g8nj
Feb 20 19:15:15.854: INFO: Got endpoints: latency-svc-f7xcn [750.549027ms]
Feb 20 19:15:15.883: INFO: Created: latency-svc-gdfx4
Feb 20 19:15:15.903: INFO: Got endpoints: latency-svc-hk77h [749.813034ms]
Feb 20 19:15:15.933: INFO: Created: latency-svc-pfgkp
Feb 20 19:15:15.955: INFO: Got endpoints: latency-svc-kbpst [750.915949ms]
Feb 20 19:15:15.985: INFO: Created: latency-svc-g2qw5
Feb 20 19:15:16.005: INFO: Got endpoints: latency-svc-8lj9k [751.940234ms]
Feb 20 19:15:16.037: INFO: Created: latency-svc-hn2g9
Feb 20 19:15:16.055: INFO: Got endpoints: latency-svc-l7vgc [751.733279ms]
Feb 20 19:15:16.088: INFO: Created: latency-svc-j9qkb
Feb 20 19:15:16.105: INFO: Got endpoints: latency-svc-kwlw5 [750.496803ms]
Feb 20 19:15:16.135: INFO: Created: latency-svc-7pl2j
Feb 20 19:15:16.153: INFO: Got endpoints: latency-svc-vx8l5 [750.117734ms]
Feb 20 19:15:16.185: INFO: Created: latency-svc-r7zm9
Feb 20 19:15:16.203: INFO: Got endpoints: latency-svc-bsrv5 [747.601783ms]
Feb 20 19:15:16.233: INFO: Created: latency-svc-jcl5q
Feb 20 19:15:16.254: INFO: Got endpoints: latency-svc-c7t8b [750.443436ms]
Feb 20 19:15:16.283: INFO: Created: latency-svc-k27vc
Feb 20 19:15:16.303: INFO: Got endpoints: latency-svc-xrs5m [749.81882ms]
Feb 20 19:15:16.333: INFO: Created: latency-svc-chn9d
Feb 20 19:15:16.353: INFO: Got endpoints: latency-svc-tr9tw [749.475766ms]
Feb 20 19:15:16.384: INFO: Created: latency-svc-5d6jb
Feb 20 19:15:16.403: INFO: Got endpoints: latency-svc-zz7xr [749.655432ms]
Feb 20 19:15:16.433: INFO: Created: latency-svc-fm899
Feb 20 19:15:16.455: INFO: Got endpoints: latency-svc-5prp4 [752.077465ms]
Feb 20 19:15:16.485: INFO: Created: latency-svc-flgdd
Feb 20 19:15:16.503: INFO: Got endpoints: latency-svc-4wxf6 [746.237423ms]
Feb 20 19:15:16.534: INFO: Created: latency-svc-nfllv
Feb 20 19:15:16.555: INFO: Got endpoints: latency-svc-8g8nj [751.006505ms]
Feb 20 19:15:16.585: INFO: Created: latency-svc-5q5gn
Feb 20 19:15:16.604: INFO: Got endpoints: latency-svc-gdfx4 [749.936779ms]
Feb 20 19:15:16.635: INFO: Created: latency-svc-xqcfg
Feb 20 19:15:16.654: INFO: Got endpoints: latency-svc-pfgkp [750.103942ms]
Feb 20 19:15:16.683: INFO: Created: latency-svc-mkfd5
Feb 20 19:15:16.705: INFO: Got endpoints: latency-svc-g2qw5 [750.0892ms]
Feb 20 19:15:16.735: INFO: Created: latency-svc-2scp4
Feb 20 19:15:16.753: INFO: Got endpoints: latency-svc-hn2g9 [747.904477ms]
Feb 20 19:15:16.784: INFO: Created: latency-svc-kbvnz
Feb 20 19:15:16.803: INFO: Got endpoints: latency-svc-j9qkb [748.461429ms]
Feb 20 19:15:16.833: INFO: Created: latency-svc-z7p5s
Feb 20 19:15:16.854: INFO: Got endpoints: latency-svc-7pl2j [748.928281ms]
Feb 20 19:15:16.884: INFO: Created: latency-svc-hc7vx
Feb 20 19:15:16.928: INFO: Got endpoints: latency-svc-r7zm9 [774.763445ms]
Feb 20 19:15:17.023: INFO: Got endpoints: latency-svc-k27vc [769.289688ms]
Feb 20 19:15:17.024: INFO: Got endpoints: latency-svc-jcl5q [821.182616ms]
Feb 20 19:15:17.030: INFO: Created: latency-svc-v295c
Feb 20 19:15:17.124: INFO: Got endpoints: latency-svc-chn9d [821.001805ms]
Feb 20 19:15:17.126: INFO: Got endpoints: latency-svc-5d6jb [772.288578ms]
Feb 20 19:15:17.131: INFO: Created: latency-svc-74sl5
Feb 20 19:15:17.152: INFO: Created: latency-svc-gtm8n
Feb 20 19:15:17.221: INFO: Got endpoints: latency-svc-fm899 [818.151305ms]
Feb 20 19:15:17.225: INFO: Got endpoints: latency-svc-flgdd [769.590925ms]
Feb 20 19:15:17.225: INFO: Created: latency-svc-p2sgh
Feb 20 19:15:17.230: INFO: Created: latency-svc-qgdcn
Feb 20 19:15:17.251: INFO: Created: latency-svc-rhj82
Feb 20 19:15:17.253: INFO: Got endpoints: latency-svc-nfllv [749.206472ms]
Feb 20 19:15:17.257: INFO: Created: latency-svc-5p8kd
Feb 20 19:15:17.282: INFO: Created: latency-svc-rvgzv
Feb 20 19:15:17.304: INFO: Got endpoints: latency-svc-5q5gn [748.61226ms]
Feb 20 19:15:17.333: INFO: Created: latency-svc-hv2df
Feb 20 19:15:17.353: INFO: Got endpoints: latency-svc-xqcfg [749.602629ms]
Feb 20 19:15:17.382: INFO: Created: latency-svc-js6fh
Feb 20 19:15:17.403: INFO: Got endpoints: latency-svc-mkfd5 [749.336412ms]
Feb 20 19:15:17.434: INFO: Created: latency-svc-xxfzx
Feb 20 19:15:17.453: INFO: Got endpoints: latency-svc-2scp4 [748.096637ms]
Feb 20 19:15:17.487: INFO: Created: latency-svc-8pwmj
Feb 20 19:15:17.503: INFO: Got endpoints: latency-svc-kbvnz [749.920136ms]
Feb 20 19:15:17.535: INFO: Created: latency-svc-xjwbl
Feb 20 19:15:17.553: INFO: Got endpoints: latency-svc-z7p5s [749.509228ms]
Feb 20 19:15:17.583: INFO: Created: latency-svc-6p4vk
Feb 20 19:15:17.604: INFO: Got endpoints: latency-svc-hc7vx [750.057562ms]
Feb 20 19:15:17.638: INFO: Created: latency-svc-pcb7q
Feb 20 19:15:17.653: INFO: Got endpoints: latency-svc-v295c [725.088553ms]
Feb 20 19:15:17.683: INFO: Created: latency-svc-9w757
Feb 20 19:15:17.703: INFO: Got endpoints: latency-svc-74sl5 [680.204286ms]
Feb 20 19:15:17.733: INFO: Created: latency-svc-9c8j8
Feb 20 19:15:17.753: INFO: Got endpoints: latency-svc-gtm8n [728.767534ms]
Feb 20 19:15:17.783: INFO: Created: latency-svc-kx452
Feb 20 19:15:17.804: INFO: Got endpoints: latency-svc-p2sgh [679.024097ms]
Feb 20 19:15:17.835: INFO: Created: latency-svc-mhbgf
Feb 20 19:15:17.853: INFO: Got endpoints: latency-svc-qgdcn [727.306103ms]
Feb 20 19:15:17.882: INFO: Created: latency-svc-fv975
Feb 20 19:15:17.903: INFO: Got endpoints: latency-svc-rhj82 [681.943503ms]
Feb 20 19:15:17.933: INFO: Created: latency-svc-k4bvd
Feb 20 19:15:17.953: INFO: Got endpoints: latency-svc-5p8kd [728.074758ms]
Feb 20 19:15:17.982: INFO: Created: latency-svc-jv85w
Feb 20 19:15:18.004: INFO: Got endpoints: latency-svc-rvgzv [750.731371ms]
Feb 20 19:15:18.035: INFO: Created: latency-svc-4z2zb
Feb 20 19:15:18.054: INFO: Got endpoints: latency-svc-hv2df [750.59408ms]
Feb 20 19:15:18.084: INFO: Created: latency-svc-b578c
Feb 20 19:15:18.103: INFO: Got endpoints: latency-svc-js6fh [750.035241ms]
Feb 20 19:15:18.133: INFO: Created: latency-svc-qgrvd
Feb 20 19:15:18.159: INFO: Got endpoints: latency-svc-xxfzx [755.755471ms]
Feb 20 19:15:18.187: INFO: Created: latency-svc-fqbsq
Feb 20 19:15:18.203: INFO: Got endpoints: latency-svc-8pwmj [750.045011ms]
Feb 20 19:15:18.233: INFO: Created: latency-svc-xbdlx
Feb 20 19:15:18.254: INFO: Got endpoints: latency-svc-xjwbl [750.193917ms]
Feb 20 19:15:18.284: INFO: Created: latency-svc-gpl49
Feb 20 19:15:18.305: INFO: Got endpoints: latency-svc-6p4vk [751.849844ms]
Feb 20 19:15:18.334: INFO: Created: latency-svc-78rjl
Feb 20 19:15:18.354: INFO: Got endpoints: latency-svc-pcb7q [749.764827ms]
Feb 20 19:15:18.382: INFO: Created: latency-svc-pths5
Feb 20 19:15:18.403: INFO: Got endpoints: latency-svc-9w757 [749.270134ms]
Feb 20 19:15:18.433: INFO: Created: latency-svc-fl86b
Feb 20 19:15:18.453: INFO: Got endpoints: latency-svc-9c8j8 [750.09821ms]
Feb 20 19:15:18.483: INFO: Created: latency-svc-t2hp4
Feb 20 19:15:18.503: INFO: Got endpoints: latency-svc-kx452 [750.098187ms]
Feb 20 19:15:18.532: INFO: Created: latency-svc-q8xq9
Feb 20 19:15:18.553: INFO: Got endpoints: latency-svc-mhbgf [749.600045ms]
Feb 20 19:15:18.589: INFO: Created: latency-svc-s92v9
Feb 20 19:15:18.603: INFO: Got endpoints: latency-svc-fv975 [749.939813ms]
Feb 20 19:15:18.632: INFO: Created: latency-svc-g9k7d
Feb 20 19:15:18.654: INFO: Got endpoints: latency-svc-k4bvd [750.210565ms]
Feb 20 19:15:18.684: INFO: Created: latency-svc-mqzh2
Feb 20 19:15:18.703: INFO: Got endpoints: latency-svc-jv85w [749.617181ms]
Feb 20 19:15:18.732: INFO: Created: latency-svc-xqjdc
Feb 20 19:15:18.753: INFO: Got endpoints: latency-svc-4z2zb [749.361756ms]
Feb 20 19:15:18.784: INFO: Created: latency-svc-ms87t
Feb 20 19:15:18.804: INFO: Got endpoints: latency-svc-b578c [749.278526ms]
Feb 20 19:15:18.833: INFO: Created: latency-svc-v85xp
Feb 20 19:15:18.853: INFO: Got endpoints: latency-svc-qgrvd [750.100576ms]
Feb 20 19:15:18.882: INFO: Created: latency-svc-7fvlc
Feb 20 19:15:18.903: INFO: Got endpoints: latency-svc-fqbsq [744.705781ms]
Feb 20 19:15:18.933: INFO: Created: latency-svc-jdqrt
Feb 20 19:15:18.953: INFO: Got endpoints: latency-svc-xbdlx [749.832607ms]
Feb 20 19:15:18.982: INFO: Created: latency-svc-b54nn
Feb 20 19:15:19.003: INFO: Got endpoints: latency-svc-gpl49 [749.873732ms]
Feb 20 19:15:19.033: INFO: Created: latency-svc-5tmhz
Feb 20 19:15:19.053: INFO: Got endpoints: latency-svc-78rjl [748.422616ms]
Feb 20 19:15:19.083: INFO: Created: latency-svc-h8729
Feb 20 19:15:19.103: INFO: Got endpoints: latency-svc-pths5 [749.714492ms]
Feb 20 19:15:19.133: INFO: Created: latency-svc-jmvmj
Feb 20 19:15:19.153: INFO: Got endpoints: latency-svc-fl86b [750.744084ms]
Feb 20 19:15:19.183: INFO: Created: latency-svc-r2vx5
Feb 20 19:15:19.242: INFO: Got endpoints: latency-svc-t2hp4 [788.548032ms]
Feb 20 19:15:19.254: INFO: Got endpoints: latency-svc-q8xq9 [750.00681ms]
Feb 20 19:15:19.274: INFO: Created: latency-svc-pfgq2
Feb 20 19:15:19.282: INFO: Created: latency-svc-7b42t
Feb 20 19:15:19.303: INFO: Got endpoints: latency-svc-s92v9 [750.010186ms]
Feb 20 19:15:19.333: INFO: Created: latency-svc-b64l7
Feb 20 19:15:19.353: INFO: Got endpoints: latency-svc-g9k7d [750.064252ms]
Feb 20 19:15:19.383: INFO: Created: latency-svc-9fzrg
Feb 20 19:15:19.403: INFO: Got endpoints: latency-svc-mqzh2 [749.030798ms]
Feb 20 19:15:19.432: INFO: Created: latency-svc-t5j5x
Feb 20 19:15:19.454: INFO: Got endpoints: latency-svc-xqjdc [750.510003ms]
Feb 20 19:15:19.483: INFO: Created: latency-svc-5sgpt
Feb 20 19:15:19.504: INFO: Got endpoints: latency-svc-ms87t [750.544583ms]
Feb 20 19:15:19.533: INFO: Created: latency-svc-zkcrn
Feb 20 19:15:19.553: INFO: Got endpoints: latency-svc-v85xp [749.713244ms]
Feb 20 19:15:19.583: INFO: Created: latency-svc-rkffd
Feb 20 19:15:19.603: INFO: Got endpoints: latency-svc-7fvlc [749.636292ms]
Feb 20 19:15:19.634: INFO: Created: latency-svc-8jb5r
Feb 20 19:15:19.653: INFO: Got endpoints: latency-svc-jdqrt [749.246295ms]
Feb 20 19:15:19.682: INFO: Created: latency-svc-6jkvq
Feb 20 19:15:19.703: INFO: Got endpoints: latency-svc-b54nn [750.151208ms]
Feb 20 19:15:19.753: INFO: Got endpoints: latency-svc-5tmhz [749.655604ms]
Feb 20 19:15:19.804: INFO: Got endpoints: latency-svc-h8729 [750.224402ms]
Feb 20 19:15:19.854: INFO: Got endpoints: latency-svc-jmvmj [750.802564ms]
Feb 20 19:15:19.904: INFO: Got endpoints: latency-svc-r2vx5 [750.089892ms]
Feb 20 19:15:19.953: INFO: Got endpoints: latency-svc-pfgq2 [711.139584ms]
Feb 20 19:15:20.003: INFO: Got endpoints: latency-svc-7b42t [749.745254ms]
Feb 20 19:15:20.054: INFO: Got endpoints: latency-svc-b64l7 [750.30174ms]
Feb 20 19:15:20.104: INFO: Got endpoints: latency-svc-9fzrg [750.206592ms]
Feb 20 19:15:20.154: INFO: Got endpoints: latency-svc-t5j5x [750.844528ms]
Feb 20 19:15:20.203: INFO: Got endpoints: latency-svc-5sgpt [749.604296ms]
Feb 20 19:15:20.254: INFO: Got endpoints: latency-svc-zkcrn [749.996434ms]
Feb 20 19:15:20.304: INFO: Got endpoints: latency-svc-rkffd [750.11905ms]
Feb 20 19:15:20.353: INFO: Got endpoints: latency-svc-8jb5r [750.098282ms]
Feb 20 19:15:20.403: INFO: Got endpoints: latency-svc-6jkvq [750.463582ms]
Feb 20 19:15:20.403: INFO: Latencies: [33.27356ms 37.465223ms 42.968135ms 47.559943ms 54.89839ms 59.163799ms 59.554207ms 59.896945ms 88.043635ms 100.502351ms 103.25154ms 103.911092ms 112.40252ms 112.797249ms 115.172348ms 130.207431ms 131.940757ms 132.024387ms 132.308672ms 132.527809ms 132.605321ms 132.806019ms 135.743705ms 139.084395ms 140.032092ms 141.635563ms 143.867129ms 146.441524ms 147.176107ms 149.113326ms 150.818975ms 153.235465ms 166.533333ms 170.898085ms 173.141423ms 193.965107ms 194.29852ms 241.373387ms 287.504504ms 290.576533ms 334.45191ms 346.261027ms 396.050795ms 445.394032ms 495.226259ms 545.024462ms 592.551289ms 643.39323ms 679.024097ms 680.204286ms 681.943503ms 682.986725ms 699.899935ms 700.569446ms 700.688289ms 711.139584ms 725.088553ms 727.306103ms 728.074758ms 728.767534ms 744.705781ms 746.237423ms 746.457109ms 746.649713ms 746.757778ms 746.808661ms 747.057986ms 747.601783ms 747.736412ms 747.904477ms 748.096637ms 748.422616ms 748.461429ms 748.61226ms 748.882638ms 748.928281ms 749.030798ms 749.206472ms 749.246295ms 749.270134ms 749.278526ms 749.32257ms 749.336412ms 749.361756ms 749.382ms 749.475766ms 749.491806ms 749.499915ms 749.509228ms 749.541427ms 749.572329ms 749.600045ms 749.602629ms 749.604296ms 749.610056ms 749.617181ms 749.617716ms 749.63602ms 749.636292ms 749.655432ms 749.655604ms 749.661874ms 749.711377ms 749.713244ms 749.714492ms 749.72891ms 749.729235ms 749.745254ms 749.761236ms 749.763047ms 749.764827ms 749.785488ms 749.804818ms 749.813034ms 749.81882ms 749.831174ms 749.832607ms 749.851626ms 749.873732ms 749.886808ms 749.900165ms 749.920136ms 749.936779ms 749.939813ms 749.948678ms 749.981025ms 749.984476ms 749.996434ms 750.00681ms 750.008431ms 750.010186ms 750.022923ms 750.035241ms 750.039154ms 750.045011ms 750.049576ms 750.057562ms 750.062562ms 750.064252ms 750.0892ms 750.089892ms 750.098187ms 750.09821ms 750.098282ms 750.100576ms 750.103942ms 750.117734ms 750.11905ms 750.151208ms 750.172862ms 750.193917ms 750.203912ms 750.206592ms 750.210565ms 750.220266ms 750.224402ms 750.232532ms 750.261074ms 750.268423ms 750.281567ms 750.284782ms 750.30174ms 750.310862ms 750.316327ms 750.354845ms 750.421162ms 750.443436ms 750.463582ms 750.496803ms 750.510003ms 750.544583ms 750.549027ms 750.59408ms 750.632891ms 750.696051ms 750.731371ms 750.744084ms 750.802564ms 750.844528ms 750.915949ms 751.006505ms 751.733279ms 751.849844ms 751.940234ms 752.077465ms 752.258219ms 752.653899ms 752.871242ms 753.003977ms 753.665454ms 755.755471ms 769.289688ms 769.590925ms 772.288578ms 774.763445ms 788.548032ms 800.469822ms 818.151305ms 821.001805ms 821.182616ms]
Feb 20 19:15:20.404: INFO: 50 %ile: 749.655604ms
Feb 20 19:15:20.404: INFO: 90 %ile: 751.006505ms
Feb 20 19:15:20.404: INFO: 99 %ile: 821.001805ms
Feb 20 19:15:20.404: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:15:20.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-h6sqg" for this suite.
Feb 20 19:15:46.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:15:46.570: INFO: namespace: e2e-tests-svc-latency-h6sqg, resource: bindings, ignored listing per whitelist
Feb 20 19:15:47.436: INFO: namespace e2e-tests-svc-latency-h6sqg deletion completed in 27.007944979s

 [SLOW TEST:39.896 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:15:47.436: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-pz5v8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-ee1b00ff-3543-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:15:48.585: INFO: Waiting up to 5m0s for pod "pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-pz5v8" to be "success or failure"
Feb 20 19:15:48.608: INFO: Pod "pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.182717ms
Feb 20 19:15:50.632: INFO: Pod "pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046855718s
STEP: Saw pod success
Feb 20 19:15:50.632: INFO: Pod "pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:15:50.655: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:15:50.714: INFO: Waiting for pod pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:15:50.736: INFO: Pod pod-secrets-ee1ec746-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:15:50.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-pz5v8" for this suite.
Feb 20 19:15:56.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:15:57.437: INFO: namespace: e2e-tests-secrets-pz5v8, resource: bindings, ignored listing per whitelist
Feb 20 19:15:57.847: INFO: namespace e2e-tests-secrets-pz5v8 deletion completed in 7.086068549s

 [SLOW TEST:10.411 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:15:57.847: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-kgw8x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-f45d7131-3543-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:15:59.085: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-kgw8x" to be "success or failure"
Feb 20 19:15:59.108: INFO: Pod "pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.96302ms
Feb 20 19:16:01.133: INFO: Pod "pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047100397s
STEP: Saw pod success
Feb 20 19:16:01.133: INFO: Pod "pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:16:01.157: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:16:01.221: INFO: Waiting for pod pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:16:01.245: INFO: Pod pod-projected-secrets-f4610972-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:16:01.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-kgw8x" for this suite.
Feb 20 19:16:07.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:08.211: INFO: namespace: e2e-tests-projected-kgw8x, resource: bindings, ignored listing per whitelist
Feb 20 19:16:08.329: INFO: namespace e2e-tests-projected-kgw8x deletion completed in 7.059679131s

 [SLOW TEST:10.482 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:16:08.329: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-xfzkg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-fa90eba4-3543-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:16:09.493: INFO: Waiting up to 5m0s for pod "pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-xfzkg" to be "success or failure"
Feb 20 19:16:09.516: INFO: Pod "pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.875284ms
Feb 20 19:16:11.539: INFO: Pod "pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046441024s
Feb 20 19:16:13.565: INFO: Pod "pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072662268s
STEP: Saw pod success
Feb 20 19:16:13.565: INFO: Pod "pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:16:13.589: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:16:13.650: INFO: Waiting for pod pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:16:13.673: INFO: Pod pod-secrets-fa950f84-3543-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:16:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xfzkg" for this suite.
Feb 20 19:16:19.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:20.098: INFO: namespace: e2e-tests-secrets-xfzkg, resource: bindings, ignored listing per whitelist
Feb 20 19:16:20.746: INFO: namespace e2e-tests-secrets-xfzkg deletion completed in 7.047714776s

 [SLOW TEST:12.417 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:16:20.746: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-x7pwv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-01f937cb-3544-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:16:21.942: INFO: Waiting up to 5m0s for pod "pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-x7pwv" to be "success or failure"
Feb 20 19:16:21.969: INFO: Pod "pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 27.077946ms
Feb 20 19:16:23.992: INFO: Pod "pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050048371s
STEP: Saw pod success
Feb 20 19:16:23.992: INFO: Pod "pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:16:24.021: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:16:24.081: INFO: Waiting for pod pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:16:24.125: INFO: Pod pod-secrets-01ff2c60-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:16:24.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-x7pwv" for this suite.
Feb 20 19:16:32.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:33.118: INFO: namespace: e2e-tests-secrets-x7pwv, resource: bindings, ignored listing per whitelist
Feb 20 19:16:33.378: INFO: namespace e2e-tests-secrets-x7pwv deletion completed in 9.153664375s

 [SLOW TEST:12.632 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:16:33.378: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-5s5n2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-5s5n2
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-5s5n2
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-5s5n2
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-5s5n2
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-5s5n2
Feb 20 19:16:36.600: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-5s5n2, name: ss-0, uid: 0abc32a2-3544-11e9-a0a3-2a5d9248fe67, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 19:16:36.762: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-5s5n2, name: ss-0, uid: 0abc32a2-3544-11e9-a0a3-2a5d9248fe67, status phase: Failed. Waiting for statefulset controller to delete.
Feb 20 19:16:36.764: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-5s5n2
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-5s5n2
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-5s5n2 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:16:40.865: INFO: Deleting all statefulset in ns e2e-tests-statefulset-5s5n2
Feb 20 19:16:40.889: INFO: Scaling statefulset ss to 0
Feb 20 19:16:50.984: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:16:51.008: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:16:51.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-5s5n2" for this suite.
Feb 20 19:16:57.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:16:57.377: INFO: namespace: e2e-tests-statefulset-5s5n2, resource: bindings, ignored listing per whitelist
Feb 20 19:16:58.106: INFO: namespace e2e-tests-statefulset-5s5n2 deletion completed in 7.002284234s

 [SLOW TEST:24.728 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:16:58.106: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jd5qx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 19:16:59.232: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-jd5qx'
Feb 20 19:17:00.479: INFO: stderr: ""
Feb 20 19:17:00.479: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 19:17:01.505: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:17:01.505: INFO: Found 0 / 1
Feb 20 19:17:02.504: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:17:02.504: INFO: Found 1 / 1
Feb 20 19:17:02.504: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 20 19:17:02.528: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:17:02.528: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 19:17:02.528: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml patch pod redis-master-9cl8q --namespace=e2e-tests-kubectl-jd5qx -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 20 19:17:02.738: INFO: stderr: ""
Feb 20 19:17:02.738: INFO: stdout: "pod/redis-master-9cl8q patched\n"
STEP: checking annotations
Feb 20 19:17:02.762: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:17:02.762: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:17:02.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jd5qx" for this suite.
Feb 20 19:17:26.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:27.502: INFO: namespace: e2e-tests-kubectl-jd5qx, resource: bindings, ignored listing per whitelist
Feb 20 19:17:27.880: INFO: namespace e2e-tests-kubectl-jd5qx deletion completed in 25.094144705s

 [SLOW TEST:29.774 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:17:27.880: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rfwwd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-rfwwd/configmap-test-2a01ae1c-3544-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:17:29.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-rfwwd" to be "success or failure"
Feb 20 19:17:29.104: INFO: Pod "pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.795243ms
Feb 20 19:17:31.128: INFO: Pod "pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046737322s
STEP: Saw pod success
Feb 20 19:17:31.128: INFO: Pod "pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:17:31.152: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39 container env-test: <nil>
STEP: delete the pod
Feb 20 19:17:31.212: INFO: Waiting for pod pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:17:31.236: INFO: Pod pod-configmaps-2a053de5-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:17:31.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rfwwd" for this suite.
Feb 20 19:17:37.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:38.043: INFO: namespace: e2e-tests-configmap-rfwwd, resource: bindings, ignored listing per whitelist
Feb 20 19:17:38.258: INFO: namespace e2e-tests-configmap-rfwwd deletion completed in 6.999286923s

 [SLOW TEST:10.378 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:17:38.259: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-phhq4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 20 19:17:39.801: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-phhq4,SelfLink:/api/v1/namespaces/e2e-tests-watch-phhq4/configmaps/e2e-watch-test-resource-version,UID:305458f3-3544-11e9-a0a3-2a5d9248fe67,ResourceVersion:15953,Generation:0,CreationTimestamp:2019-02-20 19:17:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:17:39.801: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-phhq4,SelfLink:/api/v1/namespaces/e2e-tests-watch-phhq4/configmaps/e2e-watch-test-resource-version,UID:305458f3-3544-11e9-a0a3-2a5d9248fe67,ResourceVersion:15954,Generation:0,CreationTimestamp:2019-02-20 19:17:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:17:39.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-phhq4" for this suite.
Feb 20 19:17:45.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:46.057: INFO: namespace: e2e-tests-watch-phhq4, resource: bindings, ignored listing per whitelist
Feb 20 19:17:46.819: INFO: namespace e2e-tests-watch-phhq4 deletion completed in 6.994376417s

 [SLOW TEST:8.560 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:17:46.819: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-hsqv5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:17:47.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-hsqv5" to be "success or failure"
Feb 20 19:17:47.976: INFO: Pod "downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.944905ms
Feb 20 19:17:50.000: INFO: Pod "downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046956579s
STEP: Saw pod success
Feb 20 19:17:50.000: INFO: Pod "downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:17:50.024: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:17:50.086: INFO: Waiting for pod downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:17:50.109: INFO: Pod downwardapi-volume-3544d77b-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:17:50.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-hsqv5" for this suite.
Feb 20 19:17:56.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:17:56.805: INFO: namespace: e2e-tests-downward-api-hsqv5, resource: bindings, ignored listing per whitelist
Feb 20 19:17:57.082: INFO: namespace e2e-tests-downward-api-hsqv5 deletion completed in 6.949105552s

 [SLOW TEST:10.263 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:17:57.082: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-v5cr7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:17:58.259: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-v5cr7" to be "success or failure"
Feb 20 19:17:58.282: INFO: Pod "downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.614318ms
Feb 20 19:18:00.306: INFO: Pod "downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046974511s
STEP: Saw pod success
Feb 20 19:18:00.306: INFO: Pod "downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:18:00.330: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:18:00.392: INFO: Waiting for pod downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:18:00.416: INFO: Pod downwardapi-volume-3b696ee4-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:18:00.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-v5cr7" for this suite.
Feb 20 19:18:08.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:18:08.608: INFO: namespace: e2e-tests-projected-v5cr7, resource: bindings, ignored listing per whitelist
Feb 20 19:18:09.394: INFO: namespace e2e-tests-projected-v5cr7 deletion completed in 8.953532809s

 [SLOW TEST:12.313 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:18:09.395: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-swlmn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 20 19:18:10.659: INFO: Waiting up to 5m0s for pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-swlmn" to be "success or failure"
Feb 20 19:18:10.682: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.765634ms
Feb 20 19:18:12.706: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04718994s
Feb 20 19:18:14.730: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070699279s
Feb 20 19:18:16.753: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093662936s
Feb 20 19:18:18.776: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117507334s
Feb 20 19:18:20.800: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 10.141536894s
Feb 20 19:18:22.825: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.165900101s
Feb 20 19:18:24.849: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 14.190389083s
Feb 20 19:18:26.882: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 16.222773126s
Feb 20 19:18:28.905: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 18.246518888s
Feb 20 19:18:30.929: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 20.270059408s
Feb 20 19:18:32.954: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.29540288s
Feb 20 19:18:34.979: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 24.320424361s
Feb 20 19:18:37.003: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 26.344108747s
Feb 20 19:18:39.027: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 28.368383698s
Feb 20 19:18:41.061: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 30.402205624s
Feb 20 19:18:43.087: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 32.427703963s
Feb 20 19:18:45.112: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.452673806s
STEP: Saw pod success
Feb 20 19:18:45.112: INFO: Pod "downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:18:45.135: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39 container dapi-container: <nil>
STEP: delete the pod
Feb 20 19:18:45.198: INFO: Waiting for pod downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:18:45.223: INFO: Pod downward-api-42cd8999-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:18:45.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-swlmn" for this suite.
Feb 20 19:18:51.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:18:51.980: INFO: namespace: e2e-tests-downward-api-swlmn, resource: bindings, ignored listing per whitelist
Feb 20 19:18:52.219: INFO: namespace e2e-tests-downward-api-swlmn deletion completed in 6.971675493s

 [SLOW TEST:42.824 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:18:52.219: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-8q8tv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:18:53.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-8q8tv" to be "success or failure"
Feb 20 19:18:53.537: INFO: Pod "downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 43.559317ms
Feb 20 19:18:55.561: INFO: Pod "downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067745018s
Feb 20 19:18:57.588: INFO: Pod "downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094876807s
STEP: Saw pod success
Feb 20 19:18:57.588: INFO: Pod "downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:18:57.612: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:18:57.673: INFO: Waiting for pod downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:18:57.696: INFO: Pod downwardapi-volume-5c54399b-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:18:57.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-8q8tv" for this suite.
Feb 20 19:19:05.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:06.596: INFO: namespace: e2e-tests-projected-8q8tv, resource: bindings, ignored listing per whitelist
Feb 20 19:19:06.760: INFO: namespace e2e-tests-projected-8q8tv deletion completed in 9.039950226s

 [SLOW TEST:14.541 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:19:06.760: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bm64s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-64f638ca-3544-11e9-9b2a-cafe91372a39
STEP: Creating secret with name secret-projected-all-test-volume-64f638b7-3544-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 20 19:19:08.016: INFO: Waiting up to 5m0s for pod "projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-bm64s" to be "success or failure"
Feb 20 19:19:08.039: INFO: Pod "projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.044853ms
Feb 20 19:19:10.064: INFO: Pod "projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047616016s
Feb 20 19:19:12.088: INFO: Pod "projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071837351s
STEP: Saw pod success
Feb 20 19:19:12.088: INFO: Pod "projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:19:12.112: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 20 19:19:12.170: INFO: Waiting for pod projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:19:12.194: INFO: Pod projected-volume-64f63881-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:19:12.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bm64s" for this suite.
Feb 20 19:19:18.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:19.183: INFO: namespace: e2e-tests-projected-bm64s, resource: bindings, ignored listing per whitelist
Feb 20 19:19:19.229: INFO: namespace e2e-tests-projected-bm64s deletion completed in 7.011534767s

 [SLOW TEST:12.469 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:19:19.229: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-pkj82
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 19:19:20.361: INFO: Waiting up to 5m0s for pod "pod-6c5948b1-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-pkj82" to be "success or failure"
Feb 20 19:19:20.384: INFO: Pod "pod-6c5948b1-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.734295ms
Feb 20 19:19:22.408: INFO: Pod "pod-6c5948b1-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046782873s
STEP: Saw pod success
Feb 20 19:19:22.408: INFO: Pod "pod-6c5948b1-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:19:22.434: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-6c5948b1-3544-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:19:22.499: INFO: Waiting for pod pod-6c5948b1-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:19:22.544: INFO: Pod pod-6c5948b1-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:19:22.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-pkj82" for this suite.
Feb 20 19:19:30.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:30.716: INFO: namespace: e2e-tests-emptydir-pkj82, resource: bindings, ignored listing per whitelist
Feb 20 19:19:31.558: INFO: namespace e2e-tests-emptydir-pkj82 deletion completed in 8.988226828s

 [SLOW TEST:12.329 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:19:31.558: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-prr4v
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39
Feb 20 19:19:32.678: INFO: Pod name my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39: Found 1 pods out of 1
Feb 20 19:19:32.678: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39" are running
Feb 20 19:19:34.724: INFO: Pod "my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39-h9ffm" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-20 19:19:32 +0000 UTC Reason: Message:}])
Feb 20 19:19:34.724: INFO: Trying to dial the pod
Feb 20 19:19:39.882: INFO: Controller my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39: Got expected result from replica 1 [my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39-h9ffm]: "my-hostname-basic-73adadcd-3544-11e9-9b2a-cafe91372a39-h9ffm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:19:39.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-prr4v" for this suite.
Feb 20 19:19:45.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:46.701: INFO: namespace: e2e-tests-replication-controller-prr4v, resource: bindings, ignored listing per whitelist
Feb 20 19:19:46.934: INFO: namespace e2e-tests-replication-controller-prr4v deletion completed in 7.027331009s

 [SLOW TEST:15.376 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:19:46.934: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-dlpkg
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 20 19:19:48.159: INFO: Waiting up to 5m0s for pod "pod-7cead8cd-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-dlpkg" to be "success or failure"
Feb 20 19:19:48.182: INFO: Pod "pod-7cead8cd-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.243171ms
Feb 20 19:19:50.207: INFO: Pod "pod-7cead8cd-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047806756s
STEP: Saw pod success
Feb 20 19:19:50.207: INFO: Pod "pod-7cead8cd-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:19:50.230: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-7cead8cd-3544-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:19:50.321: INFO: Waiting for pod pod-7cead8cd-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:19:50.345: INFO: Pod pod-7cead8cd-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:19:50.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-dlpkg" for this suite.
Feb 20 19:19:56.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:19:57.258: INFO: namespace: e2e-tests-emptydir-dlpkg, resource: bindings, ignored listing per whitelist
Feb 20 19:19:57.348: INFO: namespace e2e-tests-emptydir-dlpkg deletion completed in 6.960466357s

 [SLOW TEST:10.414 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:19:57.348: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-jlsn8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:19:58.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-jlsn8" to be "success or failure"
Feb 20 19:19:58.594: INFO: Pod "downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.978057ms
Feb 20 19:20:00.618: INFO: Pod "downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047004527s
STEP: Saw pod success
Feb 20 19:20:00.618: INFO: Pod "downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:20:00.641: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:20:00.722: INFO: Waiting for pod downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:20:00.746: INFO: Pod downwardapi-volume-831f6996-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:20:00.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-jlsn8" for this suite.
Feb 20 19:20:06.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:06.977: INFO: namespace: e2e-tests-downward-api-jlsn8, resource: bindings, ignored listing per whitelist
Feb 20 19:20:07.768: INFO: namespace e2e-tests-downward-api-jlsn8 deletion completed in 6.998241071s

 [SLOW TEST:10.419 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:20:07.768: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7vffn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-8954230e-3544-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:20:09.005: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-7vffn" to be "success or failure"
Feb 20 19:20:09.028: INFO: Pod "pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.932473ms
Feb 20 19:20:11.052: INFO: Pod "pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046811593s
STEP: Saw pod success
Feb 20 19:20:11.052: INFO: Pod "pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:20:11.078: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:20:11.140: INFO: Waiting for pod pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:20:11.163: INFO: Pod pod-projected-configmaps-8957b220-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:20:11.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7vffn" for this suite.
Feb 20 19:20:17.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:17.419: INFO: namespace: e2e-tests-projected-7vffn, resource: bindings, ignored listing per whitelist
Feb 20 19:20:18.192: INFO: namespace e2e-tests-projected-7vffn deletion completed in 7.004561834s

 [SLOW TEST:10.424 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:20:18.193: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-46sgp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-46sgp
Feb 20 19:20:23.409: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-46sgp
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 19:20:23.432: INFO: Initial restart count of pod liveness-http is 0
Feb 20 19:20:37.624: INFO: Restart count of pod e2e-tests-container-probe-46sgp/liveness-http is now 1 (14.192423966s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:20:37.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-46sgp" for this suite.
Feb 20 19:20:43.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:20:44.173: INFO: namespace: e2e-tests-container-probe-46sgp, resource: bindings, ignored listing per whitelist
Feb 20 19:20:44.639: INFO: namespace e2e-tests-container-probe-46sgp deletion completed in 6.962352352s

 [SLOW TEST:26.447 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:20:44.640: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-mw5rn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:20:46.057: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 20 19:20:46.129: INFO: Number of nodes with available pods: 0
Feb 20 19:20:46.129: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:20:47.177: INFO: Number of nodes with available pods: 0
Feb 20 19:20:47.177: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk is running more than one daemon pod
Feb 20 19:20:48.189: INFO: Number of nodes with available pods: 2
Feb 20 19:20:48.189: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 20 19:20:48.338: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:48.338: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:49.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:49.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:50.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:50.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:51.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:51.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:52.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:52.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:53.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:53.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:54.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:54.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:55.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:55.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:56.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:56.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:57.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:57.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:58.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:58.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:59.394: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:20:59.394: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:00.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:00.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:01.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:01.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:02.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:02.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:03.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:03.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:04.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:04.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:05.388: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:05.388: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:06.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:06.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:07.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:07.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:08.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:08.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:09.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:09.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:10.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:10.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:11.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:11.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:12.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:12.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:13.391: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:13.391: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:14.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:14.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:15.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:15.386: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:16.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:16.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:17.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:17.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:18.384: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:18.384: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:19.387: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:19.387: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:20.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:20.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:21.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:21.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:22.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:22.385: INFO: Wrong image for pod: daemon-set-lz7h6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:22.385: INFO: Pod daemon-set-lz7h6 is not available
Feb 20 19:21:23.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:23.385: INFO: Pod daemon-set-wsfhd is not available
Feb 20 19:21:24.424: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:24.424: INFO: Pod daemon-set-wsfhd is not available
Feb 20 19:21:25.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:25.385: INFO: Pod daemon-set-wsfhd is not available
Feb 20 19:21:26.421: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:27.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:28.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:29.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:30.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:31.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:32.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:33.387: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:34.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:35.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:36.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:37.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:38.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:39.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:40.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:41.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:42.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:43.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:44.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:45.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:46.388: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:47.391: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:48.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:49.389: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:50.386: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:51.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:52.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:53.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:54.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:55.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:56.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:57.385: INFO: Wrong image for pod: daemon-set-8jdlj. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 20 19:21:57.385: INFO: Pod daemon-set-8jdlj is not available
Feb 20 19:21:58.385: INFO: Pod daemon-set-pjtbp is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 20 19:21:58.456: INFO: Number of nodes with available pods: 1
Feb 20 19:21:58.456: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd is running more than one daemon pod
Feb 20 19:21:59.504: INFO: Number of nodes with available pods: 1
Feb 20 19:21:59.504: INFO: Node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd is running more than one daemon pod
Feb 20 19:22:00.504: INFO: Number of nodes with available pods: 2
Feb 20 19:22:00.505: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-mw5rn, will wait for the garbage collector to delete the pods
Feb 20 19:22:00.725: INFO: Deleting {extensions DaemonSet} daemon-set took: 25.952646ms
Feb 20 19:22:00.825: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.215416ms
Feb 20 19:22:13.049: INFO: Number of nodes with available pods: 0
Feb 20 19:22:13.049: INFO: Number of running nodes: 0, number of available pods: 0
Feb 20 19:22:13.072: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mw5rn/daemonsets","resourceVersion":"16775"},"items":null}

Feb 20 19:22:13.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mw5rn/pods","resourceVersion":"16775"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:22:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mw5rn" for this suite.
Feb 20 19:22:21.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:22:21.488: INFO: namespace: e2e-tests-daemonsets-mw5rn, resource: bindings, ignored listing per whitelist
Feb 20 19:22:22.203: INFO: namespace e2e-tests-daemonsets-mw5rn deletion completed in 9.010440581s

 [SLOW TEST:97.563 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:22:22.203: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qxrbp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 20 19:22:26.033: INFO: Successfully updated pod "annotationupdated96cbc72-3544-11e9-9b2a-cafe91372a39"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:22:28.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qxrbp" for this suite.
Feb 20 19:22:52.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:22:52.807: INFO: namespace: e2e-tests-downward-api-qxrbp, resource: bindings, ignored listing per whitelist
Feb 20 19:22:53.224: INFO: namespace e2e-tests-downward-api-qxrbp deletion completed in 25.09833547s

 [SLOW TEST:31.021 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:22:53.224: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-d8bg7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:22:54.450: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 20 19:22:54.497: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-d8bg7/daemonsets","resourceVersion":"16892"},"items":null}

Feb 20 19:22:54.520: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-d8bg7/pods","resourceVersion":"16892"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:22:54.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-d8bg7" for this suite.
Feb 20 19:23:00.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:01.218: INFO: namespace: e2e-tests-daemonsets-d8bg7, resource: bindings, ignored listing per whitelist
Feb 20 19:23:01.566: INFO: namespace e2e-tests-daemonsets-d8bg7 deletion completed in 6.951439468s

S [SKIPPING] [8.342 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 20 19:22:54.450: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:23:01.566: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-xb268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f0f7d252-3544-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:23:02.884: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-xb268" to be "success or failure"
Feb 20 19:23:02.907: INFO: Pod "pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.959448ms
Feb 20 19:23:04.932: INFO: Pod "pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047387161s
STEP: Saw pod success
Feb 20 19:23:04.932: INFO: Pod "pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:23:04.955: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:23:05.020: INFO: Waiting for pod pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:23:05.044: INFO: Pod pod-projected-configmaps-f0fb69b1-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:23:05.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-xb268" for this suite.
Feb 20 19:23:11.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:11.609: INFO: namespace: e2e-tests-projected-xb268, resource: bindings, ignored listing per whitelist
Feb 20 19:23:12.162: INFO: namespace e2e-tests-projected-xb268 deletion completed in 7.094370409s

 [SLOW TEST:10.596 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:23:12.162: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-tpxb6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 20 19:23:13.377: INFO: Waiting up to 5m0s for pod "pod-f73c9903-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-tpxb6" to be "success or failure"
Feb 20 19:23:13.400: INFO: Pod "pod-f73c9903-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.323411ms
Feb 20 19:23:15.425: INFO: Pod "pod-f73c9903-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047552415s
STEP: Saw pod success
Feb 20 19:23:15.425: INFO: Pod "pod-f73c9903-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:23:15.448: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-f73c9903-3544-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:23:15.508: INFO: Waiting for pod pod-f73c9903-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:23:15.531: INFO: Pod pod-f73c9903-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:23:15.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tpxb6" for this suite.
Feb 20 19:23:21.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:22.492: INFO: namespace: e2e-tests-emptydir-tpxb6, resource: bindings, ignored listing per whitelist
Feb 20 19:23:22.540: INFO: namespace e2e-tests-emptydir-tpxb6 deletion completed in 6.984018973s

 [SLOW TEST:10.378 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:23:22.540: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-946bn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 19:23:23.661: INFO: Waiting up to 5m0s for pod "pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-946bn" to be "success or failure"
Feb 20 19:23:23.688: INFO: Pod "pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 27.415126ms
Feb 20 19:23:25.713: INFO: Pod "pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05182483s
STEP: Saw pod success
Feb 20 19:23:25.713: INFO: Pod "pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:23:25.736: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:23:25.797: INFO: Waiting for pod pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:23:25.820: INFO: Pod pod-fd5ddd35-3544-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:23:25.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-946bn" for this suite.
Feb 20 19:23:31.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:23:32.252: INFO: namespace: e2e-tests-emptydir-946bn, resource: bindings, ignored listing per whitelist
Feb 20 19:23:32.828: INFO: namespace e2e-tests-emptydir-946bn deletion completed in 6.984681542s

 [SLOW TEST:10.289 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:23:32.828: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-6jtsl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-6jtsl
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 20 19:23:33.933: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 20 19:24:00.336: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.1.203:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6jtsl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:24:00.336: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:24:00.873: INFO: Found all expected endpoints: [netserver-0]
Feb 20 19:24:00.906: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.0.66:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-6jtsl PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:24:00.906: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:24:01.461: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:24:01.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-6jtsl" for this suite.
Feb 20 19:24:25.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:24:26.376: INFO: namespace: e2e-tests-pod-network-test-6jtsl, resource: bindings, ignored listing per whitelist
Feb 20 19:24:26.446: INFO: namespace e2e-tests-pod-network-test-6jtsl deletion completed in 24.96112771s

 [SLOW TEST:53.618 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:24:26.447: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-kv2w9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 20 19:24:27.676: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17183,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 20 19:24:27.676: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17184,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 20 19:24:27.676: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17186,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 20 19:24:37.843: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17207,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 20 19:24:37.843: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17208,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 20 19:24:37.843: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-kv2w9,SelfLink:/api/v1/namespaces/e2e-tests-watch-kv2w9/configmaps/e2e-watch-test-label-changed,UID:23786bfc-3545-11e9-a0a3-2a5d9248fe67,ResourceVersion:17209,Generation:0,CreationTimestamp:2019-02-20 19:24:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:24:37.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-kv2w9" for this suite.
Feb 20 19:24:43.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:24:44.670: INFO: namespace: e2e-tests-watch-kv2w9, resource: bindings, ignored listing per whitelist
Feb 20 19:24:44.875: INFO: namespace e2e-tests-watch-kv2w9 deletion completed in 7.006643776s

 [SLOW TEST:18.429 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:24:44.876: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hwgq2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 20 19:24:45.934: INFO: namespace e2e-tests-kubectl-hwgq2
Feb 20 19:24:45.934: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-hwgq2'
Feb 20 19:24:46.366: INFO: stderr: ""
Feb 20 19:24:46.366: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 20 19:24:47.390: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:24:47.390: INFO: Found 0 / 1
Feb 20 19:24:48.391: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:24:48.391: INFO: Found 1 / 1
Feb 20 19:24:48.391: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 20 19:24:48.415: INFO: Selector matched 1 pods for map[app:redis]
Feb 20 19:24:48.415: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 20 19:24:48.415: INFO: wait on redis-master startup in e2e-tests-kubectl-hwgq2 
Feb 20 19:24:48.415: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml logs redis-master-l55cf redis-master --namespace=e2e-tests-kubectl-hwgq2'
Feb 20 19:24:48.647: INFO: stderr: ""
Feb 20 19:24:48.647: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 20 Feb 19:24:47.371 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 20 Feb 19:24:47.371 # Server started, Redis version 3.2.12\n1:M 20 Feb 19:24:47.371 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 20 Feb 19:24:47.371 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 20 19:24:48.647: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-hwgq2'
Feb 20 19:24:48.834: INFO: stderr: ""
Feb 20 19:24:48.834: INFO: stdout: "service/rm2 exposed\n"
Feb 20 19:24:48.857: INFO: Service rm2 in namespace e2e-tests-kubectl-hwgq2 found.
STEP: exposing service
Feb 20 19:24:50.904: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-hwgq2'
Feb 20 19:24:51.124: INFO: stderr: ""
Feb 20 19:24:51.124: INFO: stdout: "service/rm3 exposed\n"
Feb 20 19:24:51.148: INFO: Service rm3 in namespace e2e-tests-kubectl-hwgq2 found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:24:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hwgq2" for this suite.
Feb 20 19:25:17.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:25:17.973: INFO: namespace: e2e-tests-kubectl-hwgq2, resource: bindings, ignored listing per whitelist
Feb 20 19:25:18.206: INFO: namespace e2e-tests-kubectl-hwgq2 deletion completed in 24.987897245s

 [SLOW TEST:33.330 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:25:18.206: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-c7tvq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-c7tvq
Feb 20 19:25:21.513: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-c7tvq
STEP: checking the pod's current state and verifying that restartCount is present
Feb 20 19:25:21.537: INFO: Initial restart count of pod liveness-http is 0
Feb 20 19:25:39.778: INFO: Restart count of pod e2e-tests-container-probe-c7tvq/liveness-http is now 1 (18.240645581s elapsed)
Feb 20 19:26:00.018: INFO: Restart count of pod e2e-tests-container-probe-c7tvq/liveness-http is now 2 (38.480129462s elapsed)
Feb 20 19:26:20.269: INFO: Restart count of pod e2e-tests-container-probe-c7tvq/liveness-http is now 3 (58.73134076s elapsed)
Feb 20 19:26:40.515: INFO: Restart count of pod e2e-tests-container-probe-c7tvq/liveness-http is now 4 (1m18.977763262s elapsed)
Feb 20 19:27:41.250: INFO: Restart count of pod e2e-tests-container-probe-c7tvq/liveness-http is now 5 (2m19.712150565s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:27:41.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-c7tvq" for this suite.
Feb 20 19:27:47.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:27:48.210: INFO: namespace: e2e-tests-container-probe-c7tvq, resource: bindings, ignored listing per whitelist
Feb 20 19:27:48.256: INFO: namespace e2e-tests-container-probe-c7tvq deletion completed in 6.953407426s

 [SLOW TEST:150.050 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:27:48.256: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-fqz5s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 20 19:27:49.568: INFO: Waiting up to 5m0s for pod "pod-9bdc210c-3545-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-fqz5s" to be "success or failure"
Feb 20 19:27:49.591: INFO: Pod "pod-9bdc210c-3545-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.032748ms
Feb 20 19:27:51.615: INFO: Pod "pod-9bdc210c-3545-11e9-9b2a-cafe91372a39": Phase="Running", Reason="", readiness=true. Elapsed: 2.047079766s
Feb 20 19:27:53.639: INFO: Pod "pod-9bdc210c-3545-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070916532s
STEP: Saw pod success
Feb 20 19:27:53.639: INFO: Pod "pod-9bdc210c-3545-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:27:53.662: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-9bdc210c-3545-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:27:53.722: INFO: Waiting for pod pod-9bdc210c-3545-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:27:53.745: INFO: Pod pod-9bdc210c-3545-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:27:53.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-fqz5s" for this suite.
Feb 20 19:27:59.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:28:00.518: INFO: namespace: e2e-tests-emptydir-fqz5s, resource: bindings, ignored listing per whitelist
Feb 20 19:28:00.792: INFO: namespace e2e-tests-emptydir-fqz5s deletion completed in 7.022847436s

 [SLOW TEST:12.536 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:28:00.792: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fn9mt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-a3422b0e-3545-11e9-9b2a-cafe91372a39
STEP: Creating secret with name s-test-opt-upd-a3422b70-3545-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a3422b0e-3545-11e9-9b2a-cafe91372a39
STEP: Updating secret s-test-opt-upd-a3422b70-3545-11e9-9b2a-cafe91372a39
STEP: Creating secret with name s-test-opt-create-a3422b8b-3545-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:28:06.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fn9mt" for this suite.
Feb 20 19:28:28.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:28:29.349: INFO: namespace: e2e-tests-projected-fn9mt, resource: bindings, ignored listing per whitelist
Feb 20 19:28:29.639: INFO: namespace e2e-tests-projected-fn9mt deletion completed in 23.038267859s

 [SLOW TEST:28.847 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:28:29.639: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-wlchv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-hx2vn
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-9h4sd
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:28:37.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-wlchv" for this suite.
Feb 20 19:28:43.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:28:43.974: INFO: namespace: e2e-tests-namespaces-wlchv, resource: bindings, ignored listing per whitelist
Feb 20 19:28:44.560: INFO: namespace e2e-tests-namespaces-wlchv deletion completed in 7.006365029s
STEP: Destroying namespace "e2e-tests-nsdeletetest-hx2vn" for this suite.
Feb 20 19:28:44.584: INFO: Namespace e2e-tests-nsdeletetest-hx2vn was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-9h4sd" for this suite.
Feb 20 19:28:50.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:28:51.279: INFO: namespace: e2e-tests-nsdeletetest-9h4sd, resource: bindings, ignored listing per whitelist
Feb 20 19:28:51.537: INFO: namespace e2e-tests-nsdeletetest-9h4sd deletion completed in 6.953225888s

 [SLOW TEST:21.898 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:28:51.537: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-d6wgc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:28:54.796: INFO: Waiting up to 5m0s for pod "client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-pods-d6wgc" to be "success or failure"
Feb 20 19:28:54.820: INFO: Pod "client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.501997ms
Feb 20 19:28:56.844: INFO: Pod "client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047788395s
STEP: Saw pod success
Feb 20 19:28:56.844: INFO: Pod "client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:28:56.867: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39 container env3cont: <nil>
STEP: delete the pod
Feb 20 19:28:56.927: INFO: Waiting for pod client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:28:56.950: INFO: Pod client-envvars-c2bc463e-3545-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:28:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-d6wgc" for this suite.
Feb 20 19:29:47.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:29:47.847: INFO: namespace: e2e-tests-pods-d6wgc, resource: bindings, ignored listing per whitelist
Feb 20 19:29:47.939: INFO: namespace e2e-tests-pods-d6wgc deletion completed in 50.965140727s

 [SLOW TEST:56.402 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:29:47.939: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-wpc64
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-wpc64
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-wpc64
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-wpc64
Feb 20 19:29:49.318: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 20 19:29:59.343: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 20 19:29:59.367: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:30:00.184: INFO: stderr: ""
Feb 20 19:30:00.184: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:30:00.184: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:30:00.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 20 19:30:10.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:30:10.233: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:30:10.326: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999645s
Feb 20 19:30:11.351: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.976435729s
Feb 20 19:30:12.375: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.95207048s
Feb 20 19:30:13.399: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.927818442s
Feb 20 19:30:14.424: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.903755055s
Feb 20 19:30:15.448: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.879329145s
Feb 20 19:30:16.472: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.855135246s
Feb 20 19:30:17.496: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.830931293s
Feb 20 19:30:18.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.806495938s
Feb 20 19:30:19.545: INFO: Verifying statefulset ss doesn't scale past 1 for another 782.436247ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-wpc64
Feb 20 19:30:20.569: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:21.205: INFO: stderr: ""
Feb 20 19:30:21.205: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:30:21.205: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:30:21.229: INFO: Found 1 stateful pods, waiting for 3
Feb 20 19:30:31.253: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:30:31.253: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 20 19:30:31.253: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 20 19:30:31.299: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:30:32.039: INFO: stderr: ""
Feb 20 19:30:32.039: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:30:32.039: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:30:32.039: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:30:32.757: INFO: stderr: ""
Feb 20 19:30:32.757: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:30:32.757: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:30:32.757: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 20 19:30:33.517: INFO: stderr: ""
Feb 20 19:30:33.517: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 20 19:30:33.517: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 20 19:30:33.517: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:30:33.540: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 20 19:30:43.588: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:30:43.589: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:30:43.589: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 20 19:30:43.659: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999473s
Feb 20 19:30:44.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976561355s
Feb 20 19:30:45.709: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952283741s
Feb 20 19:30:46.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.925117357s
Feb 20 19:30:47.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.901073403s
Feb 20 19:30:48.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.8757906s
Feb 20 19:30:49.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.851339177s
Feb 20 19:30:50.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.82676064s
Feb 20 19:30:51.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.802479713s
Feb 20 19:30:52.883: INFO: Verifying statefulset ss doesn't scale past 3 for another 778.389299ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-wpc64
Feb 20 19:30:53.907: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:54.815: INFO: stderr: ""
Feb 20 19:30:54.815: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:30:54.815: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:30:54.816: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:55.518: INFO: stderr: ""
Feb 20 19:30:55.518: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 20 19:30:55.518: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 20 19:30:55.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:30:55.857: INFO: rc: 1
Feb 20 19:30:55.857: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0020d3980 exit status 1 <nil> <nil> true [0xc0015d7590 0xc0015d75e0 0xc0015d7630] [0xc0015d7590 0xc0015d75e0 0xc0015d7630] [0xc0015d75c0 0xc0015d7620] [0x932420 0x932420] 0xc001bc6f00 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 20 19:31:05.858: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:06.053: INFO: rc: 1
Feb 20 19:31:06.053: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020d3c20 exit status 1 <nil> <nil> true [0xc0015d7648 0xc0015d7690 0xc0015d76c8] [0xc0015d7648 0xc0015d7690 0xc0015d76c8] [0xc0015d7670 0xc0015d76b8] [0x932420 0x932420] 0xc001bc7200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:31:16.054: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:16.233: INFO: rc: 1
Feb 20 19:31:16.233: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0020d3e30 exit status 1 <nil> <nil> true [0xc0015d76e8 0xc0015d7730 0xc0015d7768] [0xc0015d76e8 0xc0015d7730 0xc0015d7768] [0xc0015d7710 0xc0015d7758] [0x932420 0x932420] 0xc001bc7500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:31:26.234: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:26.626: INFO: rc: 1
Feb 20 19:31:26.626: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d3e0 exit status 1 <nil> <nil> true [0xc0000f2220 0xc0000f2318 0xc0000f2370] [0xc0000f2220 0xc0000f2318 0xc0000f2370] [0xc0000f22d8 0xc0000f2368] [0x932420 0x932420] 0xc001f3e360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:31:36.626: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:36.799: INFO: rc: 1
Feb 20 19:31:36.799: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00249e240 exit status 1 <nil> <nil> true [0xc001d12000 0xc001d12018 0xc001d12030] [0xc001d12000 0xc001d12018 0xc001d12030] [0xc001d12010 0xc001d12028] [0x932420 0x932420] 0xc002082240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:31:46.799: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:46.982: INFO: rc: 1
Feb 20 19:31:46.982: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00249e510 exit status 1 <nil> <nil> true [0xc001d12038 0xc001d12058 0xc001d12070] [0xc001d12038 0xc001d12058 0xc001d12070] [0xc001d12048 0xc001d12068] [0x932420 0x932420] 0xc0020825a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:31:56.982: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:31:57.159: INFO: rc: 1
Feb 20 19:31:57.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d680 exit status 1 <nil> <nil> true [0xc0000f2388 0xc0000f2458 0xc0000f2528] [0xc0000f2388 0xc0000f2458 0xc0000f2528] [0xc0000f23e0 0xc0000f24c8] [0x932420 0x932420] 0xc001f3e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:07.159: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:07.355: INFO: rc: 1
Feb 20 19:32:07.355: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d8f0 exit status 1 <nil> <nil> true [0xc0000f2530 0xc0000f2560 0xc0000f2608] [0xc0000f2530 0xc0000f2560 0xc0000f2608] [0xc0000f2550 0xc0000f25f0] [0x932420 0x932420] 0xc001f3f140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:17.355: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:17.493: INFO: rc: 1
Feb 20 19:32:17.494: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131a300 exit status 1 <nil> <nil> true [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa180 0xc0006fa240] [0x932420 0x932420] 0xc001ad6480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:27.494: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:27.647: INFO: rc: 1
Feb 20 19:32:27.647: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00249e9f0 exit status 1 <nil> <nil> true [0xc001d12078 0xc001d12090 0xc001d120a8] [0xc001d12078 0xc001d12090 0xc001d120a8] [0xc001d12088 0xc001d120a0] [0x932420 0x932420] 0xc0020828a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:37.648: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:37.868: INFO: rc: 1
Feb 20 19:32:37.868: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00070acc0 exit status 1 <nil> <nil> true [0xc001612000 0xc001612018 0xc001612030] [0xc001612000 0xc001612018 0xc001612030] [0xc001612010 0xc001612028] [0x932420 0x932420] 0xc001de4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:47.868: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:48.047: INFO: rc: 1
Feb 20 19:32:48.047: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098db90 exit status 1 <nil> <nil> true [0xc0000f2618 0xc0000f2660 0xc0000f2760] [0xc0000f2618 0xc0000f2660 0xc0000f2760] [0xc0000f2648 0xc0000f2718] [0x932420 0x932420] 0xc001f3f440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:32:58.047: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:32:58.193: INFO: rc: 1
Feb 20 19:32:58.193: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131a810 exit status 1 <nil> <nil> true [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa288 0xc0006fa378] [0x932420 0x932420] 0xc001ad6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:08.194: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:08.404: INFO: rc: 1
Feb 20 19:33:08.404: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00249ef30 exit status 1 <nil> <nil> true [0xc001d120b0 0xc001d120c8 0xc001d120e0] [0xc001d120b0 0xc001d120c8 0xc001d120e0] [0xc001d120c0 0xc001d120d8] [0x932420 0x932420] 0xc002082ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:18.404: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:18.582: INFO: rc: 1
Feb 20 19:33:18.582: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00107a000 exit status 1 <nil> <nil> true [0xc001612038 0xc001612050 0xc001612068] [0xc001612038 0xc001612050 0xc001612068] [0xc001612048 0xc001612060] [0x932420 0x932420] 0xc001de4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:28.582: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:28.873: INFO: rc: 1
Feb 20 19:33:28.873: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131af60 exit status 1 <nil> <nil> true [0xc0006fa4c8 0xc0006fa570 0xc0006fa6b8] [0xc0006fa4c8 0xc0006fa570 0xc0006fa6b8] [0xc0006fa568 0xc0006fa670] [0x932420 0x932420] 0xc001ad6a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:38.874: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:39.098: INFO: rc: 1
Feb 20 19:33:39.098: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00107a270 exit status 1 <nil> <nil> true [0xc001612000 0xc001612018 0xc001612030] [0xc001612000 0xc001612018 0xc001612030] [0xc001612010 0xc001612028] [0x932420 0x932420] 0xc001de4240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:49.098: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:49.338: INFO: rc: 1
Feb 20 19:33:49.338: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d410 exit status 1 <nil> <nil> true [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2258 0xc0000f2358] [0x932420 0x932420] 0xc001f3e360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:33:59.338: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:33:59.554: INFO: rc: 1
Feb 20 19:33:59.554: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00107a510 exit status 1 <nil> <nil> true [0xc001612038 0xc001612050 0xc001612068] [0xc001612038 0xc001612050 0xc001612068] [0xc001612048 0xc001612060] [0x932420 0x932420] 0xc001de4540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:34:09.554: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:34:09.773: INFO: rc: 1
Feb 20 19:34:09.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00107a930 exit status 1 <nil> <nil> true [0xc001612070 0xc001612088 0xc0016120a0] [0xc001612070 0xc001612088 0xc0016120a0] [0xc001612080 0xc001612098] [0x932420 0x932420] 0xc001de4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:34:19.774: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:34:19.922: INFO: rc: 1
Feb 20 19:34:19.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00070bb60 exit status 1 <nil> <nil> true [0xc001d12000 0xc001d12018 0xc001d12030] [0xc001d12000 0xc001d12018 0xc001d12030] [0xc001d12010 0xc001d12028] [0x932420 0x932420] 0xc002082240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:34:29.922: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:34:30.094: INFO: rc: 1
Feb 20 19:34:30.094: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00249e090 exit status 1 <nil> <nil> true [0xc001d12038 0xc001d12058 0xc001d12070] [0xc001d12038 0xc001d12058 0xc001d12070] [0xc001d12048 0xc001d12068] [0x932420 0x932420] 0xc0020825a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:34:40.094: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:34:40.308: INFO: rc: 1
Feb 20 19:34:40.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131a330 exit status 1 <nil> <nil> true [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa138 0xc0006fa1e8 0xc0006fa258] [0xc0006fa180 0xc0006fa240] [0x932420 0x932420] 0xc001ad6480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:34:50.309: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:34:50.476: INFO: rc: 1
Feb 20 19:34:50.476: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131aba0 exit status 1 <nil> <nil> true [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa260 0xc0006fa310 0xc0006fa3b0] [0xc0006fa288 0xc0006fa378] [0x932420 0x932420] 0xc001ad6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:00.476: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:00.643: INFO: rc: 1
Feb 20 19:35:00.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d710 exit status 1 <nil> <nil> true [0xc0000f2370 0xc0000f23e0 0xc0000f24c8] [0xc0000f2370 0xc0000f23e0 0xc0000f24c8] [0xc0000f2390 0xc0000f2460] [0x932420 0x932420] 0xc001f3e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:10.643: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:10.812: INFO: rc: 1
Feb 20 19:35:10.812: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131b1d0 exit status 1 <nil> <nil> true [0xc0006fa6d0 0xc0006fa780 0xc0006fa7f8] [0xc0006fa6d0 0xc0006fa780 0xc0006fa7f8] [0xc0006fa740 0xc0006fa7c8] [0x932420 0x932420] 0xc001ad7e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:20.812: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:20.957: INFO: rc: 1
Feb 20 19:35:20.957: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d9e0 exit status 1 <nil> <nil> true [0xc0000f2528 0xc0000f2550 0xc0000f25f0] [0xc0000f2528 0xc0000f2550 0xc0000f25f0] [0xc0000f2538 0xc0000f25b0] [0x932420 0x932420] 0xc001f3f140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:30.958: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:31.117: INFO: rc: 1
Feb 20 19:35:31.117: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00070ac30 exit status 1 <nil> <nil> true [0xc0006fa148 0xc0006fa208 0xc0006fa260] [0xc0006fa148 0xc0006fa208 0xc0006fa260] [0xc0006fa1e8 0xc0006fa258] [0x932420 0x932420] 0xc001ad6480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:41.117: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:41.290: INFO: rc: 1
Feb 20 19:35:41.290: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00131a000 exit status 1 <nil> <nil> true [0xc0006fa280 0xc0006fa360 0xc0006fa3d8] [0xc0006fa280 0xc0006fa360 0xc0006fa3d8] [0xc0006fa310 0xc0006fa3b0] [0x932420 0x932420] 0xc001ad6780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:35:51.290: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:35:51.540: INFO: rc: 1
Feb 20 19:35:51.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00098d3e0 exit status 1 <nil> <nil> true [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2000 0xc0000f22d8 0xc0000f2368] [0xc0000f2258 0xc0000f2358] [0x932420 0x932420] 0xc001f3e360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 20 19:36:01.540: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-wpc64 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 20 19:36:01.732: INFO: rc: 1
Feb 20 19:36:01.732: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 20 19:36:01.732: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 20 19:36:01.804: INFO: Deleting all statefulset in ns e2e-tests-statefulset-wpc64
Feb 20 19:36:01.827: INFO: Scaling statefulset ss to 0
Feb 20 19:36:01.898: INFO: Waiting for statefulset status.replicas updated to 0
Feb 20 19:36:01.921: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:36:01.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-wpc64" for this suite.
Feb 20 19:36:08.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:08.965: INFO: namespace: e2e-tests-statefulset-wpc64, resource: bindings, ignored listing per whitelist
Feb 20 19:36:08.988: INFO: namespace e2e-tests-statefulset-wpc64 deletion completed in 6.972227348s

 [SLOW TEST:381.049 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:36:08.989: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-dkhdg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 20 19:36:12.156: INFO: Pod pod-hostip-c62d49ce-3546-11e9-9b2a-cafe91372a39 has hostIP: 10.250.0.3
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:36:12.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-dkhdg" for this suite.
Feb 20 19:36:34.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:34.984: INFO: namespace: e2e-tests-pods-dkhdg, resource: bindings, ignored listing per whitelist
Feb 20 19:36:35.164: INFO: namespace e2e-tests-pods-dkhdg deletion completed in 22.984002136s

 [SLOW TEST:26.175 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:36:35.164: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-tbg9l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-d5cc1b4b-3546-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:36:36.291: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-tbg9l" to be "success or failure"
Feb 20 19:36:36.315: INFO: Pod "pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.586801ms
Feb 20 19:36:38.340: INFO: Pod "pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048957078s
STEP: Saw pod success
Feb 20 19:36:38.340: INFO: Pod "pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:36:38.365: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:36:38.440: INFO: Waiting for pod pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:36:38.465: INFO: Pod pod-projected-configmaps-d5cfb90a-3546-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:36:38.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-tbg9l" for this suite.
Feb 20 19:36:44.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:45.407: INFO: namespace: e2e-tests-projected-tbg9l, resource: bindings, ignored listing per whitelist
Feb 20 19:36:45.478: INFO: namespace e2e-tests-projected-tbg9l deletion completed in 6.987583716s

 [SLOW TEST:10.313 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:36:45.478: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-kd68n
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 20 19:36:46.660: INFO: Waiting up to 5m0s for pod "pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-kd68n" to be "success or failure"
Feb 20 19:36:46.682: INFO: Pod "pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.659443ms
Feb 20 19:36:48.707: INFO: Pod "pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047300512s
STEP: Saw pod success
Feb 20 19:36:48.707: INFO: Pod "pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:36:48.730: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:36:48.824: INFO: Waiting for pod pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:36:48.847: INFO: Pod pod-dbfdc97f-3546-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:36:48.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-kd68n" for this suite.
Feb 20 19:36:54.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:36:55.130: INFO: namespace: e2e-tests-emptydir-kd68n, resource: bindings, ignored listing per whitelist
Feb 20 19:36:55.876: INFO: namespace e2e-tests-emptydir-kd68n deletion completed in 7.004204468s

 [SLOW TEST:10.398 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:36:55.876: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-mm72q
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 20 19:36:56.933: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 20 19:36:56.980: INFO: Waiting for terminating namespaces to be deleted...
Feb 20 19:36:57.003: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-9h8kk before test
Feb 20 19:36:57.037: INFO: addons-kubernetes-dashboard-5f64f76bd-h2vfh from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 20 19:36:57.037: INFO: calico-node-mmtdl from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 19:36:57.037: INFO: node-exporter-c4nt2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 19:36:57.037: INFO: addons-nginx-ingress-controller-55d976867d-ttlp2 from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 20 19:36:57.037: INFO: blackbox-exporter-64f6f7f998-h9cxk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 20 19:36:57.037: INFO: coredns-5f4748c5f-5qhjk from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container coredns ready: true, restart count 0
Feb 20 19:36:57.037: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-sn6mb from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 20 19:36:57.037: INFO: metrics-server-6c5b747679-8tgqt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container metrics-server ready: true, restart count 0
Feb 20 19:36:57.037: INFO: addons-kube-lego-648f8c9f5c-c9t8z from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container kube-lego ready: true, restart count 0
Feb 20 19:36:57.037: INFO: kube-proxy-2lxpj from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 20 19:36:57.037: INFO: vpn-shoot-56b45cd8c8-2p7dt from kube-system started at 2019-02-20 17:53:46 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.037: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 20 19:36:57.037: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd before test
Feb 20 19:36:57.096: INFO: calico-node-6skx5 from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.096: INFO: 	Container calico-node ready: true, restart count 0
Feb 20 19:36:57.096: INFO: node-exporter-prqpl from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.096: INFO: 	Container node-exporter ready: true, restart count 0
Feb 20 19:36:57.096: INFO: kube-proxy-j2qvq from kube-system started at 2019-02-20 17:53:53 +0000 UTC (1 container statuses recorded)
Feb 20 19:36:57.096: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e37e6c19-3546-11e9-9b2a-cafe91372a39 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e37e6c19-3546-11e9-9b2a-cafe91372a39 off the node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e37e6c19-3546-11e9-9b2a-cafe91372a39
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:37:01.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-mm72q" for this suite.
Feb 20 19:37:15.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:37:15.677: INFO: namespace: e2e-tests-sched-pred-mm72q, resource: bindings, ignored listing per whitelist
Feb 20 19:37:16.493: INFO: namespace e2e-tests-sched-pred-mm72q deletion completed in 15.032646362s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

 [SLOW TEST:20.617 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:37:16.493: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-mjqm7
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-ee7b22b0-3546-11e9-9b2a-cafe91372a39
STEP: Creating configMap with name cm-test-opt-upd-ee7b22fa-3546-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ee7b22b0-3546-11e9-9b2a-cafe91372a39
STEP: Updating configmap cm-test-opt-upd-ee7b22fa-3546-11e9-9b2a-cafe91372a39
STEP: Creating configMap with name cm-test-opt-create-ee7b2311-3546-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:38:29.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-mjqm7" for this suite.
Feb 20 19:38:53.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:38:54.228: INFO: namespace: e2e-tests-configmap-mjqm7, resource: bindings, ignored listing per whitelist
Feb 20 19:38:54.553: INFO: namespace e2e-tests-configmap-mjqm7 deletion completed in 25.032993961s

 [SLOW TEST:98.060 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:38:54.553: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-2lmrn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-xk7h6
STEP: Creating secret with name secret-test-28e13527-3547-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:38:56.097: INFO: Waiting up to 5m0s for pod "pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-2lmrn" to be "success or failure"
Feb 20 19:38:56.124: INFO: Pod "pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 27.095763ms
Feb 20 19:38:58.148: INFO: Pod "pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051214262s
STEP: Saw pod success
Feb 20 19:38:58.148: INFO: Pod "pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:38:58.172: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:38:58.233: INFO: Waiting for pod pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:38:58.256: INFO: Pod pod-secrets-2923228a-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:38:58.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-2lmrn" for this suite.
Feb 20 19:39:06.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:06.643: INFO: namespace: e2e-tests-secrets-2lmrn, resource: bindings, ignored listing per whitelist
Feb 20 19:39:07.352: INFO: namespace e2e-tests-secrets-2lmrn deletion completed in 9.071460687s
STEP: Destroying namespace "e2e-tests-secret-namespace-xk7h6" for this suite.
Feb 20 19:39:13.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:13.823: INFO: namespace: e2e-tests-secret-namespace-xk7h6, resource: bindings, ignored listing per whitelist
Feb 20 19:39:14.310: INFO: namespace e2e-tests-secret-namespace-xk7h6 deletion completed in 6.958476613s

 [SLOW TEST:19.757 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:39:14.311: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-ztmtp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 20 19:39:15.464: INFO: Waiting up to 5m0s for pod "client-containers-34af6552-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-containers-ztmtp" to be "success or failure"
Feb 20 19:39:15.487: INFO: Pod "client-containers-34af6552-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.039237ms
Feb 20 19:39:17.511: INFO: Pod "client-containers-34af6552-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046725675s
STEP: Saw pod success
Feb 20 19:39:17.511: INFO: Pod "client-containers-34af6552-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:39:17.534: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod client-containers-34af6552-3547-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:39:17.596: INFO: Waiting for pod client-containers-34af6552-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:39:17.619: INFO: Pod client-containers-34af6552-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:39:17.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-ztmtp" for this suite.
Feb 20 19:39:23.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:23.998: INFO: namespace: e2e-tests-containers-ztmtp, resource: bindings, ignored listing per whitelist
Feb 20 19:39:24.645: INFO: namespace e2e-tests-containers-ztmtp deletion completed in 7.001983678s

 [SLOW TEST:10.333 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:39:24.645: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-lc8w5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 20 19:39:25.732: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml cluster-info'
Feb 20 19:39:26.387: INFO: stderr: ""
Feb 20 19:39:26.387: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:39:26.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-lc8w5" for this suite.
Feb 20 19:39:32.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:33.334: INFO: namespace: e2e-tests-kubectl-lc8w5, resource: bindings, ignored listing per whitelist
Feb 20 19:39:33.402: INFO: namespace e2e-tests-kubectl-lc8w5 deletion completed in 6.990699794s

 [SLOW TEST:8.757 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:39:33.402: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fmwn9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:39:34.655: INFO: Waiting up to 5m0s for pod "downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-fmwn9" to be "success or failure"
Feb 20 19:39:34.677: INFO: Pod "downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.364852ms
Feb 20 19:39:36.700: INFO: Pod "downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.045755065s
STEP: Saw pod success
Feb 20 19:39:36.700: INFO: Pod "downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:39:36.723: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:39:36.782: INFO: Waiting for pod downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:39:36.808: INFO: Pod downwardapi-volume-401fc9d4-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:39:36.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fmwn9" for this suite.
Feb 20 19:39:42.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:43.585: INFO: namespace: e2e-tests-projected-fmwn9, resource: bindings, ignored listing per whitelist
Feb 20 19:39:43.891: INFO: namespace e2e-tests-projected-fmwn9 deletion completed in 7.059923753s

 [SLOW TEST:10.489 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:39:43.891: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-kbh2z
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 20 19:39:45.062: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:39:45.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-kbh2z" for this suite.
Feb 20 19:39:53.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:39:53.938: INFO: namespace: e2e-tests-custom-resource-definition-kbh2z, resource: bindings, ignored listing per whitelist
Feb 20 19:39:54.260: INFO: namespace e2e-tests-custom-resource-definition-kbh2z deletion completed in 8.990306309s

 [SLOW TEST:10.369 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:39:54.260: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-26kch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0220 19:40:35.604036   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 19:40:35.604: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:40:35.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-26kch" for this suite.
Feb 20 19:40:41.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:40:42.086: INFO: namespace: e2e-tests-gc-26kch, resource: bindings, ignored listing per whitelist
Feb 20 19:40:42.569: INFO: namespace e2e-tests-gc-26kch deletion completed in 6.942083339s

 [SLOW TEST:48.309 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:40:42.570: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-q7l9x
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:40:43.752: INFO: Waiting up to 5m0s for pod "downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-q7l9x" to be "success or failure"
Feb 20 19:40:43.775: INFO: Pod "downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.805835ms
Feb 20 19:40:45.799: INFO: Pod "downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046783379s
Feb 20 19:40:47.823: INFO: Pod "downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07078502s
STEP: Saw pod success
Feb 20 19:40:47.823: INFO: Pod "downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:40:47.847: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:40:47.906: INFO: Waiting for pod downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:40:47.930: INFO: Pod downwardapi-volume-694f3af5-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:40:47.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q7l9x" for this suite.
Feb 20 19:40:54.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:40:54.806: INFO: namespace: e2e-tests-downward-api-q7l9x, resource: bindings, ignored listing per whitelist
Feb 20 19:40:54.942: INFO: namespace e2e-tests-downward-api-q7l9x deletion completed in 6.989024139s

 [SLOW TEST:12.373 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:40:54.943: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-kssrp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 20 19:40:56.058: INFO: Waiting up to 5m0s for pod "client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-containers-kssrp" to be "success or failure"
Feb 20 19:40:56.080: INFO: Pod "client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.443957ms
Feb 20 19:40:58.104: INFO: Pod "client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046495151s
Feb 20 19:41:00.128: INFO: Pod "client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070148621s
STEP: Saw pod success
Feb 20 19:41:00.128: INFO: Pod "client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:41:00.156: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:41:00.240: INFO: Waiting for pod client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:41:00.264: INFO: Pod client-containers-70a4fc1d-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:41:00.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-kssrp" for this suite.
Feb 20 19:41:06.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:07.209: INFO: namespace: e2e-tests-containers-kssrp, resource: bindings, ignored listing per whitelist
Feb 20 19:41:07.305: INFO: namespace e2e-tests-containers-kssrp deletion completed in 7.0168037s

 [SLOW TEST:12.362 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:41:07.305: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-kdrv9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 20 19:41:09.216: INFO: Waiting up to 5m0s for pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx" in namespace "e2e-tests-svcaccounts-kdrv9" to be "success or failure"
Feb 20 19:41:09.239: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx": Phase="Pending", Reason="", readiness=false. Elapsed: 22.760138ms
Feb 20 19:41:11.263: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046440373s
Feb 20 19:41:13.287: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070257715s
STEP: Saw pod success
Feb 20 19:41:13.287: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx" satisfied condition "success or failure"
Feb 20 19:41:13.313: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx container token-test: <nil>
STEP: delete the pod
Feb 20 19:41:13.374: INFO: Waiting for pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx to disappear
Feb 20 19:41:13.397: INFO: Pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-stgfx no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 20 19:41:13.422: INFO: Waiting up to 5m0s for pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt" in namespace "e2e-tests-svcaccounts-kdrv9" to be "success or failure"
Feb 20 19:41:13.445: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt": Phase="Pending", Reason="", readiness=false. Elapsed: 22.667401ms
Feb 20 19:41:15.468: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046588629s
Feb 20 19:41:17.492: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070502573s
STEP: Saw pod success
Feb 20 19:41:17.492: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt" satisfied condition "success or failure"
Feb 20 19:41:17.516: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt container root-ca-test: <nil>
STEP: delete the pod
Feb 20 19:41:17.574: INFO: Waiting for pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt to disappear
Feb 20 19:41:17.597: INFO: Pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-l98kt no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 20 19:41:17.621: INFO: Waiting up to 5m0s for pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp" in namespace "e2e-tests-svcaccounts-kdrv9" to be "success or failure"
Feb 20 19:41:17.645: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp": Phase="Pending", Reason="", readiness=false. Elapsed: 23.307433ms
Feb 20 19:41:19.668: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046735672s
Feb 20 19:41:21.692: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070894127s
STEP: Saw pod success
Feb 20 19:41:21.692: INFO: Pod "pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp" satisfied condition "success or failure"
Feb 20 19:41:21.715: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp container namespace-test: <nil>
STEP: delete the pod
Feb 20 19:41:21.775: INFO: Waiting for pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp to disappear
Feb 20 19:41:21.798: INFO: Pod pod-service-account-787ca006-3547-11e9-9b2a-cafe91372a39-kr7rp no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:41:21.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-kdrv9" for this suite.
Feb 20 19:41:27.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:28.045: INFO: namespace: e2e-tests-svcaccounts-kdrv9, resource: bindings, ignored listing per whitelist
Feb 20 19:41:28.876: INFO: namespace e2e-tests-svcaccounts-kdrv9 deletion completed in 7.054641062s

 [SLOW TEST:21.571 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:41:28.877: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-d6c9d
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 1 pods
STEP: Gathering metrics
W0220 19:41:30.668696   29647 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 20 19:41:30.668: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:41:30.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-d6c9d" for this suite.
Feb 20 19:41:36.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:36.992: INFO: namespace: e2e-tests-gc-d6c9d, resource: bindings, ignored listing per whitelist
Feb 20 19:41:37.698: INFO: namespace e2e-tests-gc-d6c9d deletion completed in 7.005728618s

 [SLOW TEST:8.822 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:41:37.698: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-phswb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-8a27ee19-3547-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:41:38.884: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-phswb" to be "success or failure"
Feb 20 19:41:38.907: INFO: Pod "pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.037889ms
Feb 20 19:41:40.931: INFO: Pod "pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046711299s
STEP: Saw pod success
Feb 20 19:41:40.931: INFO: Pod "pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:41:40.954: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:41:41.022: INFO: Waiting for pod pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:41:41.046: INFO: Pod pod-configmaps-8a2b8506-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:41:41.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-phswb" for this suite.
Feb 20 19:41:49.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:49.210: INFO: namespace: e2e-tests-configmap-phswb, resource: bindings, ignored listing per whitelist
Feb 20 19:41:50.016: INFO: namespace e2e-tests-configmap-phswb deletion completed in 8.945218858s

 [SLOW TEST:12.318 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:41:50.016: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-kwf7l
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-917ccb3c-3547-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:41:51.183: INFO: Waiting up to 5m0s for pod "pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-kwf7l" to be "success or failure"
Feb 20 19:41:51.206: INFO: Pod "pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.934194ms
Feb 20 19:41:53.230: INFO: Pod "pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047402739s
STEP: Saw pod success
Feb 20 19:41:53.230: INFO: Pod "pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:41:53.254: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39 container secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:41:53.312: INFO: Waiting for pod pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:41:53.335: INFO: Pod pod-secrets-91805dd4-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:41:53.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-kwf7l" for this suite.
Feb 20 19:41:59.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:41:59.523: INFO: namespace: e2e-tests-secrets-kwf7l, resource: bindings, ignored listing per whitelist
Feb 20 19:42:00.309: INFO: namespace e2e-tests-secrets-kwf7l deletion completed in 6.950451845s

 [SLOW TEST:10.293 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:42:00.310: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-clhfk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:42:01.430: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:01.652: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 20 19:42:01.652: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Feb 20 19:42:01.702: INFO: scanned /root for discovery docs: <nil>
Feb 20 19:42:01.702: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:15.924: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 20 19:42:15.924: INFO: stdout: "Created e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef\nScaling up e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 20 19:42:15.924: INFO: stdout: "Created e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef\nScaling up e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 20 19:42:15.925: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:16.184: INFO: stderr: ""
Feb 20 19:42:16.184: INFO: stdout: "e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef-8czj4 "
Feb 20 19:42:16.184: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef-8czj4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:16.562: INFO: stderr: ""
Feb 20 19:42:16.562: INFO: stdout: "true"
Feb 20 19:42:16.562: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml get pods e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef-8czj4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:16.867: INFO: stderr: ""
Feb 20 19:42:16.867: INFO: stdout: "nginx:1.14-alpine"
Feb 20 19:42:16.867: INFO: e2e-test-nginx-rc-c97f20aecdc4148b6a8a3c31e3e682ef-8czj4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Feb 20 19:42:16.867: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-clhfk'
Feb 20 19:42:17.107: INFO: stderr: ""
Feb 20 19:42:17.107: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:42:17.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-clhfk" for this suite.
Feb 20 19:42:33.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:42:34.055: INFO: namespace: e2e-tests-kubectl-clhfk, resource: bindings, ignored listing per whitelist
Feb 20 19:42:34.124: INFO: namespace e2e-tests-kubectl-clhfk deletion completed in 16.990762816s

 [SLOW TEST:33.815 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:42:34.125: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-pgzbf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:42:35.231: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-pgzbf'
Feb 20 19:42:35.412: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 20 19:42:35.413: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Feb 20 19:42:35.435: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-pgzbf'
Feb 20 19:42:35.613: INFO: stderr: ""
Feb 20 19:42:35.613: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:42:35.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-pgzbf" for this suite.
Feb 20 19:42:59.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:00.065: INFO: namespace: e2e-tests-kubectl-pgzbf, resource: bindings, ignored listing per whitelist
Feb 20 19:43:00.718: INFO: namespace e2e-tests-kubectl-pgzbf deletion completed in 25.081903888s

 [SLOW TEST:26.594 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:43:00.719: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bjth4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 20 19:43:01.955: INFO: Waiting up to 5m0s for pod "pod-bbaf5808-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-bjth4" to be "success or failure"
Feb 20 19:43:01.978: INFO: Pod "pod-bbaf5808-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.440008ms
Feb 20 19:43:04.013: INFO: Pod "pod-bbaf5808-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.057541148s
STEP: Saw pod success
Feb 20 19:43:04.013: INFO: Pod "pod-bbaf5808-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:43:04.040: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-bbaf5808-3547-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:43:04.116: INFO: Waiting for pod pod-bbaf5808-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:43:04.139: INFO: Pod pod-bbaf5808-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:43:04.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bjth4" for this suite.
Feb 20 19:43:10.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:10.679: INFO: namespace: e2e-tests-emptydir-bjth4, resource: bindings, ignored listing per whitelist
Feb 20 19:43:11.192: INFO: namespace e2e-tests-emptydir-bjth4 deletion completed in 7.02900635s

 [SLOW TEST:10.474 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:43:11.193: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-jd4pw
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-c1e22065-3547-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:43:12.379: INFO: Waiting up to 5m0s for pod "pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-secrets-jd4pw" to be "success or failure"
Feb 20 19:43:12.401: INFO: Pod "pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.578482ms
Feb 20 19:43:14.425: INFO: Pod "pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046160258s
STEP: Saw pod success
Feb 20 19:43:14.425: INFO: Pod "pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:43:14.448: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39 container secret-env-test: <nil>
STEP: delete the pod
Feb 20 19:43:14.520: INFO: Waiting for pod pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:43:14.543: INFO: Pod pod-secrets-c1e5cb32-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:43:14.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-jd4pw" for this suite.
Feb 20 19:43:20.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:20.819: INFO: namespace: e2e-tests-secrets-jd4pw, resource: bindings, ignored listing per whitelist
Feb 20 19:43:21.559: INFO: namespace e2e-tests-secrets-jd4pw deletion completed in 6.992676484s

 [SLOW TEST:10.367 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:43:21.559: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-vgrcd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-kst7m
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 20 19:43:32.397: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-kldld
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:43:50.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-vgrcd" for this suite.
Feb 20 19:43:56.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:43:57.141: INFO: namespace: e2e-tests-namespaces-vgrcd, resource: bindings, ignored listing per whitelist
Feb 20 19:43:57.700: INFO: namespace e2e-tests-namespaces-vgrcd deletion completed in 7.022088507s
STEP: Destroying namespace "e2e-tests-nsdeletetest-kst7m" for this suite.
Feb 20 19:43:57.724: INFO: Namespace e2e-tests-nsdeletetest-kst7m was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-kldld" for this suite.
Feb 20 19:44:05.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:06.640: INFO: namespace: e2e-tests-nsdeletetest-kldld, resource: bindings, ignored listing per whitelist
Feb 20 19:44:06.686: INFO: namespace e2e-tests-nsdeletetest-kldld deletion completed in 8.962604931s

 [SLOW TEST:45.127 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:44:06.687: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6m6cf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:44:07.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-6m6cf" to be "success or failure"
Feb 20 19:44:07.881: INFO: Pod "downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.883432ms
Feb 20 19:44:09.905: INFO: Pod "downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.04685231s
STEP: Saw pod success
Feb 20 19:44:09.905: INFO: Pod "downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:44:09.929: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:44:09.987: INFO: Waiting for pod downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:44:10.011: INFO: Pod downwardapi-volume-e2f72014-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:44:10.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6m6cf" for this suite.
Feb 20 19:44:16.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:16.666: INFO: namespace: e2e-tests-downward-api-6m6cf, resource: bindings, ignored listing per whitelist
Feb 20 19:44:16.994: INFO: namespace e2e-tests-downward-api-6m6cf deletion completed in 6.958685127s

 [SLOW TEST:10.307 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:44:16.994: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-kf2tb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 20 19:44:20.795: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39"
Feb 20 19:44:20.796: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-pods-kf2tb" to be "terminated due to deadline exceeded"
Feb 20 19:44:20.819: INFO: Pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39": Phase="Running", Reason="", readiness=true. Elapsed: 23.036979ms
Feb 20 19:44:22.842: INFO: Pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39": Phase="Running", Reason="", readiness=true. Elapsed: 2.046508247s
Feb 20 19:44:24.867: INFO: Pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.071544461s
Feb 20 19:44:24.867: INFO: Pod "pod-update-activedeadlineseconds-e91a1502-3547-11e9-9b2a-cafe91372a39" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:44:24.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-kf2tb" for this suite.
Feb 20 19:44:30.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:31.605: INFO: namespace: e2e-tests-pods-kf2tb, resource: bindings, ignored listing per whitelist
Feb 20 19:44:31.848: INFO: namespace e2e-tests-pods-kf2tb deletion completed in 6.957430237s

 [SLOW TEST:14.854 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:44:31.848: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rx4vt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:44:32.960: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-rx4vt" to be "success or failure"
Feb 20 19:44:32.983: INFO: Pod "downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.628289ms
Feb 20 19:44:35.006: INFO: Pod "downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045416497s
Feb 20 19:44:37.030: INFO: Pod "downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0693561s
STEP: Saw pod success
Feb 20 19:44:37.030: INFO: Pod "downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:44:37.056: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:44:37.122: INFO: Waiting for pod downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:44:37.146: INFO: Pod downwardapi-volume-f1ed8a2e-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:44:37.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rx4vt" for this suite.
Feb 20 19:44:43.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:43.401: INFO: namespace: e2e-tests-projected-rx4vt, resource: bindings, ignored listing per whitelist
Feb 20 19:44:44.214: INFO: namespace e2e-tests-projected-rx4vt deletion completed in 7.04485636s

 [SLOW TEST:12.366 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:44:44.214: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-5qsbw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-f9517978-3547-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:44:45.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-5qsbw" to be "success or failure"
Feb 20 19:44:45.407: INFO: Pod "pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.904981ms
Feb 20 19:44:47.431: INFO: Pod "pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046774823s
STEP: Saw pod success
Feb 20 19:44:47.431: INFO: Pod "pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:44:47.455: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:44:47.515: INFO: Waiting for pod pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:44:47.538: INFO: Pod pod-projected-configmaps-f9553810-3547-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:44:47.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-5qsbw" for this suite.
Feb 20 19:44:53.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:44:53.956: INFO: namespace: e2e-tests-projected-5qsbw, resource: bindings, ignored listing per whitelist
Feb 20 19:44:54.602: INFO: namespace e2e-tests-projected-5qsbw deletion completed in 7.039204877s

 [SLOW TEST:10.388 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:44:54.602: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-8fnwg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 20 19:45:00.018: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:00.041: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:02.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:02.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:04.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:04.067: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:06.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:06.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:08.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:08.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:10.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:10.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:12.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:12.066: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:14.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:14.064: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:16.043: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:16.072: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:18.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:18.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:20.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:20.065: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:22.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:22.066: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 20 19:45:24.041: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 20 19:45:24.068: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:45:24.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-8fnwg" for this suite.
Feb 20 19:45:48.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:45:48.604: INFO: namespace: e2e-tests-container-lifecycle-hook-8fnwg, resource: bindings, ignored listing per whitelist
Feb 20 19:45:49.070: INFO: namespace e2e-tests-container-lifecycle-hook-8fnwg deletion completed in 24.977972351s

 [SLOW TEST:54.467 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:45:49.070: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7xrjr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:45:50.351: INFO: Waiting up to 5m0s for pod "downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-downward-api-7xrjr" to be "success or failure"
Feb 20 19:45:50.375: INFO: Pod "downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 23.253557ms
Feb 20 19:45:52.399: INFO: Pod "downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047441649s
STEP: Saw pod success
Feb 20 19:45:52.399: INFO: Pod "downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:45:52.423: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:45:52.484: INFO: Waiting for pod downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:45:52.506: INFO: Pod downwardapi-volume-200e79a9-3548-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:45:52.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7xrjr" for this suite.
Feb 20 19:45:58.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:45:59.430: INFO: namespace: e2e-tests-downward-api-7xrjr, resource: bindings, ignored listing per whitelist
Feb 20 19:45:59.636: INFO: namespace e2e-tests-downward-api-7xrjr deletion completed in 7.086038313s

 [SLOW TEST:10.566 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:45:59.636: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-7288g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-2655f3a5-3548-11e9-9b2a-cafe91372a39
STEP: Creating configMap with name cm-test-opt-upd-2655f3ed-3548-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2655f3a5-3548-11e9-9b2a-cafe91372a39
STEP: Updating configmap cm-test-opt-upd-2655f3ed-3548-11e9-9b2a-cafe91372a39
STEP: Creating configMap with name cm-test-opt-create-2655f402-3548-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:47:10.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-7288g" for this suite.
Feb 20 19:47:32.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:47:33.036: INFO: namespace: e2e-tests-projected-7288g, resource: bindings, ignored listing per whitelist
Feb 20 19:47:33.570: INFO: namespace e2e-tests-projected-7288g deletion completed in 23.013923734s

 [SLOW TEST:93.934 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:47:33.570: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-g8hkx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 20 19:47:34.657: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-g8hkx" to be "success or failure"
Feb 20 19:47:34.680: INFO: Pod "downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.980822ms
Feb 20 19:47:36.704: INFO: Pod "downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046719629s
STEP: Saw pod success
Feb 20 19:47:36.704: INFO: Pod "downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:47:36.728: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39 container client-container: <nil>
STEP: delete the pod
Feb 20 19:47:36.786: INFO: Waiting for pod downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:47:36.809: INFO: Pod downwardapi-volume-5e3a4d6c-3548-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:47:36.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-g8hkx" for this suite.
Feb 20 19:47:42.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:47:43.511: INFO: namespace: e2e-tests-projected-g8hkx, resource: bindings, ignored listing per whitelist
Feb 20 19:47:43.786: INFO: namespace e2e-tests-projected-g8hkx deletion completed in 6.9533421s

 [SLOW TEST:10.215 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:47:43.786: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-mkvr9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 20 19:47:51.128: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:51.128: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:51.747: INFO: Exec stderr: ""
Feb 20 19:47:51.747: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:51.747: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:52.302: INFO: Exec stderr: ""
Feb 20 19:47:52.302: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:52.302: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:52.840: INFO: Exec stderr: ""
Feb 20 19:47:52.840: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:52.840: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:53.347: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 20 19:47:53.347: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:53.347: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:53.929: INFO: Exec stderr: ""
Feb 20 19:47:53.929: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:53.929: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:54.499: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 20 19:47:54.499: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:54.499: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:55.070: INFO: Exec stderr: ""
Feb 20 19:47:55.070: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:55.070: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:55.602: INFO: Exec stderr: ""
Feb 20 19:47:55.602: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:55.602: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:56.248: INFO: Exec stderr: ""
Feb 20 19:47:56.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-mkvr9 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 20 19:47:56.248: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
Feb 20 19:47:56.781: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:47:56.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-mkvr9" for this suite.
Feb 20 19:48:46.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:48:46.988: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-mkvr9, resource: bindings, ignored listing per whitelist
Feb 20 19:48:47.838: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-mkvr9 deletion completed in 51.032440888s

 [SLOW TEST:64.053 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:48:47.839: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-frklw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-frklw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-frklw to expose endpoints map[]
Feb 20 19:48:48.980: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-frklw exposes endpoints map[] (23.366787ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-frklw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-frklw to expose endpoints map[pod1:[80]]
Feb 20 19:48:51.145: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-frklw exposes endpoints map[pod1:[80]] (2.13847543s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-frklw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-frklw to expose endpoints map[pod1:[80] pod2:[80]]
Feb 20 19:48:53.381: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-frklw exposes endpoints map[pod1:[80] pod2:[80]] (2.211908062s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-frklw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-frklw to expose endpoints map[pod2:[80]]
Feb 20 19:48:53.452: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-frklw exposes endpoints map[pod2:[80]] (46.000913ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-frklw
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-frklw to expose endpoints map[]
Feb 20 19:48:53.500: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-frklw exposes endpoints map[] (22.722119ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:48:53.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-frklw" for this suite.
Feb 20 19:49:17.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:18.341: INFO: namespace: e2e-tests-services-frklw, resource: bindings, ignored listing per whitelist
Feb 20 19:49:18.504: INFO: namespace e2e-tests-services-frklw deletion completed in 24.949598393s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

 [SLOW TEST:30.665 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:49:18.504: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-qdj7m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 20 19:49:19.625: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:49:23.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-qdj7m" for this suite.
Feb 20 19:49:29.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:29.586: INFO: namespace: e2e-tests-init-container-qdj7m, resource: bindings, ignored listing per whitelist
Feb 20 19:49:30.167: INFO: namespace e2e-tests-init-container-qdj7m deletion completed in 6.950792316s

 [SLOW TEST:11.663 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:49:30.167: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-tl2rr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 20 19:49:31.258: INFO: Waiting up to 5m0s for pod "pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-emptydir-tl2rr" to be "success or failure"
Feb 20 19:49:31.281: INFO: Pod "pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.585579ms
Feb 20 19:49:33.305: INFO: Pod "pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046803797s
Feb 20 19:49:35.329: INFO: Pod "pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070845298s
STEP: Saw pod success
Feb 20 19:49:35.329: INFO: Pod "pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:49:35.352: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39 container test-container: <nil>
STEP: delete the pod
Feb 20 19:49:35.413: INFO: Waiting for pod pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:49:35.436: INFO: Pod pod-a3ba26cd-3548-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:49:35.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tl2rr" for this suite.
Feb 20 19:49:41.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:42.180: INFO: namespace: e2e-tests-emptydir-tl2rr, resource: bindings, ignored listing per whitelist
Feb 20 19:49:42.411: INFO: namespace e2e-tests-emptydir-tl2rr deletion completed in 6.951630633s

 [SLOW TEST:12.244 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:49:42.411: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-rzmjq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:49:43.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-rzmjq" for this suite.
Feb 20 19:49:49.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:49:50.268: INFO: namespace: e2e-tests-services-rzmjq, resource: bindings, ignored listing per whitelist
Feb 20 19:49:50.597: INFO: namespace e2e-tests-services-rzmjq deletion completed in 7.01784696s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

 [SLOW TEST:8.186 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:49:50.597: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-fxpq4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-fxpq4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fxpq4 to expose endpoints map[]
Feb 20 19:49:51.875: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fxpq4 exposes endpoints map[] (22.770824ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-fxpq4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fxpq4 to expose endpoints map[pod1:[100]]
Feb 20 19:49:54.057: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fxpq4 exposes endpoints map[pod1:[100]] (2.15511484s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-fxpq4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fxpq4 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 20 19:49:56.295: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fxpq4 exposes endpoints map[pod1:[100] pod2:[101]] (2.213284575s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-fxpq4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fxpq4 to expose endpoints map[pod2:[101]]
Feb 20 19:49:56.365: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fxpq4 exposes endpoints map[pod2:[101]] (46.316832ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-fxpq4
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-fxpq4 to expose endpoints map[]
Feb 20 19:49:56.414: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-fxpq4 exposes endpoints map[] (23.161798ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:49:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-fxpq4" for this suite.
Feb 20 19:50:20.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:50:21.334: INFO: namespace: e2e-tests-services-fxpq4, resource: bindings, ignored listing per whitelist
Feb 20 19:50:21.426: INFO: namespace e2e-tests-services-fxpq4 deletion completed in 24.956856539s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

 [SLOW TEST:30.829 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:50:21.426: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rgchg
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-c262a03a-3548-11e9-9b2a-cafe91372a39
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c262a03a-3548-11e9-9b2a-cafe91372a39
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:51:46.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rgchg" for this suite.
Feb 20 19:52:08.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:52:09.124: INFO: namespace: e2e-tests-configmap-rgchg, resource: bindings, ignored listing per whitelist
Feb 20 19:52:09.689: INFO: namespace e2e-tests-configmap-rgchg deletion completed in 22.972276955s

 [SLOW TEST:108.263 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:52:09.689: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-tzq99
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-02db0314-3549-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume configMaps
Feb 20 19:52:10.880: INFO: Waiting up to 5m0s for pod "pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-configmap-tzq99" to be "success or failure"
Feb 20 19:52:10.903: INFO: Pod "pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.977182ms
Feb 20 19:52:12.927: INFO: Pod "pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046637476s
STEP: Saw pod success
Feb 20 19:52:12.927: INFO: Pod "pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:52:12.950: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 20 19:52:13.011: INFO: Waiting for pod pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:52:13.034: INFO: Pod pod-configmaps-02de992f-3549-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:52:13.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-tzq99" for this suite.
Feb 20 19:52:19.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:52:19.515: INFO: namespace: e2e-tests-configmap-tzq99, resource: bindings, ignored listing per whitelist
Feb 20 19:52:20.089: INFO: namespace e2e-tests-configmap-tzq99 deletion completed in 7.030469424s

 [SLOW TEST:10.400 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:52:20.089: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jh9lt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 20 19:52:21.161: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-jh9lt'
Feb 20 19:52:22.485: INFO: stderr: ""
Feb 20 19:52:22.485: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Feb 20 19:52:22.508: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-gcp-w3ymx.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-jh9lt'
Feb 20 19:52:32.986: INFO: stderr: ""
Feb 20 19:52:32.986: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:52:32.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jh9lt" for this suite.
Feb 20 19:52:39.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:52:39.411: INFO: namespace: e2e-tests-kubectl-jh9lt, resource: bindings, ignored listing per whitelist
Feb 20 19:52:39.975: INFO: namespace e2e-tests-kubectl-jh9lt deletion completed in 6.964290779s

 [SLOW TEST:19.886 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 20 19:52:39.975: INFO: >>> kubeConfig: /tmp/build/c6a29f40/git-kubernetes_publish_conf_test_results-master_master/scripts/gcp_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lsxwm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-14fa954a-3549-11e9-9b2a-cafe91372a39
STEP: Creating a pod to test consume secrets
Feb 20 19:52:41.287: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39" in namespace "e2e-tests-projected-lsxwm" to be "success or failure"
Feb 20 19:52:41.309: INFO: Pod "pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39": Phase="Pending", Reason="", readiness=false. Elapsed: 22.514457ms
Feb 20 19:52:43.333: INFO: Pod "pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.046624773s
STEP: Saw pod success
Feb 20 19:52:43.334: INFO: Pod "pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39" satisfied condition "success or failure"
Feb 20 19:52:43.357: INFO: Trying to get logs from node shoot--it--pub-gcp-w3ymx-cpu-worker-z1-6755fbd4b5-t2ktd pod pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 20 19:52:43.415: INFO: Waiting for pod pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39 to disappear
Feb 20 19:52:43.441: INFO: Pod pod-projected-secrets-14fe35d2-3549-11e9-9b2a-cafe91372a39 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 20 19:52:43.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lsxwm" for this suite.
Feb 20 19:52:51.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 20 19:52:52.286: INFO: namespace: e2e-tests-projected-lsxwm, resource: bindings, ignored listing per whitelist
Feb 20 19:52:52.474: INFO: namespace e2e-tests-projected-lsxwm deletion completed in 9.009515334s

 [SLOW TEST:12.499 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSFeb 20 19:52:52.474: INFO: Running AfterSuite actions on all node
Feb 20 19:52:52.474: INFO: Running AfterSuite actions on node 1
Feb 20 19:52:52.474: INFO: Skipping dumping logs from cluster

Ran 187 of 2011 Specs in 6121.845 seconds
SUCCESS! -- 187 Passed | 0 Failed | 0 Flaked | 0 Pending | 1824 Skipped PASS

Ginkgo ran 1 suite in 1h42m2.640155903s
Test Suite Passed
