Conformance test: not doing test setup.
Feb 13 22:47:30.148: INFO: Overriding default scale value of zero to 1
Feb 13 22:47:30.148: INFO: Overriding default milliseconds value of zero to 5000
I0213 22:47:30.542818    3058 e2e.go:304] Starting e2e run "57ec673c-2fe1-11e9-b8b1-3a3e684b6200" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1550098049 - Will randomize all specs
Will run 188 of 2011 specs

Feb 13 22:47:30.698: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 22:47:30.702: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 13 22:47:30.821: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 13 22:47:30.946: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 13 22:47:30.946: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 13 22:47:30.946: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 13 22:47:30.975: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 13 22:47:30.975: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Feb 13 22:47:30.975: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Feb 13 22:47:30.975: INFO: e2e test version: v1.12.5
Feb 13 22:47:30.996: INFO: kube-apiserver version: v1.12.5
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:47:30.996: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
Feb 13 22:47:31.972: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Feb 13 22:47:32.047: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5b6ck
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
[It] should create an rc from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 22:47:32.235: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-5b6ck'
Feb 13 22:47:32.418: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 13 22:47:32.418: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Feb 13 22:47:32.462: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-qtk7c]
Feb 13 22:47:32.462: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-qtk7c" in namespace "e2e-tests-kubectl-5b6ck" to be "running and ready"
Feb 13 22:47:32.485: INFO: Pod "e2e-test-nginx-rc-qtk7c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.879695ms
Feb 13 22:47:34.508: INFO: Pod "e2e-test-nginx-rc-qtk7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045615545s
Feb 13 22:47:36.531: INFO: Pod "e2e-test-nginx-rc-qtk7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068104297s
Feb 13 22:47:38.574: INFO: Pod "e2e-test-nginx-rc-qtk7c": Phase="Running", Reason="", readiness=true. Elapsed: 6.111435797s
Feb 13 22:47:38.574: INFO: Pod "e2e-test-nginx-rc-qtk7c" satisfied condition "running and ready"
Feb 13 22:47:38.574: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-qtk7c]
Feb 13 22:47:38.574: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs rc/e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5b6ck'
Feb 13 22:47:38.973: INFO: stderr: ""
Feb 13 22:47:38.973: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1251
Feb 13 22:47:38.973: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-5b6ck'
Feb 13 22:47:39.197: INFO: stderr: ""
Feb 13 22:47:39.197: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:47:39.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5b6ck" for this suite.
Feb 13 22:48:03.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:48:03.804: INFO: namespace: e2e-tests-kubectl-5b6ck, resource: bindings, ignored listing per whitelist
Feb 13 22:48:04.258: INFO: namespace e2e-tests-kubectl-5b6ck deletion completed in 25.025992537s

• [SLOW TEST:33.262 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run rc
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:48:04.258: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wpltf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:48:05.470: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-wpltf" to be "success or failure"
Feb 13 22:48:05.492: INFO: Pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.904782ms
Feb 13 22:48:07.519: INFO: Pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048383779s
Feb 13 22:48:09.541: INFO: Pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07036685s
Feb 13 22:48:11.564: INFO: Pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.09353742s
STEP: Saw pod success
Feb 13 22:48:11.564: INFO: Pod "downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:48:11.586: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 22:48:11.658: INFO: Waiting for pod downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:48:11.687: INFO: Pod downwardapi-volume-6d012376-2fe1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:48:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wpltf" for this suite.
Feb 13 22:48:46.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:48:47.177: INFO: namespace: e2e-tests-projected-wpltf, resource: bindings, ignored listing per whitelist
Feb 13 22:48:47.594: INFO: namespace e2e-tests-projected-wpltf deletion completed in 35.884981597s

• [SLOW TEST:43.336 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:48:47.595: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mc9rc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support proxy with --port 0  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting the proxy server
Feb 13 22:48:48.726: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:48:48.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mc9rc" for this suite.
Feb 13 22:48:54.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:48:55.492: INFO: namespace: e2e-tests-kubectl-mc9rc, resource: bindings, ignored listing per whitelist
Feb 13 22:48:55.890: INFO: namespace e2e-tests-kubectl-mc9rc deletion completed in 6.965023977s

• [SLOW TEST:8.295 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support proxy with --port 0  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:48:55.890: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vrkjl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1042
STEP: creating the pod
Feb 13 22:48:57.011: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:48:57.403: INFO: stderr: ""
Feb 13 22:48:57.404: INFO: stdout: "pod/pause created\n"
Feb 13 22:48:57.404: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 13 22:48:57.404: INFO: Waiting up to 5m0s for pod "pause" in namespace "e2e-tests-kubectl-vrkjl" to be "running and ready"
Feb 13 22:48:57.426: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.143655ms
Feb 13 22:48:59.449: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045009581s
Feb 13 22:49:01.471: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.067847569s
Feb 13 22:49:01.471: INFO: Pod "pause" satisfied condition "running and ready"
Feb 13 22:49:01.471: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 13 22:49:01.472: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml label pods pause testing-label=testing-label-value --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:01.686: INFO: stderr: ""
Feb 13 22:49:01.686: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 13 22:49:01.686: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:01.869: INFO: stderr: ""
Feb 13 22:49:01.869: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 13 22:49:01.869: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml label pods pause testing-label- --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:02.081: INFO: stderr: ""
Feb 13 22:49:02.081: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 13 22:49:02.081: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod pause -L testing-label --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:02.242: INFO: stderr: ""
Feb 13 22:49:02.242: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1048
STEP: using delete to clean up resources
Feb 13 22:49:02.243: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:02.449: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 22:49:02.450: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 13 22:49:02.450: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=pause --no-headers --namespace=e2e-tests-kubectl-vrkjl'
Feb 13 22:49:02.653: INFO: stderr: "No resources found.\n"
Feb 13 22:49:02.653: INFO: stdout: ""
Feb 13 22:49:02.653: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=pause --namespace=e2e-tests-kubectl-vrkjl -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 13 22:49:02.822: INFO: stderr: ""
Feb 13 22:49:02.822: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:49:02.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vrkjl" for this suite.
Feb 13 22:49:08.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:49:09.143: INFO: namespace: e2e-tests-kubectl-vrkjl, resource: bindings, ignored listing per whitelist
Feb 13 22:49:09.818: INFO: namespace e2e-tests-kubectl-vrkjl deletion completed in 6.973774347s

• [SLOW TEST:13.928 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl label
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update the label on a resource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:49:09.819: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-7qrrk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/wheezy_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-7qrrk.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7qrrk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7qrrk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_udp@kubernetes.default;test -n "$$(dig +tcp +noall +answer +search kubernetes.default A)" && echo OK > /results/jessie_tcp@kubernetes.default;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_udp@kubernetes.default.svc;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc;test -n "$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;test -n "$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;test -n "$$(getent hosts dns-querier-1.dns-test-service.e2e-tests-dns-7qrrk.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.e2e-tests-dns-7qrrk.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-7qrrk.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 13 22:49:44.526: INFO: DNS probes using e2e-tests-dns-7qrrk/dns-test-940ae52b-2fe1-11e9-b8b1-3a3e684b6200 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:49:44.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-7qrrk" for this suite.
Feb 13 22:49:50.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:49:51.253: INFO: namespace: e2e-tests-dns-7qrrk, resource: bindings, ignored listing per whitelist
Feb 13 22:49:51.582: INFO: namespace e2e-tests-dns-7qrrk deletion completed in 6.994045765s

• [SLOW TEST:41.764 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:49:51.583: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-9gzvs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:49:52.779: INFO: (0) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 128.505106ms)
Feb 13 22:49:52.806: INFO: (1) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.315082ms)
Feb 13 22:49:52.831: INFO: (2) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.569044ms)
Feb 13 22:49:52.856: INFO: (3) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.964755ms)
Feb 13 22:49:52.881: INFO: (4) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.23316ms)
Feb 13 22:49:52.907: INFO: (5) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.699392ms)
Feb 13 22:49:52.932: INFO: (6) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.93426ms)
Feb 13 22:49:52.957: INFO: (7) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.585561ms)
Feb 13 22:49:52.983: INFO: (8) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.13524ms)
Feb 13 22:49:53.009: INFO: (9) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.686784ms)
Feb 13 22:49:53.034: INFO: (10) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.812861ms)
Feb 13 22:49:53.059: INFO: (11) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.111311ms)
Feb 13 22:49:53.085: INFO: (12) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.201139ms)
Feb 13 22:49:53.110: INFO: (13) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.707964ms)
Feb 13 22:49:53.136: INFO: (14) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.362087ms)
Feb 13 22:49:53.161: INFO: (15) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.417786ms)
Feb 13 22:49:53.186: INFO: (16) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.023755ms)
Feb 13 22:49:53.212: INFO: (17) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.167708ms)
Feb 13 22:49:53.242: INFO: (18) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 29.80634ms)
Feb 13 22:49:53.267: INFO: (19) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg:10250/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.019544ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:49:53.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-9gzvs" for this suite.
Feb 13 22:49:59.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:49:59.575: INFO: namespace: e2e-tests-proxy-9gzvs, resource: bindings, ignored listing per whitelist
Feb 13 22:50:00.219: INFO: namespace e2e-tests-proxy-9gzvs deletion completed in 6.929592184s

• [SLOW TEST:8.636 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:50:00.219: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-s5grh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 13 22:50:08.074: INFO: Successfully updated pod "pod-update-b2204f96-2fe1-11e9-b8b1-3a3e684b6200"
STEP: verifying the updated pod is in kubernetes
Feb 13 22:50:08.119: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:50:08.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-s5grh" for this suite.
Feb 13 22:50:32.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:50:33.100: INFO: namespace: e2e-tests-pods-s5grh, resource: bindings, ignored listing per whitelist
Feb 13 22:50:33.144: INFO: namespace e2e-tests-pods-s5grh deletion completed in 24.970970417s

• [SLOW TEST:32.925 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:50:33.144: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-q5l4b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service endpoint-test2 in namespace e2e-tests-services-q5l4b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-q5l4b to expose endpoints map[]
Feb 13 22:50:34.274: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-q5l4b exposes endpoints map[] (23.697278ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-q5l4b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-q5l4b to expose endpoints map[pod1:[80]]
Feb 13 22:50:38.548: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-q5l4b exposes endpoints map[pod1:[80]] (4.237478115s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-q5l4b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-q5l4b to expose endpoints map[pod1:[80] pod2:[80]]
Feb 13 22:50:41.842: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-q5l4b exposes endpoints map[pod1:[80] pod2:[80]] (3.269939862s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-q5l4b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-q5l4b to expose endpoints map[pod2:[80]]
Feb 13 22:50:41.909: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-q5l4b exposes endpoints map[pod2:[80]] (43.986834ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-q5l4b
STEP: waiting up to 3m0s for service endpoint-test2 in namespace e2e-tests-services-q5l4b to expose endpoints map[]
Feb 13 22:50:41.960: INFO: successfully validated that service endpoint-test2 in namespace e2e-tests-services-q5l4b exposes endpoints map[] (26.388028ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:50:42.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-q5l4b" for this suite.
Feb 13 22:51:04.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:51:04.400: INFO: namespace: e2e-tests-services-q5l4b, resource: bindings, ignored listing per whitelist
Feb 13 22:51:05.011: INFO: namespace e2e-tests-services-q5l4b deletion completed in 22.986421159s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:31.866 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:51:05.011: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-49jnp
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 13 22:51:06.147: INFO: Waiting up to 5m0s for pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-49jnp" to be "success or failure"
Feb 13 22:51:06.170: INFO: Pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.635188ms
Feb 13 22:51:08.194: INFO: Pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046497105s
Feb 13 22:51:10.216: INFO: Pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069041784s
Feb 13 22:51:12.239: INFO: Pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092342877s
STEP: Saw pod success
Feb 13 22:51:12.239: INFO: Pod "downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:51:12.261: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 13 22:51:12.334: INFO: Waiting for pod downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:51:12.356: INFO: Pod downward-api-d8b22ae4-2fe1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:51:12.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-49jnp" for this suite.
Feb 13 22:51:18.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:51:18.556: INFO: namespace: e2e-tests-downward-api-49jnp, resource: bindings, ignored listing per whitelist
Feb 13 22:51:19.358: INFO: namespace e2e-tests-downward-api-49jnp deletion completed in 6.979806094s

• [SLOW TEST:14.347 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:51:19.358: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-bcjdj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:51:20.507: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 13 22:51:20.556: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 13 22:51:24.606: INFO: Creating deployment "test-rolling-update-deployment"
Feb 13 22:51:24.629: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 13 22:51:24.690: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Feb 13 22:51:26.735: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 13 22:51:26.757: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685695084, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685695084, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685695084, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685695084, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-65b7695dcf\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 22:51:28.780: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 22:51:28.847: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:e2e-tests-deployment-bcjdj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bcjdj/deployments/test-rolling-update-deployment,UID:e3b76a79-2fe1-11e9-9d38-36c391ece45e,ResourceVersion:6047,Generation:1,CreationTimestamp:2019-02-13 22:51:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-13 22:51:24 +0000 UTC 2019-02-13 22:51:24 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-13 22:51:28 +0000 UTC 2019-02-13 22:51:24 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-65b7695dcf" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 13 22:51:28.872: INFO: New ReplicaSet "test-rolling-update-deployment-65b7695dcf" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf,GenerateName:,Namespace:e2e-tests-deployment-bcjdj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bcjdj/replicasets/test-rolling-update-deployment-65b7695dcf,UID:e3bda2e6-2fe1-11e9-9d38-36c391ece45e,ResourceVersion:6040,Generation:1,CreationTimestamp:2019-02-13 22:51:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e3b76a79-2fe1-11e9-9d38-36c391ece45e 0xc00144f687 0xc00144f688}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 13 22:51:28.872: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 13 22:51:28.872: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:e2e-tests-deployment-bcjdj,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-bcjdj/replicasets/test-rolling-update-controller,UID:e145cc13-2fe1-11e9-9d38-36c391ece45e,ResourceVersion:6046,Generation:2,CreationTimestamp:2019-02-13 22:51:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e3b76a79-2fe1-11e9-9d38-36c391ece45e 0xc00144f5bf 0xc00144f5d0}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 22:51:28.895: INFO: Pod "test-rolling-update-deployment-65b7695dcf-xr9rj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-65b7695dcf-xr9rj,GenerateName:test-rolling-update-deployment-65b7695dcf-,Namespace:e2e-tests-deployment-bcjdj,SelfLink:/api/v1/namespaces/e2e-tests-deployment-bcjdj/pods/test-rolling-update-deployment-65b7695dcf-xr9rj,UID:e3be24c4-2fe1-11e9-9d38-36c391ece45e,ResourceVersion:6039,Generation:0,CreationTimestamp:2019-02-13 22:51:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 65b7695dcf,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.10/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-65b7695dcf e3bda2e6-2fe1-11e9-9d38-36c391ece45e 0xc00144ff17 0xc00144ff18}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8xsx2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8xsx2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8xsx2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144ff80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144ffa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:51:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:51:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:51:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 22:51:24 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.10,StartTime:2019-02-13 22:51:24 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-13 22:51:28 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://852fa138e760541d1f046562dd08a0a9ee38c1f572117d9c4a698788a369cce4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:51:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-bcjdj" for this suite.
Feb 13 22:51:34.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:51:35.209: INFO: namespace: e2e-tests-deployment-bcjdj, resource: bindings, ignored listing per whitelist
Feb 13 22:51:35.852: INFO: namespace e2e-tests-deployment-bcjdj deletion completed in 6.933384911s

• [SLOW TEST:16.494 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:51:35.852: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ltgng
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 13 22:51:37.040: INFO: Waiting up to 5m0s for pod "downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-ltgng" to be "success or failure"
Feb 13 22:51:37.070: INFO: Pod "downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 30.449158ms
Feb 13 22:51:39.093: INFO: Pod "downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053105791s
Feb 13 22:51:41.115: INFO: Pod "downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075785674s
STEP: Saw pod success
Feb 13 22:51:41.115: INFO: Pod "downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:51:41.138: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 13 22:51:41.228: INFO: Waiting for pod downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:51:41.250: INFO: Pod downward-api-eb1c12e5-2fe1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:51:41.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ltgng" for this suite.
Feb 13 22:51:47.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:51:48.131: INFO: namespace: e2e-tests-downward-api-ltgng, resource: bindings, ignored listing per whitelist
Feb 13 22:51:48.225: INFO: namespace e2e-tests-downward-api-ltgng deletion completed in 6.952522939s

• [SLOW TEST:12.373 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:51:48.225: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-rphnj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:51:49.332: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-rphnj" to be "success or failure"
Feb 13 22:51:49.354: INFO: Pod "downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.114049ms
Feb 13 22:51:51.377: INFO: Pod "downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045707143s
Feb 13 22:51:53.400: INFO: Pod "downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068401563s
STEP: Saw pod success
Feb 13 22:51:53.400: INFO: Pod "downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:51:53.423: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 22:51:53.494: INFO: Waiting for pod downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:51:53.530: INFO: Pod downwardapi-volume-f26fbc6d-2fe1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:51:53.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-rphnj" for this suite.
Feb 13 22:51:59.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:51:59.925: INFO: namespace: e2e-tests-downward-api-rphnj, resource: bindings, ignored listing per whitelist
Feb 13 22:52:00.535: INFO: namespace e2e-tests-downward-api-rphnj deletion completed in 6.98313184s

• [SLOW TEST:12.310 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:52:00.535: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-cp5g6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 13 22:52:01.906: INFO: Number of nodes with available pods: 0
Feb 13 22:52:01.906: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:52:02.952: INFO: Number of nodes with available pods: 0
Feb 13 22:52:02.952: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:52:03.953: INFO: Number of nodes with available pods: 0
Feb 13 22:52:03.953: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:52:04.952: INFO: Number of nodes with available pods: 0
Feb 13 22:52:04.952: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:52:05.961: INFO: Number of nodes with available pods: 0
Feb 13 22:52:05.961: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:52:06.951: INFO: Number of nodes with available pods: 2
Feb 13 22:52:06.951: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 13 22:52:07.081: INFO: Number of nodes with available pods: 1
Feb 13 22:52:07.081: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn is running more than one daemon pod
Feb 13 22:52:08.126: INFO: Number of nodes with available pods: 1
Feb 13 22:52:08.126: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn is running more than one daemon pod
Feb 13 22:52:09.152: INFO: Number of nodes with available pods: 1
Feb 13 22:52:09.152: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn is running more than one daemon pod
Feb 13 22:52:10.135: INFO: Number of nodes with available pods: 2
Feb 13 22:52:10.135: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-cp5g6, will wait for the garbage collector to delete the pods
Feb 13 22:52:10.275: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.665217ms
Feb 13 22:52:10.375: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.266319ms
Feb 13 22:52:54.897: INFO: Number of nodes with available pods: 0
Feb 13 22:52:54.897: INFO: Number of running nodes: 0, number of available pods: 0
Feb 13 22:52:54.921: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-cp5g6/daemonsets","resourceVersion":"6309"},"items":null}

Feb 13 22:52:54.943: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-cp5g6/pods","resourceVersion":"6309"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:52:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-cp5g6" for this suite.
Feb 13 22:53:01.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:53:01.295: INFO: namespace: e2e-tests-daemonsets-cp5g6, resource: bindings, ignored listing per whitelist
Feb 13 22:53:01.962: INFO: namespace e2e-tests-daemonsets-cp5g6 deletion completed in 6.928636599s

• [SLOW TEST:61.427 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:53:01.962: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-zczsb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 22:53:03.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-zczsb" to be "success or failure"
Feb 13 22:53:03.192: INFO: Pod "downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 28.919878ms
Feb 13 22:53:05.215: INFO: Pod "downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05182278s
Feb 13 22:53:07.238: INFO: Pod "downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075085871s
STEP: Saw pod success
Feb 13 22:53:07.238: INFO: Pod "downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:53:07.260: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 22:53:07.360: INFO: Waiting for pod downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:53:07.382: INFO: Pod downwardapi-volume-1e716caa-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:53:07.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-zczsb" for this suite.
Feb 13 22:53:13.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:53:14.231: INFO: namespace: e2e-tests-projected-zczsb, resource: bindings, ignored listing per whitelist
Feb 13 22:53:14.361: INFO: namespace e2e-tests-projected-zczsb deletion completed in 6.954850039s

• [SLOW TEST:12.399 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set DefaultMode on files [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:53:14.362: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-g2hgq
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's args
Feb 13 22:53:15.629: INFO: Waiting up to 5m0s for pod "var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-var-expansion-g2hgq" to be "success or failure"
Feb 13 22:53:15.651: INFO: Pod "var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.337052ms
Feb 13 22:53:17.674: INFO: Pod "var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045144987s
Feb 13 22:53:19.697: INFO: Pod "var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067824014s
STEP: Saw pod success
Feb 13 22:53:19.697: INFO: Pod "var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:53:19.719: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 13 22:53:19.779: INFO: Waiting for pod var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:53:19.818: INFO: Pod var-expansion-25df9859-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:53:19.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-g2hgq" for this suite.
Feb 13 22:53:27.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:53:28.575: INFO: namespace: e2e-tests-var-expansion-g2hgq, resource: bindings, ignored listing per whitelist
Feb 13 22:53:28.795: INFO: namespace e2e-tests-var-expansion-g2hgq deletion completed in 8.954880103s

• [SLOW TEST:14.434 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:53:28.795: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-p8n77
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-2e68ec39-2fe2-11e9-b8b1-3a3e684b6200
STEP: Creating configMap with name cm-test-opt-upd-2e68ec78-2fe2-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2e68ec39-2fe2-11e9-b8b1-3a3e684b6200
STEP: Updating configmap cm-test-opt-upd-2e68ec78-2fe2-11e9-b8b1-3a3e684b6200
STEP: Creating configMap with name cm-test-opt-create-2e68ec8e-2fe2-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:55:08.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-p8n77" for this suite.
Feb 13 22:55:32.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:55:32.475: INFO: namespace: e2e-tests-projected-p8n77, resource: bindings, ignored listing per whitelist
Feb 13 22:55:33.241: INFO: namespace e2e-tests-projected-p8n77 deletion completed in 24.969956055s

• [SLOW TEST:124.446 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:55:33.242: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-7ttl2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1306
[It] should support rolling-update to same image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 22:55:34.437: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:34.776: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 13 22:55:34.776: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Feb 13 22:55:34.821: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 13 22:55:34.847: INFO: scanned /root for discovery docs: <nil>
Feb 13 22:55:34.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:51.089: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 13 22:55:51.089: INFO: stdout: "Created e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d\nScaling up e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Feb 13 22:55:51.089: INFO: stdout: "Created e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d\nScaling up e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Feb 13 22:55:51.089: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:51.273: INFO: stderr: ""
Feb 13 22:55:51.273: INFO: stdout: "e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d-hc4pm "
Feb 13 22:55:51.273: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d-hc4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:51.449: INFO: stderr: ""
Feb 13 22:55:51.449: INFO: stdout: "true"
Feb 13 22:55:51.449: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d-hc4pm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:51.618: INFO: stderr: ""
Feb 13 22:55:51.618: INFO: stdout: "nginx:1.14-alpine"
Feb 13 22:55:51.618: INFO: e2e-test-nginx-rc-528769d8f63e63f058d34b0463e5886d-hc4pm is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
Feb 13 22:55:51.618: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete rc e2e-test-nginx-rc --namespace=e2e-tests-kubectl-7ttl2'
Feb 13 22:55:51.813: INFO: stderr: ""
Feb 13 22:55:51.813: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:55:51.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-7ttl2" for this suite.
Feb 13 22:56:23.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:56:23.961: INFO: namespace: e2e-tests-kubectl-7ttl2, resource: bindings, ignored listing per whitelist
Feb 13 22:56:24.323: INFO: namespace e2e-tests-kubectl-7ttl2 deletion completed in 32.486641787s

• [SLOW TEST:51.082 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl rolling-update
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support rolling-update to same image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:56:24.324: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-tkskl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0213 22:56:55.606339    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 22:56:55.606: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:56:55.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tkskl" for this suite.
Feb 13 22:57:03.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:57:04.160: INFO: namespace: e2e-tests-gc-tkskl, resource: bindings, ignored listing per whitelist
Feb 13 22:57:04.632: INFO: namespace e2e-tests-gc-tkskl deletion completed in 9.002993194s

• [SLOW TEST:40.308 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:57:04.632: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hg2f2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-af1be705-2fe2-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 22:57:05.896: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-hg2f2" to be "success or failure"
Feb 13 22:57:05.922: INFO: Pod "pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 26.168286ms
Feb 13 22:57:07.946: INFO: Pod "pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049436661s
Feb 13 22:57:09.969: INFO: Pod "pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072548206s
STEP: Saw pod success
Feb 13 22:57:09.969: INFO: Pod "pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:57:09.991: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 13 22:57:10.053: INFO: Waiting for pod pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:57:10.075: INFO: Pod pod-projected-secrets-af1f6bfc-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:57:10.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hg2f2" for this suite.
Feb 13 22:57:16.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:57:16.335: INFO: namespace: e2e-tests-projected-hg2f2, resource: bindings, ignored listing per whitelist
Feb 13 22:57:17.067: INFO: namespace e2e-tests-projected-hg2f2 deletion completed in 6.969046489s

• [SLOW TEST:12.435 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:57:17.067: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-x8sbf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1347
[It] should create a deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 22:57:18.209: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=e2e-tests-kubectl-x8sbf'
Feb 13 22:57:18.449: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 13 22:57:18.449: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1352
Feb 13 22:57:20.512: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-x8sbf'
Feb 13 22:57:20.725: INFO: stderr: ""
Feb 13 22:57:20.725: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:57:20.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-x8sbf" for this suite.
Feb 13 22:58:26.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:58:27.414: INFO: namespace: e2e-tests-kubectl-x8sbf, resource: bindings, ignored listing per whitelist
Feb 13 22:58:27.699: INFO: namespace e2e-tests-kubectl-x8sbf deletion completed in 1m6.942879457s

• [SLOW TEST:70.632 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:58:27.699: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-vb6c8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 13 22:58:28.840: INFO: Waiting up to 5m0s for pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-vb6c8" to be "success or failure"
Feb 13 22:58:28.862: INFO: Pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.248074ms
Feb 13 22:58:30.886: INFO: Pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045973406s
Feb 13 22:58:32.908: INFO: Pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068615431s
Feb 13 22:58:34.932: INFO: Pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091942558s
STEP: Saw pod success
Feb 13 22:58:34.932: INFO: Pod "pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:58:34.954: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 22:58:35.040: INFO: Waiting for pod pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:58:35.062: INFO: Pod pod-e08fc754-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:58:35.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-vb6c8" for this suite.
Feb 13 22:58:41.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:58:41.420: INFO: namespace: e2e-tests-emptydir-vb6c8, resource: bindings, ignored listing per whitelist
Feb 13 22:58:42.031: INFO: namespace e2e-tests-emptydir-vb6c8 deletion completed in 6.945534284s

• [SLOW TEST:14.331 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:58:42.031: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-cx4ll
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override command
Feb 13 22:58:43.141: INFO: Waiting up to 5m0s for pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-containers-cx4ll" to be "success or failure"
Feb 13 22:58:43.171: INFO: Pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 30.39221ms
Feb 13 22:58:45.195: INFO: Pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053835864s
Feb 13 22:58:47.218: INFO: Pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077019053s
Feb 13 22:58:49.241: INFO: Pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.100421061s
STEP: Saw pod success
Feb 13 22:58:49.241: INFO: Pod "client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:58:49.263: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 22:58:49.326: INFO: Waiting for pod client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:58:49.359: INFO: Pod client-containers-e915e648-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:58:49.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-cx4ll" for this suite.
Feb 13 22:58:55.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:58:56.273: INFO: namespace: e2e-tests-containers-cx4ll, resource: bindings, ignored listing per whitelist
Feb 13 22:58:56.317: INFO: namespace e2e-tests-containers-cx4ll deletion completed in 6.934865154s

• [SLOW TEST:14.286 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:58:56.317: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-d7nv6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-f1ad2bba-2fe2-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 13 22:58:57.584: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-d7nv6" to be "success or failure"
Feb 13 22:58:57.606: INFO: Pod "pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.728223ms
Feb 13 22:58:59.629: INFO: Pod "pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044735738s
Feb 13 22:59:01.652: INFO: Pod "pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068277065s
STEP: Saw pod success
Feb 13 22:59:01.652: INFO: Pod "pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:59:01.674: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 22:59:01.740: INFO: Waiting for pod pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:59:01.762: INFO: Pod pod-projected-configmaps-f1b0a158-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:59:01.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-d7nv6" for this suite.
Feb 13 22:59:07.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:59:08.703: INFO: namespace: e2e-tests-projected-d7nv6, resource: bindings, ignored listing per whitelist
Feb 13 22:59:08.704: INFO: namespace e2e-tests-projected-d7nv6 deletion completed in 6.919604637s

• [SLOW TEST:12.386 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:59:08.704: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-5cjjl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 13 22:59:09.834: INFO: Waiting up to 5m0s for pod "pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-5cjjl" to be "success or failure"
Feb 13 22:59:09.856: INFO: Pod "pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.501871ms
Feb 13 22:59:11.880: INFO: Pod "pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046036106s
Feb 13 22:59:13.903: INFO: Pod "pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069012373s
STEP: Saw pod success
Feb 13 22:59:13.903: INFO: Pod "pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 22:59:13.925: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 22:59:14.039: INFO: Waiting for pod pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 22:59:14.084: INFO: Pod pod-f8ff2dc3-2fe2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 22:59:14.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-5cjjl" for this suite.
Feb 13 22:59:20.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 22:59:20.828: INFO: namespace: e2e-tests-emptydir-5cjjl, resource: bindings, ignored listing per whitelist
Feb 13 22:59:21.610: INFO: namespace e2e-tests-emptydir-5cjjl deletion completed in 7.50308887s

• [SLOW TEST:12.906 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 22:59:21.610: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-28gx6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 22:59:22.817: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 13 22:59:22.889: INFO: Number of nodes with available pods: 0
Feb 13 22:59:22.889: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:59:23.935: INFO: Number of nodes with available pods: 0
Feb 13 22:59:23.935: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:59:24.935: INFO: Number of nodes with available pods: 0
Feb 13 22:59:24.935: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 22:59:25.935: INFO: Number of nodes with available pods: 2
Feb 13 22:59:25.935: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 13 22:59:26.098: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:26.098: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:27.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:27.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:28.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:28.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:29.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:29.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:30.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:30.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:31.147: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:31.147: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:32.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:32.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:33.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:33.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:34.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:34.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:35.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:35.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:36.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:36.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:37.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:37.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:38.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:38.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:39.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:39.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:40.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:40.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:41.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:41.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:42.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:42.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:43.143: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:43.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:44.147: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:44.147: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:45.145: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:45.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:46.258: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:46.258: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:47.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:47.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:48.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:48.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:49.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:49.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:50.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:50.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:51.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:51.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:52.145: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:52.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:53.153: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:53.153: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:54.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:54.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:55.145: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:55.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:56.145: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:56.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:57.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:57.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:58.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:58.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:59.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 22:59:59.145: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 22:59:59.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:00.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:00.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:00.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:01.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:01.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:01.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:02.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:02.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:02.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:03.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:03.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:03.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:04.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:04.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:04.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:05.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:05.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:05.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:06.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:06.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:06.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:07.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:07.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:07.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:08.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:08.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:08.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:09.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:09.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:09.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:10.144: INFO: Wrong image for pod: daemon-set-6j8w2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:10.144: INFO: Pod daemon-set-6j8w2 is not available
Feb 13 23:00:10.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:11.144: INFO: Pod daemon-set-smxcw is not available
Feb 13 23:00:11.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:12.144: INFO: Pod daemon-set-smxcw is not available
Feb 13 23:00:12.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:13.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:14.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:15.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:16.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:17.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:18.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:19.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:20.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:21.147: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:22.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:23.145: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:24.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:25.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:26.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:27.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:28.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:29.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:30.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:31.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:32.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:33.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:34.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:35.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:36.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:37.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:38.152: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:39.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:40.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:41.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:42.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:43.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:44.144: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:45.220: INFO: Wrong image for pod: daemon-set-sx96q. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1.
Feb 13 23:00:45.220: INFO: Pod daemon-set-sx96q is not available
Feb 13 23:00:46.144: INFO: Pod daemon-set-bd966 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 13 23:00:46.212: INFO: Number of nodes with available pods: 1
Feb 13 23:00:46.212: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 23:00:47.257: INFO: Number of nodes with available pods: 1
Feb 13 23:00:47.257: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 23:00:48.257: INFO: Number of nodes with available pods: 1
Feb 13 23:00:48.257: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 23:00:49.257: INFO: Number of nodes with available pods: 1
Feb 13 23:00:49.257: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 23:00:50.257: INFO: Number of nodes with available pods: 1
Feb 13 23:00:50.258: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 13 23:00:51.269: INFO: Number of nodes with available pods: 2
Feb 13 23:00:51.269: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-28gx6, will wait for the garbage collector to delete the pods
Feb 13 23:00:51.480: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.762659ms
Feb 13 23:00:51.581: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.270837ms
Feb 13 23:01:04.904: INFO: Number of nodes with available pods: 0
Feb 13 23:01:04.904: INFO: Number of running nodes: 0, number of available pods: 0
Feb 13 23:01:04.926: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-28gx6/daemonsets","resourceVersion":"7626"},"items":null}

Feb 13 23:01:04.951: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-28gx6/pods","resourceVersion":"7626"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:01:05.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-28gx6" for this suite.
Feb 13 23:01:11.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:01:11.946: INFO: namespace: e2e-tests-daemonsets-28gx6, resource: bindings, ignored listing per whitelist
Feb 13 23:01:12.033: INFO: namespace e2e-tests-daemonsets-28gx6 deletion completed in 6.993275625s

• [SLOW TEST:110.423 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:01:12.034: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-phpxl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-428fcf85-2fe3-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:01:13.296: INFO: Waiting up to 5m0s for pod "pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-phpxl" to be "success or failure"
Feb 13 23:01:13.318: INFO: Pod "pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.951491ms
Feb 13 23:01:15.341: INFO: Pod "pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045390734s
Feb 13 23:01:17.371: INFO: Pod "pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074522852s
STEP: Saw pod success
Feb 13 23:01:17.371: INFO: Pod "pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:01:17.393: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:01:17.455: INFO: Waiting for pod pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:01:17.494: INFO: Pod pod-secrets-42934b03-2fe3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:01:17.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-phpxl" for this suite.
Feb 13 23:01:25.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:01:26.279: INFO: namespace: e2e-tests-secrets-phpxl, resource: bindings, ignored listing per whitelist
Feb 13 23:01:26.478: INFO: namespace e2e-tests-secrets-phpxl deletion completed in 8.961922829s

• [SLOW TEST:14.445 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:01:26.479: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-d9d6t
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1402
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 13 23:01:28.432: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-d9d6t'
Feb 13 23:01:30.087: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 13 23:01:30.087: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1407
Feb 13 23:01:30.109: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete jobs e2e-test-nginx-job --namespace=e2e-tests-kubectl-d9d6t'
Feb 13 23:01:30.313: INFO: stderr: ""
Feb 13 23:01:30.313: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:01:30.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-d9d6t" for this suite.
Feb 13 23:01:36.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:01:36.591: INFO: namespace: e2e-tests-kubectl-d9d6t, resource: bindings, ignored listing per whitelist
Feb 13 23:01:37.302: INFO: namespace e2e-tests-kubectl-d9d6t deletion completed in 6.966518078s

• [SLOW TEST:10.824 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image when restart is OnFailure  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:01:37.303: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-kkmvp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating cluster-info
Feb 13 23:01:38.409: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml cluster-info'
Feb 13 23:01:38.577: INFO: stderr: ""
Feb 13 23:01:38.577: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:01:38.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-kkmvp" for this suite.
Feb 13 23:01:44.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:01:44.916: INFO: namespace: e2e-tests-kubectl-kkmvp, resource: bindings, ignored listing per whitelist
Feb 13 23:01:45.518: INFO: namespace e2e-tests-kubectl-kkmvp deletion completed in 6.918336603s

• [SLOW TEST:8.215 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl cluster-info
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:01:45.518: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-pcmzq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 13 23:01:51.368: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5683b925-2fe3-11e9-b8b1-3a3e684b6200"
Feb 13 23:01:51.368: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5683b925-2fe3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-pods-pcmzq" to be "terminated due to deadline exceeded"
Feb 13 23:01:51.391: INFO: Pod "pod-update-activedeadlineseconds-5683b925-2fe3-11e9-b8b1-3a3e684b6200": Phase="Running", Reason="", readiness=true. Elapsed: 22.135012ms
Feb 13 23:01:53.414: INFO: Pod "pod-update-activedeadlineseconds-5683b925-2fe3-11e9-b8b1-3a3e684b6200": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.04504378s
Feb 13 23:01:53.414: INFO: Pod "pod-update-activedeadlineseconds-5683b925-2fe3-11e9-b8b1-3a3e684b6200" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:01:53.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-pcmzq" for this suite.
Feb 13 23:02:01.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:02:01.801: INFO: namespace: e2e-tests-pods-pcmzq, resource: bindings, ignored listing per whitelist
Feb 13 23:02:02.394: INFO: namespace e2e-tests-pods-pcmzq deletion completed in 8.95765487s

• [SLOW TEST:16.876 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:02:02.394: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-gmcz8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:02:03.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-gmcz8" to be "success or failure"
Feb 13 23:02:03.564: INFO: Pod "downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.092764ms
Feb 13 23:02:05.598: INFO: Pod "downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059186345s
Feb 13 23:02:07.625: INFO: Pod "downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086067296s
STEP: Saw pod success
Feb 13 23:02:07.625: INFO: Pod "downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:02:07.648: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:02:07.709: INFO: Waiting for pod downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:02:07.737: INFO: Pod downwardapi-volume-60879a08-2fe3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:02:07.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-gmcz8" for this suite.
Feb 13 23:02:15.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:02:16.551: INFO: namespace: e2e-tests-downward-api-gmcz8, resource: bindings, ignored listing per whitelist
Feb 13 23:02:16.680: INFO: namespace e2e-tests-downward-api-gmcz8 deletion completed in 8.920206613s

• [SLOW TEST:14.286 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:02:16.680: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-bw264
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-691dea55-2fe3-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-691dea55-2fe3-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:03:36.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-bw264" for this suite.
Feb 13 23:03:59.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:03:59.284: INFO: namespace: e2e-tests-configmap-bw264, resource: bindings, ignored listing per whitelist
Feb 13 23:03:59.981: INFO: namespace e2e-tests-configmap-bw264 deletion completed in 23.01484445s

• [SLOW TEST:103.301 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:03:59.982: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-7r2np
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 13 23:04:01.222: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 13 23:04:01.267: INFO: Waiting for terminating namespaces to be deleted...
Feb 13 23:04:01.289: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg before test
Feb 13 23:04:01.332: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-r49j7 from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 13 23:04:01.332: INFO: coredns-5f4748c5f-6746w from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container coredns ready: true, restart count 0
Feb 13 23:04:01.332: INFO: metrics-server-cf4dd5768-cphxg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container metrics-server ready: true, restart count 0
Feb 13 23:04:01.332: INFO: blackbox-exporter-64f6f7f998-7p6v6 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 13 23:04:01.332: INFO: kube-proxy-9n7qg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 13 23:04:01.332: INFO: calico-node-plz52 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 23:04:01.332: INFO: vpn-shoot-84746d495b-vvflf from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 13 23:04:01.332: INFO: addons-nginx-ingress-controller-55d976867d-bcwjt from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 13 23:04:01.332: INFO: addons-kube-lego-648f8c9f5c-f8bfm from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container kube-lego ready: true, restart count 0
Feb 13 23:04:01.332: INFO: addons-kubernetes-dashboard-5f64f76bd-kpc78 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 13 23:04:01.332: INFO: node-exporter-xz5f7 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.332: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 23:04:01.332: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn before test
Feb 13 23:04:01.386: INFO: node-exporter-tvzmp from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.387: INFO: 	Container node-exporter ready: true, restart count 0
Feb 13 23:04:01.387: INFO: calico-node-pwznt from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.387: INFO: 	Container calico-node ready: true, restart count 0
Feb 13 23:04:01.387: INFO: kube-proxy-wnnpd from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 13 23:04:01.387: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a93f63b6-2fe3-11e9-b8b1-3a3e684b6200 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-a93f63b6-2fe3-11e9-b8b1-3a3e684b6200 off the node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a93f63b6-2fe3-11e9-b8b1-3a3e684b6200
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:04:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-7r2np" for this suite.
Feb 13 23:04:23.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:04:24.134: INFO: namespace: e2e-tests-sched-pred-7r2np, resource: bindings, ignored listing per whitelist
Feb 13 23:04:24.738: INFO: namespace e2e-tests-sched-pred-7r2np deletion completed in 14.996869562s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:24.757 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:04:24.738: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bdlh8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 13 23:04:25.928: INFO: Waiting up to 5m0s for pod "pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-bdlh8" to be "success or failure"
Feb 13 23:04:25.950: INFO: Pod "pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.810568ms
Feb 13 23:04:27.974: INFO: Pod "pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046248896s
Feb 13 23:04:29.998: INFO: Pod "pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069876318s
STEP: Saw pod success
Feb 13 23:04:29.998: INFO: Pod "pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:04:30.020: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 23:04:30.089: INFO: Waiting for pod pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:04:30.111: INFO: Pod pod-b567215f-2fe3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:04:30.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bdlh8" for this suite.
Feb 13 23:04:36.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:04:36.576: INFO: namespace: e2e-tests-emptydir-bdlh8, resource: bindings, ignored listing per whitelist
Feb 13 23:04:37.169: INFO: namespace e2e-tests-emptydir-bdlh8 deletion completed in 7.035890098s

• [SLOW TEST:12.431 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:04:37.170: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-k4c4s
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 13 23:04:38.334: INFO: Waiting up to 5m0s for pod "pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-k4c4s" to be "success or failure"
Feb 13 23:04:38.361: INFO: Pod "pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 26.997047ms
Feb 13 23:04:40.384: INFO: Pod "pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050270952s
Feb 13 23:04:42.407: INFO: Pod "pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072787519s
STEP: Saw pod success
Feb 13 23:04:42.407: INFO: Pod "pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:04:42.429: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 23:04:42.490: INFO: Waiting for pod pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:04:42.511: INFO: Pod pod-bccc3ca6-2fe3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:04:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-k4c4s" for this suite.
Feb 13 23:04:48.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:04:48.966: INFO: namespace: e2e-tests-emptydir-k4c4s, resource: bindings, ignored listing per whitelist
Feb 13 23:04:49.552: INFO: namespace e2e-tests-emptydir-k4c4s deletion completed in 7.017557067s

• [SLOW TEST:12.382 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:04:49.552: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-gw9h8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:05:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-gw9h8" for this suite.
Feb 13 23:06:12.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:06:13.536: INFO: namespace: e2e-tests-container-probe-gw9h8, resource: bindings, ignored listing per whitelist
Feb 13 23:06:13.719: INFO: namespace e2e-tests-container-probe-gw9h8 deletion completed in 22.929293357s

• [SLOW TEST:84.167 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:06:13.719: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-fz8dj
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:204
[It] should be submitted and removed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:06:14.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-fz8dj" for this suite.
Feb 13 23:06:37.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:06:37.723: INFO: namespace: e2e-tests-pods-fz8dj, resource: bindings, ignored listing per whitelist
Feb 13 23:06:37.986: INFO: namespace e2e-tests-pods-fz8dj deletion completed in 22.993468665s

• [SLOW TEST:24.267 seconds]
[k8s.io] [sig-node] Pods Extended
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  [k8s.io] Pods Set QOS Class
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be submitted and removed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:06:37.986: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gpjtq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-04be6627-2fe4-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 13 23:06:39.067: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-gpjtq" to be "success or failure"
Feb 13 23:06:39.128: INFO: Pod "pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 60.477321ms
Feb 13 23:06:41.151: INFO: Pod "pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08378541s
Feb 13 23:06:43.175: INFO: Pod "pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107089914s
STEP: Saw pod success
Feb 13 23:06:43.175: INFO: Pod "pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:06:43.197: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:06:43.269: INFO: Waiting for pod pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:06:43.291: INFO: Pod pod-projected-configmaps-04c1ccaa-2fe4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:06:43.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gpjtq" for this suite.
Feb 13 23:06:49.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:06:49.877: INFO: namespace: e2e-tests-projected-gpjtq, resource: bindings, ignored listing per whitelist
Feb 13 23:06:50.293: INFO: namespace e2e-tests-projected-gpjtq deletion completed in 6.968423266s

• [SLOW TEST:12.307 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:06:50.294: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-dsrqn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-dsrqn
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StaefulSet
Feb 13 23:06:51.479: INFO: Found 0 stateful pods, waiting for 3
Feb 13 23:07:01.503: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:01.503: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:01.503: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 13 23:07:01.626: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 13 23:07:01.730: INFO: Updating stateful set ss2
Feb 13 23:07:01.787: INFO: Waiting for Pod e2e-tests-statefulset-dsrqn/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Feb 13 23:07:11.941: INFO: Found 2 stateful pods, waiting for 3
Feb 13 23:07:21.965: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:21.965: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:21.965: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 13 23:07:31.965: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:31.965: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 13 23:07:31.965: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 13 23:07:32.068: INFO: Updating stateful set ss2
Feb 13 23:07:32.121: INFO: Waiting for Pod e2e-tests-statefulset-dsrqn/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:07:42.167: INFO: Waiting for Pod e2e-tests-statefulset-dsrqn/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:07:52.232: INFO: Updating stateful set ss2
Feb 13 23:07:52.283: INFO: Waiting for StatefulSet e2e-tests-statefulset-dsrqn/ss2 to complete update
Feb 13 23:07:52.283: INFO: Waiting for Pod e2e-tests-statefulset-dsrqn/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Feb 13 23:08:02.330: INFO: Waiting for StatefulSet e2e-tests-statefulset-dsrqn/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 13 23:08:12.330: INFO: Deleting all statefulset in ns e2e-tests-statefulset-dsrqn
Feb 13 23:08:12.352: INFO: Scaling statefulset ss2 to 0
Feb 13 23:08:42.457: INFO: Waiting for statefulset status.replicas updated to 0
Feb 13 23:08:42.479: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:08:42.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-dsrqn" for this suite.
Feb 13 23:08:48.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:08:49.194: INFO: namespace: e2e-tests-statefulset-dsrqn, resource: bindings, ignored listing per whitelist
Feb 13 23:08:49.601: INFO: namespace e2e-tests-statefulset-dsrqn deletion completed in 7.017312846s

• [SLOW TEST:119.307 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:08:49.601: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-q9j7g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:08:50.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-q9j7g" to be "success or failure"
Feb 13 23:08:50.858: INFO: Pod "downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.216562ms
Feb 13 23:08:52.881: INFO: Pod "downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044789279s
Feb 13 23:08:54.904: INFO: Pod "downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067772158s
STEP: Saw pod success
Feb 13 23:08:54.904: INFO: Pod "downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:08:54.926: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:08:55.011: INFO: Waiting for pod downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:08:55.033: INFO: Pod downwardapi-volume-534cbc36-2fe4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:08:55.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-q9j7g" for this suite.
Feb 13 23:09:01.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:01.439: INFO: namespace: e2e-tests-downward-api-q9j7g, resource: bindings, ignored listing per whitelist
Feb 13 23:09:02.041: INFO: namespace e2e-tests-downward-api-q9j7g deletion completed in 6.985336452s

• [SLOW TEST:12.440 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:09:02.041: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-h7926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 13 23:09:03.126: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:09:08.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-h7926" for this suite.
Feb 13 23:09:14.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:15.433: INFO: namespace: e2e-tests-init-container-h7926, resource: bindings, ignored listing per whitelist
Feb 13 23:09:15.546: INFO: namespace e2e-tests-init-container-h7926 deletion completed in 6.977140866s

• [SLOW TEST:13.506 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:09:15.547: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-nz5kt
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-62bedef3-2fe4-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:09:16.772: INFO: Waiting up to 5m0s for pod "pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-nz5kt" to be "success or failure"
Feb 13 23:09:16.796: INFO: Pod "pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 24.64364ms
Feb 13 23:09:18.820: INFO: Pod "pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048213201s
Feb 13 23:09:20.843: INFO: Pod "pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07083583s
STEP: Saw pod success
Feb 13 23:09:20.843: INFO: Pod "pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:09:20.864: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:09:20.932: INFO: Waiting for pod pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:09:20.955: INFO: Pod pod-secrets-62c25005-2fe4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:09:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-nz5kt" for this suite.
Feb 13 23:09:29.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:09:29.199: INFO: namespace: e2e-tests-secrets-nz5kt, resource: bindings, ignored listing per whitelist
Feb 13 23:09:30.344: INFO: namespace e2e-tests-secrets-nz5kt deletion completed in 9.354504246s

• [SLOW TEST:14.798 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:09:30.345: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-t8vq5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-t8vq5
Feb 13 23:09:35.582: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-t8vq5
STEP: checking the pod's current state and verifying that restartCount is present
Feb 13 23:09:35.604: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:16:22.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-t8vq5" for this suite.
Feb 13 23:16:28.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:16:29.281: INFO: namespace: e2e-tests-container-probe-t8vq5, resource: bindings, ignored listing per whitelist
Feb 13 23:16:29.433: INFO: namespace e2e-tests-container-probe-t8vq5 deletion completed in 6.959183126s

• [SLOW TEST:419.088 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:16:29.433: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-887kh
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-654de80f-2fe5-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:16:30.560: INFO: Waiting up to 5m0s for pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-887kh" to be "success or failure"
Feb 13 23:16:30.581: INFO: Pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.612586ms
Feb 13 23:16:32.604: INFO: Pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044886835s
Feb 13 23:16:34.628: INFO: Pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068112551s
Feb 13 23:16:36.650: INFO: Pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090746374s
STEP: Saw pod success
Feb 13 23:16:36.650: INFO: Pod "pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:16:36.672: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:16:36.759: INFO: Waiting for pod pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:16:36.792: INFO: Pod pod-secrets-6551312c-2fe5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:16:36.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-887kh" for this suite.
Feb 13 23:16:42.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:16:43.561: INFO: namespace: e2e-tests-secrets-887kh, resource: bindings, ignored listing per whitelist
Feb 13 23:16:43.806: INFO: namespace e2e-tests-secrets-887kh deletion completed in 6.992119936s

• [SLOW TEST:14.373 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:16:43.807: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-mvkx6
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:16:44.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-mvkx6" to be "success or failure"
Feb 13 23:16:44.948: INFO: Pod "downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.37274ms
Feb 13 23:16:46.971: INFO: Pod "downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044297397s
Feb 13 23:16:48.993: INFO: Pod "downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066673327s
STEP: Saw pod success
Feb 13 23:16:48.994: INFO: Pod "downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:16:49.015: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:16:49.077: INFO: Waiting for pod downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:16:49.099: INFO: Pod downwardapi-volume-6de16686-2fe5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:16:49.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-mvkx6" for this suite.
Feb 13 23:16:55.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:16:55.715: INFO: namespace: e2e-tests-downward-api-mvkx6, resource: bindings, ignored listing per whitelist
Feb 13 23:16:56.074: INFO: namespace e2e-tests-downward-api-mvkx6 deletion completed in 6.953606337s

• [SLOW TEST:12.268 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:16:56.075: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-lg4kz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-752d04d6-2fe5-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 13 23:16:57.190: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-lg4kz" to be "success or failure"
Feb 13 23:16:57.235: INFO: Pod "pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 44.78703ms
Feb 13 23:16:59.257: INFO: Pod "pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066854378s
Feb 13 23:17:01.280: INFO: Pod "pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089357259s
STEP: Saw pod success
Feb 13 23:17:01.280: INFO: Pod "pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:17:01.302: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:17:01.372: INFO: Waiting for pod pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:17:01.394: INFO: Pod pod-projected-configmaps-7530c91c-2fe5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:17:01.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-lg4kz" for this suite.
Feb 13 23:17:07.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:17:07.882: INFO: namespace: e2e-tests-projected-lg4kz, resource: bindings, ignored listing per whitelist
Feb 13 23:17:08.364: INFO: namespace e2e-tests-projected-lg4kz deletion completed in 6.947043279s

• [SLOW TEST:12.289 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:17:08.364: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-n9l7z
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-n9l7z
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 13 23:17:09.696: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 13 23:17:38.145: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.58:8080/dial?request=hostName&protocol=http&host=100.96.0.17&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-n9l7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 23:17:38.145: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 23:17:38.747: INFO: Waiting for endpoints: map[]
Feb 13 23:17:38.771: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.58:8080/dial?request=hostName&protocol=http&host=100.96.1.57&port=8080&tries=1'] Namespace:e2e-tests-pod-network-test-n9l7z PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 13 23:17:38.771: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 13 23:17:39.290: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:17:39.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-n9l7z" for this suite.
Feb 13 23:18:03.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:18:03.713: INFO: namespace: e2e-tests-pod-network-test-n9l7z, resource: bindings, ignored listing per whitelist
Feb 13 23:18:04.277: INFO: namespace e2e-tests-pod-network-test-n9l7z deletion completed in 24.963731696s

• [SLOW TEST:55.913 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:18:04.277: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-ffqkh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 13 23:18:10.192: INFO: Successfully updated pod "annotationupdate9ddd06e7-2fe5-11e9-b8b1-3a3e684b6200"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:18:12.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-ffqkh" for this suite.
Feb 13 23:18:36.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:18:36.804: INFO: namespace: e2e-tests-projected-ffqkh, resource: bindings, ignored listing per whitelist
Feb 13 23:18:37.261: INFO: namespace e2e-tests-projected-ffqkh deletion completed in 24.983298997s

• [SLOW TEST:32.984 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:18:37.262: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bgwhh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b18a2f8a-2fe5-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b18a2f8a-2fe5-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:18:46.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bgwhh" for this suite.
Feb 13 23:31:12.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:31:12.693: INFO: namespace: e2e-tests-projected-bgwhh, resource: bindings, ignored listing per whitelist
Feb 13 23:31:13.241: INFO: namespace e2e-tests-projected-bgwhh deletion completed in 12m26.320448925s

• [SLOW TEST:755.979 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:31:13.241: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-events-n98xv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 13 23:31:18.626: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-7435c9f5-2fe7-11e9-b8b1-3a3e684b6200,GenerateName:,Namespace:e2e-tests-events-n98xv,SelfLink:/api/v1/namespaces/e2e-tests-events-n98xv/pods/send-events-7435c9f5-2fe7-11e9-b8b1-3a3e684b6200,UID:7436760c-2fe7-11e9-9d38-36c391ece45e,ResourceVersion:11779,Generation:0,CreationTimestamp:2019-02-13 23:31:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 514384395,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.61/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-v47kc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-v47kc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-v47kc true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001de07d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001de07f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:31:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:31:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:31:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:31:14 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.61,StartTime:2019-02-13 23:31:14 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-02-13 23:31:16 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://02ebadce94cd859f4e90baa3b389116ed545770ecf3d4b606816f07451a37e95}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Feb 13 23:31:20.650: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 13 23:31:22.674: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:31:22.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-events-n98xv" for this suite.
Feb 13 23:32:02.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:03.288: INFO: namespace: e2e-tests-events-n98xv, resource: bindings, ignored listing per whitelist
Feb 13 23:32:03.660: INFO: namespace e2e-tests-events-n98xv deletion completed in 40.938240059s

• [SLOW TEST:50.420 seconds]
[k8s.io] [sig-node] Events
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:32:03.661: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-8cz2g
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:32:04.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-8cz2g" for this suite.
Feb 13 23:32:10.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:11.287: INFO: namespace: e2e-tests-services-8cz2g, resource: bindings, ignored listing per whitelist
Feb 13 23:32:11.769: INFO: namespace e2e-tests-services-8cz2g deletion completed in 6.914669859s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:8.108 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:32:11.769: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-d67s4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 13 23:32:12.830: INFO: Waiting up to 5m0s for pod "pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-d67s4" to be "success or failure"
Feb 13 23:32:12.851: INFO: Pod "pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.822237ms
Feb 13 23:32:14.875: INFO: Pod "pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045035674s
Feb 13 23:32:16.910: INFO: Pod "pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.080353304s
STEP: Saw pod success
Feb 13 23:32:16.910: INFO: Pod "pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:32:16.937: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 23:32:17.022: INFO: Waiting for pod pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:32:17.049: INFO: Pod pod-96f435a8-2fe7-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:32:17.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-d67s4" for this suite.
Feb 13 23:32:23.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:23.471: INFO: namespace: e2e-tests-emptydir-d67s4, resource: bindings, ignored listing per whitelist
Feb 13 23:32:24.098: INFO: namespace e2e-tests-emptydir-d67s4 deletion completed in 7.026957608s

• [SLOW TEST:12.329 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:32:24.098: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-d8l5m
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-d8l5m/configmap-test-9e592d62-2fe7-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 13 23:32:25.281: INFO: Waiting up to 5m0s for pod "pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-d8l5m" to be "success or failure"
Feb 13 23:32:25.302: INFO: Pod "pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.654517ms
Feb 13 23:32:27.325: INFO: Pod "pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044463468s
Feb 13 23:32:29.348: INFO: Pod "pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06720939s
STEP: Saw pod success
Feb 13 23:32:29.348: INFO: Pod "pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:32:29.370: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200 container env-test: <nil>
STEP: delete the pod
Feb 13 23:32:29.434: INFO: Waiting for pod pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:32:29.464: INFO: Pod pod-configmaps-9e600051-2fe7-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:32:29.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-d8l5m" for this suite.
Feb 13 23:32:35.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:36.291: INFO: namespace: e2e-tests-configmap-d8l5m, resource: bindings, ignored listing per whitelist
Feb 13 23:32:36.422: INFO: namespace e2e-tests-configmap-d8l5m deletion completed in 6.936102886s

• [SLOW TEST:12.324 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via environment variable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:32:36.422: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2lgqg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:32:37.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-2lgqg" to be "success or failure"
Feb 13 23:32:37.561: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.754523ms
Feb 13 23:32:39.584: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044446727s
Feb 13 23:32:41.607: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068373563s
Feb 13 23:32:43.630: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090734511s
Feb 13 23:32:45.653: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113541706s
Feb 13 23:32:47.676: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.136502335s
STEP: Saw pod success
Feb 13 23:32:47.676: INFO: Pod "downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:32:47.697: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:32:47.766: INFO: Waiting for pod downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:32:47.788: INFO: Pod downwardapi-volume-a5ae8631-2fe7-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:32:47.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2lgqg" for this suite.
Feb 13 23:32:53.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:32:54.288: INFO: namespace: e2e-tests-downward-api-2lgqg, resource: bindings, ignored listing per whitelist
Feb 13 23:49:01.399: INFO: Couldn't delete ns: "e2e-tests-downward-api-2lgqg": Get https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-downward-api-2lgqg/persistentvolumeclaims: read tcp 10.254.0.202:58998->168.63.23.100:443: read: connection timed out (&url.Error{Op:"Get", URL:"https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-downward-api-2lgqg/persistentvolumeclaims", Err:(*net.OpError)(0xc001cec000)})

• Failure in Spec Teardown (AfterEach) [984.978 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] [AfterEach]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 13 23:49:01.399: Couldn't delete ns: "e2e-tests-downward-api-2lgqg": Get https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-downward-api-2lgqg/persistentvolumeclaims: read tcp 10.254.0.202:58998->168.63.23.100:443: read: connection timed out (&url.Error{Op:"Get", URL:"https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com/api/v1/namespaces/e2e-tests-downward-api-2lgqg/persistentvolumeclaims", Err:(*net.OpError)(0xc001cec000)})

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:343
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:49:01.400: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-dln45
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:49:02.733: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-dln45" to be "success or failure"
Feb 13 23:49:02.764: INFO: Pod "downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 31.118059ms
Feb 13 23:49:04.786: INFO: Pod "downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053261891s
Feb 13 23:49:06.808: INFO: Pod "downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075527753s
STEP: Saw pod success
Feb 13 23:49:06.808: INFO: Pod "downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:49:06.830: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:49:06.894: INFO: Waiting for pod downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:49:06.915: INFO: Pod downwardapi-volume-f0e7324c-2fe9-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:49:06.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-dln45" for this suite.
Feb 13 23:49:13.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:49:13.715: INFO: namespace: e2e-tests-downward-api-dln45, resource: bindings, ignored listing per whitelist
Feb 13 23:49:13.872: INFO: namespace e2e-tests-downward-api-dln45 deletion completed in 6.934346077s

• [SLOW TEST:12.472 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:49:13.872: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-tffs8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 13 23:49:23.252: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:23.275: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:25.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:25.301: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:27.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:27.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:29.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:29.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:31.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:31.298: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:33.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:33.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:35.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:35.298: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:37.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:37.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:39.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:39.298: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:41.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:41.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:43.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:43.396: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:45.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:45.297: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:47.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:47.298: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:49.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:49.325: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 13 23:49:51.275: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 13 23:49:51.297: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:49:51.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-tffs8" for this suite.
Feb 13 23:50:15.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:15.878: INFO: namespace: e2e-tests-container-lifecycle-hook-tffs8, resource: bindings, ignored listing per whitelist
Feb 13 23:50:16.263: INFO: namespace e2e-tests-container-lifecycle-hook-tffs8 deletion completed in 24.94419173s

• [SLOW TEST:62.391 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:50:16.263: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-tmjmx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should create and stop a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 13 23:50:17.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:19.035: INFO: stderr: ""
Feb 13 23:50:19.035: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 13 23:50:19.035: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:19.230: INFO: stderr: ""
Feb 13 23:50:19.230: INFO: stdout: "update-demo-nautilus-68v6p update-demo-nautilus-sp82s "
Feb 13 23:50:19.230: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-68v6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:19.400: INFO: stderr: ""
Feb 13 23:50:19.400: INFO: stdout: ""
Feb 13 23:50:19.400: INFO: update-demo-nautilus-68v6p is created but not running
Feb 13 23:50:24.400: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:24.575: INFO: stderr: ""
Feb 13 23:50:24.575: INFO: stdout: "update-demo-nautilus-68v6p update-demo-nautilus-sp82s "
Feb 13 23:50:24.575: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-68v6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:24.752: INFO: stderr: ""
Feb 13 23:50:24.752: INFO: stdout: "true"
Feb 13 23:50:24.752: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-68v6p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:24.933: INFO: stderr: ""
Feb 13 23:50:24.933: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 13 23:50:24.933: INFO: validating pod update-demo-nautilus-68v6p
Feb 13 23:50:25.048: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 13 23:50:25.048: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 13 23:50:25.049: INFO: update-demo-nautilus-68v6p is verified up and running
Feb 13 23:50:25.049: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-sp82s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:25.219: INFO: stderr: ""
Feb 13 23:50:25.219: INFO: stdout: "true"
Feb 13 23:50:25.219: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-sp82s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:25.395: INFO: stderr: ""
Feb 13 23:50:25.395: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 13 23:50:25.395: INFO: validating pod update-demo-nautilus-sp82s
Feb 13 23:50:25.506: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 13 23:50:25.506: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 13 23:50:25.506: INFO: update-demo-nautilus-sp82s is verified up and running
STEP: using delete to clean up resources
Feb 13 23:50:25.506: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:25.703: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 13 23:50:25.703: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 13 23:50:25.703: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-tmjmx'
Feb 13 23:50:25.898: INFO: stderr: "No resources found.\n"
Feb 13 23:50:25.898: INFO: stdout: ""
Feb 13 23:50:25.898: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-tmjmx -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 13 23:50:26.076: INFO: stderr: ""
Feb 13 23:50:26.076: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:50:26.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-tmjmx" for this suite.
Feb 13 23:50:32.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:32.485: INFO: namespace: e2e-tests-kubectl-tmjmx, resource: bindings, ignored listing per whitelist
Feb 13 23:50:33.291: INFO: namespace e2e-tests-kubectl-tmjmx deletion completed in 7.192979401s

• [SLOW TEST:17.027 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:50:33.291: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-9h2md
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:50:34.499: INFO: Creating deployment "test-recreate-deployment"
Feb 13 23:50:34.522: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 13 23:50:34.580: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 13 23:50:34.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-recreate-deployment-79f694ff59\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:50:36.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685698634, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-79f694ff59\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 13 23:50:38.625: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 13 23:50:38.669: INFO: Updating deployment test-recreate-deployment
Feb 13 23:50:38.669: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 13 23:50:39.080: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:e2e-tests-deployment-9h2md,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9h2md/deployments/test-recreate-deployment,UID:279ddfa0-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14233,Generation:2,CreationTimestamp:2019-02-13 23:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-02-13 23:50:39 +0000 UTC 2019-02-13 23:50:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-13 23:50:39 +0000 UTC 2019-02-13 23:50:34 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-7cf749666b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Feb 13 23:50:39.122: INFO: New ReplicaSet "test-recreate-deployment-7cf749666b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b,GenerateName:,Namespace:e2e-tests-deployment-9h2md,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9h2md/replicasets/test-recreate-deployment-7cf749666b,UID:2a42bc53-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14231,Generation:1,CreationTimestamp:2019-02-13 23:50:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 279ddfa0-2fea-11e9-9d38-36c391ece45e 0xc001e2e817 0xc001e2e818}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:50:39.122: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 13 23:50:39.122: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-79f694ff59,GenerateName:,Namespace:e2e-tests-deployment-9h2md,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-9h2md/replicasets/test-recreate-deployment-79f694ff59,UID:279f34e0-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14224,Generation:2,CreationTimestamp:2019-02-13 23:50:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 279ddfa0-2fea-11e9-9d38-36c391ece45e 0xc001e2e757 0xc001e2e758}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 79f694ff59,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 13 23:50:39.223: INFO: Pod "test-recreate-deployment-7cf749666b-v8pmp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7cf749666b-v8pmp,GenerateName:test-recreate-deployment-7cf749666b-,Namespace:e2e-tests-deployment-9h2md,SelfLink:/api/v1/namespaces/e2e-tests-deployment-9h2md/pods/test-recreate-deployment-7cf749666b-v8pmp,UID:2a448e1f-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14229,Generation:0,CreationTimestamp:2019-02-13 23:50:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7cf749666b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-7cf749666b 2a42bc53-2fea-11e9-9d38-36c391ece45e 0xc001e2f067 0xc001e2f068}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-jt9fn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jt9fn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jt9fn true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001e2f0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001e2f0f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:50:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:50:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:50:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-13 23:50:38 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-13 23:50:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:50:39.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-9h2md" for this suite.
Feb 13 23:50:47.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:48.221: INFO: namespace: e2e-tests-deployment-9h2md, resource: bindings, ignored listing per whitelist
Feb 13 23:50:48.263: INFO: namespace e2e-tests-deployment-9h2md deletion completed in 8.941210676s

• [SLOW TEST:14.972 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:50:48.263: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-vlrm9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 13 23:50:49.502: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vlrm9,SelfLink:/api/v1/namespaces/e2e-tests-watch-vlrm9/configmaps/e2e-watch-test-watch-closed,UID:308526dd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14274,Generation:0,CreationTimestamp:2019-02-13 23:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 13 23:50:49.502: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vlrm9,SelfLink:/api/v1/namespaces/e2e-tests-watch-vlrm9/configmaps/e2e-watch-test-watch-closed,UID:308526dd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14275,Generation:0,CreationTimestamp:2019-02-13 23:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 13 23:50:49.589: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vlrm9,SelfLink:/api/v1/namespaces/e2e-tests-watch-vlrm9/configmaps/e2e-watch-test-watch-closed,UID:308526dd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14276,Generation:0,CreationTimestamp:2019-02-13 23:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 13 23:50:49.589: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-vlrm9,SelfLink:/api/v1/namespaces/e2e-tests-watch-vlrm9/configmaps/e2e-watch-test-watch-closed,UID:308526dd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:14277,Generation:0,CreationTimestamp:2019-02-13 23:50:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:50:49.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-vlrm9" for this suite.
Feb 13 23:50:55.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:50:56.223: INFO: namespace: e2e-tests-watch-vlrm9, resource: bindings, ignored listing per whitelist
Feb 13 23:50:56.550: INFO: namespace e2e-tests-watch-vlrm9 deletion completed in 6.937869045s

• [SLOW TEST:8.287 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:50:56.550: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-q7krx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name projected-secret-test-3582182a-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:50:57.863: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-q7krx" to be "success or failure"
Feb 13 23:50:57.907: INFO: Pod "pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 43.621411ms
Feb 13 23:50:59.929: INFO: Pod "pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066119758s
Feb 13 23:51:01.963: INFO: Pod "pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100500784s
STEP: Saw pod success
Feb 13 23:51:01.963: INFO: Pod "pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:51:01.985: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:51:02.053: INFO: Waiting for pod pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:51:02.074: INFO: Pod pod-projected-secrets-3585674a-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:51:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-q7krx" for this suite.
Feb 13 23:51:08.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:51:08.798: INFO: namespace: e2e-tests-projected-q7krx, resource: bindings, ignored listing per whitelist
Feb 13 23:51:09.091: INFO: namespace e2e-tests-projected-q7krx deletion completed in 6.994595537s

• [SLOW TEST:12.541 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:51:09.091: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-4qdp2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 13 23:51:18.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:18.368: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:20.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:20.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:22.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:22.393: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:24.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:24.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:26.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:26.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:28.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:28.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:30.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:30.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:32.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:32.392: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:34.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:34.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:36.370: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:36.395: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:38.370: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:38.392: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:40.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:40.391: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 13 23:51:42.369: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 13 23:51:42.391: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:51:42.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-4qdp2" for this suite.
Feb 13 23:52:06.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:07.111: INFO: namespace: e2e-tests-container-lifecycle-hook-4qdp2, resource: bindings, ignored listing per whitelist
Feb 13 23:52:07.392: INFO: namespace e2e-tests-container-lifecycle-hook-4qdp2 deletion completed in 24.934582869s

• [SLOW TEST:58.301 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:52:07.393: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-mr6wl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0213 23:52:18.880813    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:52:18.880: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:52:18.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-mr6wl" for this suite.
Feb 13 23:52:26.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:27.258: INFO: namespace: e2e-tests-gc-mr6wl, resource: bindings, ignored listing per whitelist
Feb 13 23:52:27.905: INFO: namespace e2e-tests-gc-mr6wl deletion completed in 9.0021061s

• [SLOW TEST:20.512 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Projected 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:52:27.905: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-t9vj9
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 13 23:52:29.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-t9vj9" to be "success or failure"
Feb 13 23:52:29.073: INFO: Pod "downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.245435ms
Feb 13 23:52:31.096: INFO: Pod "downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044651042s
Feb 13 23:52:33.120: INFO: Pod "downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068106044s
STEP: Saw pod success
Feb 13 23:52:33.120: INFO: Pod "downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:52:33.141: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 13 23:52:33.238: INFO: Waiting for pod downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:52:33.259: INFO: Pod downwardapi-volume-6be0ed8f-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:52:33.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-t9vj9" for this suite.
Feb 13 23:52:41.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:41.606: INFO: namespace: e2e-tests-projected-t9vj9, resource: bindings, ignored listing per whitelist
Feb 13 23:52:42.212: INFO: namespace e2e-tests-projected-t9vj9 deletion completed in 8.929852055s

• [SLOW TEST:14.307 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:52:42.212: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-c9ks4
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-74657edf-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:52:43.363: INFO: Waiting up to 5m0s for pod "pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-c9ks4" to be "success or failure"
Feb 13 23:52:43.385: INFO: Pod "pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.440759ms
Feb 13 23:52:45.408: INFO: Pod "pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04419022s
Feb 13 23:52:47.431: INFO: Pod "pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067583386s
STEP: Saw pod success
Feb 13 23:52:47.431: INFO: Pod "pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:52:47.453: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:52:47.523: INFO: Waiting for pod pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:52:47.545: INFO: Pod pod-secrets-7468cd15-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:52:47.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-c9ks4" for this suite.
Feb 13 23:52:53.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:52:53.998: INFO: namespace: e2e-tests-secrets-c9ks4, resource: bindings, ignored listing per whitelist
Feb 13 23:52:54.453: INFO: namespace e2e-tests-secrets-c9ks4 deletion completed in 6.886417102s

• [SLOW TEST:12.241 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:52:54.453: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qplkh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 13 23:52:55.517: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml version --client'
Feb 13 23:52:55.588: INFO: stderr: ""
Feb 13 23:52:55.589: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-13T22:45:47Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Feb 13 23:52:55.609: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-qplkh'
Feb 13 23:52:56.075: INFO: stderr: ""
Feb 13 23:52:56.076: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 13 23:52:56.076: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-qplkh'
Feb 13 23:52:56.398: INFO: stderr: ""
Feb 13 23:52:56.398: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 13 23:52:57.421: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:52:57.421: INFO: Found 0 / 1
Feb 13 23:52:58.421: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:52:58.421: INFO: Found 0 / 1
Feb 13 23:52:59.422: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:52:59.422: INFO: Found 1 / 1
Feb 13 23:52:59.422: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 13 23:52:59.443: INFO: Selector matched 1 pods for map[app:redis]
Feb 13 23:52:59.443: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 13 23:52:59.443: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe pod redis-master-rbl2h --namespace=e2e-tests-kubectl-qplkh'
Feb 13 23:52:59.664: INFO: stderr: ""
Feb 13 23:52:59.664: INFO: stdout: "Name:               redis-master-rbl2h\nNamespace:          e2e-tests-kubectl-qplkh\nPriority:           0\nPriorityClassName:  <none>\nNode:               shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn/10.250.0.5\nStart Time:         Wed, 13 Feb 2019 23:52:56 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 100.96.1.80/32\n                    kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 100.96.1.80\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://9a6ffca467be6fa2dec5735459eb23f20d4d5208dde5065332b6f186f380f30d\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 13 Feb 2019 23:52:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-drg9l (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-drg9l:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-drg9l\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                          Message\n  ----    ------     ----  ----                                                          -------\n  Normal  Scheduled  3s    default-scheduler                                             Successfully assigned e2e-tests-kubectl-qplkh/redis-master-rbl2h to shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn\n  Normal  Pulled     2s    kubelet, shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Created container\n  Normal  Started    1s    kubelet, shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Started container\n"
Feb 13 23:52:59.664: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe rc redis-master --namespace=e2e-tests-kubectl-qplkh'
Feb 13 23:52:59.939: INFO: stderr: ""
Feb 13 23:52:59.939: INFO: stdout: "Name:         redis-master\nNamespace:    e2e-tests-kubectl-qplkh\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-rbl2h\n"
Feb 13 23:52:59.939: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe service redis-master --namespace=e2e-tests-kubectl-qplkh'
Feb 13 23:53:00.236: INFO: stderr: ""
Feb 13 23:53:00.236: INFO: stdout: "Name:              redis-master\nNamespace:         e2e-tests-kubectl-qplkh\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.65.181.6\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.80:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 13 23:53:00.258: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg'
Feb 13 23:53:00.559: INFO: stderr: ""
Feb 13 23:53:00.559: INFO: stdout: "Name:               shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_DS2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=westeurope\n                    failure-domain.beta.kubernetes.io/zone=0\n                    kubernetes.io/hostname=shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg\n                    kubernetes.io/role=node\n                    node-role.kubernetes.io/node=\n                    worker.garden.sapcloud.io/group=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.4/19\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 13 Feb 2019 22:10:09 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 13 Feb 2019 22:11:17 +0000   Wed, 13 Feb 2019 22:11:17 +0000   RouteCreated                 RouteController created a route\n  OutOfDisk            False   Wed, 13 Feb 2019 23:52:51 +0000   Wed, 13 Feb 2019 22:10:09 +0000   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure       False   Wed, 13 Feb 2019 23:52:51 +0000   Wed, 13 Feb 2019 22:10:09 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 13 Feb 2019 23:52:51 +0000   Wed, 13 Feb 2019 22:10:09 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 13 Feb 2019 23:52:51 +0000   Wed, 13 Feb 2019 22:10:09 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 13 Feb 2019 23:52:51 +0000   Wed, 13 Feb 2019 22:10:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.4\n  Hostname:    shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg\nCapacity:\n attachable-volumes-azure-disk:  8\n cpu:                            2\n ephemeral-storage:              33136428Ki\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         7115804Ki\n pods:                           110\nAllocatable:\n attachable-volumes-azure-disk:  8\n cpu:                            1920m\n ephemeral-storage:              32235117134\n hugepages-1Gi:                  0\n hugepages-2Mi:                  0\n memory:                         5848512302\n pods:                           110\nSystem Info:\n Machine ID:                 473d2d9a599e438db8564f92f79b3a1b\n System UUID:                89E850AF-35D5-5641-8E07-638838FD3A70\n Boot ID:                    03a78999-1039-4ab1-afbd-53d2c39e0626\n Kernel Version:             4.14.96-coreos\n OS Image:                   Container Linux by CoreOS 1967.5.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.12.5\n Kube-Proxy Version:         v1.12.5\nPodCIDR:                     100.96.0.0/24\nProviderID:                  azure:///subscriptions/00d2caa5-cd29-46f7-845a-2f8ee0360ef5/resourceGroups/shoot--it--pub-az-6i4kt/providers/Microsoft.Compute/virtualMachines/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                                                               ------------  ----------  ---------------  -------------\n  kube-system                addons-kube-lego-648f8c9f5c-f8bfm                                  20m (1%)      50m (2%)    8Mi (0%)         32Mi (0%)\n  kube-system                addons-kubernetes-dashboard-5f64f76bd-kpc78                        50m (2%)      100m (5%)   50Mi (0%)        256Mi (4%)\n  kube-system                addons-nginx-ingress-controller-55d976867d-bcwjt                   100m (5%)     2 (104%)    100Mi (1%)       800Mi (14%)\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-r49j7    0 (0%)        0 (0%)      0 (0%)           0 (0%)\n  kube-system                blackbox-exporter-64f6f7f998-7p6v6                                 5m (0%)       10m (0%)    5Mi (0%)         35Mi (0%)\n  kube-system                calico-node-plz52                                                  100m (5%)     500m (26%)  100Mi (1%)       700Mi (12%)\n  kube-system                coredns-5f4748c5f-6746w                                            50m (2%)      100m (5%)   15Mi (0%)        100Mi (1%)\n  kube-system                kube-proxy-9n7qg                                                   20m (1%)      900m (46%)  64Mi (1%)        200Mi (3%)\n  kube-system                metrics-server-cf4dd5768-cphxg                                     20m (1%)      80m (4%)    100Mi (1%)       400Mi (7%)\n  kube-system                node-exporter-xz5f7                                                5m (0%)       15m (0%)    10Mi (0%)        50Mi (0%)\n  kube-system                vpn-shoot-84746d495b-vvflf                                         50m (2%)      100m (5%)   50Mi (0%)        100Mi (1%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                       Requests    Limits\n  --------                       --------    ------\n  cpu                            420m (21%)  3855m (200%)\n  memory                         502Mi (9%)  2673Mi (47%)\n  attachable-volumes-azure-disk  0           0\nEvents:                          <none>\n"
Feb 13 23:53:00.559: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml describe namespace e2e-tests-kubectl-qplkh'
Feb 13 23:53:00.840: INFO: stderr: ""
Feb 13 23:53:00.840: INFO: stdout: "Name:         e2e-tests-kubectl-qplkh\nLabels:       e2e-framework=kubectl\n              e2e-run=57ec673c-2fe1-11e9-b8b1-3a3e684b6200\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:53:00.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qplkh" for this suite.
Feb 13 23:53:24.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:53:25.475: INFO: namespace: e2e-tests-kubectl-qplkh, resource: bindings, ignored listing per whitelist
Feb 13 23:53:25.839: INFO: namespace e2e-tests-kubectl-qplkh deletion completed in 24.976182571s

• [SLOW TEST:31.386 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl describe
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:53:25.840: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-cch5d
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-8e6ed848-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:53:27.051: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-cch5d" to be "success or failure"
Feb 13 23:53:27.073: INFO: Pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.314852ms
Feb 13 23:53:29.095: INFO: Pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043803398s
Feb 13 23:53:31.118: INFO: Pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066180225s
Feb 13 23:53:33.139: INFO: Pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088100027s
STEP: Saw pod success
Feb 13 23:53:33.140: INFO: Pod "pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:53:33.167: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:53:33.226: INFO: Waiting for pod pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:53:33.247: INFO: Pod pod-projected-secrets-8e721669-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:53:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-cch5d" for this suite.
Feb 13 23:53:39.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:53:39.615: INFO: namespace: e2e-tests-projected-cch5d, resource: bindings, ignored listing per whitelist
Feb 13 23:53:40.184: INFO: namespace e2e-tests-projected-cch5d deletion completed in 6.91516995s

• [SLOW TEST:14.345 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:53:40.185: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-zmh25
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-96f60371-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:53:41.364: INFO: Waiting up to 5m0s for pod "pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-zmh25" to be "success or failure"
Feb 13 23:53:41.385: INFO: Pod "pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.227955ms
Feb 13 23:53:43.408: INFO: Pod "pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043680327s
Feb 13 23:53:45.430: INFO: Pod "pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06582193s
STEP: Saw pod success
Feb 13 23:53:45.430: INFO: Pod "pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:53:45.451: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200 container secret-env-test: <nil>
STEP: delete the pod
Feb 13 23:53:45.510: INFO: Waiting for pod pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:53:45.532: INFO: Pod pod-secrets-96f951e0-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:53:45.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-zmh25" for this suite.
Feb 13 23:53:51.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:53:52.081: INFO: namespace: e2e-tests-secrets-zmh25, resource: bindings, ignored listing per whitelist
Feb 13 23:53:52.552: INFO: namespace e2e-tests-secrets-zmh25 deletion completed in 6.997292023s

• [SLOW TEST:12.367 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:53:52.552: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-services-c4cm5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:84
[It] should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating service multi-endpoint-test in namespace e2e-tests-services-c4cm5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c4cm5 to expose endpoints map[]
Feb 13 23:53:53.779: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c4cm5 exposes endpoints map[] (20.962834ms elapsed)
STEP: Creating pod pod1 in namespace e2e-tests-services-c4cm5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c4cm5 to expose endpoints map[pod1:[100]]
Feb 13 23:53:57.016: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c4cm5 exposes endpoints map[pod1:[100]] (3.191281314s elapsed)
STEP: Creating pod pod2 in namespace e2e-tests-services-c4cm5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c4cm5 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 13 23:54:00.301: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c4cm5 exposes endpoints map[pod1:[100] pod2:[101]] (3.260516711s elapsed)
STEP: Deleting pod pod1 in namespace e2e-tests-services-c4cm5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c4cm5 to expose endpoints map[pod2:[101]]
Feb 13 23:54:00.368: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c4cm5 exposes endpoints map[pod2:[101]] (43.58425ms elapsed)
STEP: Deleting pod pod2 in namespace e2e-tests-services-c4cm5
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-c4cm5 to expose endpoints map[]
Feb 13 23:54:00.421: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-c4cm5 exposes endpoints map[] (24.309255ms elapsed)
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:54:00.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-services-c4cm5" for this suite.
Feb 13 23:54:24.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:54:24.754: INFO: namespace: e2e-tests-services-c4cm5, resource: bindings, ignored listing per whitelist
Feb 13 23:54:25.431: INFO: namespace e2e-tests-services-c4cm5 deletion completed in 24.945376421s
[AfterEach] [sig-network] Services
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:89

• [SLOW TEST:32.879 seconds]
[sig-network] Services
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:54:25.431: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-fc6fj
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 13 23:54:26.634: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15017,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 13 23:54:26.634: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15018,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 13 23:54:26.634: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15019,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 13 23:54:36.794: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15041,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 13 23:54:36.794: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15042,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 13 23:54:36.794: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-fc6fj,SelfLink:/api/v1/namespaces/e2e-tests-watch-fc6fj/configmaps/e2e-watch-test-label-changed,UID:b1ea61bd-2fea-11e9-9d38-36c391ece45e,ResourceVersion:15043,Generation:0,CreationTimestamp:2019-02-13 23:54:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:54:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-fc6fj" for this suite.
Feb 13 23:54:42.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:54:43.324: INFO: namespace: e2e-tests-watch-fc6fj, resource: bindings, ignored listing per whitelist
Feb 13 23:54:43.750: INFO: namespace e2e-tests-watch-fc6fj deletion completed in 6.933510319s

• [SLOW TEST:18.319 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:54:43.750: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-8dnrf
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0213 23:55:24.988227    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 13 23:55:24.988: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:55:24.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-8dnrf" for this suite.
Feb 13 23:55:33.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:55:33.559: INFO: namespace: e2e-tests-gc-8dnrf, resource: bindings, ignored listing per whitelist
Feb 13 23:55:33.989: INFO: namespace e2e-tests-gc-8dnrf deletion completed in 8.979394318s

• [SLOW TEST:50.239 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:55:33.989: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-xn9l6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-dad8d663-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:55:35.248: INFO: Waiting up to 5m0s for pod "pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-xn9l6" to be "success or failure"
Feb 13 23:55:35.269: INFO: Pod "pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.12798ms
Feb 13 23:55:37.324: INFO: Pod "pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.075819034s
Feb 13 23:55:39.347: INFO: Pod "pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098850227s
STEP: Saw pod success
Feb 13 23:55:39.347: INFO: Pod "pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:55:39.368: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:55:39.444: INFO: Waiting for pod pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:55:39.473: INFO: Pod pod-secrets-dadc3c23-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:55:39.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-xn9l6" for this suite.
Feb 13 23:55:45.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:55:46.350: INFO: namespace: e2e-tests-secrets-xn9l6, resource: bindings, ignored listing per whitelist
Feb 13 23:55:46.545: INFO: namespace e2e-tests-secrets-xn9l6 deletion completed in 7.048471308s

• [SLOW TEST:12.556 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:55:46.545: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-ldfth
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
Feb 13 23:55:48.212: INFO: created pod pod-service-account-defaultsa
Feb 13 23:55:48.213: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 13 23:55:48.235: INFO: created pod pod-service-account-mountsa
Feb 13 23:55:48.235: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 13 23:55:48.257: INFO: created pod pod-service-account-nomountsa
Feb 13 23:55:48.257: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 13 23:55:48.280: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 13 23:55:48.280: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 13 23:55:48.305: INFO: created pod pod-service-account-mountsa-mountspec
Feb 13 23:55:48.305: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 13 23:55:48.330: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 13 23:55:48.330: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 13 23:55:48.352: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 13 23:55:48.352: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 13 23:55:48.389: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 13 23:55:48.389: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 13 23:55:48.411: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 13 23:55:48.411: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:55:48.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-ldfth" for this suite.
Feb 13 23:56:12.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:56:13.304: INFO: namespace: e2e-tests-svcaccounts-ldfth, resource: bindings, ignored listing per whitelist
Feb 13 23:56:13.329: INFO: namespace e2e-tests-svcaccounts-ldfth deletion completed in 24.896180449s

• [SLOW TEST:26.784 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:56:13.329: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-jpfqr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-f244c498-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 13 23:56:14.541: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-jpfqr" to be "success or failure"
Feb 13 23:56:14.562: INFO: Pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 20.955496ms
Feb 13 23:56:16.584: INFO: Pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043226315s
Feb 13 23:56:18.607: INFO: Pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0653526s
Feb 13 23:56:20.629: INFO: Pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.087611517s
STEP: Saw pod success
Feb 13 23:56:20.629: INFO: Pod "pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:56:20.650: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 13 23:56:20.869: INFO: Waiting for pod pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:56:20.892: INFO: Pod pod-projected-secrets-f2480e6e-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:56:20.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-jpfqr" for this suite.
Feb 13 23:56:26.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:56:27.347: INFO: namespace: e2e-tests-projected-jpfqr, resource: bindings, ignored listing per whitelist
Feb 13 23:56:27.803: INFO: namespace e2e-tests-projected-jpfqr deletion completed in 6.888904158s

• [SLOW TEST:14.474 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:56:27.803: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rw9r7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-fadaef5c-2fea-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 13 23:56:28.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-rw9r7" to be "success or failure"
Feb 13 23:56:28.969: INFO: Pod "pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.673327ms
Feb 13 23:56:30.991: INFO: Pod "pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043948353s
Feb 13 23:56:33.014: INFO: Pod "pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066819356s
STEP: Saw pod success
Feb 13 23:56:33.014: INFO: Pod "pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:56:33.036: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 13 23:56:33.157: INFO: Waiting for pod pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:56:33.178: INFO: Pod pod-configmaps-fade374f-2fea-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:56:33.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rw9r7" for this suite.
Feb 13 23:56:39.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:56:39.964: INFO: namespace: e2e-tests-configmap-rw9r7, resource: bindings, ignored listing per whitelist
Feb 13 23:56:40.161: INFO: namespace e2e-tests-configmap-rw9r7 deletion completed in 6.925371543s

• [SLOW TEST:12.357 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:56:40.161: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-gp7bv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test use defaults
Feb 13 23:56:41.352: INFO: Waiting up to 5m0s for pod "client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-containers-gp7bv" to be "success or failure"
Feb 13 23:56:41.373: INFO: Pod "client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.398903ms
Feb 13 23:56:43.395: INFO: Pod "client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043143431s
Feb 13 23:56:45.417: INFO: Pod "client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06534365s
STEP: Saw pod success
Feb 13 23:56:45.417: INFO: Pod "client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 13 23:56:45.439: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 13 23:56:45.508: INFO: Waiting for pod client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200 to disappear
Feb 13 23:56:45.529: INFO: Pod client-containers-0242f3da-2feb-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:56:45.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-gp7bv" for this suite.
Feb 13 23:56:51.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:56:52.274: INFO: namespace: e2e-tests-containers-gp7bv, resource: bindings, ignored listing per whitelist
Feb 13 23:56:52.486: INFO: namespace e2e-tests-containers-gp7bv deletion completed in 6.930719161s

• [SLOW TEST:12.325 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:56:52.487: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-8fmm8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-mpm6r in namespace e2e-tests-proxy-8fmm8
I0213 23:56:53.768448    3058 runners.go:180] Created replication controller with name: proxy-service-mpm6r, namespace: e2e-tests-proxy-8fmm8, replica count: 1
I0213 23:56:54.818939    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:56:55.819170    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:56:56.819415    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:56:57.819650    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:56:58.819960    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0213 23:56:59.820205    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0213 23:57:00.820575    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0213 23:57:01.820890    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0213 23:57:02.821127    3058 runners.go:180] proxy-service-mpm6r Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 13 23:57:02.843: INFO: setup took 9.140039579s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 13 23:57:02.877: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 33.713891ms)
Feb 13 23:57:02.878: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 34.570638ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 35.584916ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 35.507939ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 35.707427ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 35.651301ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 35.650256ms)
Feb 13 23:57:02.879: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 35.70579ms)
Feb 13 23:57:02.880: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 36.850682ms)
Feb 13 23:57:02.882: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 39.335147ms)
Feb 13 23:57:02.884: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 40.81892ms)
Feb 13 23:57:02.894: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 50.750511ms)
Feb 13 23:57:02.894: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 50.836701ms)
Feb 13 23:57:02.895: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 52.185501ms)
Feb 13 23:57:02.941: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 98.154655ms)
Feb 13 23:57:02.941: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 98.156443ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 26.812253ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 26.860947ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 26.92164ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 26.756166ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.034779ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 26.759974ms)
Feb 13 23:57:02.968: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.80545ms)
Feb 13 23:57:02.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 28.821775ms)
Feb 13 23:57:02.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 28.954322ms)
Feb 13 23:57:02.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 29.011379ms)
Feb 13 23:57:02.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 28.909593ms)
Feb 13 23:57:02.970: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 28.707835ms)
Feb 13 23:57:02.973: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 31.638119ms)
Feb 13 23:57:02.976: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 34.089117ms)
Feb 13 23:57:02.976: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 34.08319ms)
Feb 13 23:57:02.978: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 36.003301ms)
Feb 13 23:57:03.004: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 25.870998ms)
Feb 13 23:57:03.004: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.992573ms)
Feb 13 23:57:03.004: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.931197ms)
Feb 13 23:57:03.004: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.969226ms)
Feb 13 23:57:03.004: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.098166ms)
Feb 13 23:57:03.006: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.902158ms)
Feb 13 23:57:03.006: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.931999ms)
Feb 13 23:57:03.006: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 27.887549ms)
Feb 13 23:57:03.006: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 28.101851ms)
Feb 13 23:57:03.008: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 30.331884ms)
Feb 13 23:57:03.008: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 30.245429ms)
Feb 13 23:57:03.010: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 32.464612ms)
Feb 13 23:57:03.010: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 32.337459ms)
Feb 13 23:57:03.010: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 32.404285ms)
Feb 13 23:57:03.010: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 32.477282ms)
Feb 13 23:57:03.013: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 34.671575ms)
Feb 13 23:57:03.038: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.239517ms)
Feb 13 23:57:03.038: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.433841ms)
Feb 13 23:57:03.038: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.27754ms)
Feb 13 23:57:03.038: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.273193ms)
Feb 13 23:57:03.038: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.332296ms)
Feb 13 23:57:03.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 27.546125ms)
Feb 13 23:57:03.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.624976ms)
Feb 13 23:57:03.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.569206ms)
Feb 13 23:57:03.040: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.559015ms)
Feb 13 23:57:03.043: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 30.002236ms)
Feb 13 23:57:03.043: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 30.080764ms)
Feb 13 23:57:03.043: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 30.132514ms)
Feb 13 23:57:03.043: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 30.100315ms)
Feb 13 23:57:03.043: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 30.148596ms)
Feb 13 23:57:03.046: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 33.239366ms)
Feb 13 23:57:03.047: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 34.034664ms)
Feb 13 23:57:03.072: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.245017ms)
Feb 13 23:57:03.072: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.310865ms)
Feb 13 23:57:03.072: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.306297ms)
Feb 13 23:57:03.073: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 25.303842ms)
Feb 13 23:57:03.073: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.358184ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.579054ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.576492ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.649039ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.70305ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 27.631842ms)
Feb 13 23:57:03.075: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.823772ms)
Feb 13 23:57:03.077: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.790724ms)
Feb 13 23:57:03.077: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 29.939535ms)
Feb 13 23:57:03.079: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 32.026967ms)
Feb 13 23:57:03.079: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.912477ms)
Feb 13 23:57:03.079: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 31.975009ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 24.456374ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 24.959288ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.687339ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 24.772408ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 24.652445ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 24.745881ms)
Feb 13 23:57:03.104: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 24.849906ms)
Feb 13 23:57:03.106: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.031101ms)
Feb 13 23:57:03.107: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.33767ms)
Feb 13 23:57:03.108: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 29.226345ms)
Feb 13 23:57:03.109: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 29.10654ms)
Feb 13 23:57:03.109: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.096316ms)
Feb 13 23:57:03.109: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.23781ms)
Feb 13 23:57:03.109: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 29.425651ms)
Feb 13 23:57:03.110: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 31.111054ms)
Feb 13 23:57:03.115: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 36.082282ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 25.809838ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 25.500963ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.451453ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 25.749255ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.558835ms)
Feb 13 23:57:03.141: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.62094ms)
Feb 13 23:57:03.143: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.353038ms)
Feb 13 23:57:03.143: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 27.345641ms)
Feb 13 23:57:03.143: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.332043ms)
Feb 13 23:57:03.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 29.367186ms)
Feb 13 23:57:03.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.354474ms)
Feb 13 23:57:03.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.359165ms)
Feb 13 23:57:03.145: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 29.561468ms)
Feb 13 23:57:03.147: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.657179ms)
Feb 13 23:57:03.147: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 31.61052ms)
Feb 13 23:57:03.150: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 33.996118ms)
Feb 13 23:57:03.175: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.517292ms)
Feb 13 23:57:03.176: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.687185ms)
Feb 13 23:57:03.176: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 25.570769ms)
Feb 13 23:57:03.176: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.582726ms)
Feb 13 23:57:03.176: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 25.731288ms)
Feb 13 23:57:03.177: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 26.712374ms)
Feb 13 23:57:03.177: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.589918ms)
Feb 13 23:57:03.177: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.53096ms)
Feb 13 23:57:03.177: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 26.824219ms)
Feb 13 23:57:03.179: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.445372ms)
Feb 13 23:57:03.179: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.248435ms)
Feb 13 23:57:03.179: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 29.191283ms)
Feb 13 23:57:03.179: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 29.143859ms)
Feb 13 23:57:03.179: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 29.514668ms)
Feb 13 23:57:03.182: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 31.86979ms)
Feb 13 23:57:03.184: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 34.124778ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.124355ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.189439ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 27.274245ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.133919ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.371057ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.477995ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 27.288749ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.387575ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 27.896708ms)
Feb 13 23:57:03.212: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.089204ms)
Feb 13 23:57:03.214: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 28.646058ms)
Feb 13 23:57:03.214: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 28.832049ms)
Feb 13 23:57:03.216: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.486003ms)
Feb 13 23:57:03.216: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 31.078064ms)
Feb 13 23:57:03.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 33.549223ms)
Feb 13 23:57:03.218: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 33.59086ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 24.931838ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 24.970694ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 24.829659ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.886638ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.959667ms)
Feb 13 23:57:03.244: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.274016ms)
Feb 13 23:57:03.245: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 26.371741ms)
Feb 13 23:57:03.245: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 26.532874ms)
Feb 13 23:57:03.245: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 26.697295ms)
Feb 13 23:57:03.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 29.180451ms)
Feb 13 23:57:03.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 28.888662ms)
Feb 13 23:57:03.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 29.238909ms)
Feb 13 23:57:03.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 28.94583ms)
Feb 13 23:57:03.248: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 28.991266ms)
Feb 13 23:57:03.250: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.127478ms)
Feb 13 23:57:03.255: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 36.076352ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.497389ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 25.624786ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 25.025753ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 25.580679ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.538045ms)
Feb 13 23:57:03.281: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.980064ms)
Feb 13 23:57:03.283: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.621353ms)
Feb 13 23:57:03.283: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.820225ms)
Feb 13 23:57:03.283: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.564837ms)
Feb 13 23:57:03.285: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.771538ms)
Feb 13 23:57:03.285: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.899575ms)
Feb 13 23:57:03.285: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 29.961833ms)
Feb 13 23:57:03.285: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 30.095517ms)
Feb 13 23:57:03.285: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 29.946356ms)
Feb 13 23:57:03.287: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 31.528134ms)
Feb 13 23:57:03.292: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 36.320823ms)
Feb 13 23:57:03.317: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.177542ms)
Feb 13 23:57:03.317: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.168024ms)
Feb 13 23:57:03.317: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 24.71082ms)
Feb 13 23:57:03.317: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.135463ms)
Feb 13 23:57:03.317: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 24.609626ms)
Feb 13 23:57:03.319: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 26.784085ms)
Feb 13 23:57:03.319: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.964178ms)
Feb 13 23:57:03.319: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.396562ms)
Feb 13 23:57:03.320: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 26.819663ms)
Feb 13 23:57:03.320: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.841689ms)
Feb 13 23:57:03.321: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.51209ms)
Feb 13 23:57:03.322: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 28.744069ms)
Feb 13 23:57:03.323: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.190096ms)
Feb 13 23:57:03.327: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 34.697411ms)
Feb 13 23:57:03.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 37.550219ms)
Feb 13 23:57:03.329: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 37.281235ms)
Feb 13 23:57:03.356: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 25.385031ms)
Feb 13 23:57:03.356: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.516487ms)
Feb 13 23:57:03.356: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.783032ms)
Feb 13 23:57:03.356: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.009032ms)
Feb 13 23:57:03.356: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.679455ms)
Feb 13 23:57:03.358: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 28.45497ms)
Feb 13 23:57:03.358: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.284199ms)
Feb 13 23:57:03.358: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 28.14139ms)
Feb 13 23:57:03.358: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.996774ms)
Feb 13 23:57:03.360: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.935666ms)
Feb 13 23:57:03.360: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.487817ms)
Feb 13 23:57:03.360: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 30.328815ms)
Feb 13 23:57:03.360: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 29.897852ms)
Feb 13 23:57:03.360: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 30.464505ms)
Feb 13 23:57:03.362: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 31.501186ms)
Feb 13 23:57:03.367: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 37.787647ms)
Feb 13 23:57:03.394: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.659112ms)
Feb 13 23:57:03.394: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.465455ms)
Feb 13 23:57:03.394: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 25.81382ms)
Feb 13 23:57:03.394: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.608066ms)
Feb 13 23:57:03.394: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.514809ms)
Feb 13 23:57:03.396: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 28.146522ms)
Feb 13 23:57:03.396: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.454373ms)
Feb 13 23:57:03.396: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.517563ms)
Feb 13 23:57:03.396: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.549422ms)
Feb 13 23:57:03.396: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.601134ms)
Feb 13 23:57:03.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 30.254878ms)
Feb 13 23:57:03.398: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 30.53174ms)
Feb 13 23:57:03.401: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 33.276252ms)
Feb 13 23:57:03.401: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 32.748794ms)
Feb 13 23:57:03.403: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 34.595917ms)
Feb 13 23:57:03.405: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 36.578695ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.21975ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.077463ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 25.035769ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 24.997202ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.178349ms)
Feb 13 23:57:03.430: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 25.101291ms)
Feb 13 23:57:03.432: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 27.184655ms)
Feb 13 23:57:03.432: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.132675ms)
Feb 13 23:57:03.432: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 27.175959ms)
Feb 13 23:57:03.432: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.117692ms)
Feb 13 23:57:03.434: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 29.042303ms)
Feb 13 23:57:03.434: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 29.082796ms)
Feb 13 23:57:03.434: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.32867ms)
Feb 13 23:57:03.436: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 30.862205ms)
Feb 13 23:57:03.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 32.964593ms)
Feb 13 23:57:03.438: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 32.919336ms)
Feb 13 23:57:03.463: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.900032ms)
Feb 13 23:57:03.463: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 24.606439ms)
Feb 13 23:57:03.463: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 24.720219ms)
Feb 13 23:57:03.463: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 24.77481ms)
Feb 13 23:57:03.463: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 24.645267ms)
Feb 13 23:57:03.465: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.801417ms)
Feb 13 23:57:03.465: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 26.939734ms)
Feb 13 23:57:03.465: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.121359ms)
Feb 13 23:57:03.465: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.991917ms)
Feb 13 23:57:03.467: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 28.486671ms)
Feb 13 23:57:03.467: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 28.528055ms)
Feb 13 23:57:03.467: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 28.868767ms)
Feb 13 23:57:03.469: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 31.230274ms)
Feb 13 23:57:03.469: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.275778ms)
Feb 13 23:57:03.471: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 32.86258ms)
Feb 13 23:57:03.471: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 33.11127ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 26.014471ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.916044ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 25.99785ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.878532ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 26.00543ms)
Feb 13 23:57:03.497: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.967368ms)
Feb 13 23:57:03.499: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.935323ms)
Feb 13 23:57:03.499: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 27.874373ms)
Feb 13 23:57:03.499: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.841466ms)
Feb 13 23:57:03.499: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.822639ms)
Feb 13 23:57:03.502: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 30.180841ms)
Feb 13 23:57:03.502: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 30.121399ms)
Feb 13 23:57:03.502: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 30.282911ms)
Feb 13 23:57:03.502: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 30.288821ms)
Feb 13 23:57:03.504: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 32.223317ms)
Feb 13 23:57:03.506: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 34.400412ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 25.902594ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.935968ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.99046ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.999059ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.119436ms)
Feb 13 23:57:03.532: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 26.037488ms)
Feb 13 23:57:03.534: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 27.791029ms)
Feb 13 23:57:03.534: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.75975ms)
Feb 13 23:57:03.534: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 27.828048ms)
Feb 13 23:57:03.534: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 27.999073ms)
Feb 13 23:57:03.534: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 27.898818ms)
Feb 13 23:57:03.536: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 30.227299ms)
Feb 13 23:57:03.536: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 30.239946ms)
Feb 13 23:57:03.539: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 32.439688ms)
Feb 13 23:57:03.539: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 32.517569ms)
Feb 13 23:57:03.545: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 38.712723ms)
Feb 13 23:57:03.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.885631ms)
Feb 13 23:57:03.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 25.934812ms)
Feb 13 23:57:03.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 25.982891ms)
Feb 13 23:57:03.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 25.883215ms)
Feb 13 23:57:03.571: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 25.95062ms)
Feb 13 23:57:03.576: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 31.310776ms)
Feb 13 23:57:03.576: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 31.295951ms)
Feb 13 23:57:03.576: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.45309ms)
Feb 13 23:57:03.576: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 31.345303ms)
Feb 13 23:57:03.576: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 31.360027ms)
Feb 13 23:57:03.577: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 31.526968ms)
Feb 13 23:57:03.577: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 31.423117ms)
Feb 13 23:57:03.577: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 31.413857ms)
Feb 13 23:57:03.577: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 31.405594ms)
Feb 13 23:57:03.578: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 32.447589ms)
Feb 13 23:57:03.583: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 38.331045ms)
Feb 13 23:57:03.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:1080/proxy/... (200; 26.594755ms)
Feb 13 23:57:03.610: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 26.700433ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/http:proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 26.756586ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:162/proxy/: bar (200; 26.943375ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname2/proxy/: tls qux (200; 27.301289ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:462/proxy/: tls qux (200; 27.190271ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:460/proxy/: tls baz (200; 27.177259ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:160/proxy/: foo (200; 27.031475ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh:1080/proxy/rewri... (200; 27.060145ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/proxy-service-mpm6r-zdrrh/proxy/rewriteme"... (200; 27.317761ms)
Feb 13 23:57:03.611: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/: <a href="/api/v1/namespaces/e2e-tests-proxy-8fmm8/pods/https:proxy-service-mpm6r-zdrrh:443/proxy/... (200; 27.129158ms)
Feb 13 23:57:03.613: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/https:proxy-service-mpm6r:tlsportname1/proxy/: tls baz (200; 29.214356ms)
Feb 13 23:57:03.616: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname2/proxy/: bar (200; 31.65381ms)
Feb 13 23:57:03.616: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname1/proxy/: foo (200; 31.748455ms)
Feb 13 23:57:03.616: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/http:proxy-service-mpm6r:portname2/proxy/: bar (200; 31.830935ms)
Feb 13 23:57:03.616: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-8fmm8/services/proxy-service-mpm6r:portname1/proxy/: foo (200; 31.856902ms)
STEP: deleting { ReplicationController} proxy-service-mpm6r in namespace e2e-tests-proxy-8fmm8, will wait for the garbage collector to delete the pods
Feb 13 23:57:03.712: INFO: Deleting { ReplicationController} proxy-service-mpm6r took: 24.065319ms
Feb 13 23:57:03.812: INFO: Terminating { ReplicationController} proxy-service-mpm6r pods took: 100.325212ms
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 13 23:57:10.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-8fmm8" for this suite.
Feb 13 23:57:16.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 13 23:57:16.865: INFO: namespace: e2e-tests-proxy-8fmm8, resource: bindings, ignored listing per whitelist
Feb 13 23:57:17.632: INFO: namespace e2e-tests-proxy-8fmm8 deletion completed in 6.897074807s

• [SLOW TEST:25.145 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 13 23:57:17.632: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-28dsx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-28dsx
Feb 13 23:57:22.779: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-28dsx
STEP: checking the pod's current state and verifying that restartCount is present
Feb 13 23:57:22.800: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:01:24.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-28dsx" for this suite.
Feb 14 00:01:30.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:01:30.597: INFO: namespace: e2e-tests-container-probe-28dsx, resource: bindings, ignored listing per whitelist
Feb 14 00:01:31.279: INFO: namespace e2e-tests-container-probe-28dsx deletion completed in 6.9192526s

• [SLOW TEST:253.648 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:01:31.280: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-l26lc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name secret-test-map-afc28dd7-2feb-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 14 00:01:32.455: INFO: Waiting up to 5m0s for pod "pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-l26lc" to be "success or failure"
Feb 14 00:01:32.476: INFO: Pod "pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.091953ms
Feb 14 00:01:34.499: INFO: Pod "pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043605183s
Feb 14 00:01:36.522: INFO: Pod "pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066305264s
STEP: Saw pod success
Feb 14 00:01:36.522: INFO: Pod "pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:01:36.543: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:01:36.608: INFO: Waiting for pod pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:01:36.629: INFO: Pod pod-secrets-afc5daf9-2feb-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:01:36.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-l26lc" for this suite.
Feb 14 00:02:09.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:02:10.579: INFO: namespace: e2e-tests-secrets-l26lc, resource: bindings, ignored listing per whitelist
Feb 14 00:02:10.731: INFO: namespace e2e-tests-secrets-l26lc deletion completed in 34.08031836s

• [SLOW TEST:39.452 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:02:10.732: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-5lsm7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test env composition
Feb 14 00:02:11.944: INFO: Waiting up to 5m0s for pod "var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-var-expansion-5lsm7" to be "success or failure"
Feb 14 00:02:11.965: INFO: Pod "var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.440978ms
Feb 14 00:02:13.989: INFO: Pod "var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04449991s
Feb 14 00:02:16.011: INFO: Pod "var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066771057s
STEP: Saw pod success
Feb 14 00:02:16.011: INFO: Pod "var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:02:16.033: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:02:16.095: INFO: Waiting for pod var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:02:16.116: INFO: Pod var-expansion-c74f52c4-2feb-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:02:16.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-5lsm7" for this suite.
Feb 14 00:02:22.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:02:22.463: INFO: namespace: e2e-tests-var-expansion-5lsm7, resource: bindings, ignored listing per whitelist
Feb 14 00:02:23.081: INFO: namespace e2e-tests-var-expansion-5lsm7 deletion completed in 6.942798075s

• [SLOW TEST:12.349 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:02:23.081: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-bprtr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-bprtr
Feb 14 00:02:30.260: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-bprtr
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 00:02:30.282: INFO: Initial restart count of pod liveness-http is 0
Feb 14 00:03:21.580: INFO: Restart count of pod e2e-tests-container-probe-bprtr/liveness-http is now 2 (51.298069987s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:03:21.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-bprtr" for this suite.
Feb 14 00:03:27.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:03:28.615: INFO: namespace: e2e-tests-container-probe-bprtr, resource: bindings, ignored listing per whitelist
Feb 14 00:03:28.636: INFO: namespace e2e-tests-container-probe-bprtr deletion completed in 6.973440687s

• [SLOW TEST:65.555 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:03:28.637: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-8zh7m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 00:03:29.804: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:03:35.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-8zh7m" for this suite.
Feb 14 00:03:41.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:03:42.174: INFO: namespace: e2e-tests-init-container-8zh7m, resource: bindings, ignored listing per whitelist
Feb 14 00:03:42.395: INFO: namespace e2e-tests-init-container-8zh7m deletion completed in 6.924695785s

• [SLOW TEST:13.759 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartNever pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:03:42.396: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gvkf5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1511
[It] should update a single-container pod's image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:03:43.530: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gvkf5'
Feb 14 00:03:44.894: INFO: stderr: ""
Feb 14 00:03:44.894: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Feb 14 00:03:49.945: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pod e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gvkf5 -o json'
Feb 14 00:03:50.144: INFO: stderr: ""
Feb 14 00:03:50.144: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.108/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-02-14T00:03:44Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"e2e-tests-kubectl-gvkf5\",\n        \"resourceVersion\": \"16543\",\n        \"selfLink\": \"/api/v1/namespaces/e2e-tests-kubectl-gvkf5/pods/e2e-test-nginx-pod\",\n        \"uid\": \"feb5c5b0-2feb-11e9-9d38-36c391ece45e\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9vlwq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"nodeName\": \"shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9vlwq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9vlwq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:03:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:03:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:03:48Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-02-14T00:03:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e2ceb632502c1ff257e1a2362e3f45eabb9b5efad297fd49ef6fc246976b97a4\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-02-14T00:03:47Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.5\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.108\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-02-14T00:03:44Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 14 00:03:50.144: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml replace -f - --namespace=e2e-tests-kubectl-gvkf5'
Feb 14 00:03:50.530: INFO: stderr: ""
Feb 14 00:03:50.530: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
Feb 14 00:03:50.552: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gvkf5'
Feb 14 00:03:54.910: INFO: stderr: ""
Feb 14 00:03:54.910: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:03:54.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gvkf5" for this suite.
Feb 14 00:04:01.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:01.154: INFO: namespace: e2e-tests-kubectl-gvkf5, resource: bindings, ignored listing per whitelist
Feb 14 00:04:01.878: INFO: namespace e2e-tests-kubectl-gvkf5 deletion completed in 6.945304374s

• [SLOW TEST:19.482 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl replace
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should update a single-container pod's image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:04:01.878: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-rpvzr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-09879a1c-2fec-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:04:03.064: INFO: Waiting up to 5m0s for pod "pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-rpvzr" to be "success or failure"
Feb 14 00:04:03.086: INFO: Pod "pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.299166ms
Feb 14 00:04:05.110: INFO: Pod "pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045559259s
Feb 14 00:04:07.134: INFO: Pod "pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069782651s
STEP: Saw pod success
Feb 14 00:04:07.134: INFO: Pod "pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:04:07.157: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:04:07.296: INFO: Waiting for pod pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:04:07.317: INFO: Pod pod-configmaps-098af3a1-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:04:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-rpvzr" for this suite.
Feb 14 00:04:13.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:13.900: INFO: namespace: e2e-tests-configmap-rpvzr, resource: bindings, ignored listing per whitelist
Feb 14 00:04:14.318: INFO: namespace e2e-tests-configmap-rpvzr deletion completed in 6.978656612s

• [SLOW TEST:12.440 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:04:14.318: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-tdng8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 14 00:04:15.630: INFO: Waiting up to 5m0s for pod "pod-1108526a-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-tdng8" to be "success or failure"
Feb 14 00:04:15.650: INFO: Pod "pod-1108526a-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 20.832067ms
Feb 14 00:04:17.674: INFO: Pod "pod-1108526a-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044866079s
Feb 14 00:04:19.697: INFO: Pod "pod-1108526a-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067588112s
STEP: Saw pod success
Feb 14 00:04:19.697: INFO: Pod "pod-1108526a-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:04:19.718: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-1108526a-2fec-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:04:19.837: INFO: Waiting for pod pod-1108526a-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:04:19.868: INFO: Pod pod-1108526a-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:04:19.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-tdng8" for this suite.
Feb 14 00:04:25.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:26.362: INFO: namespace: e2e-tests-emptydir-tdng8, resource: bindings, ignored listing per whitelist
Feb 14 00:04:26.840: INFO: namespace e2e-tests-emptydir-tdng8 deletion completed in 6.949627311s

• [SLOW TEST:12.522 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:04:26.840: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-855s6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:04:27.985: INFO: Waiting up to 5m0s for pod "downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-855s6" to be "success or failure"
Feb 14 00:04:28.006: INFO: Pod "downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.399601ms
Feb 14 00:04:30.029: INFO: Pod "downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044142436s
Feb 14 00:04:32.064: INFO: Pod "downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078896551s
STEP: Saw pod success
Feb 14 00:04:32.064: INFO: Pod "downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:04:32.085: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:04:32.321: INFO: Waiting for pod downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:04:32.342: INFO: Pod downward-api-18657d7d-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:04:32.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-855s6" for this suite.
Feb 14 00:04:38.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:39.094: INFO: namespace: e2e-tests-downward-api-855s6, resource: bindings, ignored listing per whitelist
Feb 14 00:04:39.312: INFO: namespace e2e-tests-downward-api-855s6 deletion completed in 6.948503991s

• [SLOW TEST:12.472 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:04:39.312: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-zjvgm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:04:40.717: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 00:04:44.763: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 00:04:48.947: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:e2e-tests-deployment-zjvgm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zjvgm/deployments/test-cleanup-deployment,UID:22702464-2fec-11e9-9d38-36c391ece45e,ResourceVersion:16762,Generation:1,CreationTimestamp:2019-02-14 00:04:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-14 00:04:44 +0000 UTC 2019-02-14 00:04:44 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-14 00:04:47 +0000 UTC 2019-02-14 00:04:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-755f6b95cc" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 14 00:04:48.969: INFO: New ReplicaSet "test-cleanup-deployment-755f6b95cc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc,GenerateName:,Namespace:e2e-tests-deployment-zjvgm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-zjvgm/replicasets/test-cleanup-deployment-755f6b95cc,UID:22725e63-2fec-11e9-9d38-36c391ece45e,ResourceVersion:16755,Generation:1,CreationTimestamp:2019-02-14 00:04:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 22702464-2fec-11e9-9d38-36c391ece45e 0xc002794f77 0xc002794f78}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 00:04:48.992: INFO: Pod "test-cleanup-deployment-755f6b95cc-7rghg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-755f6b95cc-7rghg,GenerateName:test-cleanup-deployment-755f6b95cc-,Namespace:e2e-tests-deployment-zjvgm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-zjvgm/pods/test-cleanup-deployment-755f6b95cc-7rghg,UID:22731148-2fec-11e9-9d38-36c391ece45e,ResourceVersion:16754,Generation:0,CreationTimestamp:2019-02-14 00:04:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 755f6b95cc,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.113/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-755f6b95cc 22725e63-2fec-11e9-9d38-36c391ece45e 0xc0027956b7 0xc0027956b8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-8vsrp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8vsrp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8vsrp true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002795720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002795740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:04:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:04:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:04:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:04:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.113,StartTime:2019-02-14 00:04:44 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-14 00:04:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://8a5d5c39de6e53b65c64bd8c707f3f5407e504acbb50725096ad960200b77f8d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:04:48.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-zjvgm" for this suite.
Feb 14 00:04:55.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:04:55.417: INFO: namespace: e2e-tests-deployment-zjvgm, resource: bindings, ignored listing per whitelist
Feb 14 00:04:55.915: INFO: namespace e2e-tests-deployment-zjvgm deletion completed in 6.900158341s

• [SLOW TEST:16.602 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:04:55.915: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-l7zln
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-29b6718f-2fec-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:04:57.059: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-l7zln" to be "success or failure"
Feb 14 00:04:57.080: INFO: Pod "pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 20.977961ms
Feb 14 00:04:59.102: INFO: Pod "pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04349163s
Feb 14 00:05:01.124: INFO: Pod "pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065276837s
STEP: Saw pod success
Feb 14 00:05:01.124: INFO: Pod "pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:05:01.146: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:05:01.227: INFO: Waiting for pod pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:05:01.250: INFO: Pod pod-projected-configmaps-29b9be54-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:05:01.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-l7zln" for this suite.
Feb 14 00:05:07.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:07.653: INFO: namespace: e2e-tests-projected-l7zln, resource: bindings, ignored listing per whitelist
Feb 14 00:05:08.189: INFO: namespace e2e-tests-projected-l7zln deletion completed in 6.916654715s

• [SLOW TEST:12.274 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:05:08.189: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-fzfn2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:05:09.241: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-fzfn2" to be "success or failure"
Feb 14 00:05:09.284: INFO: Pod "downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 43.29758ms
Feb 14 00:05:11.307: INFO: Pod "downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065774405s
Feb 14 00:05:13.329: INFO: Pod "downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088628978s
STEP: Saw pod success
Feb 14 00:05:13.329: INFO: Pod "downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:05:13.351: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:05:13.474: INFO: Waiting for pod downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:05:13.501: INFO: Pod downwardapi-volume-30fcad6b-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:05:13.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-fzfn2" for this suite.
Feb 14 00:05:21.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:05:21.917: INFO: namespace: e2e-tests-downward-api-fzfn2, resource: bindings, ignored listing per whitelist
Feb 14 00:05:22.460: INFO: namespace e2e-tests-downward-api-fzfn2 deletion completed in 8.931630095s

• [SLOW TEST:14.271 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:05:22.460: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-cljsr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-cljsr
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 00:05:23.514: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 00:05:49.959: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.117:8080/dial?request=hostName&protocol=udp&host=100.96.0.32&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-cljsr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:05:49.959: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:05:55.525: INFO: Waiting for endpoints: map[]
Feb 14 00:05:55.547: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.117:8080/dial?request=hostName&protocol=udp&host=100.96.1.116&port=8081&tries=1'] Namespace:e2e-tests-pod-network-test-cljsr PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:05:55.547: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:05:56.073: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:05:56.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-cljsr" for this suite.
Feb 14 00:06:20.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:20.407: INFO: namespace: e2e-tests-pod-network-test-cljsr, resource: bindings, ignored listing per whitelist
Feb 14 00:06:21.055: INFO: namespace e2e-tests-pod-network-test-cljsr deletion completed in 24.960001611s

• [SLOW TEST:58.595 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:06:21.056: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-plmsl
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 14 00:06:22.138: INFO: Waiting up to 5m0s for pod "pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-plmsl" to be "success or failure"
Feb 14 00:06:22.183: INFO: Pod "pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 45.09404ms
Feb 14 00:06:24.206: INFO: Pod "pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067784912s
Feb 14 00:06:26.233: INFO: Pod "pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094526617s
STEP: Saw pod success
Feb 14 00:06:26.233: INFO: Pod "pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:06:26.254: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:06:26.366: INFO: Waiting for pod pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:06:26.388: INFO: Pod pod-5c6fd5d4-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:06:26.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-plmsl" for this suite.
Feb 14 00:06:32.503: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:32.670: INFO: namespace: e2e-tests-emptydir-plmsl, resource: bindings, ignored listing per whitelist
Feb 14 00:06:33.336: INFO: namespace e2e-tests-emptydir-plmsl deletion completed in 6.92651307s

• [SLOW TEST:12.280 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:06:33.336: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-9k8bs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-63e93aaa-2fec-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:06:34.701: INFO: Waiting up to 5m0s for pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-9k8bs" to be "success or failure"
Feb 14 00:06:34.732: INFO: Pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 30.857648ms
Feb 14 00:06:36.754: INFO: Pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053249511s
Feb 14 00:06:38.776: INFO: Pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075529105s
Feb 14 00:06:40.801: INFO: Pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.10044323s
STEP: Saw pod success
Feb 14 00:06:40.801: INFO: Pod "pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:06:40.823: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:06:40.895: INFO: Waiting for pod pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:06:40.916: INFO: Pod pod-configmaps-63ecd1d4-2fec-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:06:40.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-9k8bs" for this suite.
Feb 14 00:06:47.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:47.769: INFO: namespace: e2e-tests-configmap-9k8bs, resource: bindings, ignored listing per whitelist
Feb 14 00:06:47.897: INFO: namespace e2e-tests-configmap-9k8bs deletion completed in 6.957896468s

• [SLOW TEST:14.561 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:06:47.897: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-proxy-j8n6t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:06:49.148: INFO: (0) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 28.83178ms)
Feb 14 00:06:49.194: INFO: (1) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 45.905063ms)
Feb 14 00:06:49.219: INFO: (2) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.793774ms)
Feb 14 00:06:49.244: INFO: (3) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.908613ms)
Feb 14 00:06:49.268: INFO: (4) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.593842ms)
Feb 14 00:06:49.293: INFO: (5) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.206727ms)
Feb 14 00:06:49.317: INFO: (6) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.206599ms)
Feb 14 00:06:49.341: INFO: (7) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.276691ms)
Feb 14 00:06:49.366: INFO: (8) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.6439ms)
Feb 14 00:06:49.391: INFO: (9) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.807365ms)
Feb 14 00:06:49.415: INFO: (10) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.686488ms)
Feb 14 00:06:49.440: INFO: (11) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.358984ms)
Feb 14 00:06:49.465: INFO: (12) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.052613ms)
Feb 14 00:06:49.496: INFO: (13) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 30.733723ms)
Feb 14 00:06:49.521: INFO: (14) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 25.199251ms)
Feb 14 00:06:49.546: INFO: (15) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.962616ms)
Feb 14 00:06:49.573: INFO: (16) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 26.375097ms)
Feb 14 00:06:49.601: INFO: (17) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 27.992649ms)
Feb 14 00:06:49.625: INFO: (18) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.578753ms)
Feb 14 00:06:49.650: INFO: (19) /api/v1/nodes/shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg/proxy/logs/: <pre>
<a href="azure/">azure/</a>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<... (200; 24.721471ms)
[AfterEach] version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:06:49.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-proxy-j8n6t" for this suite.
Feb 14 00:06:55.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:06:56.080: INFO: namespace: e2e-tests-proxy-j8n6t, resource: bindings, ignored listing per whitelist
Feb 14 00:06:56.725: INFO: namespace e2e-tests-proxy-j8n6t deletion completed in 7.05238344s

• [SLOW TEST:8.828 seconds]
[sig-network] Proxy
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:06:56.725: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-btv9m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-btv9m
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-btv9m
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-btv9m
Feb 14 00:06:58.185: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Feb 14 00:07:08.214: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 14 00:07:08.236: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:07:09.002: INFO: stderr: ""
Feb 14 00:07:09.002: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:07:09.002: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:07:09.030: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:07:09.030: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:07:09.348: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999421s
Feb 14 00:07:10.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.974519048s
Feb 14 00:07:11.396: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952467585s
Feb 14 00:07:12.419: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.926184146s
Feb 14 00:07:13.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.90323274s
Feb 14 00:07:14.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.880150767s
Feb 14 00:07:15.493: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.856195552s
Feb 14 00:07:16.516: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.829064616s
Feb 14 00:07:17.539: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.806040391s
Feb 14 00:07:18.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 782.867506ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-btv9m
Feb 14 00:07:19.586: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:20.342: INFO: stderr: ""
Feb 14 00:07:20.342: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:07:20.342: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:07:20.342: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:21.117: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 14 00:07:21.117: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:07:21.117: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:07:21.117: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:21.832: INFO: stderr: "mv: can't rename '/tmp/index.html': No such file or directory\n"
Feb 14 00:07:21.832: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:07:21.832: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:07:21.854: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:21.854: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:07:21.854: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 14 00:07:21.876: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:07:22.563: INFO: stderr: ""
Feb 14 00:07:22.563: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:07:22.563: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:07:22.563: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:07:23.363: INFO: stderr: ""
Feb 14 00:07:23.363: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:07:23.363: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:07:23.363: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:07:24.002: INFO: stderr: ""
Feb 14 00:07:24.002: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:07:24.002: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:07:24.002: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:07:24.024: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 14 00:07:34.069: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:07:34.069: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:07:34.069: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:07:34.139: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:34.139: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:34.139: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:34.139: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:34.139: INFO: 
Feb 14 00:07:34.139: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:35.162: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:35.162: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:35.162: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:35.162: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:35.162: INFO: 
Feb 14 00:07:35.162: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:36.185: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:36.185: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:36.185: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:36.185: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:36.185: INFO: 
Feb 14 00:07:36.185: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:37.209: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:37.209: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:37.209: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:37.209: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:37.210: INFO: 
Feb 14 00:07:37.210: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:38.233: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:38.233: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:38.233: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:38.233: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:38.233: INFO: 
Feb 14 00:07:38.233: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:39.256: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:39.256: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:39.256: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:39.256: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:39.256: INFO: 
Feb 14 00:07:39.256: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:40.279: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:40.279: INFO: ss-0  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:06:58 +0000 UTC  }]
Feb 14 00:07:40.279: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:40.279: INFO: ss-2  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:40.279: INFO: 
Feb 14 00:07:40.279: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 14 00:07:41.302: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:41.302: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:41.302: INFO: 
Feb 14 00:07:41.302: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 00:07:42.325: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:42.325: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:42.325: INFO: 
Feb 14 00:07:42.325: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 14 00:07:43.348: INFO: POD   NODE                                                 PHASE    GRACE  CONDITIONS
Feb 14 00:07:43.348: INFO: ss-1  shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:07:09 +0000 UTC  }]
Feb 14 00:07:43.348: INFO: 
Feb 14 00:07:43.348: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-btv9m
Feb 14 00:07:44.370: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:44.787: INFO: rc: 1
Feb 14 00:07:44.787: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00188e7e0 exit status 1 <nil> <nil> true [0xc00163c450 0xc00163c468 0xc00163c480] [0xc00163c450 0xc00163c468 0xc00163c480] [0xc00163c460 0xc00163c478] [0x932420 0x932420] 0xc0013896e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Feb 14 00:07:54.787: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:07:54.990: INFO: rc: 1
Feb 14 00:07:54.990: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00188ea80 exit status 1 <nil> <nil> true [0xc00163c488 0xc00163c4a0 0xc00163c4b8] [0xc00163c488 0xc00163c4a0 0xc00163c4b8] [0xc00163c498 0xc00163c4b0] [0x932420 0x932420] 0xc00223e0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:08:04.991: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:05.157: INFO: rc: 1
Feb 14 00:08:05.157: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fe4870 exit status 1 <nil> <nil> true [0xc001666650 0xc001666668 0xc001666680] [0xc001666650 0xc001666668 0xc001666680] [0xc001666660 0xc001666678] [0x932420 0x932420] 0xc00113d860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:08:15.158: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:15.335: INFO: rc: 1
Feb 14 00:08:15.335: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002008660 exit status 1 <nil> <nil> true [0xc001d1e050 0xc001d1e088 0xc001d1e0b0] [0xc001d1e050 0xc001d1e088 0xc001d1e0b0] [0xc001d1e080 0xc001d1e0a8] [0x932420 0x932420] 0xc00146ec60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:08:25.342: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:30.526: INFO: rc: 1
Feb 14 00:08:30.526: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002008900 exit status 1 <nil> <nil> true [0xc001d1e0c0 0xc001d1e0d8 0xc001d1e0f0] [0xc001d1e0c0 0xc001d1e0d8 0xc001d1e0f0] [0xc001d1e0d0 0xc001d1e0e8] [0x932420 0x932420] 0xc00146ef60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:08:40.526: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:40.688: INFO: rc: 1
Feb 14 00:08:40.688: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0026dc270 exit status 1 <nil> <nil> true [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4010 0xc001bc4028] [0x932420 0x932420] 0xc001388240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:08:50.688: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:08:50.865: INFO: rc: 1
Feb 14 00:08:50.865: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb04e0 exit status 1 <nil> <nil> true [0xc001772000 0xc001772030 0xc001772070] [0xc001772000 0xc001772030 0xc001772070] [0xc001772028 0xc001772068] [0x932420 0x932420] 0xc002756240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:00.866: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:01.029: INFO: rc: 1
Feb 14 00:09:01.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb07e0 exit status 1 <nil> <nil> true [0xc001772090 0xc0017720e0 0xc001772110] [0xc001772090 0xc0017720e0 0xc001772110] [0xc0017720c8 0xc0017720f0] [0x932420 0x932420] 0xc0027565a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:11.029: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:11.196: INFO: rc: 1
Feb 14 00:09:11.196: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5e2a0 exit status 1 <nil> <nil> true [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f2478 0xc0000f2718] [0x932420 0x932420] 0xc001436300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:21.196: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:21.379: INFO: rc: 1
Feb 14 00:09:21.379: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c2d0 exit status 1 <nil> <nil> true [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28090 0xc000a280e0] [0x932420 0x932420] 0xc001066f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:31.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:31.539: INFO: rc: 1
Feb 14 00:09:31.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c720 exit status 1 <nil> <nil> true [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a28110 0xc000a28130] [0x932420 0x932420] 0xc001067260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:41.540: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:41.705: INFO: rc: 1
Feb 14 00:09:41.705: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c9c0 exit status 1 <nil> <nil> true [0xc000a28168 0xc000a281a0 0xc000a28260] [0xc000a28168 0xc000a281a0 0xc000a28260] [0xc000a28188 0xc000a28228] [0x932420 0x932420] 0xc0010675c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:09:51.705: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:09:51.890: INFO: rc: 1
Feb 14 00:09:51.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5e540 exit status 1 <nil> <nil> true [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f28b0 0xc0000f2978] [0x932420 0x932420] 0xc001436840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:01.891: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:02.095: INFO: rc: 1
Feb 14 00:10:02.095: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5eb40 exit status 1 <nil> <nil> true [0xc0000f29a8 0xc0000f2a60 0xc0000f2c00] [0xc0000f29a8 0xc0000f2a60 0xc0000f2c00] [0xc0000f29f8 0xc0000f2b80] [0x932420 0x932420] 0xc001436cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:12.096: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:12.275: INFO: rc: 1
Feb 14 00:10:12.275: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271cc90 exit status 1 <nil> <nil> true [0xc000a28278 0xc000a282d8 0xc000a28380] [0xc000a28278 0xc000a282d8 0xc000a28380] [0xc000a282c8 0xc000a28350] [0x932420 0x932420] 0xc0010678c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:22.275: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:22.436: INFO: rc: 1
Feb 14 00:10:22.436: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0026dc5a0 exit status 1 <nil> <nil> true [0xc001bc4038 0xc001bc4050 0xc001bc4068] [0xc001bc4038 0xc001bc4050 0xc001bc4068] [0xc001bc4048 0xc001bc4060] [0x932420 0x932420] 0xc001388540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:32.436: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:32.656: INFO: rc: 1
Feb 14 00:10:32.656: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb0510 exit status 1 <nil> <nil> true [0xc001772020 0xc001772050 0xc001772090] [0xc001772020 0xc001772050 0xc001772090] [0xc001772030 0xc001772070] [0x932420 0x932420] 0xc002756240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:42.657: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:42.822: INFO: rc: 1
Feb 14 00:10:42.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb0840 exit status 1 <nil> <nil> true [0xc0017720a8 0xc0017720e8 0xc001772128] [0xc0017720a8 0xc0017720e8 0xc001772128] [0xc0017720e0 0xc001772110] [0x932420 0x932420] 0xc0027565a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:10:52.822: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:10:52.989: INFO: rc: 1
Feb 14 00:10:52.990: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb0ae0 exit status 1 <nil> <nil> true [0xc001772130 0xc001772160 0xc0017721b0] [0xc001772130 0xc001772160 0xc0017721b0] [0xc001772140 0xc001772190] [0x932420 0x932420] 0xc0027568a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:02.990: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:03.204: INFO: rc: 1
Feb 14 00:11:03.204: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c270 exit status 1 <nil> <nil> true [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4010 0xc001bc4028] [0x932420 0x932420] 0xc001388240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:13.204: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:13.377: INFO: rc: 1
Feb 14 00:11:13.377: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5e270 exit status 1 <nil> <nil> true [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28090 0xc000a280e0] [0x932420 0x932420] 0xc001066f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:23.377: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:23.539: INFO: rc: 1
Feb 14 00:11:23.539: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0026dc360 exit status 1 <nil> <nil> true [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f2478 0xc0000f2718] [0x932420 0x932420] 0xc001436300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:33.539: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:33.735: INFO: rc: 1
Feb 14 00:11:33.735: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb0d80 exit status 1 <nil> <nil> true [0xc0017721c8 0xc001772208 0xc001772220] [0xc0017721c8 0xc001772208 0xc001772220] [0xc001772200 0xc001772218] [0x932420 0x932420] 0xc002756c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:43.736: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:43.950: INFO: rc: 1
Feb 14 00:11:43.950: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000cb1020 exit status 1 <nil> <nil> true [0xc001772228 0xc001772240 0xc001772278] [0xc001772228 0xc001772240 0xc001772278] [0xc001772238 0xc001772260] [0x932420 0x932420] 0xc002756fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:11:53.950: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:11:54.114: INFO: rc: 1
Feb 14 00:11:54.114: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5e570 exit status 1 <nil> <nil> true [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a28110 0xc000a28130] [0x932420 0x932420] 0xc001067260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:12:04.114: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:12:04.292: INFO: rc: 1
Feb 14 00:12:04.292: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c690 exit status 1 <nil> <nil> true [0xc001bc4038 0xc001bc4050 0xc001bc4068] [0xc001bc4038 0xc001bc4050 0xc001bc4068] [0xc001bc4048 0xc001bc4060] [0x932420 0x932420] 0xc001388540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:12:14.292: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:12:14.458: INFO: rc: 1
Feb 14 00:12:14.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00271c9f0 exit status 1 <nil> <nil> true [0xc001bc4070 0xc001bc4088 0xc001bc40a0] [0xc001bc4070 0xc001bc4088 0xc001bc40a0] [0xc001bc4080 0xc001bc4098] [0x932420 0x932420] 0xc001388840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:12:24.458: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:12:24.626: INFO: rc: 1
Feb 14 00:12:24.626: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0026dc630 exit status 1 <nil> <nil> true [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f28b0 0xc0000f2978] [0x932420 0x932420] 0xc001436840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:12:34.627: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:12:34.804: INFO: rc: 1
Feb 14 00:12:34.804: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc000e5e2a0 exit status 1 <nil> <nil> true [0xc000a28088 0xc000a280d8 0xc000a280f8] [0xc000a28088 0xc000a280d8 0xc000a280f8] [0xc000a280c0 0xc000a280f0] [0x932420 0x932420] 0xc001066f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

Feb 14 00:12:44.804: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-btv9m ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:12:45.051: INFO: rc: 1
Feb 14 00:12:45.051: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
Feb 14 00:12:45.051: INFO: Scaling statefulset ss to 0
Feb 14 00:12:45.116: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 00:12:45.137: INFO: Deleting all statefulset in ns e2e-tests-statefulset-btv9m
Feb 14 00:12:45.158: INFO: Scaling statefulset ss to 0
Feb 14 00:12:45.230: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:12:45.252: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:12:45.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-btv9m" for this suite.
Feb 14 00:12:51.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:12:51.981: INFO: namespace: e2e-tests-statefulset-btv9m, resource: bindings, ignored listing per whitelist
Feb 14 00:12:52.265: INFO: namespace e2e-tests-statefulset-btv9m deletion completed in 6.925341603s

• [SLOW TEST:355.539 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:12:52.265: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-rk5gx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:12:53.365: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 14 00:12:57.425: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 14 00:12:59.448: INFO: Creating deployment "test-rollover-deployment"
Feb 14 00:12:59.503: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 14 00:13:01.547: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 14 00:13:01.590: INFO: Ensure that both replica sets have 1 created replica
Feb 14 00:13:01.634: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 14 00:13:01.678: INFO: Updating deployment test-rollover-deployment
Feb 14 00:13:01.678: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 14 00:13:03.728: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 14 00:13:03.772: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 14 00:13:03.815: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:03.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699981, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:05.858: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:05.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699985, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:07.863: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:07.863: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699985, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:09.859: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:09.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699985, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:11.859: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:11.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699985, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:13.860: INFO: all replica sets need to contain the pod-template-hash label
Feb 14 00:13:13.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699985, loc:(*time.Location)(0x78fbda0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685699979, loc:(*time.Location)(0x78fbda0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5b76ff8c4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 14 00:13:15.860: INFO: 
Feb 14 00:13:15.860: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 00:13:15.926: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:e2e-tests-deployment-rk5gx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rk5gx/deployments/test-rollover-deployment,UID:4944b7e1-2fed-11e9-9d38-36c391ece45e,ResourceVersion:18034,Generation:2,CreationTimestamp:2019-02-14 00:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-02-14 00:12:59 +0000 UTC 2019-02-14 00:12:59 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-02-14 00:13:15 +0000 UTC 2019-02-14 00:12:59 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5b76ff8c4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Feb 14 00:13:15.950: INFO: New ReplicaSet "test-rollover-deployment-5b76ff8c4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4,GenerateName:,Namespace:e2e-tests-deployment-rk5gx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rk5gx/replicasets/test-rollover-deployment-5b76ff8c4,UID:4a964702-2fed-11e9-9d38-36c391ece45e,ResourceVersion:18027,Generation:2,CreationTimestamp:2019-02-14 00:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4944b7e1-2fed-11e9-9d38-36c391ece45e 0xc00121fc90 0xc00121fc91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Feb 14 00:13:15.950: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 14 00:13:15.950: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:e2e-tests-deployment-rk5gx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rk5gx/replicasets/test-rollover-controller,UID:459c0064-2fed-11e9-9d38-36c391ece45e,ResourceVersion:18033,Generation:2,CreationTimestamp:2019-02-14 00:12:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4944b7e1-2fed-11e9-9d38-36c391ece45e 0xc00121fbd7 0xc00121fbd8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 00:13:15.950: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6975f4fb87,GenerateName:,Namespace:e2e-tests-deployment-rk5gx,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-rk5gx/replicasets/test-rollover-deployment-6975f4fb87,UID:494706df-2fed-11e9-9d38-36c391ece45e,ResourceVersion:17989,Generation:2,CreationTimestamp:2019-02-14 00:12:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 4944b7e1-2fed-11e9-9d38-36c391ece45e 0xc00121fec7 0xc00121fec8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6975f4fb87,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 00:13:15.972: INFO: Pod "test-rollover-deployment-5b76ff8c4-vfr4x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5b76ff8c4-vfr4x,GenerateName:test-rollover-deployment-5b76ff8c4-,Namespace:e2e-tests-deployment-rk5gx,SelfLink:/api/v1/namespaces/e2e-tests-deployment-rk5gx/pods/test-rollover-deployment-5b76ff8c4-vfr4x,UID:4aa0d0aa-2fed-11e9-9d38-36c391ece45e,ResourceVersion:18004,Generation:0,CreationTimestamp:2019-02-14 00:13:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5b76ff8c4,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.124/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5b76ff8c4 4a964702-2fed-11e9-9d38-36c391ece45e 0xc00178ced0 0xc00178ced1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-hzbgs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hzbgs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hzbgs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00178cf30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00178cf50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:13:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:13:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:13:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:13:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.124,StartTime:2019-02-14 00:13:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-02-14 00:13:04 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b8ca3d5bc34bc7f74684e4038120728cffc4b00e024ef194eae981630b1d91f9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:13:15.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-rk5gx" for this suite.
Feb 14 00:13:22.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:13:22.302: INFO: namespace: e2e-tests-deployment-rk5gx, resource: bindings, ignored listing per whitelist
Feb 14 00:13:22.992: INFO: namespace e2e-tests-deployment-rk5gx deletion completed in 6.993705054s

• [SLOW TEST:30.727 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:13:22.992: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-xcxhs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-xcxhs
Feb 14 00:13:28.129: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-xcxhs
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 00:13:28.150: INFO: Initial restart count of pod liveness-http is 0
Feb 14 00:13:44.353: INFO: Restart count of pod e2e-tests-container-probe-xcxhs/liveness-http is now 1 (16.202814049s elapsed)
Feb 14 00:14:04.584: INFO: Restart count of pod e2e-tests-container-probe-xcxhs/liveness-http is now 2 (36.43375601s elapsed)
Feb 14 00:14:22.788: INFO: Restart count of pod e2e-tests-container-probe-xcxhs/liveness-http is now 3 (54.63729792s elapsed)
Feb 14 00:14:43.023: INFO: Restart count of pod e2e-tests-container-probe-xcxhs/liveness-http is now 4 (1m14.87290279s elapsed)
Feb 14 00:16:24.941: INFO: Restart count of pod e2e-tests-container-probe-xcxhs/liveness-http is now 5 (2m56.790172149s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:16:24.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-xcxhs" for this suite.
Feb 14 00:16:31.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:16:31.592: INFO: namespace: e2e-tests-container-probe-xcxhs, resource: bindings, ignored listing per whitelist
Feb 14 00:16:32.054: INFO: namespace e2e-tests-container-probe-xcxhs deletion completed in 7.024241553s

• [SLOW TEST:189.062 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:16:32.054: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mwmdf
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-map-c8ab28e0-2fed-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:16:33.249: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-mwmdf" to be "success or failure"
Feb 14 00:16:33.271: INFO: Pod "pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.613479ms
Feb 14 00:16:35.293: INFO: Pod "pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044262355s
Feb 14 00:16:37.317: INFO: Pod "pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067513445s
STEP: Saw pod success
Feb 14 00:16:37.317: INFO: Pod "pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:16:37.338: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:16:37.411: INFO: Waiting for pod pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:16:37.432: INFO: Pod pod-projected-configmaps-c8ae8826-2fed-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:16:37.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mwmdf" for this suite.
Feb 14 00:16:43.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:16:43.851: INFO: namespace: e2e-tests-projected-mwmdf, resource: bindings, ignored listing per whitelist
Feb 14 00:16:44.370: INFO: namespace e2e-tests-projected-mwmdf deletion completed in 6.916296191s

• [SLOW TEST:12.316 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:16:44.370: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-bwjxx
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating pod
Feb 14 00:16:49.825: INFO: Pod pod-hostip-d0213f1f-2fed-11e9-b8b1-3a3e684b6200 has hostIP: 10.250.0.5
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:16:49.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-bwjxx" for this suite.
Feb 14 00:17:09.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:17:10.524: INFO: namespace: e2e-tests-pods-bwjxx, resource: bindings, ignored listing per whitelist
Feb 14 00:17:10.936: INFO: namespace e2e-tests-pods-bwjxx deletion completed in 21.088191637s

• [SLOW TEST:26.566 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should get a host IP [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:17:10.936: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-54bkp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 14 00:17:12.109: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 00:17:12.153: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 00:17:12.174: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg before test
Feb 14 00:17:12.220: INFO: kube-proxy-9n7qg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.220: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 00:17:12.220: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-r49j7 from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.220: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 14 00:17:12.220: INFO: vpn-shoot-84746d495b-vvflf from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.220: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 14 00:17:12.221: INFO: metrics-server-cf4dd5768-cphxg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 00:17:12.221: INFO: blackbox-exporter-64f6f7f998-7p6v6 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 14 00:17:12.221: INFO: coredns-5f4748c5f-6746w from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container coredns ready: true, restart count 0
Feb 14 00:17:12.221: INFO: addons-kube-lego-648f8c9f5c-f8bfm from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container kube-lego ready: true, restart count 0
Feb 14 00:17:12.221: INFO: addons-kubernetes-dashboard-5f64f76bd-kpc78 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 14 00:17:12.221: INFO: node-exporter-xz5f7 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:17:12.221: INFO: calico-node-plz52 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:17:12.221: INFO: addons-nginx-ingress-controller-55d976867d-bcwjt from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.221: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 14 00:17:12.221: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn before test
Feb 14 00:17:12.280: INFO: calico-node-pwznt from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.281: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 00:17:12.281: INFO: node-exporter-tvzmp from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.281: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 00:17:12.281: INFO: kube-proxy-wnnpd from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 00:17:12.281: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.158312dbe2e679b6], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:17:13.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-54bkp" for this suite.
Feb 14 00:17:46.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:17:46.962: INFO: namespace: e2e-tests-sched-pred-54bkp, resource: bindings, ignored listing per whitelist
Feb 14 00:17:47.315: INFO: namespace e2e-tests-sched-pred-54bkp deletion completed in 33.89520272s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:36.379 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:17:47.316: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-ndqwq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-zrfb
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 00:17:48.573: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zrfb" in namespace "e2e-tests-subpath-ndqwq" to be "success or failure"
Feb 14 00:17:48.599: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.241801ms
Feb 14 00:17:50.622: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048584567s
Feb 14 00:17:52.644: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071459214s
Feb 14 00:17:54.667: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 6.094233799s
Feb 14 00:17:56.689: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 8.116492421s
Feb 14 00:17:58.713: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 10.139894696s
Feb 14 00:18:00.736: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 12.162600578s
Feb 14 00:18:02.758: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 14.185275151s
Feb 14 00:18:04.780: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 16.207243697s
Feb 14 00:18:06.805: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 18.231653311s
Feb 14 00:18:08.829: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 20.255680386s
Feb 14 00:18:10.857: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Running", Reason="", readiness=false. Elapsed: 22.283750106s
Feb 14 00:18:12.879: INFO: Pod "pod-subpath-test-configmap-zrfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.306258002s
STEP: Saw pod success
Feb 14 00:18:12.879: INFO: Pod "pod-subpath-test-configmap-zrfb" satisfied condition "success or failure"
Feb 14 00:18:12.901: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-subpath-test-configmap-zrfb container test-container-subpath-configmap-zrfb: <nil>
STEP: delete the pod
Feb 14 00:18:12.971: INFO: Waiting for pod pod-subpath-test-configmap-zrfb to disappear
Feb 14 00:18:12.991: INFO: Pod pod-subpath-test-configmap-zrfb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zrfb
Feb 14 00:18:12.991: INFO: Deleting pod "pod-subpath-test-configmap-zrfb" in namespace "e2e-tests-subpath-ndqwq"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:18:13.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-ndqwq" for this suite.
Feb 14 00:18:19.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:18:19.610: INFO: namespace: e2e-tests-subpath-ndqwq, resource: bindings, ignored listing per whitelist
Feb 14 00:18:20.133: INFO: namespace e2e-tests-subpath-ndqwq deletion completed in 7.087449044s

• [SLOW TEST:32.818 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:18:20.134: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-s7xlm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-s7xlm
[It] Should recreate evicted statefulset [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace e2e-tests-statefulset-s7xlm
STEP: Creating statefulset with conflicting port in namespace e2e-tests-statefulset-s7xlm
STEP: Waiting until pod test-pod will start running in namespace e2e-tests-statefulset-s7xlm
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace e2e-tests-statefulset-s7xlm
Feb 14 00:18:25.372: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s7xlm, name: ss-0, uid: 0b3cc37d-2fee-11e9-9d38-36c391ece45e, status phase: Pending. Waiting for statefulset controller to delete.
Feb 14 00:18:34.818: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s7xlm, name: ss-0, uid: 0b3cc37d-2fee-11e9-9d38-36c391ece45e, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 00:18:34.822: INFO: Observed stateful pod in namespace: e2e-tests-statefulset-s7xlm, name: ss-0, uid: 0b3cc37d-2fee-11e9-9d38-36c391ece45e, status phase: Failed. Waiting for statefulset controller to delete.
Feb 14 00:18:34.829: INFO: Observed delete event for stateful pod ss-0 in namespace e2e-tests-statefulset-s7xlm
STEP: Removing pod with conflicting port in namespace e2e-tests-statefulset-s7xlm
STEP: Waiting when stateful pod ss-0 will be recreated in namespace e2e-tests-statefulset-s7xlm and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 00:18:38.980: INFO: Deleting all statefulset in ns e2e-tests-statefulset-s7xlm
Feb 14 00:18:39.002: INFO: Scaling statefulset ss to 0
Feb 14 00:18:49.105: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:18:49.128: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:18:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-s7xlm" for this suite.
Feb 14 00:18:57.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:18:57.436: INFO: namespace: e2e-tests-statefulset-s7xlm, resource: bindings, ignored listing per whitelist
Feb 14 00:18:58.193: INFO: namespace e2e-tests-statefulset-s7xlm deletion completed in 8.97251713s

• [SLOW TEST:38.059 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Should recreate evicted statefulset [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:18:58.193: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-deployment-fdcnm
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:18:59.223: INFO: Creating deployment "nginx-deployment"
Feb 14 00:18:59.245: INFO: Waiting for observed generation 1
Feb 14 00:19:01.297: INFO: Waiting for all required pods to come up
Feb 14 00:19:01.333: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 14 00:19:09.378: INFO: Waiting for deployment "nginx-deployment" to complete
Feb 14 00:19:09.438: INFO: Updating deployment "nginx-deployment" with a non-existent image
Feb 14 00:19:09.487: INFO: Updating deployment nginx-deployment
Feb 14 00:19:09.487: INFO: Waiting for observed generation 2
Feb 14 00:19:11.573: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 14 00:19:11.595: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 14 00:19:11.616: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 14 00:19:11.681: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 14 00:19:11.681: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 14 00:19:11.703: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Feb 14 00:19:11.745: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Feb 14 00:19:11.745: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Feb 14 00:19:11.790: INFO: Updating deployment nginx-deployment
Feb 14 00:19:11.790: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Feb 14 00:19:11.857: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 14 00:19:13.921: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Feb 14 00:19:13.966: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fdcnm/deployments/nginx-deployment,UID:1fb5fa9a-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19122,Generation:3,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-02-14 00:19:11 +0000 UTC 2019-02-14 00:19:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-02-14 00:19:12 +0000 UTC 2019-02-14 00:18:59 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-7dc8f79789" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Feb 14 00:19:13.989: INFO: New ReplicaSet "nginx-deployment-7dc8f79789" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789,GenerateName:,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fdcnm/replicasets/nginx-deployment-7dc8f79789,UID:25d1a588-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19116,Generation:3,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1fb5fa9a-2fee-11e9-9d38-36c391ece45e 0xc001454057 0xc001454058}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Feb 14 00:19:13.989: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Feb 14 00:19:13.989: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b,GenerateName:,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/apis/apps/v1/namespaces/e2e-tests-deployment-fdcnm/replicasets/nginx-deployment-7f9675fb8b,UID:1fb88d46-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19114,Generation:3,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1fb5fa9a-2fee-11e9-9d38-36c391ece45e 0xc001454117 0xc001454118}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Feb 14 00:19:14.013: INFO: Pod "nginx-deployment-7dc8f79789-48jdt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-48jdt,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-48jdt,UID:273b155f-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19130,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc001a176f7 0xc001a176f8}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a17760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a17780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-9wpxp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-9wpxp,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-9wpxp,UID:25ecb35c-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19052,Generation:0,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc001a17840 0xc001a17841}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a178b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a178d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-c6gs5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-c6gs5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-c6gs5,UID:27359e4e-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19091,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc001a179a0 0xc001a179a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a17a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a17a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-gh2vj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-gh2vj,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-gh2vj,UID:27380c40-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19113,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144e360 0xc00144e361}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144e3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144e3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-k6qnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-k6qnl,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-k6qnl,UID:25d37447-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19038,Generation:0,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144e5e0 0xc00144e5e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144e710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144e730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-nxhq5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-nxhq5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-nxhq5,UID:273b17e4-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19120,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144e920 0xc00144e921}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144e990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144e9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-q6jcz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-q6jcz,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-q6jcz,UID:273b158c-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19129,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144eac0 0xc00144eac1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144eeb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144eed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-qpf4v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-qpf4v,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-qpf4v,UID:273b1041-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19119,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144ef90 0xc00144ef91}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-sgwc5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-sgwc5,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-sgwc5,UID:2737fce5-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19125,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144f140 0xc00144f141}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f1b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f1d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.014: INFO: Pod "nginx-deployment-7dc8f79789-smq5g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-smq5g,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-smq5g,UID:25ea764c-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19080,Generation:0,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.41/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144f2a0 0xc00144f2a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7dc8f79789-vhjpl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-vhjpl,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-vhjpl,UID:2745466a-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19133,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144f3f0 0xc00144f3f1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7dc8f79789-vmf4c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-vmf4c,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-vmf4c,UID:25d93708-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19135,Generation:0,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.40/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144f600 0xc00144f601}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.40,StartTime:2019-02-14 00:19:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7dc8f79789-zf84k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7dc8f79789-zf84k,GenerateName:nginx-deployment-7dc8f79789-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7dc8f79789-zf84k,UID:25d930dd-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19043,Generation:0,CreationTimestamp:2019-02-14 00:19:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7dc8f79789,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7dc8f79789 25d1a588-2fee-11e9-9d38-36c391ece45e 0xc00144f8d0 0xc00144f8d1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144f940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144f960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:09 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7f9675fb8b-2pxb4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-2pxb4,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-2pxb4,UID:2735a0d0-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19118,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00144fa20 0xc00144fa21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144fa80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144faa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7f9675fb8b-47tls" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-47tls,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-47tls,UID:1fc28b44-2fee-11e9-9d38-36c391ece45e,ResourceVersion:18996,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.129/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00144fc30 0xc00144fc31}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144fcd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144fcf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.129,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:06 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://9eca1020845787ac9029c77f39867a0335c1cdaafd267d2939e8223c408a25c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7f9675fb8b-4cwxj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-4cwxj,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-4cwxj,UID:27380bfc-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19128,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00144fdb0 0xc00144fdb1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144fe10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144fe30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7f9675fb8b-6w7rr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-6w7rr,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-6w7rr,UID:27380aaa-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19127,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00144fee0 0xc00144fee1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00144ff40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00144ff60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.015: INFO: Pod "nginx-deployment-7f9675fb8b-766hk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-766hk,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-766hk,UID:1fbe27a2-2fee-11e9-9d38-36c391ece45e,ResourceVersion:18984,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.39/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc0015780b0 0xc0015780b1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001578140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001578160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.39,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://eb3968260721fa243676c245b955b285de0fb3cac84882c93f39000f073c8b58}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-979qv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-979qv,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-979qv,UID:1fbb0d58-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19012,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.130/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001578480 0xc001578481}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001578560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001578580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.130,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://d8b08d5dedd29c4d981ff9d385c42eae7d75d7a7fd83542b74048f6bddd3032a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-9l94x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-9l94x,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-9l94x,UID:1fc5024a-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19004,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.133/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001578a00 0xc001578a01}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001578ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001578c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.133,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6daa3da1d636c12b9d091c7bdfc2de60be1e51fc2bd690764caf04791b8fa6f5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-b2ksf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-b2ksf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-b2ksf,UID:2735a917-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19110,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001578ce0 0xc001578ce1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001578db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001578dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-fm2x7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-fm2x7,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-fm2x7,UID:273802ba-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19117,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001579010 0xc001579011}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001579070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001579090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-g4wks" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-g4wks,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-g4wks,UID:1fbe0693-2fee-11e9-9d38-36c391ece45e,ResourceVersion:18990,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.37/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc0015792a0 0xc0015792a1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001579300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001579320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.37,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6be50dbeeadc5fe281909ca7ede7d80f7294a732f3b63d3627b15a0aac5343f4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-k458t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-k458t,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-k458t,UID:27455b0a-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19136,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001579750 0xc001579751}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0015797b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015797d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.016: INFO: Pod "nginx-deployment-7f9675fb8b-l7hgg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-l7hgg,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-l7hgg,UID:27455241-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19121,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001579a50 0xc001579a51}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001579b20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001579b40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.018: INFO: Pod "nginx-deployment-7f9675fb8b-mn6dl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-mn6dl,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-mn6dl,UID:1fc4f2e7-2fee-11e9-9d38-36c391ece45e,ResourceVersion:18987,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001579cc0 0xc001579cc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001579e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001579e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.38,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://3a48237d0db63dba0ff499866dfdf9d396cdf99bd523ad241b8a8a15a20c7b77}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.018: INFO: Pod "nginx-deployment-7f9675fb8b-nln74" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-nln74,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-nln74,UID:27455140-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19124,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc001579f20 0xc001579f21}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117a010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117a030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.018: INFO: Pod "nginx-deployment-7f9675fb8b-nxlnf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-nxlnf,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-nxlnf,UID:2745551b-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19123,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117a0e0 0xc00117a0e1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117a400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117a4f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:12 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.018: INFO: Pod "nginx-deployment-7f9675fb8b-prq9l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-prq9l,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-prq9l,UID:2732d1ec-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19111,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117a860 0xc00117a861}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117aa60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117aa80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.019: INFO: Pod "nginx-deployment-7f9675fb8b-r556p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-r556p,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-r556p,UID:1fc4fe5c-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19016,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.134/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117abc0 0xc00117abc1}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117ac90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117acb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:100.96.1.134,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://a2b9970101f017db40f9f23d4ec8646011ce3381fb79dd8d7017c8a3e872a258}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.019: INFO: Pod "nginx-deployment-7f9675fb8b-rbl5k" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-rbl5k,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-rbl5k,UID:1fbc32d7-2fee-11e9-9d38-36c391ece45e,ResourceVersion:18989,Generation:0,CreationTimestamp:2019-02-14 00:18:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117ae60 0xc00117ae61}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117aef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117af10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:18:59 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:100.96.0.36,StartTime:2019-02-14 00:18:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-02-14 00:19:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://ab26ab75305565e11a9f425036c06d076c07c53776ac6dc2de21b2483832722e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.019: INFO: Pod "nginx-deployment-7f9675fb8b-tjl8f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-tjl8f,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-tjl8f,UID:27455295-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19134,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117b100 0xc00117b101}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117b160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117b180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.5,PodIP:,StartTime:2019-02-14 00:19:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Feb 14 00:19:14.019: INFO: Pod "nginx-deployment-7f9675fb8b-xjz5v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7f9675fb8b-xjz5v,GenerateName:nginx-deployment-7f9675fb8b-,Namespace:e2e-tests-deployment-fdcnm,SelfLink:/api/v1/namespaces/e2e-tests-deployment-fdcnm/pods/nginx-deployment-7f9675fb8b-xjz5v,UID:2737fc6a-2fee-11e9-9d38-36c391ece45e,ResourceVersion:19112,Generation:0,CreationTimestamp:2019-02-14 00:19:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7f9675fb8b,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7f9675fb8b 1fb88d46-2fee-11e9-9d38-36c391ece45e 0xc00117b340 0xc00117b341}],Finalizers:[],ClusterName:,Initializers:nil,},Spec:PodSpec{Volumes:[{default-token-bs5fs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bs5fs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bs5fs true /var/run/secrets/kubernetes.io/serviceaccount  <nil>}] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00117b3b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00117b510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-02-14 00:19:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.4,PodIP:,StartTime:2019-02-14 00:19:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:19:14.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-deployment-fdcnm" for this suite.
Feb 14 00:19:22.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:19:22.314: INFO: namespace: e2e-tests-deployment-fdcnm, resource: bindings, ignored listing per whitelist
Feb 14 00:19:23.024: INFO: namespace e2e-tests-deployment-fdcnm deletion completed in 8.982625418s

• [SLOW TEST:24.831 seconds]
[sig-apps] Deployment
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:19:23.024: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-kd5vs
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:19:56.264: INFO: Container started at 2019-02-14 00:19:33 +0000 UTC, pod became ready at 2019-02-14 00:19:55 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:19:56.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-kd5vs" for this suite.
Feb 14 00:20:20.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:20:21.038: INFO: namespace: e2e-tests-container-probe-kd5vs, resource: bindings, ignored listing per whitelist
Feb 14 00:20:21.234: INFO: namespace e2e-tests-container-probe-kd5vs deletion completed in 24.948576714s

• [SLOW TEST:58.210 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:20:21.235: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-vg66l
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
Feb 14 00:20:26.478: INFO: running pod: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-submit-remove-513cddd0-2fee-11e9-b8b1-3a3e684b6200", GenerateName:"", Namespace:"e2e-tests-pods-vg66l", SelfLink:"/api/v1/namespaces/e2e-tests-pods-vg66l/pods/pod-submit-remove-513cddd0-2fee-11e9-b8b1-3a3e684b6200", UID:"51440a68-2fee-11e9-9d38-36c391ece45e", ResourceVersion:"19390", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685700422, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"317619670"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.140/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hwkz2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00212e1c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container(nil), Containers:[]v1.Container{v1.Container{Name:"nginx", Image:"docker.io/library/nginx:1.14-alpine", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwkz2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001fe1628), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00217d560), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fe1660)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001fe1680)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001fe1688), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Running", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685700422, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685700425, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"ContainersReady", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685700425, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685700422, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.140", StartTime:(*v1.Time)(0xc001ea7f80), InitContainerStatuses:[]v1.ContainerStatus(nil), ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"nginx", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc001ea7fa0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:0, Image:"nginx:1.14-alpine", ImageID:"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632", ContainerID:"docker://dd35d26a8461b4cdc64be96ef0b737e34c2d176d1f98808595f5db6965c360be"}}, QOSClass:"BestEffort"}}
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 14 00:20:31.534: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:20:31.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-vg66l" for this suite.
Feb 14 00:20:39.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:20:40.363: INFO: namespace: e2e-tests-pods-vg66l, resource: bindings, ignored listing per whitelist
Feb 14 00:20:40.577: INFO: namespace e2e-tests-pods-vg66l deletion completed in 8.99867812s

• [SLOW TEST:19.342 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be submitted and removed [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:20:40.577: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-probe-qvxct
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod liveness-exec in namespace e2e-tests-container-probe-qvxct
Feb 14 00:20:45.888: INFO: Started pod liveness-exec in namespace e2e-tests-container-probe-qvxct
STEP: checking the pod's current state and verifying that restartCount is present
Feb 14 00:20:45.910: INFO: Initial restart count of pod liveness-exec is 0
Feb 14 00:21:40.552: INFO: Restart count of pod e2e-tests-container-probe-qvxct/liveness-exec is now 1 (54.640662965s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:21:40.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-probe-qvxct" for this suite.
Feb 14 00:21:46.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:21:46.987: INFO: namespace: e2e-tests-container-probe-qvxct, resource: bindings, ignored listing per whitelist
Feb 14 00:21:47.590: INFO: namespace e2e-tests-container-probe-qvxct deletion completed in 6.986969673s

• [SLOW TEST:67.014 seconds]
[k8s.io] Probing container
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:21:47.590: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-nj4gp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 00:21:48.706: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:21:54.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-nj4gp" for this suite.
Feb 14 00:22:24.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:22:25.623: INFO: namespace: e2e-tests-init-container-nj4gp, resource: bindings, ignored listing per whitelist
Feb 14 00:22:25.776: INFO: namespace e2e-tests-init-container-nj4gp deletion completed in 30.955974322s

• [SLOW TEST:38.185 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should invoke init containers on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:22:25.776: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svc-latency-sdc98
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating replication controller svc-latency-rc in namespace e2e-tests-svc-latency-sdc98
I0214 00:22:27.013937    3058 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: e2e-tests-svc-latency-sdc98, replica count: 1
I0214 00:22:28.064477    3058 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0214 00:22:29.064724    3058 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0214 00:22:30.064967    3058 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 14 00:22:30.244: INFO: Created: latency-svc-7l59m
Feb 14 00:22:30.260: INFO: Got endpoints: latency-svc-7l59m [57.978776ms]
Feb 14 00:22:30.304: INFO: Created: latency-svc-vzhpn
Feb 14 00:22:30.341: INFO: Got endpoints: latency-svc-vzhpn [81.137291ms]
Feb 14 00:22:30.343: INFO: Created: latency-svc-7864t
Feb 14 00:22:30.357: INFO: Got endpoints: latency-svc-7864t [97.082977ms]
Feb 14 00:22:30.380: INFO: Created: latency-svc-m2tb7
Feb 14 00:22:30.392: INFO: Got endpoints: latency-svc-m2tb7 [132.159851ms]
Feb 14 00:22:30.406: INFO: Created: latency-svc-ntdmh
Feb 14 00:22:30.419: INFO: Got endpoints: latency-svc-ntdmh [154.31432ms]
Feb 14 00:22:30.432: INFO: Created: latency-svc-n5hhk
Feb 14 00:22:30.521: INFO: Got endpoints: latency-svc-n5hhk [255.685933ms]
Feb 14 00:22:30.525: INFO: Created: latency-svc-jktg7
Feb 14 00:22:30.529: INFO: Got endpoints: latency-svc-jktg7 [264.030679ms]
Feb 14 00:22:30.623: INFO: Created: latency-svc-djtkp
Feb 14 00:22:30.626: INFO: Got endpoints: latency-svc-djtkp [361.35748ms]
Feb 14 00:22:30.642: INFO: Created: latency-svc-bk47l
Feb 14 00:22:30.654: INFO: Got endpoints: latency-svc-bk47l [389.406251ms]
Feb 14 00:22:30.668: INFO: Created: latency-svc-gpghx
Feb 14 00:22:30.680: INFO: Got endpoints: latency-svc-gpghx [415.213988ms]
Feb 14 00:22:30.694: INFO: Created: latency-svc-f9tpn
Feb 14 00:22:30.706: INFO: Got endpoints: latency-svc-f9tpn [440.635666ms]
Feb 14 00:22:30.733: INFO: Created: latency-svc-bwhzq
Feb 14 00:22:30.752: INFO: Created: latency-svc-ff98s
Feb 14 00:22:30.752: INFO: Got endpoints: latency-svc-bwhzq [486.388754ms]
Feb 14 00:22:30.756: INFO: Got endpoints: latency-svc-ff98s [490.684634ms]
Feb 14 00:22:30.769: INFO: Created: latency-svc-27nrc
Feb 14 00:22:30.782: INFO: Got endpoints: latency-svc-27nrc [517.034871ms]
Feb 14 00:22:30.796: INFO: Created: latency-svc-6fxmf
Feb 14 00:22:30.809: INFO: Got endpoints: latency-svc-6fxmf [544.223993ms]
Feb 14 00:22:30.823: INFO: Created: latency-svc-wckvp
Feb 14 00:22:30.856: INFO: Got endpoints: latency-svc-wckvp [596.151475ms]
Feb 14 00:22:30.894: INFO: Created: latency-svc-r7bkg
Feb 14 00:22:30.898: INFO: Got endpoints: latency-svc-r7bkg [556.274834ms]
Feb 14 00:22:30.932: INFO: Created: latency-svc-qjc79
Feb 14 00:22:30.945: INFO: Got endpoints: latency-svc-qjc79 [587.485582ms]
Feb 14 00:22:31.009: INFO: Created: latency-svc-qnp65
Feb 14 00:22:31.011: INFO: Got endpoints: latency-svc-qnp65 [618.637578ms]
Feb 14 00:22:31.030: INFO: Created: latency-svc-fz4lh
Feb 14 00:22:31.044: INFO: Got endpoints: latency-svc-fz4lh [624.185447ms]
Feb 14 00:22:31.056: INFO: Created: latency-svc-45x9g
Feb 14 00:22:31.068: INFO: Got endpoints: latency-svc-45x9g [547.571845ms]
Feb 14 00:22:31.082: INFO: Created: latency-svc-d9d7r
Feb 14 00:22:31.103: INFO: Got endpoints: latency-svc-d9d7r [573.794873ms]
Feb 14 00:22:31.151: INFO: Created: latency-svc-vzvqv
Feb 14 00:22:31.168: INFO: Got endpoints: latency-svc-vzvqv [541.257236ms]
Feb 14 00:22:31.168: INFO: Created: latency-svc-jffk8
Feb 14 00:22:31.172: INFO: Got endpoints: latency-svc-jffk8 [517.662124ms]
Feb 14 00:22:31.186: INFO: Created: latency-svc-tq6mv
Feb 14 00:22:31.198: INFO: Got endpoints: latency-svc-tq6mv [517.940912ms]
Feb 14 00:22:31.212: INFO: Created: latency-svc-dpnp8
Feb 14 00:22:31.225: INFO: Got endpoints: latency-svc-dpnp8 [519.45124ms]
Feb 14 00:22:31.240: INFO: Created: latency-svc-n2kqz
Feb 14 00:22:31.280: INFO: Got endpoints: latency-svc-n2kqz [528.345733ms]
Feb 14 00:22:31.281: INFO: Created: latency-svc-87m6j
Feb 14 00:22:31.285: INFO: Got endpoints: latency-svc-87m6j [529.550927ms]
Feb 14 00:22:31.304: INFO: Created: latency-svc-bgpq6
Feb 14 00:22:31.313: INFO: Got endpoints: latency-svc-bgpq6 [531.100284ms]
Feb 14 00:22:31.328: INFO: Created: latency-svc-cfznb
Feb 14 00:22:31.341: INFO: Got endpoints: latency-svc-cfznb [531.473379ms]
Feb 14 00:22:31.355: INFO: Created: latency-svc-2lxzl
Feb 14 00:22:31.367: INFO: Got endpoints: latency-svc-2lxzl [510.430423ms]
Feb 14 00:22:31.432: INFO: Created: latency-svc-l6x7b
Feb 14 00:22:31.434: INFO: Got endpoints: latency-svc-l6x7b [536.490389ms]
Feb 14 00:22:31.454: INFO: Created: latency-svc-vwrnl
Feb 14 00:22:31.467: INFO: Got endpoints: latency-svc-vwrnl [522.086775ms]
Feb 14 00:22:31.481: INFO: Created: latency-svc-l28nn
Feb 14 00:22:31.493: INFO: Got endpoints: latency-svc-l28nn [481.858894ms]
Feb 14 00:22:31.507: INFO: Created: latency-svc-f2tpl
Feb 14 00:22:31.520: INFO: Got endpoints: latency-svc-f2tpl [476.103803ms]
Feb 14 00:22:31.582: INFO: Created: latency-svc-kknvl
Feb 14 00:22:31.597: INFO: Got endpoints: latency-svc-kknvl [528.875238ms]
Feb 14 00:22:31.598: INFO: Created: latency-svc-ww7cr
Feb 14 00:22:31.617: INFO: Got endpoints: latency-svc-ww7cr [514.184587ms]
Feb 14 00:22:31.638: INFO: Created: latency-svc-lx8kr
Feb 14 00:22:31.659: INFO: Got endpoints: latency-svc-lx8kr [491.184052ms]
Feb 14 00:22:31.679: INFO: Created: latency-svc-7dxg7
Feb 14 00:22:31.720: INFO: Got endpoints: latency-svc-7dxg7 [547.463188ms]
Feb 14 00:22:31.721: INFO: Created: latency-svc-5whgn
Feb 14 00:22:31.726: INFO: Got endpoints: latency-svc-5whgn [527.572776ms]
Feb 14 00:22:31.745: INFO: Created: latency-svc-dbn9k
Feb 14 00:22:31.757: INFO: Got endpoints: latency-svc-dbn9k [532.046562ms]
Feb 14 00:22:31.771: INFO: Created: latency-svc-4d4mx
Feb 14 00:22:31.783: INFO: Got endpoints: latency-svc-4d4mx [503.424765ms]
Feb 14 00:22:31.798: INFO: Created: latency-svc-6dvk2
Feb 14 00:22:31.820: INFO: Got endpoints: latency-svc-6dvk2 [534.60912ms]
Feb 14 00:22:31.862: INFO: Created: latency-svc-cp2p8
Feb 14 00:22:31.864: INFO: Got endpoints: latency-svc-cp2p8 [550.870956ms]
Feb 14 00:22:31.898: INFO: Created: latency-svc-bn2z6
Feb 14 00:22:31.910: INFO: Got endpoints: latency-svc-bn2z6 [568.717507ms]
Feb 14 00:22:31.923: INFO: Created: latency-svc-8gczx
Feb 14 00:22:31.936: INFO: Got endpoints: latency-svc-8gczx [568.474962ms]
Feb 14 00:22:31.953: INFO: Created: latency-svc-qnxhs
Feb 14 00:22:31.993: INFO: Got endpoints: latency-svc-qnxhs [558.665878ms]
Feb 14 00:22:31.994: INFO: Created: latency-svc-zf2kb
Feb 14 00:22:32.000: INFO: Got endpoints: latency-svc-zf2kb [533.314279ms]
Feb 14 00:22:32.013: INFO: Created: latency-svc-lcxlv
Feb 14 00:22:32.026: INFO: Got endpoints: latency-svc-lcxlv [532.963772ms]
Feb 14 00:22:32.037: INFO: Created: latency-svc-7bwmb
Feb 14 00:22:32.052: INFO: Got endpoints: latency-svc-7bwmb [531.842928ms]
Feb 14 00:22:32.065: INFO: Created: latency-svc-vlt74
Feb 14 00:22:32.078: INFO: Got endpoints: latency-svc-vlt74 [480.561265ms]
Feb 14 00:22:32.091: INFO: Created: latency-svc-bz74c
Feb 14 00:22:32.129: INFO: Got endpoints: latency-svc-bz74c [512.002985ms]
Feb 14 00:22:32.135: INFO: Created: latency-svc-28w6n
Feb 14 00:22:32.149: INFO: Created: latency-svc-chwb8
Feb 14 00:22:32.150: INFO: Got endpoints: latency-svc-28w6n [491.3776ms]
Feb 14 00:22:32.163: INFO: Got endpoints: latency-svc-chwb8 [443.377648ms]
Feb 14 00:22:32.188: INFO: Created: latency-svc-j4zgh
Feb 14 00:22:32.208: INFO: Got endpoints: latency-svc-j4zgh [481.70705ms]
Feb 14 00:22:32.299: INFO: Created: latency-svc-n977d
Feb 14 00:22:32.315: INFO: Got endpoints: latency-svc-n977d [557.462028ms]
Feb 14 00:22:32.315: INFO: Created: latency-svc-v587d
Feb 14 00:22:32.327: INFO: Got endpoints: latency-svc-v587d [543.433749ms]
Feb 14 00:22:32.340: INFO: Created: latency-svc-rffj6
Feb 14 00:22:32.353: INFO: Got endpoints: latency-svc-rffj6 [532.787108ms]
Feb 14 00:22:32.365: INFO: Created: latency-svc-58vd4
Feb 14 00:22:32.376: INFO: Got endpoints: latency-svc-58vd4 [512.008011ms]
Feb 14 00:22:32.390: INFO: Created: latency-svc-7tbl9
Feb 14 00:22:32.423: INFO: Got endpoints: latency-svc-7tbl9 [512.776789ms]
Feb 14 00:22:32.437: INFO: Created: latency-svc-wgmrw
Feb 14 00:22:32.450: INFO: Got endpoints: latency-svc-wgmrw [514.084748ms]
Feb 14 00:22:32.475: INFO: Created: latency-svc-8mcsl
Feb 14 00:22:32.482: INFO: Got endpoints: latency-svc-8mcsl [489.009523ms]
Feb 14 00:22:32.497: INFO: Created: latency-svc-2hstd
Feb 14 00:22:32.510: INFO: Got endpoints: latency-svc-2hstd [509.055386ms]
Feb 14 00:22:32.574: INFO: Created: latency-svc-w6x4m
Feb 14 00:22:32.577: INFO: Got endpoints: latency-svc-w6x4m [550.848363ms]
Feb 14 00:22:32.597: INFO: Created: latency-svc-6z6gn
Feb 14 00:22:32.609: INFO: Got endpoints: latency-svc-6z6gn [557.254747ms]
Feb 14 00:22:32.623: INFO: Created: latency-svc-892bf
Feb 14 00:22:32.636: INFO: Got endpoints: latency-svc-892bf [557.657331ms]
Feb 14 00:22:32.649: INFO: Created: latency-svc-m7vxv
Feb 14 00:22:32.662: INFO: Got endpoints: latency-svc-m7vxv [532.956323ms]
Feb 14 00:22:32.714: INFO: Created: latency-svc-cqv92
Feb 14 00:22:32.730: INFO: Created: latency-svc-7drwm
Feb 14 00:22:32.730: INFO: Got endpoints: latency-svc-cqv92 [579.956106ms]
Feb 14 00:22:32.742: INFO: Got endpoints: latency-svc-7drwm [578.671354ms]
Feb 14 00:22:32.776: INFO: Created: latency-svc-8k84f
Feb 14 00:22:32.788: INFO: Got endpoints: latency-svc-8k84f [580.786849ms]
Feb 14 00:22:32.802: INFO: Created: latency-svc-tbjgb
Feb 14 00:22:32.839: INFO: Got endpoints: latency-svc-tbjgb [524.592146ms]
Feb 14 00:22:32.840: INFO: Created: latency-svc-f78gj
Feb 14 00:22:32.845: INFO: Got endpoints: latency-svc-f78gj [518.20881ms]
Feb 14 00:22:32.864: INFO: Created: latency-svc-kvhfw
Feb 14 00:22:32.867: INFO: Got endpoints: latency-svc-kvhfw [514.524975ms]
Feb 14 00:22:32.881: INFO: Created: latency-svc-tdxm4
Feb 14 00:22:32.893: INFO: Got endpoints: latency-svc-tdxm4 [516.973291ms]
Feb 14 00:22:32.907: INFO: Created: latency-svc-pf6nb
Feb 14 00:22:32.933: INFO: Created: latency-svc-wqqnc
Feb 14 00:22:32.933: INFO: Got endpoints: latency-svc-pf6nb [510.687299ms]
Feb 14 00:22:32.980: INFO: Got endpoints: latency-svc-wqqnc [529.868367ms]
Feb 14 00:22:32.996: INFO: Created: latency-svc-8pcxs
Feb 14 00:22:33.008: INFO: Got endpoints: latency-svc-8pcxs [525.374244ms]
Feb 14 00:22:33.022: INFO: Created: latency-svc-fzrjn
Feb 14 00:22:33.033: INFO: Got endpoints: latency-svc-fzrjn [523.850942ms]
Feb 14 00:22:33.048: INFO: Created: latency-svc-dbqgn
Feb 14 00:22:33.069: INFO: Got endpoints: latency-svc-dbqgn [491.996915ms]
Feb 14 00:22:33.118: INFO: Created: latency-svc-2dxzd
Feb 14 00:22:33.135: INFO: Got endpoints: latency-svc-2dxzd [525.503469ms]
Feb 14 00:22:33.135: INFO: Created: latency-svc-k5xlx
Feb 14 00:22:33.146: INFO: Got endpoints: latency-svc-k5xlx [510.729647ms]
Feb 14 00:22:33.159: INFO: Created: latency-svc-bldzq
Feb 14 00:22:33.172: INFO: Got endpoints: latency-svc-bldzq [509.877149ms]
Feb 14 00:22:33.185: INFO: Created: latency-svc-vqqt5
Feb 14 00:22:33.197: INFO: Got endpoints: latency-svc-vqqt5 [466.372349ms]
Feb 14 00:22:33.210: INFO: Created: latency-svc-bs9dr
Feb 14 00:22:33.248: INFO: Got endpoints: latency-svc-bs9dr [505.950405ms]
Feb 14 00:22:33.249: INFO: Created: latency-svc-wkjjf
Feb 14 00:22:33.253: INFO: Got endpoints: latency-svc-wkjjf [464.86957ms]
Feb 14 00:22:33.269: INFO: Created: latency-svc-nxpmp
Feb 14 00:22:33.281: INFO: Got endpoints: latency-svc-nxpmp [441.881366ms]
Feb 14 00:22:33.295: INFO: Created: latency-svc-6mmmj
Feb 14 00:22:33.307: INFO: Got endpoints: latency-svc-6mmmj [461.327071ms]
Feb 14 00:22:33.319: INFO: Created: latency-svc-f5dxh
Feb 14 00:22:33.331: INFO: Got endpoints: latency-svc-f5dxh [463.13219ms]
Feb 14 00:22:33.343: INFO: Created: latency-svc-pb6q9
Feb 14 00:22:33.382: INFO: Got endpoints: latency-svc-pb6q9 [488.864063ms]
Feb 14 00:22:33.393: INFO: Created: latency-svc-qz68g
Feb 14 00:22:33.406: INFO: Got endpoints: latency-svc-qz68g [472.96079ms]
Feb 14 00:22:33.422: INFO: Created: latency-svc-597l7
Feb 14 00:22:33.435: INFO: Got endpoints: latency-svc-597l7 [455.625573ms]
Feb 14 00:22:33.450: INFO: Created: latency-svc-xttnr
Feb 14 00:22:33.465: INFO: Got endpoints: latency-svc-xttnr [457.123898ms]
Feb 14 00:22:33.531: INFO: Created: latency-svc-z8jcc
Feb 14 00:22:33.533: INFO: Got endpoints: latency-svc-z8jcc [499.777972ms]
Feb 14 00:22:33.570: INFO: Created: latency-svc-4tz67
Feb 14 00:22:33.593: INFO: Got endpoints: latency-svc-4tz67 [524.31036ms]
Feb 14 00:22:33.615: INFO: Created: latency-svc-r998j
Feb 14 00:22:33.628: INFO: Got endpoints: latency-svc-r998j [493.622604ms]
Feb 14 00:22:33.653: INFO: Created: latency-svc-t6fqd
Feb 14 00:22:33.656: INFO: Got endpoints: latency-svc-t6fqd [510.046607ms]
Feb 14 00:22:33.675: INFO: Created: latency-svc-wx67d
Feb 14 00:22:33.687: INFO: Got endpoints: latency-svc-wx67d [515.195504ms]
Feb 14 00:22:33.702: INFO: Created: latency-svc-jc9th
Feb 14 00:22:33.714: INFO: Got endpoints: latency-svc-jc9th [517.578664ms]
Feb 14 00:22:33.728: INFO: Created: latency-svc-6j2dl
Feb 14 00:22:33.750: INFO: Got endpoints: latency-svc-6j2dl [502.505164ms]
Feb 14 00:22:33.791: INFO: Created: latency-svc-6w2gk
Feb 14 00:22:33.808: INFO: Got endpoints: latency-svc-6w2gk [554.485903ms]
Feb 14 00:22:33.808: INFO: Created: latency-svc-jl5d7
Feb 14 00:22:33.835: INFO: Created: latency-svc-f6jpn
Feb 14 00:22:33.835: INFO: Got endpoints: latency-svc-jl5d7 [553.928529ms]
Feb 14 00:22:33.861: INFO: Created: latency-svc-hnm9n
Feb 14 00:22:33.886: INFO: Created: latency-svc-r8llc
Feb 14 00:22:33.886: INFO: Got endpoints: latency-svc-f6jpn [579.509759ms]
Feb 14 00:22:33.926: INFO: Got endpoints: latency-svc-hnm9n [595.687628ms]
Feb 14 00:22:33.960: INFO: Created: latency-svc-vhwsb
Feb 14 00:22:33.991: INFO: Created: latency-svc-kzjfw
Feb 14 00:22:33.991: INFO: Got endpoints: latency-svc-r8llc [609.105422ms]
Feb 14 00:22:34.017: INFO: Created: latency-svc-69c97
Feb 14 00:22:34.051: INFO: Got endpoints: latency-svc-vhwsb [644.259141ms]
Feb 14 00:22:34.052: INFO: Created: latency-svc-gpmrx
Feb 14 00:22:34.076: INFO: Created: latency-svc-p2jgl
Feb 14 00:22:34.102: INFO: Created: latency-svc-h4f4f
Feb 14 00:22:34.131: INFO: Created: latency-svc-9v9x2
Feb 14 00:22:34.131: INFO: Got endpoints: latency-svc-kzjfw [695.218282ms]
Feb 14 00:22:34.131: INFO: Got endpoints: latency-svc-69c97 [665.800858ms]
Feb 14 00:22:34.194: INFO: Got endpoints: latency-svc-gpmrx [660.180689ms]
Feb 14 00:22:34.202: INFO: Created: latency-svc-c7cf8
Feb 14 00:22:34.230: INFO: Created: latency-svc-c6xkj
Feb 14 00:22:34.230: INFO: Got endpoints: latency-svc-p2jgl [636.853042ms]
Feb 14 00:22:34.256: INFO: Created: latency-svc-6kclh
Feb 14 00:22:34.283: INFO: Created: latency-svc-8spbv
Feb 14 00:22:34.283: INFO: Got endpoints: latency-svc-h4f4f [654.651929ms]
Feb 14 00:22:34.325: INFO: Got endpoints: latency-svc-9v9x2 [668.926025ms]
Feb 14 00:22:34.329: INFO: Created: latency-svc-rxdfq
Feb 14 00:22:34.381: INFO: Got endpoints: latency-svc-c7cf8 [693.743477ms]
Feb 14 00:22:34.382: INFO: Created: latency-svc-plqxh
Feb 14 00:22:34.408: INFO: Created: latency-svc-6j459
Feb 14 00:22:34.487: INFO: Created: latency-svc-rvkf2
Feb 14 00:22:34.487: INFO: Got endpoints: latency-svc-6kclh [736.433902ms]
Feb 14 00:22:34.487: INFO: Got endpoints: latency-svc-c6xkj [772.730228ms]
Feb 14 00:22:34.504: INFO: Created: latency-svc-dmh9r
Feb 14 00:22:34.531: INFO: Created: latency-svc-wktv6
Feb 14 00:22:34.531: INFO: Got endpoints: latency-svc-8spbv [722.785179ms]
Feb 14 00:22:34.557: INFO: Created: latency-svc-rmzq7
Feb 14 00:22:34.584: INFO: Got endpoints: latency-svc-rxdfq [748.389069ms]
Feb 14 00:22:34.585: INFO: Created: latency-svc-gl4tx
Feb 14 00:22:34.627: INFO: Created: latency-svc-znt5c
Feb 14 00:22:34.627: INFO: Got endpoints: latency-svc-plqxh [740.867226ms]
Feb 14 00:22:34.656: INFO: Created: latency-svc-x2r96
Feb 14 00:22:34.686: INFO: Got endpoints: latency-svc-6j459 [759.381472ms]
Feb 14 00:22:34.687: INFO: Created: latency-svc-bzfbf
Feb 14 00:22:34.711: INFO: Created: latency-svc-w8m6l
Feb 14 00:22:34.755: INFO: Got endpoints: latency-svc-rvkf2 [763.969348ms]
Feb 14 00:22:34.757: INFO: Created: latency-svc-svnv9
Feb 14 00:22:34.782: INFO: Got endpoints: latency-svc-dmh9r [731.678069ms]
Feb 14 00:22:34.783: INFO: Created: latency-svc-s5dbv
Feb 14 00:22:34.825: INFO: Created: latency-svc-hlcql
Feb 14 00:22:34.836: INFO: Got endpoints: latency-svc-wktv6 [705.738467ms]
Feb 14 00:22:34.851: INFO: Created: latency-svc-7l9w2
Feb 14 00:22:34.901: INFO: Got endpoints: latency-svc-rmzq7 [770.704115ms]
Feb 14 00:22:34.903: INFO: Created: latency-svc-mpzjj
Feb 14 00:22:34.921: INFO: Created: latency-svc-rlbv2
Feb 14 00:22:34.940: INFO: Got endpoints: latency-svc-gl4tx [746.046278ms]
Feb 14 00:22:34.940: INFO: Created: latency-svc-xhbqm
Feb 14 00:22:34.966: INFO: Created: latency-svc-5d9bs
Feb 14 00:22:34.988: INFO: Got endpoints: latency-svc-znt5c [757.664062ms]
Feb 14 00:22:35.032: INFO: Created: latency-svc-hqxxg
Feb 14 00:22:35.032: INFO: Got endpoints: latency-svc-x2r96 [748.688944ms]
Feb 14 00:22:35.048: INFO: Created: latency-svc-rkf8t
Feb 14 00:22:35.066: INFO: Created: latency-svc-5vcz5
Feb 14 00:22:35.079: INFO: Got endpoints: latency-svc-bzfbf [753.087856ms]
Feb 14 00:22:35.097: INFO: Created: latency-svc-7q82t
Feb 14 00:22:35.122: INFO: Created: latency-svc-f79zz
Feb 14 00:22:35.157: INFO: Got endpoints: latency-svc-w8m6l [775.34098ms]
Feb 14 00:22:35.158: INFO: Created: latency-svc-6jrjv
Feb 14 00:22:35.176: INFO: Got endpoints: latency-svc-svnv9 [689.309098ms]
Feb 14 00:22:35.197: INFO: Created: latency-svc-tg7m9
Feb 14 00:22:35.224: INFO: Created: latency-svc-rkqlf
Feb 14 00:22:35.237: INFO: Got endpoints: latency-svc-s5dbv [749.355173ms]
Feb 14 00:22:35.293: INFO: Got endpoints: latency-svc-hlcql [762.622364ms]
Feb 14 00:22:35.312: INFO: Created: latency-svc-b6ndl
Feb 14 00:22:35.325: INFO: Got endpoints: latency-svc-7l9w2 [740.937575ms]
Feb 14 00:22:35.342: INFO: Created: latency-svc-gdqsq
Feb 14 00:22:35.372: INFO: Created: latency-svc-658jt
Feb 14 00:22:35.383: INFO: Got endpoints: latency-svc-mpzjj [755.86192ms]
Feb 14 00:22:35.440: INFO: Got endpoints: latency-svc-rlbv2 [754.366311ms]
Feb 14 00:22:35.460: INFO: Created: latency-svc-6wmxv
Feb 14 00:22:35.476: INFO: Got endpoints: latency-svc-xhbqm [720.139929ms]
Feb 14 00:22:35.495: INFO: Created: latency-svc-76h2q
Feb 14 00:22:35.532: INFO: Got endpoints: latency-svc-5d9bs [749.982653ms]
Feb 14 00:22:35.569: INFO: Created: latency-svc-wbfr2
Feb 14 00:22:35.589: INFO: Got endpoints: latency-svc-hqxxg [752.473423ms]
Feb 14 00:22:35.589: INFO: Created: latency-svc-k579k
Feb 14 00:22:35.630: INFO: Got endpoints: latency-svc-rkf8t [728.259248ms]
Feb 14 00:22:35.630: INFO: Created: latency-svc-sm2bm
Feb 14 00:22:35.717: INFO: Created: latency-svc-4smrc
Feb 14 00:22:35.717: INFO: Got endpoints: latency-svc-5vcz5 [777.313032ms]
Feb 14 00:22:35.725: INFO: Got endpoints: latency-svc-7q82t [737.763488ms]
Feb 14 00:22:35.756: INFO: Created: latency-svc-ckjdj
Feb 14 00:22:35.781: INFO: Created: latency-svc-khkvq
Feb 14 00:22:35.782: INFO: Got endpoints: latency-svc-f79zz [749.9211ms]
Feb 14 00:22:35.865: INFO: Created: latency-svc-l2sqg
Feb 14 00:22:35.865: INFO: Got endpoints: latency-svc-6jrjv [786.362303ms]
Feb 14 00:22:35.875: INFO: Got endpoints: latency-svc-tg7m9 [718.658372ms]
Feb 14 00:22:35.909: INFO: Created: latency-svc-rz84f
Feb 14 00:22:35.934: INFO: Created: latency-svc-ndjzl
Feb 14 00:22:35.934: INFO: Got endpoints: latency-svc-rkqlf [757.986235ms]
Feb 14 00:22:36.022: INFO: Got endpoints: latency-svc-b6ndl [785.387232ms]
Feb 14 00:22:36.022: INFO: Created: latency-svc-ptrl2
Feb 14 00:22:36.025: INFO: Got endpoints: latency-svc-gdqsq [731.932248ms]
Feb 14 00:22:36.062: INFO: Created: latency-svc-297r4
Feb 14 00:22:36.088: INFO: Created: latency-svc-gqndx
Feb 14 00:22:36.089: INFO: Got endpoints: latency-svc-658jt [763.609878ms]
Feb 14 00:22:36.166: INFO: Created: latency-svc-vx77n
Feb 14 00:22:36.166: INFO: Got endpoints: latency-svc-6wmxv [782.587718ms]
Feb 14 00:22:36.175: INFO: Got endpoints: latency-svc-76h2q [734.924383ms]
Feb 14 00:22:36.206: INFO: Created: latency-svc-6fbsj
Feb 14 00:22:36.231: INFO: Created: latency-svc-62kq7
Feb 14 00:22:36.231: INFO: Got endpoints: latency-svc-wbfr2 [755.57383ms]
Feb 14 00:22:36.302: INFO: Got endpoints: latency-svc-k579k [769.28712ms]
Feb 14 00:22:36.323: INFO: Created: latency-svc-4lstg
Feb 14 00:22:36.336: INFO: Got endpoints: latency-svc-sm2bm [746.824032ms]
Feb 14 00:22:36.371: INFO: Created: latency-svc-64f8h
Feb 14 00:22:36.422: INFO: Got endpoints: latency-svc-4smrc [792.159902ms]
Feb 14 00:22:36.425: INFO: Created: latency-svc-mcqv7
Feb 14 00:22:36.428: INFO: Got endpoints: latency-svc-ckjdj [710.522576ms]
Feb 14 00:22:36.461: INFO: Created: latency-svc-bjkwj
Feb 14 00:22:36.486: INFO: Created: latency-svc-b8hhj
Feb 14 00:22:36.487: INFO: Got endpoints: latency-svc-khkvq [762.068043ms]
Feb 14 00:22:36.558: INFO: Got endpoints: latency-svc-l2sqg [776.5245ms]
Feb 14 00:22:36.559: INFO: Created: latency-svc-pdmr2
Feb 14 00:22:36.576: INFO: Got endpoints: latency-svc-rz84f [710.581837ms]
Feb 14 00:22:36.600: INFO: Created: latency-svc-cc86g
Feb 14 00:22:36.625: INFO: Created: latency-svc-6f4x7
Feb 14 00:22:36.636: INFO: Got endpoints: latency-svc-ndjzl [761.060704ms]
Feb 14 00:22:36.691: INFO: Got endpoints: latency-svc-ptrl2 [757.112581ms]
Feb 14 00:22:36.732: INFO: Created: latency-svc-kv772
Feb 14 00:22:36.732: INFO: Got endpoints: latency-svc-297r4 [709.984588ms]
Feb 14 00:22:36.759: INFO: Created: latency-svc-w7kq2
Feb 14 00:22:36.787: INFO: Created: latency-svc-c6bbc
Feb 14 00:22:36.787: INFO: Got endpoints: latency-svc-gqndx [761.602544ms]
Feb 14 00:22:36.834: INFO: Got endpoints: latency-svc-vx77n [745.105059ms]
Feb 14 00:22:36.851: INFO: Created: latency-svc-v79ph
Feb 14 00:22:36.894: INFO: Got endpoints: latency-svc-6fbsj [727.986842ms]
Feb 14 00:22:36.894: INFO: Created: latency-svc-gmgql
Feb 14 00:22:36.932: INFO: Got endpoints: latency-svc-62kq7 [756.414025ms]
Feb 14 00:22:36.932: INFO: Created: latency-svc-qhpgg
Feb 14 00:22:36.976: INFO: Got endpoints: latency-svc-4lstg [744.345413ms]
Feb 14 00:22:36.994: INFO: Created: latency-svc-pc6h9
Feb 14 00:22:37.036: INFO: Got endpoints: latency-svc-64f8h [733.855951ms]
Feb 14 00:22:37.036: INFO: Created: latency-svc-ld29k
Feb 14 00:22:37.075: INFO: Created: latency-svc-49xxp
Feb 14 00:22:37.118: INFO: Got endpoints: latency-svc-mcqv7 [782.622235ms]
Feb 14 00:22:37.126: INFO: Got endpoints: latency-svc-bjkwj [704.544634ms]
Feb 14 00:22:37.158: INFO: Created: latency-svc-wvl2q
Feb 14 00:22:37.191: INFO: Got endpoints: latency-svc-b8hhj [763.814409ms]
Feb 14 00:22:37.213: INFO: Created: latency-svc-rdxnw
Feb 14 00:22:37.259: INFO: Got endpoints: latency-svc-pdmr2 [771.34247ms]
Feb 14 00:22:37.275: INFO: Created: latency-svc-j72cw
Feb 14 00:22:37.287: INFO: Got endpoints: latency-svc-cc86g [728.516398ms]
Feb 14 00:22:37.307: INFO: Created: latency-svc-x6qr2
Feb 14 00:22:37.335: INFO: Created: latency-svc-gz9d8
Feb 14 00:22:37.335: INFO: Got endpoints: latency-svc-6f4x7 [759.172791ms]
Feb 14 00:22:37.393: INFO: Got endpoints: latency-svc-kv772 [756.27999ms]
Feb 14 00:22:37.394: INFO: Created: latency-svc-gf7pb
Feb 14 00:22:37.434: INFO: Got endpoints: latency-svc-w7kq2 [742.215912ms]
Feb 14 00:22:37.434: INFO: Created: latency-svc-zntzp
Feb 14 00:22:37.474: INFO: Created: latency-svc-kdx4n
Feb 14 00:22:37.542: INFO: Got endpoints: latency-svc-v79ph [755.156315ms]
Feb 14 00:22:37.542: INFO: Got endpoints: latency-svc-c6bbc [810.335792ms]
Feb 14 00:22:37.582: INFO: Created: latency-svc-8jfvv
Feb 14 00:22:37.582: INFO: Got endpoints: latency-svc-gmgql [748.304123ms]
Feb 14 00:22:37.609: INFO: Created: latency-svc-85qqg
Feb 14 00:22:37.636: INFO: Created: latency-svc-ttkrr
Feb 14 00:22:37.636: INFO: Got endpoints: latency-svc-qhpgg [741.959638ms]
Feb 14 00:22:37.676: INFO: Got endpoints: latency-svc-pc6h9 [743.664901ms]
Feb 14 00:22:37.694: INFO: Created: latency-svc-dmdx8
Feb 14 00:22:37.722: INFO: Created: latency-svc-glmq4
Feb 14 00:22:37.734: INFO: Got endpoints: latency-svc-49xxp [697.872315ms]
Feb 14 00:22:37.772: INFO: Created: latency-svc-nh4sj
Feb 14 00:22:37.820: INFO: Got endpoints: latency-svc-ld29k [844.128859ms]
Feb 14 00:22:37.825: INFO: Got endpoints: latency-svc-wvl2q [706.792353ms]
Feb 14 00:22:37.861: INFO: Created: latency-svc-j2kgl
Feb 14 00:22:37.881: INFO: Got endpoints: latency-svc-rdxnw [754.225261ms]
Feb 14 00:22:37.894: INFO: Created: latency-svc-qs6sj
Feb 14 00:22:37.969: INFO: Created: latency-svc-gg2dg
Feb 14 00:22:37.970: INFO: Got endpoints: latency-svc-j72cw [778.138286ms]
Feb 14 00:22:37.976: INFO: Got endpoints: latency-svc-x6qr2 [717.521008ms]
Feb 14 00:22:38.010: INFO: Created: latency-svc-9nxvl
Feb 14 00:22:38.034: INFO: Created: latency-svc-nwrjs
Feb 14 00:22:38.034: INFO: Got endpoints: latency-svc-gz9d8 [747.067907ms]
Feb 14 00:22:38.119: INFO: Created: latency-svc-gsp5d
Feb 14 00:22:38.119: INFO: Got endpoints: latency-svc-gf7pb [784.51464ms]
Feb 14 00:22:38.125: INFO: Got endpoints: latency-svc-zntzp [732.344417ms]
Feb 14 00:22:38.160: INFO: Created: latency-svc-rpg25
Feb 14 00:22:38.181: INFO: Got endpoints: latency-svc-kdx4n [747.182437ms]
Feb 14 00:22:38.253: INFO: Got endpoints: latency-svc-8jfvv [710.174642ms]
Feb 14 00:22:38.276: INFO: Got endpoints: latency-svc-85qqg [733.394652ms]
Feb 14 00:22:38.326: INFO: Got endpoints: latency-svc-ttkrr [744.095988ms]
Feb 14 00:22:38.376: INFO: Got endpoints: latency-svc-dmdx8 [739.75214ms]
Feb 14 00:22:38.426: INFO: Got endpoints: latency-svc-glmq4 [750.371124ms]
Feb 14 00:22:38.487: INFO: Got endpoints: latency-svc-nh4sj [753.180573ms]
Feb 14 00:22:38.526: INFO: Got endpoints: latency-svc-j2kgl [705.958644ms]
Feb 14 00:22:38.576: INFO: Got endpoints: latency-svc-qs6sj [750.522116ms]
Feb 14 00:22:38.626: INFO: Got endpoints: latency-svc-gg2dg [744.900256ms]
Feb 14 00:22:38.676: INFO: Got endpoints: latency-svc-9nxvl [706.166836ms]
Feb 14 00:22:38.726: INFO: Got endpoints: latency-svc-nwrjs [749.348232ms]
Feb 14 00:22:38.776: INFO: Got endpoints: latency-svc-gsp5d [741.880644ms]
Feb 14 00:22:38.834: INFO: Got endpoints: latency-svc-rpg25 [715.000236ms]
Feb 14 00:22:38.835: INFO: Latencies: [81.137291ms 97.082977ms 132.159851ms 154.31432ms 255.685933ms 264.030679ms 361.35748ms 389.406251ms 415.213988ms 440.635666ms 441.881366ms 443.377648ms 455.625573ms 457.123898ms 461.327071ms 463.13219ms 464.86957ms 466.372349ms 472.96079ms 476.103803ms 480.561265ms 481.70705ms 481.858894ms 486.388754ms 488.864063ms 489.009523ms 490.684634ms 491.184052ms 491.3776ms 491.996915ms 493.622604ms 499.777972ms 502.505164ms 503.424765ms 505.950405ms 509.055386ms 509.877149ms 510.046607ms 510.430423ms 510.687299ms 510.729647ms 512.002985ms 512.008011ms 512.776789ms 514.084748ms 514.184587ms 514.524975ms 515.195504ms 516.973291ms 517.034871ms 517.578664ms 517.662124ms 517.940912ms 518.20881ms 519.45124ms 522.086775ms 523.850942ms 524.31036ms 524.592146ms 525.374244ms 525.503469ms 527.572776ms 528.345733ms 528.875238ms 529.550927ms 529.868367ms 531.100284ms 531.473379ms 531.842928ms 532.046562ms 532.787108ms 532.956323ms 532.963772ms 533.314279ms 534.60912ms 536.490389ms 541.257236ms 543.433749ms 544.223993ms 547.463188ms 547.571845ms 550.848363ms 550.870956ms 553.928529ms 554.485903ms 556.274834ms 557.254747ms 557.462028ms 557.657331ms 558.665878ms 568.474962ms 568.717507ms 573.794873ms 578.671354ms 579.509759ms 579.956106ms 580.786849ms 587.485582ms 595.687628ms 596.151475ms 609.105422ms 618.637578ms 624.185447ms 636.853042ms 644.259141ms 654.651929ms 660.180689ms 665.800858ms 668.926025ms 689.309098ms 693.743477ms 695.218282ms 697.872315ms 704.544634ms 705.738467ms 705.958644ms 706.166836ms 706.792353ms 709.984588ms 710.174642ms 710.522576ms 710.581837ms 715.000236ms 717.521008ms 718.658372ms 720.139929ms 722.785179ms 727.986842ms 728.259248ms 728.516398ms 731.678069ms 731.932248ms 732.344417ms 733.394652ms 733.855951ms 734.924383ms 736.433902ms 737.763488ms 739.75214ms 740.867226ms 740.937575ms 741.880644ms 741.959638ms 742.215912ms 743.664901ms 744.095988ms 744.345413ms 744.900256ms 745.105059ms 746.046278ms 746.824032ms 747.067907ms 747.182437ms 748.304123ms 748.389069ms 748.688944ms 749.348232ms 749.355173ms 749.9211ms 749.982653ms 750.371124ms 750.522116ms 752.473423ms 753.087856ms 753.180573ms 754.225261ms 754.366311ms 755.156315ms 755.57383ms 755.86192ms 756.27999ms 756.414025ms 757.112581ms 757.664062ms 757.986235ms 759.172791ms 759.381472ms 761.060704ms 761.602544ms 762.068043ms 762.622364ms 763.609878ms 763.814409ms 763.969348ms 769.28712ms 770.704115ms 771.34247ms 772.730228ms 775.34098ms 776.5245ms 777.313032ms 778.138286ms 782.587718ms 782.622235ms 784.51464ms 785.387232ms 786.362303ms 792.159902ms 810.335792ms 844.128859ms]
Feb 14 00:22:38.835: INFO: 50 %ile: 609.105422ms
Feb 14 00:22:38.835: INFO: 90 %ile: 762.622364ms
Feb 14 00:22:38.835: INFO: 99 %ile: 810.335792ms
Feb 14 00:22:38.835: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:22:38.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svc-latency-sdc98" for this suite.
Feb 14 00:22:58.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:22:59.031: INFO: namespace: e2e-tests-svc-latency-sdc98, resource: bindings, ignored listing per whitelist
Feb 14 00:22:59.826: INFO: namespace e2e-tests-svc-latency-sdc98 deletion completed in 20.969975882s

• [SLOW TEST:34.051 seconds]
[sig-network] Service endpoints latency
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:22:59.827: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-p4m8f
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-p4m8f
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 00:23:00.925: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 00:23:29.373: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.1.144 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-p4m8f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:23:29.373: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:23:30.912: INFO: Found all expected endpoints: [netserver-0]
Feb 14 00:23:30.937: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 100.96.0.44 8081 | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-p4m8f PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:23:30.937: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:23:32.510: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:23:32.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-p4m8f" for this suite.
Feb 14 00:23:54.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:23:55.052: INFO: namespace: e2e-tests-pod-network-test-p4m8f, resource: bindings, ignored listing per whitelist
Feb 14 00:23:55.460: INFO: namespace e2e-tests-pod-network-test-p4m8f deletion completed in 22.926720732s

• [SLOW TEST:55.633 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:23:55.460: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-k8cvg
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secret-namespace-f2d8k
STEP: Creating secret with name secret-test-d0f698b6-2fee-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 14 00:23:57.089: INFO: Waiting up to 5m0s for pod "pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-k8cvg" to be "success or failure"
Feb 14 00:23:57.113: INFO: Pod "pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 24.156937ms
Feb 14 00:23:59.145: INFO: Pod "pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056318839s
Feb 14 00:24:01.168: INFO: Pod "pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079311357s
STEP: Saw pod success
Feb 14 00:24:01.168: INFO: Pod "pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:24:01.190: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200 container secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:24:01.289: INFO: Waiting for pod pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:24:01.310: INFO: Pod pod-secrets-d138a2ac-2fee-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:24:01.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-k8cvg" for this suite.
Feb 14 00:24:07.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:07.548: INFO: namespace: e2e-tests-secrets-k8cvg, resource: bindings, ignored listing per whitelist
Feb 14 00:24:08.282: INFO: namespace e2e-tests-secrets-k8cvg deletion completed in 6.94944633s
STEP: Destroying namespace "e2e-tests-secret-namespace-f2d8k" for this suite.
Feb 14 00:24:14.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:14.558: INFO: namespace: e2e-tests-secret-namespace-f2d8k, resource: bindings, ignored listing per whitelist
Feb 14 00:24:15.233: INFO: namespace e2e-tests-secret-namespace-f2d8k deletion completed in 6.950313895s

• [SLOW TEST:19.772 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:24:15.233: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-8rjx8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap e2e-tests-configmap-8rjx8/configmap-test-dcb58157-2fee-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:24:16.358: INFO: Waiting up to 5m0s for pod "pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-8rjx8" to be "success or failure"
Feb 14 00:24:16.383: INFO: Pod "pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.053917ms
Feb 14 00:24:18.406: INFO: Pod "pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047707671s
Feb 14 00:24:20.438: INFO: Pod "pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079584135s
STEP: Saw pod success
Feb 14 00:24:20.438: INFO: Pod "pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:24:20.459: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200 container env-test: <nil>
STEP: delete the pod
Feb 14 00:24:20.558: INFO: Waiting for pod pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:24:20.579: INFO: Pod pod-configmaps-dcb8c665-2fee-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:24:20.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-8rjx8" for this suite.
Feb 14 00:24:26.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:24:27.119: INFO: namespace: e2e-tests-configmap-8rjx8, resource: bindings, ignored listing per whitelist
Feb 14 00:24:27.612: INFO: namespace e2e-tests-configmap-8rjx8 deletion completed in 7.010318096s

• [SLOW TEST:12.379 seconds]
[sig-api-machinery] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:30
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:24:27.612: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-txlgc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:24:33.342: INFO: Successfully updated pod "annotationupdatee40ec37f-2fee-11e9-b8b1-3a3e684b6200"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:24:35.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-txlgc" for this suite.
Feb 14 00:24:59.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:00.160: INFO: namespace: e2e-tests-downward-api-txlgc, resource: bindings, ignored listing per whitelist
Feb 14 00:25:00.376: INFO: namespace e2e-tests-downward-api-txlgc deletion completed in 24.949451059s

• [SLOW TEST:32.764 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:25:00.377: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-w9pgg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:25:01.440: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-w9pgg" to be "success or failure"
Feb 14 00:25:01.467: INFO: Pod "downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 27.55424ms
Feb 14 00:25:03.490: INFO: Pod "downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050336373s
Feb 14 00:25:05.513: INFO: Pod "downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072693306s
STEP: Saw pod success
Feb 14 00:25:05.513: INFO: Pod "downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:25:05.534: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:25:05.641: INFO: Waiting for pod downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:25:05.663: INFO: Pod downwardapi-volume-f797a83f-2fee-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:25:05.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-w9pgg" for this suite.
Feb 14 00:25:11.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:12.458: INFO: namespace: e2e-tests-projected-w9pgg, resource: bindings, ignored listing per whitelist
Feb 14 00:25:12.652: INFO: namespace e2e-tests-projected-w9pgg deletion completed in 6.96699412s

• [SLOW TEST:12.275 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:25:12.652: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-6gxgc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:25:13.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-6gxgc" to be "success or failure"
Feb 14 00:25:13.751: INFO: Pod "downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.244462ms
Feb 14 00:25:15.773: INFO: Pod "downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04306818s
Feb 14 00:25:17.795: INFO: Pod "downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065439261s
STEP: Saw pod success
Feb 14 00:25:17.795: INFO: Pod "downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:25:17.817: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:25:17.886: INFO: Waiting for pod downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:25:17.908: INFO: Pod downwardapi-volume-feeb292b-2fee-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:25:17.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-6gxgc" for this suite.
Feb 14 00:25:24.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:24.433: INFO: namespace: e2e-tests-downward-api-6gxgc, resource: bindings, ignored listing per whitelist
Feb 14 00:25:24.952: INFO: namespace e2e-tests-downward-api-6gxgc deletion completed in 7.022781943s

• [SLOW TEST:12.300 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:25:24.953: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mp25m
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1083
STEP: creating an rc
Feb 14 00:25:26.050: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-mp25m'
Feb 14 00:25:28.001: INFO: stderr: ""
Feb 14 00:25:28.001: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Waiting for Redis master to start.
Feb 14 00:25:29.024: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:25:29.024: INFO: Found 0 / 1
Feb 14 00:25:30.024: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:25:30.024: INFO: Found 0 / 1
Feb 14 00:25:31.023: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:25:31.024: INFO: Found 1 / 1
Feb 14 00:25:31.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 00:25:31.045: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:25:31.045: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Feb 14 00:25:31.045: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m'
Feb 14 00:25:31.258: INFO: stderr: ""
Feb 14 00:25:31.258: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 00:25:30.180 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 00:25:30.180 # Server started, Redis version 3.2.12\n1:M 14 Feb 00:25:30.180 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 00:25:30.180 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Feb 14 00:25:31.258: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m --tail=1'
Feb 14 00:25:31.636: INFO: stderr: ""
Feb 14 00:25:31.636: INFO: stdout: "1:M 14 Feb 00:25:30.180 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Feb 14 00:25:31.636: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m --limit-bytes=1'
Feb 14 00:25:31.847: INFO: stderr: ""
Feb 14 00:25:31.847: INFO: stdout: " "
STEP: exposing timestamps
Feb 14 00:25:31.847: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m --tail=1 --timestamps'
Feb 14 00:25:32.171: INFO: stderr: ""
Feb 14 00:25:32.171: INFO: stdout: "2019-02-14T00:25:30.180588681Z 1:M 14 Feb 00:25:30.180 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Feb 14 00:25:34.672: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m --since=1s'
Feb 14 00:25:34.888: INFO: stderr: ""
Feb 14 00:25:34.888: INFO: stdout: ""
Feb 14 00:25:34.888: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml log redis-master-lml8v redis-master --namespace=e2e-tests-kubectl-mp25m --since=24h'
Feb 14 00:25:35.101: INFO: stderr: ""
Feb 14 00:25:35.101: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 00:25:30.180 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 00:25:30.180 # Server started, Redis version 3.2.12\n1:M 14 Feb 00:25:30.180 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 00:25:30.180 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1088
STEP: using delete to clean up resources
Feb 14 00:25:35.101: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-mp25m'
Feb 14 00:25:35.311: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:25:35.311: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Feb 14 00:25:35.311: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=nginx --no-headers --namespace=e2e-tests-kubectl-mp25m'
Feb 14 00:25:35.513: INFO: stderr: "No resources found.\n"
Feb 14 00:25:35.513: INFO: stdout: ""
Feb 14 00:25:35.513: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=nginx --namespace=e2e-tests-kubectl-mp25m -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 00:25:35.687: INFO: stderr: ""
Feb 14 00:25:35.687: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:25:35.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mp25m" for this suite.
Feb 14 00:25:41.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:42.563: INFO: namespace: e2e-tests-kubectl-mp25m, resource: bindings, ignored listing per whitelist
Feb 14 00:25:42.650: INFO: namespace e2e-tests-kubectl-mp25m deletion completed in 6.92853973s

• [SLOW TEST:17.698 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl logs
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should be able to retrieve and filter logs  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:25:42.651: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-hostpath-bdrtn
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test hostPath mode
Feb 14 00:25:43.824: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "e2e-tests-hostpath-bdrtn" to be "success or failure"
Feb 14 00:25:43.849: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 24.593814ms
Feb 14 00:25:45.871: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046668104s
Feb 14 00:25:47.894: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069387201s
Feb 14 00:25:49.916: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.092065801s
STEP: Saw pod success
Feb 14 00:25:49.916: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 14 00:25:49.938: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 14 00:25:50.006: INFO: Waiting for pod pod-host-path-test to disappear
Feb 14 00:25:50.028: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:25:50.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-hostpath-bdrtn" for this suite.
Feb 14 00:25:56.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:25:56.342: INFO: namespace: e2e-tests-hostpath-bdrtn, resource: bindings, ignored listing per whitelist
Feb 14 00:25:56.969: INFO: namespace e2e-tests-hostpath-bdrtn deletion completed in 6.919444543s

• [SLOW TEST:14.318 seconds]
[sig-storage] HostPath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:25:56.969: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-2jfdx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:25:58.430: INFO: Waiting up to 5m0s for pod "downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-2jfdx" to be "success or failure"
Feb 14 00:25:58.451: INFO: Pod "downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.286042ms
Feb 14 00:26:00.475: INFO: Pod "downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044788374s
Feb 14 00:26:02.498: INFO: Pod "downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067334676s
STEP: Saw pod success
Feb 14 00:26:02.498: INFO: Pod "downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:26:02.519: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:26:02.589: INFO: Waiting for pod downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:26:02.610: INFO: Pod downward-api-198fb7ac-2fef-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:26:02.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-2jfdx" for this suite.
Feb 14 00:26:08.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:26:08.927: INFO: namespace: e2e-tests-downward-api-2jfdx, resource: bindings, ignored listing per whitelist
Feb 14 00:26:09.551: INFO: namespace e2e-tests-downward-api-2jfdx deletion completed in 6.918568817s

• [SLOW TEST:12.581 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:26:09.551: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-fdxfm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating secret e2e-tests-secrets-fdxfm/secret-test-20e9efda-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 14 00:26:10.787: INFO: Waiting up to 5m0s for pod "pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-secrets-fdxfm" to be "success or failure"
Feb 14 00:26:10.812: INFO: Pod "pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.483665ms
Feb 14 00:26:12.835: INFO: Pod "pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048155142s
Feb 14 00:26:14.858: INFO: Pod "pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071380725s
STEP: Saw pod success
Feb 14 00:26:14.858: INFO: Pod "pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:26:14.880: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200 container env-test: <nil>
STEP: delete the pod
Feb 14 00:26:14.940: INFO: Waiting for pod pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:26:14.961: INFO: Pod pod-configmaps-20ed47c2-2fef-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:26:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-fdxfm" for this suite.
Feb 14 00:26:21.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:26:21.572: INFO: namespace: e2e-tests-secrets-fdxfm, resource: bindings, ignored listing per whitelist
Feb 14 00:26:21.997: INFO: namespace e2e-tests-secrets-fdxfm deletion completed in 7.014274691s

• [SLOW TEST:12.447 seconds]
[sig-api-machinery] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:26:21.998: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-gg4ch
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-2846d0af-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:26:23.140: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-gg4ch" to be "success or failure"
Feb 14 00:26:23.161: INFO: Pod "pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.060454ms
Feb 14 00:26:25.184: INFO: Pod "pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044176413s
Feb 14 00:26:27.206: INFO: Pod "pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065953471s
STEP: Saw pod success
Feb 14 00:26:27.206: INFO: Pod "pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:26:27.227: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:26:27.387: INFO: Waiting for pod pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:26:27.408: INFO: Pod pod-projected-configmaps-284a4ce9-2fef-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:26:27.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-gg4ch" for this suite.
Feb 14 00:26:33.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:26:33.770: INFO: namespace: e2e-tests-projected-gg4ch, resource: bindings, ignored listing per whitelist
Feb 14 00:26:34.373: INFO: namespace e2e-tests-projected-gg4ch deletion completed in 6.943500486s

• [SLOW TEST:12.376 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:26:34.374: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-klp8b
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-2fb38a5e-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating secret with name s-test-opt-upd-2fb38abb-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2fb38a5e-2fef-11e9-b8b1-3a3e684b6200
STEP: Updating secret s-test-opt-upd-2fb38abb-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating secret with name s-test-opt-create-2fb38add-2fef-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:26:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-klp8b" for this suite.
Feb 14 00:27:06.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:27:06.686: INFO: namespace: e2e-tests-projected-klp8b, resource: bindings, ignored listing per whitelist
Feb 14 00:27:07.092: INFO: namespace e2e-tests-projected-klp8b deletion completed in 22.903173549s

• [SLOW TEST:32.719 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:27:07.092: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-wnmlk
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-43287b39-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:27:08.239: INFO: Waiting up to 5m0s for pod "pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-wnmlk" to be "success or failure"
Feb 14 00:27:08.269: INFO: Pod "pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 29.510274ms
Feb 14 00:27:10.291: INFO: Pod "pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051351233s
Feb 14 00:27:12.313: INFO: Pod "pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073554747s
STEP: Saw pod success
Feb 14 00:27:12.313: INFO: Pod "pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:27:12.334: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:27:12.432: INFO: Waiting for pod pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:27:12.453: INFO: Pod pod-configmaps-432bc564-2fef-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:27:12.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-wnmlk" for this suite.
Feb 14 00:27:18.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:27:18.878: INFO: namespace: e2e-tests-configmap-wnmlk, resource: bindings, ignored listing per whitelist
Feb 14 00:27:19.375: INFO: namespace e2e-tests-configmap-wnmlk deletion completed in 6.899062197s

• [SLOW TEST:12.282 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:27:19.375: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-l5zlz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create services for rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 14 00:27:20.522: INFO: namespace e2e-tests-kubectl-l5zlz
Feb 14 00:27:20.522: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-l5zlz'
Feb 14 00:27:21.037: INFO: stderr: ""
Feb 14 00:27:21.037: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 00:27:22.060: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:27:22.060: INFO: Found 0 / 1
Feb 14 00:27:23.063: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:27:23.063: INFO: Found 0 / 1
Feb 14 00:27:24.059: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:27:24.059: INFO: Found 0 / 1
Feb 14 00:27:25.060: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:27:25.060: INFO: Found 1 / 1
Feb 14 00:27:25.060: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 14 00:27:25.082: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:27:25.082: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 00:27:25.082: INFO: wait on redis-master startup in e2e-tests-kubectl-l5zlz 
Feb 14 00:27:25.082: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml logs redis-master-4jzcz redis-master --namespace=e2e-tests-kubectl-l5zlz'
Feb 14 00:27:25.384: INFO: stderr: ""
Feb 14 00:27:25.384: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Feb 00:27:23.817 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Feb 00:27:23.817 # Server started, Redis version 3.2.12\n1:M 14 Feb 00:27:23.817 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Feb 00:27:23.817 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Feb 14 00:27:25.384: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=e2e-tests-kubectl-l5zlz'
Feb 14 00:27:25.716: INFO: stderr: ""
Feb 14 00:27:25.716: INFO: stdout: "service/rm2 exposed\n"
Feb 14 00:27:25.738: INFO: Service rm2 in namespace e2e-tests-kubectl-l5zlz found.
STEP: exposing service
Feb 14 00:27:27.783: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=e2e-tests-kubectl-l5zlz'
Feb 14 00:27:28.135: INFO: stderr: ""
Feb 14 00:27:28.135: INFO: stdout: "service/rm3 exposed\n"
Feb 14 00:27:28.156: INFO: Service rm3 in namespace e2e-tests-kubectl-l5zlz found.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:27:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-l5zlz" for this suite.
Feb 14 00:27:54.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:27:55.088: INFO: namespace: e2e-tests-kubectl-l5zlz, resource: bindings, ignored listing per whitelist
Feb 14 00:27:55.133: INFO: namespace e2e-tests-kubectl-l5zlz deletion completed in 24.91056773s

• [SLOW TEST:35.758 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl expose
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create services for rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:27:55.133: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-zs4ml
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-upd-5fdaacfb-2fef-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:28:02.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-zs4ml" for this suite.
Feb 14 00:28:26.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:28:26.955: INFO: namespace: e2e-tests-configmap-zs4ml, resource: bindings, ignored listing per whitelist
Feb 14 00:28:27.515: INFO: namespace e2e-tests-configmap-zs4ml deletion completed in 24.925136465s

• [SLOW TEST:32.382 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:28:27.515: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-qnrgl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-secret-729z
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 00:28:28.678: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-729z" in namespace "e2e-tests-subpath-qnrgl" to be "success or failure"
Feb 14 00:28:28.702: INFO: Pod "pod-subpath-test-secret-729z": Phase="Pending", Reason="", readiness=false. Elapsed: 24.774716ms
Feb 14 00:28:30.725: INFO: Pod "pod-subpath-test-secret-729z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047545988s
Feb 14 00:28:32.748: INFO: Pod "pod-subpath-test-secret-729z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06995919s
Feb 14 00:28:34.771: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 6.09303491s
Feb 14 00:28:36.793: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 8.115498256s
Feb 14 00:28:38.815: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 10.13764529s
Feb 14 00:28:40.838: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 12.159956818s
Feb 14 00:28:42.862: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 14.184097871s
Feb 14 00:28:44.888: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 16.209984429s
Feb 14 00:28:46.910: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 18.231932193s
Feb 14 00:28:48.933: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 20.254960877s
Feb 14 00:28:50.957: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 22.278936591s
Feb 14 00:28:52.979: INFO: Pod "pod-subpath-test-secret-729z": Phase="Running", Reason="", readiness=false. Elapsed: 24.301325681s
Feb 14 00:28:55.003: INFO: Pod "pod-subpath-test-secret-729z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.325540468s
STEP: Saw pod success
Feb 14 00:28:55.003: INFO: Pod "pod-subpath-test-secret-729z" satisfied condition "success or failure"
Feb 14 00:28:55.026: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-subpath-test-secret-729z container test-container-subpath-secret-729z: <nil>
STEP: delete the pod
Feb 14 00:28:55.097: INFO: Waiting for pod pod-subpath-test-secret-729z to disappear
Feb 14 00:28:55.118: INFO: Pod pod-subpath-test-secret-729z no longer exists
STEP: Deleting pod pod-subpath-test-secret-729z
Feb 14 00:28:55.118: INFO: Deleting pod "pod-subpath-test-secret-729z" in namespace "e2e-tests-subpath-qnrgl"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:28:55.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-qnrgl" for this suite.
Feb 14 00:29:01.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:01.460: INFO: namespace: e2e-tests-subpath-qnrgl, resource: bindings, ignored listing per whitelist
Feb 14 00:29:02.088: INFO: namespace e2e-tests-subpath-qnrgl deletion completed in 6.888077976s

• [SLOW TEST:34.573 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:29:02.089: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-2wvxr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 14 00:29:11.516: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:11.551: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 00:29:13.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:13.574: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 00:29:15.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:15.574: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 00:29:17.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:17.574: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 00:29:19.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:19.574: INFO: Pod pod-with-prestop-http-hook still exists
Feb 14 00:29:21.551: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 14 00:29:21.574: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:29:21.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-2wvxr" for this suite.
Feb 14 00:29:45.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:29:46.598: INFO: namespace: e2e-tests-container-lifecycle-hook-2wvxr, resource: bindings, ignored listing per whitelist
Feb 14 00:29:46.621: INFO: namespace e2e-tests-container-lifecycle-hook-2wvxr deletion completed in 24.989267682s

• [SLOW TEST:44.532 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:29:46.621: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-e2e-kubelet-etc-hosts-nk5gb
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 14 00:29:58.001: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:29:58.001: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:29:58.547: INFO: Exec stderr: ""
Feb 14 00:29:58.547: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:29:58.547: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:00.035: INFO: Exec stderr: ""
Feb 14 00:30:00.035: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:00.035: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:00.583: INFO: Exec stderr: ""
Feb 14 00:30:00.583: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:00.583: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:01.243: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 14 00:30:01.243: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:01.243: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:01.762: INFO: Exec stderr: ""
Feb 14 00:30:01.762: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:01.762: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:02.290: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 14 00:30:02.290: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:02.290: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:02.773: INFO: Exec stderr: ""
Feb 14 00:30:02.773: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:02.773: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:03.311: INFO: Exec stderr: ""
Feb 14 00:30:03.311: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:03.311: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:03.890: INFO: Exec stderr: ""
Feb 14 00:30:03.890: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-tests-e2e-kubelet-etc-hosts-nk5gb PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 00:30:03.890: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 00:30:04.391: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:30:04.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-e2e-kubelet-etc-hosts-nk5gb" for this suite.
Feb 14 00:30:54.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:30:54.610: INFO: namespace: e2e-tests-e2e-kubelet-etc-hosts-nk5gb, resource: bindings, ignored listing per whitelist
Feb 14 00:30:55.296: INFO: namespace e2e-tests-e2e-kubelet-etc-hosts-nk5gb deletion completed in 50.881978359s

• [SLOW TEST:68.675 seconds]
[k8s.io] KubeletManagedEtcHosts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:30:55.296: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-xwp69
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:30:56.612: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 14 00:30:56.658: INFO: Number of nodes with available pods: 0
Feb 14 00:30:56.658: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 14 00:30:56.776: INFO: Number of nodes with available pods: 0
Feb 14 00:30:56.776: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:30:57.799: INFO: Number of nodes with available pods: 0
Feb 14 00:30:57.799: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:30:58.800: INFO: Number of nodes with available pods: 0
Feb 14 00:30:58.800: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:30:59.798: INFO: Number of nodes with available pods: 1
Feb 14 00:30:59.798: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 14 00:30:59.900: INFO: Number of nodes with available pods: 0
Feb 14 00:30:59.900: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 14 00:30:59.946: INFO: Number of nodes with available pods: 0
Feb 14 00:30:59.946: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:00.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:00.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:01.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:01.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:02.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:02.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:03.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:03.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:04.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:04.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:05.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:05.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:06.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:06.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:07.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:07.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:08.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:08.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:09.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:09.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:10.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:10.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:11.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:11.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:12.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:12.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:13.970: INFO: Number of nodes with available pods: 0
Feb 14 00:31:13.970: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:14.978: INFO: Number of nodes with available pods: 0
Feb 14 00:31:14.978: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:15.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:15.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:16.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:16.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:17.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:17.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:18.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:18.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:19.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:19.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:20.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:20.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:21.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:21.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:22.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:22.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:23.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:23.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:24.983: INFO: Number of nodes with available pods: 0
Feb 14 00:31:24.983: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:25.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:25.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:26.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:26.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:27.970: INFO: Number of nodes with available pods: 0
Feb 14 00:31:27.970: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:28.968: INFO: Number of nodes with available pods: 0
Feb 14 00:31:28.968: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:29.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:29.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:30.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:30.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:31.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:31.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:32.970: INFO: Number of nodes with available pods: 0
Feb 14 00:31:32.970: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:33.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:33.969: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:34.969: INFO: Number of nodes with available pods: 0
Feb 14 00:31:34.970: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:31:35.968: INFO: Number of nodes with available pods: 1
Feb 14 00:31:35.968: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-xwp69, will wait for the garbage collector to delete the pods
Feb 14 00:31:36.107: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.063495ms
Feb 14 00:31:36.307: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 200.307217ms
Feb 14 00:32:14.929: INFO: Number of nodes with available pods: 0
Feb 14 00:32:14.929: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 00:32:14.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-xwp69/daemonsets","resourceVersion":"22539"},"items":null}

Feb 14 00:32:14.972: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-xwp69/pods","resourceVersion":"22539"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:32:15.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-xwp69" for this suite.
Feb 14 00:32:23.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:32:23.447: INFO: namespace: e2e-tests-daemonsets-xwp69, resource: bindings, ignored listing per whitelist
Feb 14 00:32:24.045: INFO: namespace e2e-tests-daemonsets-xwp69 deletion completed in 8.948895253s

• [SLOW TEST:88.749 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-api-machinery] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:32:24.045: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-n5qg7
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward api env vars
Feb 14 00:32:25.139: INFO: Waiting up to 5m0s for pod "downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-n5qg7" to be "success or failure"
Feb 14 00:32:25.160: INFO: Pod "downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 20.855262ms
Feb 14 00:32:27.182: INFO: Pod "downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043378705s
Feb 14 00:32:29.205: INFO: Pod "downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066008724s
STEP: Saw pod success
Feb 14 00:32:29.205: INFO: Pod "downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:32:29.227: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:32:29.327: INFO: Waiting for pod downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:32:29.352: INFO: Pod downward-api-000efcbd-2ff0-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-api-machinery] Downward API
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:32:29.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-n5qg7" for this suite.
Feb 14 00:32:35.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:32:35.835: INFO: namespace: e2e-tests-downward-api-n5qg7, resource: bindings, ignored listing per whitelist
Feb 14 00:32:36.310: INFO: namespace e2e-tests-downward-api-n5qg7 deletion completed in 6.935045458s

• [SLOW TEST:12.265 seconds]
[sig-api-machinery] Downward API
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:32:36.310: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-dns-8jwfn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/wheezy_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn.svc A)" && echo OK > /results/wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn.svc A)" && echo OK > /results/wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8jwfn.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/wheezy_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 215.7.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.7.215_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 215.7.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.7.215_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(dig +notcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_udp@dns-test-service;test -n "$$(dig +tcp +noall +answer +search dns-test-service A)" && echo OK > /results/jessie_tcp@dns-test-service;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8jwfn;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn;test -n "$$(dig +notcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn.svc A)" && echo OK > /results/jessie_udp@dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search dns-test-service.e2e-tests-dns-8jwfn.svc A)" && echo OK > /results/jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc;test -n "$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc SRV)" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.e2e-tests-dns-8jwfn.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".e2e-tests-dns-8jwfn.pod.cluster.local"}');test -n "$$(dig +notcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_udp@PodARecord;test -n "$$(dig +tcp +noall +answer +search $${podARec} A)" && echo OK > /results/jessie_tcp@PodARecord;test -n "$$(dig +notcp +noall +answer +search 215.7.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.7.215_udp@PTR;test -n "$$(dig +tcp +noall +answer +search 215.7.70.100.in-addr.arpa. PTR)" && echo OK > /results/100.70.7.215_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 14 00:32:51.707: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.752: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.779: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.806: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.834: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.859: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.898: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:51.924: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.394: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.419: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.443: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.467: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.493: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.518: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.544: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.570: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:32:52.953: INFO: Lookups using e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8jwfn jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn jessie_udp@dns-test-service.e2e-tests-dns-8jwfn.svc jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc]

Feb 14 00:33:01.620: INFO: Unable to read wheezy_udp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.644: INFO: Unable to read wheezy_tcp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.668: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.693: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.718: INFO: Unable to read wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.742: INFO: Unable to read wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.774: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:01.799: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.226: INFO: Unable to read jessie_udp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.250: INFO: Unable to read jessie_tcp@dns-test-service from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.275: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.300: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.324: INFO: Unable to read jessie_udp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.350: INFO: Unable to read jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.376: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.400: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc from pod e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200: the server could not find the requested resource (get pods dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200)
Feb 14 00:33:02.794: INFO: Lookups using e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn wheezy_udp@dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc wheezy_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.e2e-tests-dns-8jwfn jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn jessie_udp@dns-test-service.e2e-tests-dns-8jwfn.svc jessie_tcp@dns-test-service.e2e-tests-dns-8jwfn.svc jessie_udp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc jessie_tcp@_http._tcp.dns-test-service.e2e-tests-dns-8jwfn.svc]

Feb 14 00:33:13.385: INFO: DNS probes using e2e-tests-dns-8jwfn/dns-test-076ec56d-2ff0-11e9-b8b1-3a3e684b6200 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:33:13.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-dns-8jwfn" for this suite.
Feb 14 00:33:19.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:33:20.079: INFO: namespace: e2e-tests-dns-8jwfn, resource: bindings, ignored listing per whitelist
Feb 14 00:33:20.485: INFO: namespace e2e-tests-dns-8jwfn deletion completed in 6.931750083s

• [SLOW TEST:44.175 seconds]
[sig-network] DNS
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:33:20.485: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-hpcqt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-downwardapi-rfsn
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 00:33:21.684: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rfsn" in namespace "e2e-tests-subpath-hpcqt" to be "success or failure"
Feb 14 00:33:21.711: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Pending", Reason="", readiness=false. Elapsed: 26.421816ms
Feb 14 00:33:23.733: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048730749s
Feb 14 00:33:25.755: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.070880635s
Feb 14 00:33:27.778: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 6.093254343s
Feb 14 00:33:29.808: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 8.123831986s
Feb 14 00:33:31.831: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 10.146314855s
Feb 14 00:33:33.854: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 12.169423169s
Feb 14 00:33:35.878: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 14.193990452s
Feb 14 00:33:37.904: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 16.219138463s
Feb 14 00:33:39.937: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 18.252441919s
Feb 14 00:33:41.963: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 20.278111757s
Feb 14 00:33:43.989: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 22.304307299s
Feb 14 00:33:46.011: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Running", Reason="", readiness=false. Elapsed: 24.326517805s
Feb 14 00:33:48.034: INFO: Pod "pod-subpath-test-downwardapi-rfsn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.349195015s
STEP: Saw pod success
Feb 14 00:33:48.034: INFO: Pod "pod-subpath-test-downwardapi-rfsn" satisfied condition "success or failure"
Feb 14 00:33:48.055: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-subpath-test-downwardapi-rfsn container test-container-subpath-downwardapi-rfsn: <nil>
STEP: delete the pod
Feb 14 00:33:48.132: INFO: Waiting for pod pod-subpath-test-downwardapi-rfsn to disappear
Feb 14 00:33:48.152: INFO: Pod pod-subpath-test-downwardapi-rfsn no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rfsn
Feb 14 00:33:48.153: INFO: Deleting pod "pod-subpath-test-downwardapi-rfsn" in namespace "e2e-tests-subpath-hpcqt"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:33:48.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-hpcqt" for this suite.
Feb 14 00:33:54.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:33:54.636: INFO: namespace: e2e-tests-subpath-hpcqt, resource: bindings, ignored listing per whitelist
Feb 14 00:33:55.097: INFO: namespace e2e-tests-subpath-hpcqt deletion completed in 6.896348605s

• [SLOW TEST:34.612 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:33:55.097: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-4z5vg
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:34:01.012: INFO: Successfully updated pod "labelsupdate366df27e-2ff0-11e9-b8b1-3a3e684b6200"
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:34:05.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-4z5vg" for this suite.
Feb 14 00:34:27.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:34:27.421: INFO: namespace: e2e-tests-projected-4z5vg, resource: bindings, ignored listing per whitelist
Feb 14 00:34:28.024: INFO: namespace e2e-tests-projected-4z5vg deletion completed in 22.888909671s

• [SLOW TEST:32.927 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:34:28.024: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-rds66
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create and stop a working application  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating all guestbook components
Feb 14 00:34:29.115: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 14 00:34:29.115: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:29.551: INFO: stderr: ""
Feb 14 00:34:29.551: INFO: stdout: "service/redis-slave created\n"
Feb 14 00:34:29.551: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 14 00:34:29.552: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:29.896: INFO: stderr: ""
Feb 14 00:34:29.897: INFO: stdout: "service/redis-master created\n"
Feb 14 00:34:29.897: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 14 00:34:29.897: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:30.232: INFO: stderr: ""
Feb 14 00:34:30.232: INFO: stdout: "service/frontend created\n"
Feb 14 00:34:30.237: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 14 00:34:30.237: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:30.560: INFO: stderr: ""
Feb 14 00:34:30.560: INFO: stdout: "deployment.extensions/frontend created\n"
Feb 14 00:34:30.568: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 14 00:34:30.568: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:30.909: INFO: stderr: ""
Feb 14 00:34:30.909: INFO: stdout: "deployment.extensions/redis-master created\n"
Feb 14 00:34:30.909: INFO: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 14 00:34:30.909: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:34:31.243: INFO: stderr: ""
Feb 14 00:34:31.243: INFO: stdout: "deployment.extensions/redis-slave created\n"
STEP: validating guestbook app
Feb 14 00:34:31.243: INFO: Waiting for all frontend pods to be Running.
Feb 14 00:35:06.295: INFO: Waiting for frontend to serve content.
Feb 14 00:35:06.425: INFO: Trying to add a new entry to the guestbook.
Feb 14 00:35:06.559: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 14 00:35:06.612: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:06.891: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:06.892: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 00:35:06.892: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:07.120: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:07.120: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 00:35:07.120: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:07.367: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:07.367: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 00:35:07.368: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:07.587: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:07.587: INFO: stdout: "deployment.extensions \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 00:35:07.587: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:07.780: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:07.780: INFO: stdout: "deployment.extensions \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 14 00:35:07.780: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-rds66'
Feb 14 00:35:07.986: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:35:07.986: INFO: stdout: "deployment.extensions \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:35:07.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-rds66" for this suite.
Feb 14 00:35:51.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:35:52.566: INFO: namespace: e2e-tests-kubectl-rds66, resource: bindings, ignored listing per whitelist
Feb 14 00:35:52.653: INFO: namespace e2e-tests-kubectl-rds66 deletion completed in 44.614381162s

• [SLOW TEST:84.629 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Guestbook application
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create and stop a working application  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:35:52.653: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-9s5q5
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-btlzs
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Creating an uninitialized pod in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
Feb 14 00:36:06.172: INFO: error from create uninitialized namespace: Internal error occurred: object deleted while waiting for creation
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-stfpf
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:36:23.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-9s5q5" for this suite.
Feb 14 00:36:29.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:36:30.321: INFO: namespace: e2e-tests-namespaces-9s5q5, resource: bindings, ignored listing per whitelist
Feb 14 00:36:30.665: INFO: namespace e2e-tests-namespaces-9s5q5 deletion completed in 6.902872637s
STEP: Destroying namespace "e2e-tests-nsdeletetest-btlzs" for this suite.
Feb 14 00:36:30.687: INFO: Namespace e2e-tests-nsdeletetest-btlzs was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-stfpf" for this suite.
Feb 14 00:36:36.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:36:37.578: INFO: namespace: e2e-tests-nsdeletetest-stfpf, resource: bindings, ignored listing per whitelist
Feb 14 00:36:37.664: INFO: namespace e2e-tests-nsdeletetest-stfpf deletion completed in 6.977073067s

• [SLOW TEST:45.011 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:36:37.664: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-cvtt8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should scale a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a replication controller
Feb 14 00:36:38.715: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:40.284: INFO: stderr: ""
Feb 14 00:36:40.284: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:36:40.284: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:40.467: INFO: stderr: ""
Feb 14 00:36:40.467: INFO: stdout: "update-demo-nautilus-crv2b update-demo-nautilus-kmmzb "
Feb 14 00:36:40.467: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-crv2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:40.640: INFO: stderr: ""
Feb 14 00:36:40.640: INFO: stdout: ""
Feb 14 00:36:40.640: INFO: update-demo-nautilus-crv2b is created but not running
Feb 14 00:36:45.640: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:45.875: INFO: stderr: ""
Feb 14 00:36:45.875: INFO: stdout: "update-demo-nautilus-crv2b update-demo-nautilus-kmmzb "
Feb 14 00:36:45.875: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-crv2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:46.045: INFO: stderr: ""
Feb 14 00:36:46.045: INFO: stdout: "true"
Feb 14 00:36:46.045: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-crv2b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:46.227: INFO: stderr: ""
Feb 14 00:36:46.227: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:36:46.227: INFO: validating pod update-demo-nautilus-crv2b
Feb 14 00:36:46.342: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:36:46.342: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:36:46.342: INFO: update-demo-nautilus-crv2b is verified up and running
Feb 14 00:36:46.342: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:46.527: INFO: stderr: ""
Feb 14 00:36:46.527: INFO: stdout: "true"
Feb 14 00:36:46.528: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:46.719: INFO: stderr: ""
Feb 14 00:36:46.719: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:36:46.719: INFO: validating pod update-demo-nautilus-kmmzb
Feb 14 00:36:46.832: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:36:46.832: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:36:46.832: INFO: update-demo-nautilus-kmmzb is verified up and running
STEP: scaling down the replication controller
Feb 14 00:36:46.836: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:36:46.837: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:47.096: INFO: stderr: ""
Feb 14 00:36:47.096: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:36:47.096: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:47.278: INFO: stderr: ""
Feb 14 00:36:47.278: INFO: stdout: "update-demo-nautilus-crv2b update-demo-nautilus-kmmzb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 00:36:52.278: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:52.466: INFO: stderr: ""
Feb 14 00:36:52.466: INFO: stdout: "update-demo-nautilus-crv2b update-demo-nautilus-kmmzb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 00:36:57.467: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:36:57.658: INFO: stderr: ""
Feb 14 00:36:57.658: INFO: stdout: "update-demo-nautilus-crv2b update-demo-nautilus-kmmzb "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 14 00:37:02.658: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:02.832: INFO: stderr: ""
Feb 14 00:37:02.832: INFO: stdout: "update-demo-nautilus-kmmzb "
Feb 14 00:37:02.832: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:03.011: INFO: stderr: ""
Feb 14 00:37:03.011: INFO: stdout: "true"
Feb 14 00:37:03.011: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:03.268: INFO: stderr: ""
Feb 14 00:37:03.268: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:37:03.268: INFO: validating pod update-demo-nautilus-kmmzb
Feb 14 00:37:03.567: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:37:03.567: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:37:03.567: INFO: update-demo-nautilus-kmmzb is verified up and running
STEP: scaling up the replication controller
Feb 14 00:37:03.571: INFO: scanned /root for discovery docs: <nil>
Feb 14 00:37:03.571: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:04.866: INFO: stderr: ""
Feb 14 00:37:04.866: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 00:37:04.866: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:05.048: INFO: stderr: ""
Feb 14 00:37:05.048: INFO: stdout: "update-demo-nautilus-2f2rx update-demo-nautilus-kmmzb "
Feb 14 00:37:05.048: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-2f2rx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:05.219: INFO: stderr: ""
Feb 14 00:37:05.219: INFO: stdout: ""
Feb 14 00:37:05.219: INFO: update-demo-nautilus-2f2rx is created but not running
Feb 14 00:37:10.219: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:10.415: INFO: stderr: ""
Feb 14 00:37:10.415: INFO: stdout: "update-demo-nautilus-2f2rx update-demo-nautilus-kmmzb "
Feb 14 00:37:10.416: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-2f2rx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:10.592: INFO: stderr: ""
Feb 14 00:37:10.592: INFO: stdout: "true"
Feb 14 00:37:10.592: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-2f2rx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:10.772: INFO: stderr: ""
Feb 14 00:37:10.772: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:37:10.772: INFO: validating pod update-demo-nautilus-2f2rx
Feb 14 00:37:10.884: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:37:10.884: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:37:10.884: INFO: update-demo-nautilus-2f2rx is verified up and running
Feb 14 00:37:10.885: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:11.066: INFO: stderr: ""
Feb 14 00:37:11.066: INFO: stdout: "true"
Feb 14 00:37:11.066: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-kmmzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:11.238: INFO: stderr: ""
Feb 14 00:37:11.238: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 00:37:11.238: INFO: validating pod update-demo-nautilus-kmmzb
Feb 14 00:37:11.305: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 00:37:11.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 00:37:11.305: INFO: update-demo-nautilus-kmmzb is verified up and running
STEP: using delete to clean up resources
Feb 14 00:37:11.305: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete --grace-period=0 --force -f - --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:11.502: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 14 00:37:11.502: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 14 00:37:11.502: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=e2e-tests-kubectl-cvtt8'
Feb 14 00:37:11.698: INFO: stderr: "No resources found.\n"
Feb 14 00:37:11.698: INFO: stdout: ""
Feb 14 00:37:11.698: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -l name=update-demo --namespace=e2e-tests-kubectl-cvtt8 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 14 00:37:11.973: INFO: stderr: ""
Feb 14 00:37:11.973: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:37:11.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-cvtt8" for this suite.
Feb 14 00:37:36.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:37:36.322: INFO: namespace: e2e-tests-kubectl-cvtt8, resource: bindings, ignored listing per whitelist
Feb 14 00:37:36.970: INFO: namespace e2e-tests-kubectl-cvtt8 deletion completed in 24.974462018s

• [SLOW TEST:59.305 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should scale a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:37:36.970: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replicaset-ks8np
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:37:38.107: INFO: Creating ReplicaSet my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200
Feb 14 00:37:38.175: INFO: Pod name my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200: Found 0 pods out of 1
Feb 14 00:37:43.197: INFO: Pod name my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200: Found 1 pods out of 1
Feb 14 00:37:43.197: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200" is running
Feb 14 00:37:43.219: INFO: Pod "my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200-x8lnb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 00:37:38 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 00:37:40 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 00:37:40 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-02-14 00:37:38 +0000 UTC Reason: Message:}])
Feb 14 00:37:43.219: INFO: Trying to dial the pod
Feb 14 00:37:48.375: INFO: Controller my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200: Got expected result from replica 1 [my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200-x8lnb]: "my-hostname-basic-ba9deb7d-2ff0-11e9-b8b1-3a3e684b6200-x8lnb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:37:48.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replicaset-ks8np" for this suite.
Feb 14 00:37:54.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:37:55.317: INFO: namespace: e2e-tests-replicaset-ks8np, resource: bindings, ignored listing per whitelist
Feb 14 00:37:55.317: INFO: namespace e2e-tests-replicaset-ks8np deletion completed in 6.91993933s

• [SLOW TEST:18.348 seconds]
[sig-apps] ReplicaSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:37:55.318: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-tj2k9
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 14 00:37:56.662: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-tj2k9,SelfLink:/api/v1/namespaces/e2e-tests-watch-tj2k9/configmaps/e2e-watch-test-resource-version,UID:c595ecce-2ff0-11e9-9d38-36c391ece45e,ResourceVersion:23564,Generation:0,CreationTimestamp:2019-02-14 00:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 00:37:56.662: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-tj2k9,SelfLink:/api/v1/namespaces/e2e-tests-watch-tj2k9/configmaps/e2e-watch-test-resource-version,UID:c595ecce-2ff0-11e9-9d38-36c391ece45e,ResourceVersion:23565,Generation:0,CreationTimestamp:2019-02-14 00:37:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:37:56.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-tj2k9" for this suite.
Feb 14 00:38:02.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:38:03.114: INFO: namespace: e2e-tests-watch-tj2k9, resource: bindings, ignored listing per whitelist
Feb 14 00:38:03.649: INFO: namespace e2e-tests-watch-tj2k9 deletion completed in 6.96503716s

• [SLOW TEST:8.332 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:38:03.650: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-7nfqb
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating the pod
Feb 14 00:38:09.385: INFO: Successfully updated pod "labelsupdateca79a3cb-2ff0-11e9-b8b1-3a3e684b6200"
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:38:11.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-7nfqb" for this suite.
Feb 14 00:38:35.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:38:36.073: INFO: namespace: e2e-tests-downward-api-7nfqb, resource: bindings, ignored listing per whitelist
Feb 14 00:38:36.406: INFO: namespace e2e-tests-downward-api-7nfqb deletion completed in 24.937956953s

• [SLOW TEST:32.757 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:38:36.407: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-replication-controller-mf8dx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating replication controller my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200
Feb 14 00:38:37.758: INFO: Pod name my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200: Found 1 pods out of 1
Feb 14 00:38:37.758: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200" are running
Feb 14 00:38:41.817: INFO: Pod "my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200-5rwbn" is running (conditions: [])
Feb 14 00:38:41.817: INFO: Trying to dial the pod
Feb 14 00:38:46.977: INFO: Controller my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200: Got expected result from replica 1 [my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200-5rwbn]: "my-hostname-basic-de243cda-2ff0-11e9-b8b1-3a3e684b6200-5rwbn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:38:46.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-replication-controller-mf8dx" for this suite.
Feb 14 00:38:53.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:38:53.235: INFO: namespace: e2e-tests-replication-controller-mf8dx, resource: bindings, ignored listing per whitelist
Feb 14 00:38:53.900: INFO: namespace e2e-tests-replication-controller-mf8dx deletion completed in 6.898858192s

• [SLOW TEST:17.493 seconds]
[sig-apps] ReplicationController
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:38:53.900: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-fq7w2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-map-e88670a4-2ff0-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 14 00:38:55.176: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-fq7w2" to be "success or failure"
Feb 14 00:38:55.197: INFO: Pod "pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.339216ms
Feb 14 00:38:57.219: INFO: Pod "pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04344534s
Feb 14 00:38:59.255: INFO: Pod "pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079035093s
STEP: Saw pod success
Feb 14 00:38:59.255: INFO: Pod "pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:38:59.276: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:38:59.351: INFO: Waiting for pod pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:38:59.372: INFO: Pod pod-projected-secrets-e889d873-2ff0-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:38:59.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-fq7w2" for this suite.
Feb 14 00:39:05.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:39:05.698: INFO: namespace: e2e-tests-projected-fq7w2, resource: bindings, ignored listing per whitelist
Feb 14 00:39:06.324: INFO: namespace e2e-tests-projected-fq7w2 deletion completed in 6.929195389s

• [SLOW TEST:12.424 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:39:06.324: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-5fzkp
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check is all data is printed  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:39:07.407: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml version'
Feb 14 00:39:07.648: INFO: stderr: ""
Feb 14 00:39:07.648: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"archive\", BuildDate:\"2019-02-13T22:45:47Z\", GoVersion:\"go1.11.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"12\", GitVersion:\"v1.12.5\", GitCommit:\"51dd616cdd25d6ee22c83a858773b607328a18ec\", GitTreeState:\"clean\", BuildDate:\"2019-01-16T18:14:49Z\", GoVersion:\"go1.10.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:39:07.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-5fzkp" for this suite.
Feb 14 00:39:13.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:39:14.140: INFO: namespace: e2e-tests-kubectl-5fzkp, resource: bindings, ignored listing per whitelist
Feb 14 00:39:14.597: INFO: namespace e2e-tests-kubectl-5fzkp deletion completed in 6.926679739s

• [SLOW TEST:8.272 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl version
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check is all data is printed  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:39:14.597: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-jjkf5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-jjkf5
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace e2e-tests-statefulset-jjkf5
STEP: Waiting until all stateful set ss replicas will be running in namespace e2e-tests-statefulset-jjkf5
Feb 14 00:39:15.705: INFO: Found 0 stateful pods, waiting for 1
Feb 14 00:39:25.728: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 14 00:39:25.750: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:39:26.468: INFO: stderr: ""
Feb 14 00:39:26.468: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:39:26.468: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:39:26.491: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 14 00:39:36.514: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:39:36.514: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:39:36.602: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999382s
Feb 14 00:39:37.625: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.977932542s
Feb 14 00:39:38.647: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.954914856s
Feb 14 00:39:39.669: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.932220496s
Feb 14 00:39:40.692: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.910137158s
Feb 14 00:39:41.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.887718812s
Feb 14 00:39:42.737: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.864519106s
Feb 14 00:39:43.775: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.840433266s
Feb 14 00:39:44.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.804565278s
Feb 14 00:39:45.832: INFO: Verifying statefulset ss doesn't scale past 1 for another 781.94672ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace e2e-tests-statefulset-jjkf5
Feb 14 00:39:46.854: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:39:47.584: INFO: stderr: ""
Feb 14 00:39:47.584: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:39:47.584: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:39:47.607: INFO: Found 1 stateful pods, waiting for 3
Feb 14 00:39:57.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:39:57.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:39:57.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 14 00:39:57.673: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-0 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:39:58.368: INFO: stderr: ""
Feb 14 00:39:58.368: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:39:58.368: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:39:58.368: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:39:59.075: INFO: stderr: ""
Feb 14 00:39:59.075: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:39:59.075: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:39:59.075: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:39:59.780: INFO: stderr: ""
Feb 14 00:39:59.780: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:39:59.780: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 00:39:59.780: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:39:59.802: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Feb 14 00:40:09.849: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:40:09.849: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:40:09.849: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 14 00:40:09.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999468s
Feb 14 00:40:10.944: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.978042061s
Feb 14 00:40:11.967: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.954997535s
Feb 14 00:40:12.989: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.932262169s
Feb 14 00:40:14.012: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.910132079s
Feb 14 00:40:15.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.887296491s
Feb 14 00:40:16.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.863930726s
Feb 14 00:40:17.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.841439095s
Feb 14 00:40:18.104: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.818194666s
Feb 14 00:40:19.127: INFO: Verifying statefulset ss doesn't scale past 3 for another 795.312067ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacee2e-tests-statefulset-jjkf5
Feb 14 00:40:20.150: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-0 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:20.864: INFO: stderr: ""
Feb 14 00:40:20.864: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:40:20.864: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:40:20.864: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:21.590: INFO: stderr: ""
Feb 14 00:40:21.590: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:40:21.590: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 00:40:21.590: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:22.483: INFO: rc: 1
Feb 14 00:40:22.483: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (1fa654f5a70825d27f65178c0d39bdb71825a168523037fc0cd2736b3a3ac9a3)
 [] <nil> 0xc0018ff920 exit status 1 <nil> <nil> true [0xc0000f2f98 0xc0000f3080 0xc0000f3130] [0xc0000f2f98 0xc0000f3080 0xc0000f3130] [0xc0000f3040 0xc0000f3118] [0x932420 0x932420] 0xc000d74cc0 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (1fa654f5a70825d27f65178c0d39bdb71825a168523037fc0cd2736b3a3ac9a3)

error:
exit status 1

Feb 14 00:40:32.484: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:32.677: INFO: rc: 1
Feb 14 00:40:32.677: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018ffc20 exit status 1 <nil> <nil> true [0xc0000f3168 0xc0000f32b0 0xc0000f3380] [0xc0000f3168 0xc0000f32b0 0xc0000f3380] [0xc0000f3220 0xc0000f3340] [0x932420 0x932420] 0xc000d74fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:40:42.677: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:42.849: INFO: rc: 1
Feb 14 00:40:42.849: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016b2030 exit status 1 <nil> <nil> true [0xc0000f33a0 0xc0000f33e8 0xc0000f34c0] [0xc0000f33a0 0xc0000f33e8 0xc0000f34c0] [0xc0000f33d8 0xc0000f3470] [0x932420 0x932420] 0xc000d752c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:40:52.849: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:40:53.027: INFO: rc: 1
Feb 14 00:40:53.027: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001180360 exit status 1 <nil> <nil> true [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e1b8 0xc00000e640] [0x932420 0x932420] 0xc00223e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:03.027: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:03.269: INFO: rc: 1
Feb 14 00:41:03.269: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001180780 exit status 1 <nil> <nil> true [0xc00000e660 0xc00000e6f8 0xc00000e740] [0xc00000e660 0xc00000e6f8 0xc00000e740] [0xc00000e6e8 0xc00000e720] [0x932420 0x932420] 0xc00223e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:13.270: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:13.482: INFO: rc: 1
Feb 14 00:41:13.482: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5fb90 exit status 1 <nil> <nil> true [0xc001bc41e0 0xc001bc4218 0xc001bc4230] [0xc001bc41e0 0xc001bc4218 0xc001bc4230] [0xc001bc4210 0xc001bc4228] [0x932420 0x932420] 0xc00283dda0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:23.482: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:23.665: INFO: rc: 1
Feb 14 00:41:23.665: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000d34150 exit status 1 <nil> <nil> true [0xc001bc4238 0xc001bc4250 0xc001bc4280] [0xc001bc4238 0xc001bc4250 0xc001bc4280] [0xc001bc4248 0xc001bc4278] [0x932420 0x932420] 0xc001ed0de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:33.665: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:33.841: INFO: rc: 1
Feb 14 00:41:33.841: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016b22d0 exit status 1 <nil> <nil> true [0xc0000f3510 0xc0000f35c8 0xc0000f3600] [0xc0000f3510 0xc0000f35c8 0xc0000f3600] [0xc0000f35b0 0xc0000f35f0] [0x932420 0x932420] 0xc000d755c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:43.841: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:44.037: INFO: rc: 1
Feb 14 00:41:44.037: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016b2570 exit status 1 <nil> <nil> true [0xc0000f3740 0xc0000f3798 0xc0000f3808] [0xc0000f3740 0xc0000f3798 0xc0000f3808] [0xc0000f3788 0xc0000f37d8] [0x932420 0x932420] 0xc000d758c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:41:54.037: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:41:54.200: INFO: rc: 1
Feb 14 00:41:54.200: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016b2990 exit status 1 <nil> <nil> true [0xc0000f3810 0xc0000f3828 0xc0000f3890] [0xc0000f3810 0xc0000f3828 0xc0000f3890] [0xc0000f3820 0xc0000f3860] [0x932420 0x932420] 0xc000d75bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:42:04.200: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:42:04.367: INFO: rc: 1
Feb 14 00:42:04.367: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0016b2c00 exit status 1 <nil> <nil> true [0xc0000f3898 0xc0000f38b0 0xc0000f38f8] [0xc0000f3898 0xc0000f38b0 0xc0000f38f8] [0xc0000f38a8 0xc0000f38d8] [0x932420 0x932420] 0xc000d75ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:42:14.367: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:42:14.579: INFO: rc: 1
Feb 14 00:42:14.579: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e2d0 exit status 1 <nil> <nil> true [0xc000a28088 0xc000a280d8 0xc000a280f8] [0xc000a28088 0xc000a280d8 0xc000a280f8] [0xc000a280c0 0xc000a280f0] [0x932420 0x932420] 0xc00283c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:42:24.579: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:42:24.748: INFO: rc: 1
Feb 14 00:42:24.748: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00271c270 exit status 1 <nil> <nil> true [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e1b8 0xc00000e640] [0x932420 0x932420] 0xc001ad2960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:42:34.748: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:42:34.913: INFO: rc: 1
Feb 14 00:42:34.913: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018fe480 exit status 1 <nil> <nil> true [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4000 0xc001bc4018 0xc001bc4030] [0xc001bc4010 0xc001bc4028] [0x932420 0x932420] 0xc00223e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:42:44.913: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:00.404: INFO: rc: 1
Feb 14 00:43:00.404: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00271c510 exit status 1 <nil> <nil> true [0xc00000e660 0xc00000e6f8 0xc00000e740] [0xc00000e660 0xc00000e6f8 0xc00000e740] [0xc00000e6e8 0xc00000e720] [0x932420 0x932420] 0xc001ad2c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:43:10.404: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:10.576: INFO: rc: 1
Feb 14 00:43:10.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cb02a0 exit status 1 <nil> <nil> true [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f2478 0xc0000f2718] [0x932420 0x932420] 0xc001ed0f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:43:20.576: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:20.760: INFO: rc: 1
Feb 14 00:43:20.760: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00271c780 exit status 1 <nil> <nil> true [0xc00000e768 0xc00000e890 0xc00000e8f8] [0xc00000e768 0xc00000e890 0xc00000e8f8] [0xc00000e858 0xc00000e8e0] [0x932420 0x932420] 0xc001ad2f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:43:30.760: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:30.937: INFO: rc: 1
Feb 14 00:43:30.937: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cb0570 exit status 1 <nil> <nil> true [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f2868 0xc0000f2940 0xc0000f2998] [0xc0000f28b0 0xc0000f2978] [0x932420 0x932420] 0xc001ed1aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:43:40.937: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:41.308: INFO: rc: 1
Feb 14 00:43:41.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e570 exit status 1 <nil> <nil> true [0xc000a28108 0xc000a28120 0xc000a28168] [0xc000a28108 0xc000a28120 0xc000a28168] [0xc000a28118 0xc000a28148] [0x932420 0x932420] 0xc00283cd80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:43:51.308: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:43:51.517: INFO: rc: 1
Feb 14 00:43:51.517: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e810 exit status 1 <nil> <nil> true [0xc000a28180 0xc000a28218 0xc000a28278] [0xc000a28180 0xc000a28218 0xc000a28278] [0xc000a281a0 0xc000a28260] [0x932420 0x932420] 0xc00283d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:01.518: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:01.681: INFO: rc: 1
Feb 14 00:44:01.681: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cb07e0 exit status 1 <nil> <nil> true [0xc0000f29a8 0xc0000f2a60 0xc0000f2c00] [0xc0000f29a8 0xc0000f2a60 0xc0000f2c00] [0xc0000f29f8 0xc0000f2b80] [0x932420 0x932420] 0xc001ed1e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:11.690: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:11.895: INFO: rc: 1
Feb 14 00:44:11.895: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0018fe780 exit status 1 <nil> <nil> true [0xc001bc4040 0xc001bc4058 0xc001bc4070] [0xc001bc4040 0xc001bc4058 0xc001bc4070] [0xc001bc4050 0xc001bc4068] [0x932420 0x932420] 0xc00223e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:21.896: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:22.074: INFO: rc: 1
Feb 14 00:44:22.074: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000cb0270 exit status 1 <nil> <nil> true [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f21c8 0xc0000f25b8 0xc0000f2788] [0xc0000f2478 0xc0000f2718] [0x932420 0x932420] 0xc001ed0f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:32.074: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:32.251: INFO: rc: 1
Feb 14 00:44:32.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00271c2a0 exit status 1 <nil> <nil> true [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e008 0xc00000e258 0xc00000e650] [0xc00000e1b8 0xc00000e640] [0x932420 0x932420] 0xc001ad2960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:42.251: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:42.445: INFO: rc: 1
Feb 14 00:44:42.445: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e300 exit status 1 <nil> <nil> true [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28030 0xc000a280c0 0xc000a280f0] [0xc000a28090 0xc000a280e0] [0x932420 0x932420] 0xc00283c240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:44:52.445: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:44:52.642: INFO: rc: 1
Feb 14 00:44:52.642: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e600 exit status 1 <nil> <nil> true [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a280f8 0xc000a28118 0xc000a28148] [0xc000a28110 0xc000a28130] [0x932420 0x932420] 0xc00283cd80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:45:02.642: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:45:02.865: INFO: rc: 1
Feb 14 00:45:02.865: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5e8d0 exit status 1 <nil> <nil> true [0xc000a28168 0xc000a281a0 0xc000a28260] [0xc000a28168 0xc000a281a0 0xc000a28260] [0xc000a28188 0xc000a28228] [0x932420 0x932420] 0xc00283d0e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:45:12.865: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:45:13.041: INFO: rc: 1
Feb 14 00:45:13.041: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000e5eb40 exit status 1 <nil> <nil> true [0xc000a28278 0xc000a282d8 0xc000a28380] [0xc000a28278 0xc000a282d8 0xc000a28380] [0xc000a282c8 0xc000a28350] [0x932420 0x932420] 0xc00283d3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Feb 14 00:45:23.042: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-jjkf5 ss-2 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:45:23.219: INFO: rc: 1
Feb 14 00:45:23.219: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Feb 14 00:45:23.219: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 00:45:23.299: INFO: Deleting all statefulset in ns e2e-tests-statefulset-jjkf5
Feb 14 00:45:23.320: INFO: Scaling statefulset ss to 0
Feb 14 00:45:23.386: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 00:45:23.407: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:45:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-jjkf5" for this suite.
Feb 14 00:45:31.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:45:32.121: INFO: namespace: e2e-tests-statefulset-jjkf5, resource: bindings, ignored listing per whitelist
Feb 14 00:45:32.427: INFO: namespace e2e-tests-statefulset-jjkf5 deletion completed in 8.929542592s

• [SLOW TEST:377.830 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:45:32.427: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pods-5mn6s
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:132
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:45:37.674: INFO: Waiting up to 5m0s for pod "client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-pods-5mn6s" to be "success or failure"
Feb 14 00:45:37.696: INFO: Pod "client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.705497ms
Feb 14 00:45:39.718: INFO: Pod "client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044028556s
Feb 14 00:45:41.740: INFO: Pod "client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065821794s
STEP: Saw pod success
Feb 14 00:45:41.740: INFO: Pod "client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:45:41.761: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200 container env3cont: <nil>
STEP: delete the pod
Feb 14 00:45:41.845: INFO: Waiting for pod client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:45:41.867: INFO: Pod client-envvars-d87280d1-2ff1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:45:41.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pods-5mn6s" for this suite.
Feb 14 00:46:41.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:46:41.690: INFO: namespace: e2e-tests-pods-5mn6s, resource: bindings, ignored listing per whitelist
Feb 14 00:46:42.382: INFO: namespace e2e-tests-pods-5mn6s deletion completed in 1m0.493310779s

• [SLOW TEST:69.955 seconds]
[k8s.io] Pods
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should contain environment variables for services [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:46:42.382: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-hvq7z
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating projection with secret that has name projected-secret-test-ffb36728-2ff1-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume secrets
Feb 14 00:46:43.554: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-hvq7z" to be "success or failure"
Feb 14 00:46:43.575: INFO: Pod "pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.265915ms
Feb 14 00:46:45.598: INFO: Pod "pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044023413s
Feb 14 00:46:47.621: INFO: Pod "pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066779979s
STEP: Saw pod success
Feb 14 00:46:47.621: INFO: Pod "pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:46:47.642: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 14 00:46:47.711: INFO: Waiting for pod pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:46:47.732: INFO: Pod pod-projected-secrets-ffb6c2e9-2ff1-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:46:47.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-hvq7z" for this suite.
Feb 14 00:46:53.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:46:54.434: INFO: namespace: e2e-tests-projected-hvq7z, resource: bindings, ignored listing per whitelist
Feb 14 00:46:54.694: INFO: namespace e2e-tests-projected-hvq7z deletion completed in 6.940150652s

• [SLOW TEST:12.313 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:46:54.695: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-bc2nr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-projected-rkvf
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 00:46:55.902: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rkvf" in namespace "e2e-tests-subpath-bc2nr" to be "success or failure"
Feb 14 00:46:55.928: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Pending", Reason="", readiness=false. Elapsed: 25.887727ms
Feb 14 00:46:57.951: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048599659s
Feb 14 00:46:59.980: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077422148s
Feb 14 00:47:02.002: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 6.099857397s
Feb 14 00:47:04.025: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 8.122763521s
Feb 14 00:47:06.048: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 10.145921044s
Feb 14 00:47:08.071: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 12.168454322s
Feb 14 00:47:10.094: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 14.192029288s
Feb 14 00:47:12.118: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 16.215762118s
Feb 14 00:47:14.141: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 18.238672925s
Feb 14 00:47:16.164: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 20.261349856s
Feb 14 00:47:18.186: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 22.284083077s
Feb 14 00:47:20.209: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Running", Reason="", readiness=false. Elapsed: 24.30685589s
Feb 14 00:47:22.231: INFO: Pod "pod-subpath-test-projected-rkvf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.329059046s
STEP: Saw pod success
Feb 14 00:47:22.232: INFO: Pod "pod-subpath-test-projected-rkvf" satisfied condition "success or failure"
Feb 14 00:47:22.253: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-subpath-test-projected-rkvf container test-container-subpath-projected-rkvf: <nil>
STEP: delete the pod
Feb 14 00:47:22.361: INFO: Waiting for pod pod-subpath-test-projected-rkvf to disappear
Feb 14 00:47:22.382: INFO: Pod pod-subpath-test-projected-rkvf no longer exists
STEP: Deleting pod pod-subpath-test-projected-rkvf
Feb 14 00:47:22.383: INFO: Deleting pod "pod-subpath-test-projected-rkvf" in namespace "e2e-tests-subpath-bc2nr"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:47:22.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-bc2nr" for this suite.
Feb 14 00:47:28.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:47:28.938: INFO: namespace: e2e-tests-subpath-bc2nr, resource: bindings, ignored listing per whitelist
Feb 14 00:47:29.327: INFO: namespace e2e-tests-subpath-bc2nr deletion completed in 6.900314001s

• [SLOW TEST:34.633 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:47:29.328: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-6nnz4
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name projected-configmap-test-volume-1ba7d589-2ff2-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:47:30.458: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-6nnz4" to be "success or failure"
Feb 14 00:47:30.480: INFO: Pod "pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.804236ms
Feb 14 00:47:32.504: INFO: Pod "pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045640397s
Feb 14 00:47:34.526: INFO: Pod "pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067827728s
STEP: Saw pod success
Feb 14 00:47:34.526: INFO: Pod "pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:47:34.547: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:47:34.615: INFO: Waiting for pod pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:47:34.637: INFO: Pod pod-projected-configmaps-1bab1826-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:47:34.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-6nnz4" for this suite.
Feb 14 00:47:40.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:47:40.913: INFO: namespace: e2e-tests-projected-6nnz4, resource: bindings, ignored listing per whitelist
Feb 14 00:47:41.621: INFO: namespace e2e-tests-projected-6nnz4 deletion completed in 6.962087955s

• [SLOW TEST:12.293 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:47:41.621: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-2b575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:47:43.090: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"233073c4-2ff2-11e9-9d38-36c391ece45e", Controller:(*bool)(0xc00227023e), BlockOwnerDeletion:(*bool)(0xc00227023f)}}
Feb 14 00:47:43.116: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2329b78c-2ff2-11e9-9d38-36c391ece45e", Controller:(*bool)(0xc00255dfbe), BlockOwnerDeletion:(*bool)(0xc00255dfbf)}}
Feb 14 00:47:43.139: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"232d1dd7-2ff2-11e9-9d38-36c391ece45e", Controller:(*bool)(0xc0025c4a2e), BlockOwnerDeletion:(*bool)(0xc0025c4a2f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:47:48.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-2b575" for this suite.
Feb 14 00:47:54.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:47:54.874: INFO: namespace: e2e-tests-gc-2b575, resource: bindings, ignored listing per whitelist
Feb 14 00:47:55.353: INFO: namespace e2e-tests-gc-2b575 deletion completed in 7.146017874s

• [SLOW TEST:13.732 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:47:55.353: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-4tn46
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:47:56.541: INFO: Requires at least 2 nodes (not -1)
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
Feb 14 00:47:56.584: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-4tn46/daemonsets","resourceVersion":"25046"},"items":null}

Feb 14 00:47:56.606: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-4tn46/pods","resourceVersion":"25046"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:47:56.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-4tn46" for this suite.
Feb 14 00:48:02.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:48:03.100: INFO: namespace: e2e-tests-daemonsets-4tn46, resource: bindings, ignored listing per whitelist
Feb 14 00:48:03.662: INFO: namespace e2e-tests-daemonsets-4tn46 deletion completed in 6.967388299s

S [SKIPPING] [8.308 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance] [It]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699

  Feb 14 00:47:56.541: Requires at least 2 nodes (not -1)

  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:292
------------------------------
S
------------------------------
[sig-storage] Projected 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:48:03.662: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-mpl4h
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:48:04.731: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-mpl4h" to be "success or failure"
Feb 14 00:48:04.757: INFO: Pod "downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.585537ms
Feb 14 00:48:06.780: INFO: Pod "downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048460186s
Feb 14 00:48:08.802: INFO: Pod "downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071321054s
STEP: Saw pod success
Feb 14 00:48:08.802: INFO: Pod "downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:48:08.824: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:48:08.897: INFO: Waiting for pod downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:48:08.918: INFO: Pod downwardapi-volume-30193fbb-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:48:08.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-mpl4h" for this suite.
Feb 14 00:48:15.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:48:15.880: INFO: namespace: e2e-tests-projected-mpl4h, resource: bindings, ignored listing per whitelist
Feb 14 00:48:15.902: INFO: namespace e2e-tests-projected-mpl4h deletion completed in 6.962087496s

• [SLOW TEST:12.240 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:48:15.902: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-t4jfx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override arguments
Feb 14 00:48:17.050: INFO: Waiting up to 5m0s for pod "client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-containers-t4jfx" to be "success or failure"
Feb 14 00:48:17.071: INFO: Pod "client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.287087ms
Feb 14 00:48:19.094: INFO: Pod "client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043728036s
Feb 14 00:48:21.116: INFO: Pod "client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066595198s
STEP: Saw pod success
Feb 14 00:48:21.116: INFO: Pod "client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:48:21.138: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:48:21.238: INFO: Waiting for pod client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:48:21.259: INFO: Pod client-containers-3770e963-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:48:21.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-t4jfx" for this suite.
Feb 14 00:48:27.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:48:27.435: INFO: namespace: e2e-tests-containers-t4jfx, resource: bindings, ignored listing per whitelist
Feb 14 00:48:28.247: INFO: namespace e2e-tests-containers-t4jfx deletion completed in 6.959981312s

• [SLOW TEST:12.345 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:48:28.247: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-5lg5t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-3ec38f36-2ff2-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:48:29.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-5lg5t" to be "success or failure"
Feb 14 00:48:29.379: INFO: Pod "pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.771544ms
Feb 14 00:48:31.401: INFO: Pod "pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044152238s
Feb 14 00:48:33.424: INFO: Pod "pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066544698s
STEP: Saw pod success
Feb 14 00:48:33.424: INFO: Pod "pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:48:33.445: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:48:33.535: INFO: Waiting for pod pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:48:33.563: INFO: Pod pod-configmaps-3ec6d872-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:48:33.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-5lg5t" for this suite.
Feb 14 00:48:39.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:48:40.394: INFO: namespace: e2e-tests-configmap-5lg5t, resource: bindings, ignored listing per whitelist
Feb 14 00:48:40.504: INFO: namespace e2e-tests-configmap-5lg5t deletion completed in 6.916105318s

• [SLOW TEST:12.256 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:48:40.504: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-pdbmk
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name cm-test-opt-del-460ca572-2ff2-11e9-b8b1-3a3e684b6200
STEP: Creating configMap with name cm-test-opt-upd-460ca5c1-2ff2-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-460ca572-2ff2-11e9-b8b1-3a3e684b6200
STEP: Updating configmap cm-test-opt-upd-460ca5c1-2ff2-11e9-b8b1-3a3e684b6200
STEP: Creating configMap with name cm-test-opt-create-460ca5d7-2ff2-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:50:19.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-pdbmk" for this suite.
Feb 14 00:50:41.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:50:42.081: INFO: namespace: e2e-tests-configmap-pdbmk, resource: bindings, ignored listing per whitelist
Feb 14 00:50:42.699: INFO: namespace e2e-tests-configmap-pdbmk deletion completed in 22.96839709s

• [SLOW TEST:122.195 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:50:42.699: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-mp7s8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should support --unix-socket=/path  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Starting the proxy
Feb 14 00:50:44.012: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix905524506/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:50:44.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-mp7s8" for this suite.
Feb 14 00:50:50.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:50:50.667: INFO: namespace: e2e-tests-kubectl-mp7s8, resource: bindings, ignored listing per whitelist
Feb 14 00:50:51.048: INFO: namespace e2e-tests-kubectl-mp7s8 deletion completed in 6.946429926s

• [SLOW TEST:8.348 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Proxy server
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should support --unix-socket=/path  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:50:51.048: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-gthsr
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1475
[It] should create a pod from an image when restart is Never  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 00:50:52.216: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-gthsr'
Feb 14 00:50:53.565: INFO: stderr: ""
Feb 14 00:50:53.565: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
Feb 14 00:50:53.587: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete pods e2e-test-nginx-pod --namespace=e2e-tests-kubectl-gthsr'
Feb 14 00:51:00.692: INFO: stderr: ""
Feb 14 00:51:00.692: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:51:00.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-gthsr" for this suite.
Feb 14 00:51:06.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:51:07.942: INFO: namespace: e2e-tests-kubectl-gthsr, resource: bindings, ignored listing per whitelist
Feb 14 00:51:08.294: INFO: namespace e2e-tests-kubectl-gthsr deletion completed in 7.580030273s

• [SLOW TEST:17.246 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run pod
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a pod from an image when restart is Never  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:51:08.294: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-bjn7f
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:51:09.437: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-bjn7f" to be "success or failure"
Feb 14 00:51:09.463: INFO: Pod "downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 26.294831ms
Feb 14 00:51:11.488: INFO: Pod "downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051699681s
Feb 14 00:51:13.511: INFO: Pod "downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074613553s
STEP: Saw pod success
Feb 14 00:51:13.511: INFO: Pod "downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:51:13.533: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:51:13.631: INFO: Waiting for pod downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:51:13.652: INFO: Pod downwardapi-volume-9e3115de-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:51:13.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-bjn7f" for this suite.
Feb 14 00:51:19.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:51:20.486: INFO: namespace: e2e-tests-projected-bjn7f, resource: bindings, ignored listing per whitelist
Feb 14 00:51:20.571: INFO: namespace e2e-tests-projected-bjn7f deletion completed in 6.896274811s

• [SLOW TEST:12.276 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:51:20.571: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-rtgpl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:51:21.737: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-rtgpl" to be "success or failure"
Feb 14 00:51:21.758: INFO: Pod "downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.165798ms
Feb 14 00:51:23.780: INFO: Pod "downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043676751s
Feb 14 00:51:25.803: INFO: Pod "downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066059384s
STEP: Saw pod success
Feb 14 00:51:25.803: INFO: Pod "downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:51:25.824: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:51:25.893: INFO: Waiting for pod downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:51:25.914: INFO: Pod downwardapi-volume-a585faab-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:51:25.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-rtgpl" for this suite.
Feb 14 00:51:32.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:51:32.172: INFO: namespace: e2e-tests-projected-rtgpl, resource: bindings, ignored listing per whitelist
Feb 14 00:51:32.886: INFO: namespace e2e-tests-projected-rtgpl deletion completed in 6.949979023s

• [SLOW TEST:12.315 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should set mode on item file [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:51:32.886: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-86mhr
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 14 00:51:34.016: INFO: Waiting up to 5m0s for pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-86mhr" to be "success or failure"
Feb 14 00:51:34.038: INFO: Pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.658955ms
Feb 14 00:51:36.059: INFO: Pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043045331s
Feb 14 00:51:38.082: INFO: Pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065739216s
Feb 14 00:51:40.104: INFO: Pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.088304279s
STEP: Saw pod success
Feb 14 00:51:40.104: INFO: Pod "pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:51:40.126: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:51:40.187: INFO: Waiting for pod pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:51:40.209: INFO: Pod pod-acd797d4-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:51:40.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-86mhr" for this suite.
Feb 14 00:51:46.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:51:46.458: INFO: namespace: e2e-tests-emptydir-86mhr, resource: bindings, ignored listing per whitelist
Feb 14 00:51:47.192: INFO: namespace e2e-tests-emptydir-86mhr deletion completed in 6.950199832s

• [SLOW TEST:14.306 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0777,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:51:47.192: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-svcaccounts-fgfpv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: getting the auto-created API token
STEP: Creating a pod to test consume service account token
Feb 14 00:51:48.776: INFO: Waiting up to 5m0s for pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x" in namespace "e2e-tests-svcaccounts-fgfpv" to be "success or failure"
Feb 14 00:51:48.803: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x": Phase="Pending", Reason="", readiness=false. Elapsed: 26.858735ms
Feb 14 00:51:50.825: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049640276s
Feb 14 00:51:52.847: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071691754s
Feb 14 00:51:54.870: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.094266139s
STEP: Saw pod success
Feb 14 00:51:54.870: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x" satisfied condition "success or failure"
Feb 14 00:51:54.892: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x container token-test: <nil>
STEP: delete the pod
Feb 14 00:51:54.954: INFO: Waiting for pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x to disappear
Feb 14 00:51:54.982: INFO: Pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-9ks7x no longer exists
STEP: Creating a pod to test consume service account root CA
Feb 14 00:51:55.012: INFO: Waiting up to 5m0s for pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d" in namespace "e2e-tests-svcaccounts-fgfpv" to be "success or failure"
Feb 14 00:51:55.036: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.036701ms
Feb 14 00:51:57.059: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046234396s
Feb 14 00:51:59.081: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.068847892s
Feb 14 00:52:01.104: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.091631162s
STEP: Saw pod success
Feb 14 00:52:01.104: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d" satisfied condition "success or failure"
Feb 14 00:52:01.125: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d container root-ca-test: <nil>
STEP: delete the pod
Feb 14 00:52:01.195: INFO: Waiting for pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d to disappear
Feb 14 00:52:01.216: INFO: Pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-pvk9d no longer exists
STEP: Creating a pod to test consume service account namespace
Feb 14 00:52:01.238: INFO: Waiting up to 5m0s for pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh" in namespace "e2e-tests-svcaccounts-fgfpv" to be "success or failure"
Feb 14 00:52:01.265: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh": Phase="Pending", Reason="", readiness=false. Elapsed: 26.933076ms
Feb 14 00:52:03.292: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053644431s
Feb 14 00:52:05.314: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075368998s
Feb 14 00:52:07.336: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.097909698s
STEP: Saw pod success
Feb 14 00:52:07.336: INFO: Pod "pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh" satisfied condition "success or failure"
Feb 14 00:52:07.358: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh container namespace-test: <nil>
STEP: delete the pod
Feb 14 00:52:07.480: INFO: Waiting for pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh to disappear
Feb 14 00:52:07.540: INFO: Pod pod-service-account-b5a3a28a-2ff2-11e9-b8b1-3a3e684b6200-klfbh no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:52:07.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-svcaccounts-fgfpv" for this suite.
Feb 14 00:52:13.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:52:13.803: INFO: namespace: e2e-tests-svcaccounts-fgfpv, resource: bindings, ignored listing per whitelist
Feb 14 00:52:14.443: INFO: namespace e2e-tests-svcaccounts-fgfpv deletion completed in 6.879943434s

• [SLOW TEST:27.250 seconds]
[sig-auth] ServiceAccounts
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:52:14.443: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-hw4pc
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should create a job from an image, then delete the job  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: executing a command with run --rm and attach with stdin
Feb 14 00:52:15.502: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml --namespace=e2e-tests-kubectl-hw4pc run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 14 00:52:19.551: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 14 00:52:19.551: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:52:21.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-hw4pc" for this suite.
Feb 14 00:52:27.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:52:28.542: INFO: namespace: e2e-tests-kubectl-hw4pc, resource: bindings, ignored listing per whitelist
Feb 14 00:52:28.542: INFO: namespace e2e-tests-kubectl-hw4pc deletion completed in 6.926101615s

• [SLOW TEST:14.099 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run --rm job
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create a job from an image, then delete the job  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:52:28.542: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-container-lifecycle-hook-x7b88
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 14 00:52:37.878: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:37.899: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:39.899: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:39.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:41.899: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:41.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:43.899: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:43.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:45.899: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:45.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:47.900: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:47.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:49.900: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:49.922: INFO: Pod pod-with-poststart-http-hook still exists
Feb 14 00:52:51.899: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 14 00:52:51.922: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:52:51.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-container-lifecycle-hook-x7b88" for this suite.
Feb 14 00:53:14.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:53:14.353: INFO: namespace: e2e-tests-container-lifecycle-hook-x7b88, resource: bindings, ignored listing per whitelist
Feb 14 00:53:14.876: INFO: namespace e2e-tests-container-lifecycle-hook-x7b88 deletion completed in 22.931569643s

• [SLOW TEST:46.334 seconds]
[k8s.io] Container Lifecycle Hook
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  when create a pod with lifecycle hook
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:53:14.877: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-tx5bz
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0214 00:53:22.277291    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 00:53:22.277: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:53:22.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tx5bz" for this suite.
Feb 14 00:53:28.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:53:28.682: INFO: namespace: e2e-tests-gc-tx5bz, resource: bindings, ignored listing per whitelist
Feb 14 00:53:29.261: INFO: namespace e2e-tests-gc-tx5bz deletion completed in 6.962067419s

• [SLOW TEST:14.384 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:53:29.261: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-var-expansion-zcwhn
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test substitution in container's command
Feb 14 00:53:30.338: INFO: Waiting up to 5m0s for pod "var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-var-expansion-zcwhn" to be "success or failure"
Feb 14 00:53:30.364: INFO: Pod "var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.811001ms
Feb 14 00:53:32.391: INFO: Pod "var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052643755s
Feb 14 00:53:34.414: INFO: Pod "var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075407651s
STEP: Saw pod success
Feb 14 00:53:34.414: INFO: Pod "var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:53:34.435: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200 container dapi-container: <nil>
STEP: delete the pod
Feb 14 00:53:34.499: INFO: Waiting for pod var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:53:34.520: INFO: Pod var-expansion-f22cfc32-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:53:34.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-var-expansion-zcwhn" for this suite.
Feb 14 00:53:40.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:53:40.785: INFO: namespace: e2e-tests-var-expansion-zcwhn, resource: bindings, ignored listing per whitelist
Feb 14 00:53:41.476: INFO: namespace e2e-tests-var-expansion-zcwhn deletion completed in 6.933534501s

• [SLOW TEST:12.215 seconds]
[k8s.io] Variable Expansion
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:53:41.476: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-jlsnd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 14 00:53:42.546: INFO: Waiting up to 5m0s for pod "pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-jlsnd" to be "success or failure"
Feb 14 00:53:42.568: INFO: Pod "pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.498311ms
Feb 14 00:53:44.589: INFO: Pod "pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04320345s
Feb 14 00:53:46.618: INFO: Pod "pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071532735s
STEP: Saw pod success
Feb 14 00:53:46.618: INFO: Pod "pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:53:46.639: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:53:46.700: INFO: Waiting for pod pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:53:46.731: INFO: Pod pod-f9728d9b-2ff2-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:53:46.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-jlsnd" for this suite.
Feb 14 00:53:52.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:53:53.403: INFO: namespace: e2e-tests-emptydir-jlsnd, resource: bindings, ignored listing per whitelist
Feb 14 00:53:53.725: INFO: namespace e2e-tests-emptydir-jlsnd deletion completed in 6.972774652s

• [SLOW TEST:12.249 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0644,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:53:53.726: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-cvfp2
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-00c77cf4-2ff3-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 00:53:54.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-cvfp2" to be "success or failure"
Feb 14 00:53:54.892: INFO: Pod "pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.435491ms
Feb 14 00:53:56.916: INFO: Pod "pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045022266s
Feb 14 00:53:58.938: INFO: Pod "pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067020639s
STEP: Saw pod success
Feb 14 00:53:58.938: INFO: Pod "pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:53:58.959: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 00:53:59.019: INFO: Waiting for pod pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:53:59.057: INFO: Pod pod-configmaps-00cad4e7-2ff3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:53:59.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-cvfp2" for this suite.
Feb 14 00:54:05.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:54:05.471: INFO: namespace: e2e-tests-configmap-cvfp2, resource: bindings, ignored listing per whitelist
Feb 14 00:54:06.027: INFO: namespace e2e-tests-configmap-cvfp2 deletion completed in 6.947763971s

• [SLOW TEST:12.301 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:54:06.027: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-tmkwm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0214 00:54:17.253508    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 00:54:17.253: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:54:17.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-tmkwm" for this suite.
Feb 14 00:54:23.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:54:24.139: INFO: namespace: e2e-tests-gc-tmkwm, resource: bindings, ignored listing per whitelist
Feb 14 00:54:24.247: INFO: namespace e2e-tests-gc-tmkwm deletion completed in 6.971952593s

• [SLOW TEST:18.220 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:54:24.247: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-containers-sbzps
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test override all
Feb 14 00:54:25.383: INFO: Waiting up to 5m0s for pod "client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-containers-sbzps" to be "success or failure"
Feb 14 00:54:25.411: INFO: Pod "client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 27.171392ms
Feb 14 00:54:27.433: INFO: Pod "client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049386494s
Feb 14 00:54:29.455: INFO: Pod "client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071816555s
STEP: Saw pod success
Feb 14 00:54:29.455: INFO: Pod "client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:54:29.476: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:54:29.545: INFO: Waiting for pod client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:54:29.567: INFO: Pod client-containers-12fc28b3-2ff3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:54:29.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-containers-sbzps" for this suite.
Feb 14 00:54:35.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:54:36.310: INFO: namespace: e2e-tests-containers-sbzps, resource: bindings, ignored listing per whitelist
Feb 14 00:54:36.570: INFO: namespace e2e-tests-containers-sbzps deletion completed in 6.981222301s

• [SLOW TEST:12.323 seconds]
[k8s.io] Docker Containers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:54:36.570: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-wmrlv
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 14 00:54:37.639: INFO: Waiting up to 5m0s for pod "pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-wmrlv" to be "success or failure"
Feb 14 00:54:37.665: INFO: Pod "pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.68947ms
Feb 14 00:54:39.687: INFO: Pod "pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048456831s
Feb 14 00:54:41.710: INFO: Pod "pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070747341s
STEP: Saw pod success
Feb 14 00:54:41.710: INFO: Pod "pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:54:41.731: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 00:54:41.814: INFO: Waiting for pod pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:54:41.835: INFO: Pod pod-1a4a4f8d-2ff3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:54:41.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-wmrlv" for this suite.
Feb 14 00:54:47.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:54:48.376: INFO: namespace: e2e-tests-emptydir-wmrlv, resource: bindings, ignored listing per whitelist
Feb 14 00:54:48.781: INFO: namespace e2e-tests-emptydir-wmrlv deletion completed in 6.923869323s

• [SLOW TEST:12.211 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on default medium should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:54:48.781: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-daemonsets-mzjxw
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 14 00:54:50.116: INFO: Number of nodes with available pods: 0
Feb 14 00:54:50.116: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:51.161: INFO: Number of nodes with available pods: 0
Feb 14 00:54:51.162: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:52.161: INFO: Number of nodes with available pods: 0
Feb 14 00:54:52.161: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:53.160: INFO: Number of nodes with available pods: 1
Feb 14 00:54:53.160: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:54.162: INFO: Number of nodes with available pods: 2
Feb 14 00:54:54.162: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 14 00:54:54.278: INFO: Number of nodes with available pods: 1
Feb 14 00:54:54.278: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:55.334: INFO: Number of nodes with available pods: 1
Feb 14 00:54:55.334: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:56.323: INFO: Number of nodes with available pods: 1
Feb 14 00:54:56.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:57.323: INFO: Number of nodes with available pods: 1
Feb 14 00:54:57.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:58.322: INFO: Number of nodes with available pods: 1
Feb 14 00:54:58.322: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:54:59.323: INFO: Number of nodes with available pods: 1
Feb 14 00:54:59.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:00.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:00.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:01.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:01.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:02.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:02.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:03.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:03.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:04.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:04.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:05.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:05.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:06.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:06.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:07.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:07.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:08.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:08.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:09.344: INFO: Number of nodes with available pods: 1
Feb 14 00:55:09.344: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:10.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:10.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:11.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:11.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:12.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:12.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:13.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:13.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:14.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:14.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:15.338: INFO: Number of nodes with available pods: 1
Feb 14 00:55:15.338: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:16.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:16.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:17.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:17.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:18.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:18.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:19.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:19.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:20.322: INFO: Number of nodes with available pods: 1
Feb 14 00:55:20.322: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:21.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:21.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:22.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:22.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:23.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:23.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:24.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:24.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:25.322: INFO: Number of nodes with available pods: 1
Feb 14 00:55:25.322: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:26.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:26.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:27.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:27.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:28.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:28.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:29.322: INFO: Number of nodes with available pods: 1
Feb 14 00:55:29.322: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:30.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:30.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:31.326: INFO: Number of nodes with available pods: 1
Feb 14 00:55:31.326: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:32.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:32.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:33.322: INFO: Number of nodes with available pods: 1
Feb 14 00:55:33.322: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:34.324: INFO: Number of nodes with available pods: 1
Feb 14 00:55:34.324: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:35.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:35.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:36.323: INFO: Number of nodes with available pods: 1
Feb 14 00:55:36.323: INFO: Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg is running more than one daemon pod
Feb 14 00:55:37.323: INFO: Number of nodes with available pods: 2
Feb 14 00:55:37.323: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-mzjxw, will wait for the garbage collector to delete the pods
Feb 14 00:55:37.441: INFO: Deleting {extensions DaemonSet} daemon-set took: 24.273531ms
Feb 14 00:55:37.541: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 100.318691ms
Feb 14 00:56:20.763: INFO: Number of nodes with available pods: 0
Feb 14 00:56:20.763: INFO: Number of running nodes: 0, number of available pods: 0
Feb 14 00:56:20.785: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/e2e-tests-daemonsets-mzjxw/daemonsets","resourceVersion":"26533"},"items":null}

Feb 14 00:56:20.832: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/e2e-tests-daemonsets-mzjxw/pods","resourceVersion":"26533"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:56:20.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-daemonsets-mzjxw" for this suite.
Feb 14 00:56:26.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:56:27.851: INFO: namespace: e2e-tests-daemonsets-mzjxw, resource: bindings, ignored listing per whitelist
Feb 14 00:56:27.873: INFO: namespace e2e-tests-daemonsets-mzjxw deletion completed in 6.953564516s

• [SLOW TEST:99.092 seconds]
[sig-apps] Daemon set [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:56:27.874: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-custom-resource-definition-p9hw5
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
Feb 14 00:56:29.030: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:56:29.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-custom-resource-definition-p9hw5" for this suite.
Feb 14 00:56:35.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:56:36.152: INFO: namespace: e2e-tests-custom-resource-definition-p9hw5, resource: bindings, ignored listing per whitelist
Feb 14 00:56:36.800: INFO: namespace e2e-tests-custom-resource-definition-p9hw5 deletion completed in 6.991017202s

• [SLOW TEST:8.926 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:56:36.800: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-nw29p
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should add annotations for pods in rc  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating Redis RC
Feb 14 00:56:37.910: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-nw29p'
Feb 14 00:56:38.241: INFO: stderr: ""
Feb 14 00:56:38.241: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 14 00:56:39.276: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:56:39.276: INFO: Found 0 / 1
Feb 14 00:56:40.264: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:56:40.264: INFO: Found 0 / 1
Feb 14 00:56:41.263: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:56:41.263: INFO: Found 1 / 1
Feb 14 00:56:41.263: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 14 00:56:41.286: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:56:41.286: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 14 00:56:41.286: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml patch pod redis-master-9pm4m --namespace=e2e-tests-kubectl-nw29p -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 14 00:56:41.472: INFO: stderr: ""
Feb 14 00:56:41.472: INFO: stdout: "pod/redis-master-9pm4m patched\n"
STEP: checking annotations
Feb 14 00:56:41.494: INFO: Selector matched 1 pods for map[app:redis]
Feb 14 00:56:41.494: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:56:41.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-nw29p" for this suite.
Feb 14 00:57:05.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:57:06.229: INFO: namespace: e2e-tests-kubectl-nw29p, resource: bindings, ignored listing per whitelist
Feb 14 00:57:06.445: INFO: namespace e2e-tests-kubectl-nw29p deletion completed in 24.928746653s

• [SLOW TEST:29.645 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl patch
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should add annotations for pods in rc  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:57:06.445: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-vflvz
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[It] should check if v1 is in available api versions  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: validating api versions
Feb 14 00:57:07.503: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml api-versions'
Feb 14 00:57:07.704: INFO: stderr: ""
Feb 14 00:57:07.704: INFO: stdout: "admissionregistration.k8s.io/v1alpha1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:57:07.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-vflvz" for this suite.
Feb 14 00:57:13.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:57:13.961: INFO: namespace: e2e-tests-kubectl-vflvz, resource: bindings, ignored listing per whitelist
Feb 14 00:57:14.652: INFO: namespace e2e-tests-kubectl-vflvz deletion completed in 6.926486468s

• [SLOW TEST:8.207 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl api-versions
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should check if v1 is in available api versions  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:57:14.653: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-init-container-85jfh
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the pod
Feb 14 00:57:15.714: INFO: PodSpec: initContainers in spec.initContainers
Feb 14 00:58:02.840: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-788664b8-2ff3-11e9-b8b1-3a3e684b6200", GenerateName:"", Namespace:"e2e-tests-init-container-85jfh", SelfLink:"/api/v1/namespaces/e2e-tests-init-container-85jfh/pods/pod-init-788664b8-2ff3-11e9-b8b1-3a3e684b6200", UID:"7886dd47-2ff3-11e9-9d38-36c391ece45e", ResourceVersion:"26812", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63685702635, loc:(*time.Location)(0x78fbda0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"714277105"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp", "cni.projectcalico.org/podIP":"100.96.1.211/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-j427m", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020f8ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j427m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j427m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-j427m", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil)}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002236e28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0014373e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002236ea0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002236ec0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002236ec8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685702635, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685702635, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685702635, loc:(*time.Location)(0x78fbda0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63685702635, loc:(*time.Location)(0x78fbda0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.5", PodIP:"100.96.1.211", StartTime:(*v1.Time)(0xc000bf5e80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00073dea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00073df10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://67be6ebcb75f3ce65f914638ec6a016280eb051ff8fad0566e8809339648912f"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000bf5ec0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000bf5ea0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:58:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-init-container-85jfh" for this suite.
Feb 14 00:58:26.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:58:27.648: INFO: namespace: e2e-tests-init-container-85jfh, resource: bindings, ignored listing per whitelist
Feb 14 00:58:27.757: INFO: namespace e2e-tests-init-container-85jfh deletion completed in 24.894567639s

• [SLOW TEST:73.105 seconds]
[k8s.io] InitContainer [NodeConformance]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] Projected 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:58:27.758: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-sh2hq
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:58:28.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-sh2hq" to be "success or failure"
Feb 14 00:58:28.857: INFO: Pod "downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.887025ms
Feb 14 00:58:30.882: INFO: Pod "downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04698466s
Feb 14 00:58:32.904: INFO: Pod "downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069284205s
STEP: Saw pod success
Feb 14 00:58:32.904: INFO: Pod "downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:58:32.929: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:58:32.993: INFO: Waiting for pod downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:58:33.014: INFO: Pod downwardapi-volume-a417ef2b-2ff3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:58:33.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-sh2hq" for this suite.
Feb 14 00:58:39.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:58:39.940: INFO: namespace: e2e-tests-projected-sh2hq, resource: bindings, ignored listing per whitelist
Feb 14 00:58:39.940: INFO: namespace e2e-tests-projected-sh2hq deletion completed in 6.90317242s

• [SLOW TEST:12.182 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:58:39.940: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-secrets-n6xr2
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating secret with name s-test-opt-del-ab7c9514-2ff3-11e9-b8b1-3a3e684b6200
STEP: Creating secret with name s-test-opt-upd-ab7c956d-2ff3-11e9-b8b1-3a3e684b6200
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ab7c9514-2ff3-11e9-b8b1-3a3e684b6200
STEP: Updating secret s-test-opt-upd-ab7c956d-2ff3-11e9-b8b1-3a3e684b6200
STEP: Creating secret with name s-test-opt-create-ab7c9595-2ff3-11e9-b8b1-3a3e684b6200
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:58:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-secrets-n6xr2" for this suite.
Feb 14 00:59:13.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:59:14.237: INFO: namespace: e2e-tests-secrets-n6xr2, resource: bindings, ignored listing per whitelist
Feb 14 00:59:14.786: INFO: namespace e2e-tests-secrets-n6xr2 deletion completed in 24.96242319s

• [SLOW TEST:34.846 seconds]
[sig-storage] Secrets
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:59:14.786: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wcls8
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 00:59:15.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-wcls8" to be "success or failure"
Feb 14 00:59:15.860: INFO: Pod "downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.23038ms
Feb 14 00:59:17.883: INFO: Pod "downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044306793s
Feb 14 00:59:19.905: INFO: Pod "downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066652769s
STEP: Saw pod success
Feb 14 00:59:19.905: INFO: Pod "downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 00:59:19.934: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 00:59:20.032: INFO: Waiting for pod downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 00:59:20.054: INFO: Pod downwardapi-volume-c01c1f73-2ff3-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 00:59:20.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wcls8" for this suite.
Feb 14 00:59:26.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 00:59:26.719: INFO: namespace: e2e-tests-projected-wcls8, resource: bindings, ignored listing per whitelist
Feb 14 00:59:26.959: INFO: namespace e2e-tests-projected-wcls8 deletion completed in 6.882315603s

• [SLOW TEST:12.173 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 00:59:26.959: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-statefulset-ktfxt
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace e2e-tests-statefulset-ktfxt
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a new StatefulSet
Feb 14 00:59:28.086: INFO: Found 1 stateful pods, waiting for 3
Feb 14 00:59:38.110: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:59:38.110: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:59:38.110: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 14 00:59:38.175: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ktfxt ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 00:59:38.911: INFO: stderr: ""
Feb 14 00:59:38.911: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 00:59:38.911: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Feb 14 00:59:49.055: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 14 00:59:49.131: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ktfxt ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 00:59:49.834: INFO: stderr: ""
Feb 14 00:59:49.834: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 00:59:49.834: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 01:00:09.976: INFO: Waiting for StatefulSet e2e-tests-statefulset-ktfxt/ss2 to complete update
Feb 14 01:00:09.976: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Feb 14 01:00:20.021: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ktfxt ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Feb 14 01:00:20.762: INFO: stderr: ""
Feb 14 01:00:20.762: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Feb 14 01:00:20.762: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Feb 14 01:00:20.866: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 14 01:00:20.942: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml exec --namespace=e2e-tests-statefulset-ktfxt ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Feb 14 01:00:21.639: INFO: stderr: ""
Feb 14 01:00:21.639: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Feb 14 01:00:21.639: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Feb 14 01:00:31.770: INFO: Waiting for StatefulSet e2e-tests-statefulset-ktfxt/ss2 to complete update
Feb 14 01:00:31.770: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 01:00:31.770: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 01:00:41.814: INFO: Waiting for StatefulSet e2e-tests-statefulset-ktfxt/ss2 to complete update
Feb 14 01:00:41.814: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 01:00:41.814: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 01:00:51.814: INFO: Waiting for StatefulSet e2e-tests-statefulset-ktfxt/ss2 to complete update
Feb 14 01:00:51.815: INFO: Waiting for Pod e2e-tests-statefulset-ktfxt/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Feb 14 01:01:01.814: INFO: Waiting for StatefulSet e2e-tests-statefulset-ktfxt/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Feb 14 01:01:11.814: INFO: Deleting all statefulset in ns e2e-tests-statefulset-ktfxt
Feb 14 01:01:11.837: INFO: Scaling statefulset ss2 to 0
Feb 14 01:01:41.940: INFO: Waiting for statefulset status.replicas updated to 0
Feb 14 01:01:41.963: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:01:42.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-statefulset-ktfxt" for this suite.
Feb 14 01:01:48.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:01:48.613: INFO: namespace: e2e-tests-statefulset-ktfxt, resource: bindings, ignored listing per whitelist
Feb 14 01:01:49.001: INFO: namespace e2e-tests-statefulset-ktfxt deletion completed in 6.942529301s

• [SLOW TEST:142.042 seconds]
[sig-apps] StatefulSet
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should perform rolling updates and roll backs of template modifications [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:01:49.001: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-prestop-89pl6
STEP: Waiting for a default service account to be provisioned in namespace
[It] should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating server pod server in namespace e2e-tests-prestop-89pl6
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace e2e-tests-prestop-89pl6
STEP: Deleting pre-stop pod
Feb 14 01:02:05.419: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:02:05.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-prestop-89pl6" for this suite.
Feb 14 01:02:45.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:02:46.267: INFO: namespace: e2e-tests-prestop-89pl6, resource: bindings, ignored listing per whitelist
Feb 14 01:02:46.417: INFO: namespace e2e-tests-prestop-89pl6 deletion completed in 40.936318054s

• [SLOW TEST:57.416 seconds]
[k8s.io] [sig-node] PreStop
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
  should call prestop when killing a pod  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:02:46.417: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-subpath-szbch
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating pod pod-subpath-test-configmap-mmjd
STEP: Creating a pod to test atomic-volume-subpath
Feb 14 01:02:47.660: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mmjd" in namespace "e2e-tests-subpath-szbch" to be "success or failure"
Feb 14 01:02:47.688: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Pending", Reason="", readiness=false. Elapsed: 27.615147ms
Feb 14 01:02:49.710: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049593669s
Feb 14 01:02:51.732: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071890706s
Feb 14 01:02:53.754: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 6.093759163s
Feb 14 01:02:55.776: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 8.116137694s
Feb 14 01:02:57.798: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 10.138287776s
Feb 14 01:02:59.822: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 12.161540382s
Feb 14 01:03:01.854: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 14.194143465s
Feb 14 01:03:03.877: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 16.216772768s
Feb 14 01:03:05.913: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 18.253239927s
Feb 14 01:03:07.935: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 20.275271537s
Feb 14 01:03:09.958: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Running", Reason="", readiness=false. Elapsed: 22.297755341s
Feb 14 01:03:11.980: INFO: Pod "pod-subpath-test-configmap-mmjd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.319950972s
STEP: Saw pod success
Feb 14 01:03:11.980: INFO: Pod "pod-subpath-test-configmap-mmjd" satisfied condition "success or failure"
Feb 14 01:03:12.001: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-subpath-test-configmap-mmjd container test-container-subpath-configmap-mmjd: <nil>
STEP: delete the pod
Feb 14 01:03:12.637: INFO: Waiting for pod pod-subpath-test-configmap-mmjd to disappear
Feb 14 01:03:12.657: INFO: Pod pod-subpath-test-configmap-mmjd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mmjd
Feb 14 01:03:12.657: INFO: Deleting pod "pod-subpath-test-configmap-mmjd" in namespace "e2e-tests-subpath-szbch"
[AfterEach] [sig-storage] Subpath
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:03:12.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-subpath-szbch" for this suite.
Feb 14 01:03:18.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:03:19.531: INFO: namespace: e2e-tests-subpath-szbch, resource: bindings, ignored listing per whitelist
Feb 14 01:03:19.639: INFO: namespace e2e-tests-subpath-szbch deletion completed in 6.939012632s

• [SLOW TEST:33.222 seconds]
[sig-storage] Subpath
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:03:19.639: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-watch-hldfm
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 14 01:03:20.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27765,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 01:03:20.797: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27765,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 14 01:03:30.855: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27785,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 14 01:03:30.856: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27785,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 14 01:03:55.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27833,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 01:03:55.291: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27833,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 14 01:04:05.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27853,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 14 01:04:05.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-a,UID:521e7376-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27853,Generation:0,CreationTimestamp:2019-02-14 01:03:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 14 01:04:15.342: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-b,UID:72a10247-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27874,Generation:0,CreationTimestamp:2019-02-14 01:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 01:04:15.342: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-b,UID:72a10247-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27874,Generation:0,CreationTimestamp:2019-02-14 01:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 14 01:04:25.367: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-b,UID:72a10247-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27894,Generation:0,CreationTimestamp:2019-02-14 01:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 14 01:04:25.367: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:e2e-tests-watch-hldfm,SelfLink:/api/v1/namespaces/e2e-tests-watch-hldfm/configmaps/e2e-watch-test-configmap-b,UID:72a10247-2ff4-11e9-9d38-36c391ece45e,ResourceVersion:27894,Generation:0,CreationTimestamp:2019-02-14 01:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:04:35.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-watch-hldfm" for this suite.
Feb 14 01:04:41.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:04:41.691: INFO: namespace: e2e-tests-watch-hldfm, resource: bindings, ignored listing per whitelist
Feb 14 01:04:42.307: INFO: namespace e2e-tests-watch-hldfm deletion completed in 6.91627617s

• [SLOW TEST:82.668 seconds]
[sig-api-machinery] Watchers
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:04:42.307: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-qfsss
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 01:04:43.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-qfsss" to be "success or failure"
Feb 14 01:04:43.475: INFO: Pod "downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 25.526422ms
Feb 14 01:04:45.500: INFO: Pod "downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050225107s
Feb 14 01:04:47.522: INFO: Pod "downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07248312s
STEP: Saw pod success
Feb 14 01:04:47.522: INFO: Pod "downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:04:47.544: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 01:04:47.722: INFO: Waiting for pod downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:04:47.743: INFO: Pod downwardapi-volume-8361af3e-2ff4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:04:47.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-qfsss" for this suite.
Feb 14 01:05:22.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:05:23.569: INFO: namespace: e2e-tests-downward-api-qfsss, resource: bindings, ignored listing per whitelist
Feb 14 01:05:23.698: INFO: namespace e2e-tests-downward-api-qfsss deletion completed in 35.932558245s

• [SLOW TEST:41.390 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:05:23.698: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-jv4rk
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:293
[It] should do a rolling update of a replication controller  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: creating the initial replication controller
Feb 14 01:05:24.914: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml create -f - --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:26.233: INFO: stderr: ""
Feb 14 01:05:26.233: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 01:05:26.233: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:26.408: INFO: stderr: ""
Feb 14 01:05:26.408: INFO: stdout: "update-demo-nautilus-jt9r9 update-demo-nautilus-lhhdc "
Feb 14 01:05:26.408: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-jt9r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:26.576: INFO: stderr: ""
Feb 14 01:05:26.577: INFO: stdout: ""
Feb 14 01:05:26.577: INFO: update-demo-nautilus-jt9r9 is created but not running
Feb 14 01:05:31.577: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:31.760: INFO: stderr: ""
Feb 14 01:05:31.760: INFO: stdout: "update-demo-nautilus-jt9r9 update-demo-nautilus-lhhdc "
Feb 14 01:05:31.761: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-jt9r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:31.932: INFO: stderr: ""
Feb 14 01:05:31.932: INFO: stdout: "true"
Feb 14 01:05:31.932: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-jt9r9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:32.108: INFO: stderr: ""
Feb 14 01:05:32.108: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 01:05:32.108: INFO: validating pod update-demo-nautilus-jt9r9
Feb 14 01:05:32.221: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 01:05:32.221: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 01:05:32.221: INFO: update-demo-nautilus-jt9r9 is verified up and running
Feb 14 01:05:32.221: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-lhhdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:32.390: INFO: stderr: ""
Feb 14 01:05:32.390: INFO: stdout: "true"
Feb 14 01:05:32.390: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-nautilus-lhhdc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:32.559: INFO: stderr: ""
Feb 14 01:05:32.559: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 14 01:05:32.559: INFO: validating pod update-demo-nautilus-lhhdc
Feb 14 01:05:32.674: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 14 01:05:32.674: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 14 01:05:32.674: INFO: update-demo-nautilus-lhhdc is verified up and running
STEP: rolling-update to new replication controller
Feb 14 01:05:32.678: INFO: scanned /root for discovery docs: <nil>
Feb 14 01:05:32.678: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml rolling-update update-demo-nautilus --update-period=1s -f - --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:55.279: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 14 01:05:55.279: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 14 01:05:55.279: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:05:55.501: INFO: stderr: ""
Feb 14 01:05:55.501: INFO: stdout: "update-demo-kitten-pbqvg update-demo-kitten-tb6xf update-demo-nautilus-jt9r9 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 14 01:06:00.501: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:00.699: INFO: stderr: ""
Feb 14 01:06:00.699: INFO: stdout: "update-demo-kitten-pbqvg update-demo-kitten-tb6xf update-demo-nautilus-jt9r9 "
STEP: Replicas for name=update-demo: expected=2 actual=3
Feb 14 01:06:05.699: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:05.879: INFO: stderr: ""
Feb 14 01:06:05.879: INFO: stdout: "update-demo-kitten-pbqvg update-demo-kitten-tb6xf "
Feb 14 01:06:05.880: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-pbqvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:06.047: INFO: stderr: ""
Feb 14 01:06:06.047: INFO: stdout: "true"
Feb 14 01:06:06.047: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-pbqvg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:06.267: INFO: stderr: ""
Feb 14 01:06:06.267: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 01:06:06.267: INFO: validating pod update-demo-kitten-pbqvg
Feb 14 01:06:06.379: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 01:06:06.379: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 01:06:06.379: INFO: update-demo-kitten-pbqvg is verified up and running
Feb 14 01:06:06.379: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-tb6xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:06.572: INFO: stderr: ""
Feb 14 01:06:06.572: INFO: stdout: "true"
Feb 14 01:06:06.572: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml get pods update-demo-kitten-tb6xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=e2e-tests-kubectl-jv4rk'
Feb 14 01:06:06.743: INFO: stderr: ""
Feb 14 01:06:06.743: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 14 01:06:06.743: INFO: validating pod update-demo-kitten-tb6xf
Feb 14 01:06:06.858: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 14 01:06:06.858: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 14 01:06:06.858: INFO: update-demo-kitten-tb6xf is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:06:06.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-jv4rk" for this suite.
Feb 14 01:06:30.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:06:31.625: INFO: namespace: e2e-tests-kubectl-jv4rk, resource: bindings, ignored listing per whitelist
Feb 14 01:06:31.838: INFO: namespace e2e-tests-kubectl-jv4rk deletion completed in 24.958930112s

• [SLOW TEST:68.140 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Update Demo
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should do a rolling update of a replication controller  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:06:31.839: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-configmap-q6n5g
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-test-volume-map-c4a7268f-2ff4-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test consume configMaps
Feb 14 01:06:32.980: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-configmap-q6n5g" to be "success or failure"
Feb 14 01:06:33.011: INFO: Pod "pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 30.537978ms
Feb 14 01:06:35.035: INFO: Pod "pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05450735s
Feb 14 01:06:37.058: INFO: Pod "pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.077392446s
STEP: Saw pod success
Feb 14 01:06:37.058: INFO: Pod "pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:06:37.079: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 14 01:06:37.186: INFO: Waiting for pod pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:06:37.209: INFO: Pod pod-configmaps-c4aa6eac-2ff4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:06:37.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-configmap-q6n5g" for this suite.
Feb 14 01:06:43.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:06:43.940: INFO: namespace: e2e-tests-configmap-q6n5g, resource: bindings, ignored listing per whitelist
Feb 14 01:06:44.135: INFO: namespace e2e-tests-configmap-q6n5g deletion completed in 6.904293335s

• [SLOW TEST:12.297 seconds]
[sig-storage] ConfigMap
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:33
  should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:06:44.136: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-pod-network-test-4vvdd
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-4vvdd
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 14 01:06:45.212: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 14 01:07:13.605: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.0.63:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4vvdd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 01:07:13.605: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 01:07:14.161: INFO: Found all expected endpoints: [netserver-0]
Feb 14 01:07:14.184: INFO: ExecWithOptions {Command:[/bin/sh -c timeout -t 15 curl -g -q -s --connect-timeout 1 http://100.96.1.228:8080/hostName | grep -v '^\s*$'] Namespace:e2e-tests-pod-network-test-4vvdd PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 14 01:07:14.184: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
Feb 14 01:07:14.766: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:07:14.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-pod-network-test-4vvdd" for this suite.
Feb 14 01:07:38.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:07:39.500: INFO: namespace: e2e-tests-pod-network-test-4vvdd, resource: bindings, ignored listing per whitelist
Feb 14 01:07:39.774: INFO: namespace e2e-tests-pod-network-test-4vvdd deletion completed in 24.98631674s

• [SLOW TEST:55.639 seconds]
[sig-network] Networking
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [NodeConformance] [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:07:39.775: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-2ghjx
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 14 01:07:40.964: INFO: Waiting up to 5m0s for pod "pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-2ghjx" to be "success or failure"
Feb 14 01:07:40.985: INFO: Pod "pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 21.17422ms
Feb 14 01:07:43.007: INFO: Pod "pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043303397s
Feb 14 01:07:45.038: INFO: Pod "pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073925953s
STEP: Saw pod success
Feb 14 01:07:45.038: INFO: Pod "pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:07:45.059: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 01:07:45.134: INFO: Waiting for pod pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:07:45.155: INFO: Pod pod-ed302ff8-2ff4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:07:45.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-2ghjx" for this suite.
Feb 14 01:07:51.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:07:51.545: INFO: namespace: e2e-tests-emptydir-2ghjx, resource: bindings, ignored listing per whitelist
Feb 14 01:07:52.093: INFO: namespace e2e-tests-emptydir-2ghjx deletion completed in 6.91507888s

• [SLOW TEST:12.318 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (non-root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:07:52.093: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-b7qzs
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 14 01:07:53.224: INFO: Waiting up to 5m0s for pod "pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-b7qzs" to be "success or failure"
Feb 14 01:07:53.246: INFO: Pod "pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.489344ms
Feb 14 01:07:55.269: INFO: Pod "pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045103665s
Feb 14 01:07:57.292: INFO: Pod "pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068286825s
STEP: Saw pod success
Feb 14 01:07:57.292: INFO: Pod "pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:07:57.313: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 01:07:57.406: INFO: Waiting for pod pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:07:57.431: INFO: Pod pod-f47ef342-2ff4-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:07:57.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-b7qzs" for this suite.
Feb 14 01:08:03.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:08:04.997: INFO: namespace: e2e-tests-emptydir-b7qzs, resource: bindings, ignored listing per whitelist
Feb 14 01:08:05.247: INFO: namespace e2e-tests-emptydir-b7qzs deletion completed in 7.793795439s

• [SLOW TEST:13.155 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,default) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:08:05.248: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-bhmjl
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 14 01:08:36.352: INFO: Waiting up to 5m0s for pod "pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-bhmjl" to be "success or failure"
Feb 14 01:08:36.387: INFO: Pod "pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 34.464834ms
Feb 14 01:08:38.410: INFO: Pod "pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058256621s
Feb 14 01:08:40.434: INFO: Pod "pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.08200193s
STEP: Saw pod success
Feb 14 01:08:40.434: INFO: Pod "pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:08:40.457: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 01:08:40.530: INFO: Waiting for pod pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:08:40.552: INFO: Pod pod-0e33a7db-2ff5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:08:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-bhmjl" for this suite.
Feb 14 01:08:46.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:08:47.112: INFO: namespace: e2e-tests-emptydir-bhmjl, resource: bindings, ignored listing per whitelist
Feb 14 01:08:47.579: INFO: namespace e2e-tests-emptydir-bhmjl deletion completed in 6.984690276s

• [SLOW TEST:42.332 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  volume on tmpfs should have the correct mode [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:08:47.580: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-projected-wd4v2
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating configMap with name configmap-projected-all-test-volume-15858830-2ff5-11e9-b8b1-3a3e684b6200
STEP: Creating secret with name secret-projected-all-test-volume-1585881f-2ff5-11e9-b8b1-3a3e684b6200
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 14 01:08:48.678: INFO: Waiting up to 5m0s for pod "projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-projected-wd4v2" to be "success or failure"
Feb 14 01:08:48.700: INFO: Pod "projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 22.116891ms
Feb 14 01:08:50.724: INFO: Pod "projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045675093s
Feb 14 01:08:52.747: INFO: Pod "projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068871054s
STEP: Saw pod success
Feb 14 01:08:52.747: INFO: Pod "projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:08:52.769: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200 container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 14 01:08:52.831: INFO: Waiting for pod projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:08:52.853: INFO: Pod projected-volume-158587e9-2ff5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Projected
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:08:52.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-projected-wd4v2" for this suite.
Feb 14 01:08:58.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:08:59.105: INFO: namespace: e2e-tests-projected-wd4v2, resource: bindings, ignored listing per whitelist
Feb 14 01:08:59.846: INFO: namespace e2e-tests-projected-wd4v2 deletion completed in 6.964237333s

• [SLOW TEST:12.266 seconds]
[sig-storage] Projected
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:35
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:08:59.846: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-kubectl-qcrf5
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:241
[BeforeEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
[It] should create an rc or deployment from an image  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: running the image docker.io/library/nginx:1.14-alpine
Feb 14 01:09:00.995: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=e2e-tests-kubectl-qcrf5'
Feb 14 01:09:01.318: INFO: stderr: "kubectl run --generator=deployment/apps.v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl create instead.\n"
Feb 14 01:09:01.318: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1216
Feb 14 01:09:03.368: INFO: Running '/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://api.pub-az-6i4kt.it.shoot.dev.k8s-hana.ondemand.com --kubeconfig=/tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml delete deployment e2e-test-nginx-deployment --namespace=e2e-tests-kubectl-qcrf5'
Feb 14 01:09:03.570: INFO: stderr: ""
Feb 14 01:09:03.570: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:09:03.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-kubectl-qcrf5" for this suite.
Feb 14 01:11:11.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:11:12.022: INFO: namespace: e2e-tests-kubectl-qcrf5, resource: bindings, ignored listing per whitelist
Feb 14 01:11:12.552: INFO: namespace e2e-tests-kubectl-qcrf5 deletion completed in 2m8.956179689s

• [SLOW TEST:132.706 seconds]
[sig-cli] Kubectl client
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:22
  [k8s.io] Kubectl run default
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:694
    should create an rc or deployment from an image  [Conformance]
    /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:11:12.553: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-gc-4pzcv
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0214 01:11:14.527104    3058 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 14 01:11:14.527: INFO: For apiserver_request_count:
For apiserver_request_latencies_summary:
For etcd_helper_cache_entry_count:
For etcd_helper_cache_hit_count:
For etcd_helper_cache_miss_count:
For etcd_request_cache_add_latencies_summary:
For etcd_request_cache_get_latencies_summary:
For etcd_request_latencies_summary:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:11:14.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-gc-4pzcv" for this suite.
Feb 14 01:11:20.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:11:21.438: INFO: namespace: e2e-tests-gc-4pzcv, resource: bindings, ignored listing per whitelist
Feb 14 01:11:21.568: INFO: namespace e2e-tests-gc-4pzcv deletion completed in 7.008135151s

• [SLOW TEST:9.015 seconds]
[sig-api-machinery] Garbage collector
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:11:21.568: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-sched-pred-hljf7
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
Feb 14 01:11:22.624: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 14 01:11:22.668: INFO: Waiting for terminating namespaces to be deleted...
Feb 14 01:11:22.690: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg before test
Feb 14 01:11:22.733: INFO: addons-kube-lego-648f8c9f5c-f8bfm from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container kube-lego ready: true, restart count 0
Feb 14 01:11:22.733: INFO: addons-kubernetes-dashboard-5f64f76bd-kpc78 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Feb 14 01:11:22.733: INFO: calico-node-plz52 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container calico-node ready: true, restart count 0
Feb 14 01:11:22.733: INFO: kube-proxy-9n7qg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 01:11:22.733: INFO: vpn-shoot-84746d495b-vvflf from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container vpn-shoot ready: true, restart count 0
Feb 14 01:11:22.733: INFO: coredns-5f4748c5f-6746w from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container coredns ready: true, restart count 0
Feb 14 01:11:22.733: INFO: metrics-server-cf4dd5768-cphxg from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container metrics-server ready: true, restart count 0
Feb 14 01:11:22.733: INFO: blackbox-exporter-64f6f7f998-7p6v6 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container blackbox-exporter ready: true, restart count 0
Feb 14 01:11:22.733: INFO: addons-nginx-ingress-controller-55d976867d-bcwjt from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Feb 14 01:11:22.733: INFO: node-exporter-xz5f7 from kube-system started at 2019-02-13 22:10:10 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 01:11:22.733: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-r49j7 from kube-system started at 2019-02-13 22:10:09 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.733: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Feb 14 01:11:22.733: INFO: 
Logging pods the kubelet thinks is on node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn before test
Feb 14 01:11:22.796: INFO: node-exporter-tvzmp from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.796: INFO: 	Container node-exporter ready: true, restart count 0
Feb 14 01:11:22.796: INFO: kube-proxy-wnnpd from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.796: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 14 01:11:22.796: INFO: calico-node-pwznt from kube-system started at 2019-02-13 22:10:15 +0000 UTC (1 container statuses recorded)
Feb 14 01:11:22.796: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: verifying the node has the label node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
STEP: verifying the node has the label node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
Feb 14 01:11:22.954: INFO: Pod addons-kube-lego-648f8c9f5c-f8bfm requesting resource cpu=20m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod addons-kubernetes-dashboard-5f64f76bd-kpc78 requesting resource cpu=50m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod addons-nginx-ingress-controller-55d976867d-bcwjt requesting resource cpu=100m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-6498456576-r49j7 requesting resource cpu=0m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod blackbox-exporter-64f6f7f998-7p6v6 requesting resource cpu=5m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod calico-node-plz52 requesting resource cpu=100m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod calico-node-pwznt requesting resource cpu=100m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
Feb 14 01:11:22.954: INFO: Pod coredns-5f4748c5f-6746w requesting resource cpu=50m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod kube-proxy-9n7qg requesting resource cpu=20m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod kube-proxy-wnnpd requesting resource cpu=20m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
Feb 14 01:11:22.954: INFO: Pod metrics-server-cf4dd5768-cphxg requesting resource cpu=20m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod node-exporter-tvzmp requesting resource cpu=5m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
Feb 14 01:11:22.954: INFO: Pod node-exporter-xz5f7 requesting resource cpu=5m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
Feb 14 01:11:22.954: INFO: Pod vpn-shoot-84746d495b-vvflf requesting resource cpu=50m on Node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718511f3-2ff5-11e9-b8b1-3a3e684b6200.158315d0ba612c63], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-hljf7/filler-pod-718511f3-2ff5-11e9-b8b1-3a3e684b6200 to shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718511f3-2ff5-11e9-b8b1-3a3e684b6200.158315d10377a195], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718511f3-2ff5-11e9-b8b1-3a3e684b6200.158315d11a532529], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718511f3-2ff5-11e9-b8b1-3a3e684b6200.158315d125e02f44], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71891f69-2ff5-11e9-b8b1-3a3e684b6200.158315d0bbe8175d], Reason = [Scheduled], Message = [Successfully assigned e2e-tests-sched-pred-hljf7/filler-pod-71891f69-2ff5-11e9-b8b1-3a3e684b6200 to shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71891f69-2ff5-11e9-b8b1-3a3e684b6200.158315d1091e1174], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71891f69-2ff5-11e9-b8b1-3a3e684b6200.158315d12342216d], Reason = [Created], Message = [Created container]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-71891f69-2ff5-11e9-b8b1-3a3e684b6200.158315d12eab055c], Reason = [Started], Message = [Started container]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.158315d1b3fe365f], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-95bjg
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:11:28.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-sched-pred-hljf7" for this suite.
Feb 14 01:11:36.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:11:37.321: INFO: namespace: e2e-tests-sched-pred-hljf7, resource: bindings, ignored listing per whitelist
Feb 14 01:11:37.321: INFO: namespace e2e-tests-sched-pred-hljf7 deletion completed in 8.960379537s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:69

• [SLOW TEST:15.754 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:11:37.322: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-downward-api-ll5xv
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test downward API volume plugin
Feb 14 01:11:38.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-downward-api-ll5xv" to be "success or failure"
Feb 14 01:11:38.562: INFO: Pod "downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 28.641488ms
Feb 14 01:11:40.585: INFO: Pod "downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050996793s
Feb 14 01:11:42.609: INFO: Pod "downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07484214s
STEP: Saw pod success
Feb 14 01:11:42.609: INFO: Pod "downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:11:42.631: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200 container client-container: <nil>
STEP: delete the pod
Feb 14 01:11:42.702: INFO: Waiting for pod downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:11:42.723: INFO: Pod downwardapi-volume-7aca4d86-2ff5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:11:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-downward-api-ll5xv" for this suite.
Feb 14 01:11:48.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:11:49.492: INFO: namespace: e2e-tests-downward-api-ll5xv, resource: bindings, ignored listing per whitelist
Feb 14 01:11:49.690: INFO: namespace e2e-tests-downward-api-ll5xv deletion completed in 6.94387598s

• [SLOW TEST:12.368 seconds]
[sig-storage] Downward API volume
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:11:49.691: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-emptydir-xwj2t
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 14 01:11:50.836: INFO: Waiting up to 5m0s for pod "pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200" in namespace "e2e-tests-emptydir-xwj2t" to be "success or failure"
Feb 14 01:11:50.873: INFO: Pod "pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 36.886434ms
Feb 14 01:11:52.896: INFO: Pod "pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059960822s
Feb 14 01:11:54.924: INFO: Pod "pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087603498s
STEP: Saw pod success
Feb 14 01:11:54.924: INFO: Pod "pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200" satisfied condition "success or failure"
Feb 14 01:11:54.950: INFO: Trying to get logs from node shoot--it--pub-az-6i4kt-cpu-worker-549555c59f-fm9qn pod pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200 container test-container: <nil>
STEP: delete the pod
Feb 14 01:11:55.115: INFO: Waiting for pod pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200 to disappear
Feb 14 01:11:55.136: INFO: Pod pod-821f829b-2ff5-11e9-b8b1-3a3e684b6200 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:11:55.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-emptydir-xwj2t" for this suite.
Feb 14 01:12:01.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:12:01.724: INFO: namespace: e2e-tests-emptydir-xwj2t, resource: bindings, ignored listing per whitelist
Feb 14 01:12:02.117: INFO: namespace e2e-tests-emptydir-xwj2t deletion completed in 6.949193472s

• [SLOW TEST:12.426 seconds]
[sig-storage] EmptyDir volumes
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40
  should support (root,0666,tmpfs) [NodeConformance] [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:147
STEP: Creating a kubernetes client
Feb 14 01:12:02.117: INFO: >>> kubeConfig: /tmp/build/7d7241ef/git-kubernetes_publish_conf_test_results-master_master/scripts/azure_kubeconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-namespaces-v22s8
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-j79mm
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-tests-nsdeletetest-nsvn6
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:148
Feb 14 01:12:10.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-tests-namespaces-v22s8" for this suite.
Feb 14 01:12:16.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:12:17.072: INFO: namespace: e2e-tests-namespaces-v22s8, resource: bindings, ignored listing per whitelist
Feb 14 01:12:17.095: INFO: namespace e2e-tests-namespaces-v22s8 deletion completed in 6.966749369s
STEP: Destroying namespace "e2e-tests-nsdeletetest-j79mm" for this suite.
Feb 14 01:12:17.118: INFO: Namespace e2e-tests-nsdeletetest-j79mm was already deleted
STEP: Destroying namespace "e2e-tests-nsdeletetest-nsvn6" for this suite.
Feb 14 01:12:23.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 14 01:12:23.587: INFO: namespace: e2e-tests-nsdeletetest-nsvn6, resource: bindings, ignored listing per whitelist
Feb 14 01:12:24.030: INFO: namespace e2e-tests-nsdeletetest-nsvn6 deletion completed in 6.912036305s

• [SLOW TEST:21.913 seconds]
[sig-api-machinery] Namespaces [Serial]
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:699
------------------------------
SSSSSSSSSSFeb 14 01:12:24.030: INFO: Running AfterSuite actions on all node
Feb 14 01:12:24.030: INFO: Running AfterSuite actions on node 1
Feb 14 01:12:24.030: INFO: Skipping dumping logs from cluster


Summarizing 1 Failure:

[Fail] [sig-storage] Downward API volume [AfterEach] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance] 
/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:343

Ran 187 of 2011 Specs in 8693.332 seconds
SUCCESS! -- 187 Passed | 0 Failed | 1 Flaked | 0 Pending | 1824 Skipped PASS

Ginkgo ran 1 suite in 2h24m54.178375648s
Test Suite Passed
